id	category	name	description	contributor_name	contributor_github_name	contributor_orcid	collection	concerns_data_topic	purpose_detail	is_open	requires_registration	url	formal_specification	used_in_bridge2ai	responsible_organization	has_application_json	has_application_name	has_application_id	has_application_references	has_application_description	has_application_category	has_application_used_in_bridge2ai	publication	related_to	has_training_resource	has_relevant_data_substrate	has_relevant_organization	subclass_of	contribution_date
B2AI_STANDARD:1	B2AI_STANDARD:BiomedicalStandard	.ACE format	.ACE format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12|B2AI_TOPIC:13]	The ACE file format is a specification for storing data about genomic contigs. The original ACE format was developed for use with Consed, a program for viewing, editing, and finishing DNA sequence assemblies. ACE files are generated by various assembly programs, including Phrap, CAP3, Newbler, Arachne, AMOS (sequence assembly) (more specifically Minimo) and Tigr Assembler v2.	True	False	https://en.wikipedia.org/wiki/ACE_(genomic_file_format)	https://web.archive.org/web/20100609072313/http://bcr.musc.edu/manuals/CONSED.txt																
B2AI_STANDARD:2	B2AI_STANDARD:BiomedicalStandard	DMS	2023 NIH Data Management and Sharing Policy	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[policy]	[B2AI_TOPIC:5]	NIH has issued the Data Management and Sharing (DMS) policy (effective January 25, 2023) to promote the sharing of scientific data. Sharing scientific data accelerates biomedical research discovery, in part, by enabling validation of research results, providing accessibility to high-value datasets, and promoting data reuse for future research studies. Under the DMS policy, NIH expects that investigators and institutions do the following. Plan and budget for the managing and sharing of data, Submit a DMS plan for review when applying for funding, Comply with the approved DMS plan.	True	False	https://sharing.nih.gov/data-management-and-sharing-policy/about-data-management-and-sharing-policy/data-management-and-sharing-policy-overview		True	[B2AI_ORG:67]														
B2AI_STANDARD:3	B2AI_STANDARD:BiomedicalStandard	ABCD	Access to Biological Collections Data Schema	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datamodel]	[B2AI_TOPIC:1]	The Access to Biological Collections Data (ABCD) Schema is an evolving comprehensive standard for the access to and exchange of data about specimens and observations (a.k.a. primary biodiversity data). The ABCD Schema attempts to be comprehensive and highly structured, supporting data from a wide variety of databases. It is compatible with several existing data standards. Parallel structures exist so that either (or both) atomised data and free-text can be accommodated. Version 1.2 is currently in use with the GBIF (Global Biodiversity Information Facility) and BioCASE (Biological Collection Access Service for Europe) networks. Apart from the GBIF and BioCASE networks, the potential for the application of ABCD extends to internal networks, or in-house legacy data access (e.g. datasets from external sources that shall not be converted and integrated into an institution's own data, but be kept separately, though easily accessible). By defining relations between terms, ABCD is a step towards an ontology for biological collections.	True	False	https://abcd.tdwg.org/			[B2AI_ORG:93]	[{\"id\": \"B2AI_APP:1\", \"category\": \"B2AI:Application\", \"name\": \"DiSSCo Digital Specimen Architecture AI-Assisted Curation\", \"description\": \"The Digital Specimen Architecture uses FAIR Digital Object type descriptions based on TDWG standards including ABCD. This enables registered AI services to automatically discover digital specimens, execute allowed actions, and attach machine-readable annotations such as automated extraction from images, relation creation for knowledge graphs, and standardization or correction. ABCD defines the object structure and attributes in the FDO, allowing AI services to operate uniformly across collections and propagate outputs to aggregators such as GBIF and GeoCASe.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.3897/biss.7.112678\"]}, {\"id\": \"B2AI_APP:102\", \"category\": \"B2AI:Application\", \"name\": \"Hespi Computer Vision and OCR Pipeline for Herbarium Sheets\", \"description\": \"Hespi integrates object detection, OCR and handwriting recognition, and a multimodal LLM for post-processing and authority control in herbarium digitization. The pipeline explicitly situates digitization within community standards, citing ABCD alongside Darwin Core. Extracted fields from labels and sheet components are mapped to ABCD and DwC fields to produce interoperable records consumable by downstream AI and analytics systems.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.48550/arxiv.2410.08740\"]}, {\"id\": \"B2AI_APP:103\", \"category\": \"B2AI:Application\", \"name\": \"ML-Ready Benchmark Datasets via ABCD Standardization\", \"description\": \"Work on improving transcribed digital specimen data highlights ABCD and Darwin Core as the target schema for interoperable outputs and schema-based annotation systems such as AnnoSys. This ecosystem underpins ML-ready benchmark datasets for herbarium images and semi-automated workflows for data cleaning and georeferencing, facilitating training and evaluation of computer vision and NLP models while keeping outputs in ABCD and DwC formats for data exchange.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1093/database/baz129\"]}]	[DiSSCo Digital Specimen Architecture AI-Assisted Curation|Hespi Computer Vision and OCR Pipeline for Herbarium Sheets|ML-Ready Benchmark Datasets via ABCD Standardization]	[B2AI_APP:1|B2AI_APP:102|B2AI_APP:103]	[['https://doi.org/10.3897/biss.7.112678']|['https://doi.org/10.48550/arxiv.2410.08740']|['https://doi.org/10.1093/database/baz129']]	[The Digital Specimen Architecture uses FAIR Digital Object type descriptions based on TDWG standards including ABCD. This enables registered AI services to automatically discover digital specimens, execute allowed actions, and attach machine-readable annotations such as automated extraction from images, relation creation for knowledge graphs, and standardization or correction. ABCD defines the object structure and attributes in the FDO, allowing AI services to operate uniformly across collections and propagate outputs to aggregators such as GBIF and GeoCASe.|Hespi integrates object detection, OCR and handwriting recognition, and a multimodal LLM for post-processing and authority control in herbarium digitization. The pipeline explicitly situates digitization within community standards, citing ABCD alongside Darwin Core. Extracted fields from labels and sheet components are mapped to ABCD and DwC fields to produce interoperable records consumable by downstream AI and analytics systems.|Work on improving transcribed digital specimen data highlights ABCD and Darwin Core as the target schema for interoperable outputs and schema-based annotation systems such as AnnoSys. This ecosystem underpins ML-ready benchmark datasets for herbarium images and semi-automated workflows for data cleaning and georeferencing, facilitating training and evaluation of computer vision and NLP models while keeping outputs in ABCD and DwC formats for data exchange.]	[B2AI:Application|B2AI:Application|B2AI:Application]	[False|False|False]							
B2AI_STANDARD:4	B2AI_STANDARD:BiomedicalStandard	AGP	AGP format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12|B2AI_TOPIC:13]	AGP format describes the assembly of a larger sequence object from smaller objects. The large object can be a contig, a scaffold (supercontig), or a chromosome. Each line (row) of the AGP file describes a different piece of the object, and has the column entries defined below. Extended comments follow. It does not serve for either a description of how sequence reads were assembled, or a description of the alignments between components used to construct a larger object. Not all of the information in proprietary assembly files can be represented in the AGP format. It is also not for recording the spans of features like repeats or genes.	True	False	https://www.ncbi.nlm.nih.gov/assembly/agp/AGP_Specification/			[B2AI_ORG:120]														
B2AI_STANDARD:5	B2AI_STANDARD:BiomedicalStandard	AnIML	Analytical Information Markup Language	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:3]	The Analytical Information Markup Language (AnIML) is the emerging ASTM XML standard for analytical chemistry data. It is currently in pre-release form. It is a combination of a highly flexible core schema that defines XML tagging for any kind of analytical information; A set of technique definition documents. These XML files, one per analytical technique, apply tight constraints to the flexible core and in turn are defined by the Technique Schema; Extensions to Technique Definitions are possible to accommodate vendor- and institution-specific data fields. Mission Statement Our goal is to serve as the open-source development platform for a new XML standard for Analytical Chemistry Information. The project is a collaborative effort between many groups and individuals and is sanctioned by the ASTM subcommittee E13.15. http://animl.cvs.sourceforge.net/viewvc/animl/schema/animl-core.xsd	True	False	https://www.animl.org/			[B2AI_ORG:8]														
B2AI_STANDARD:6	B2AI_STANDARD:BiomedicalStandard	ARRIVE	Animal Research Reporting In Vivo Experiments	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:5]	Guidelines intended to improve the reporting of animal experiments.	True	False	https://arriveguidelines.org/			[B2AI_ORG:121]								doi:10.1371/journal.pbio.3000411						
B2AI_STANDARD:7	B2AI_STANDARD:BiomedicalStandard	aECG	Annotated ECG standard	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:37]	Provides a common means of electronically storing both the ECG wave form and associated annotations.	True	False	https://www.hl7.org/implement/standards/product_brief.cfm?product_id=70	https://www.hl7.org/implement/standards/product_brief.cfm?product_id=70		[B2AI_ORG:40]														
B2AI_STANDARD:8	B2AI_STANDARD:BiomedicalStandard	AIM	Annotation and Image Markup schema	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:9|B2AI_TOPIC:15]	The Annotation and Image Markup (AIM) project is the first initiative to propose and create a standard means of adding information and knowledge to medical images in a clinical environment, enabling automatic searching of image content. AIM provides a comprehensive solution to imaging challenges including the lack of agreed-upon syntax for annotation and markup, standardized semantics for annotations, and common formats for annotations and markup. The AIM Model captures descriptive information for images with user-generated graphical symbols into a single common information source. The project includes multiple components: the AIM Template Service (a web service for uploading and downloading AIM templates), the AIM Template Builder (a Java application for creating templates with well-defined questions and answer choices), and reference implementations like AIM on ClearCanvas Workstation. AIM captures results in terms of image regions of interest, semantic descriptions, inferences, calculations, and quantitative features derived by computer programs. It is interoperable with DICOM-SR and HL7-CDA standards while providing unique advantages through an explicit semantic model of imaging results.	True	False	https://github.com/NCIP/annotation-and-image-markup	https://github.com/NCIP/annotation-and-image-markup		[B2AI_ORG:71]	[{\"id\": \"B2AI_APP:2\", \"category\": \"B2AI:Application\", \"name\": \"PACS-to-AIM Conversion for ML-Ready Label Generation\", \"description\": \"A deployed workflow converts proprietary vendor DICOM presentation state annotations from commercial PACS systems into AIM XML using the AIM API, enabling standardization of legacy radiologist annotations for machine learning. A Python module matches lesions across longitudinal studies via 3D coordinates, producing AIM files that are imported into ePAD where lesions are linked over time and quantitative metrics are computed. This AIM conversion pipeline unlocks historical radiologist annotations from PACS for supervised training, radiomics pipelines, and generation of high-quality labeled datasets for deep learning, while ensuring interoperability and enabling large-scale analysis across institutions.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1007/s10278-019-00191-6\"]}, {\"id\": \"B2AI_APP:104\", \"category\": \"B2AI:Application\", \"name\": \"ePAD AIM-Based Radiomics and ML Data Platform\", \"description\": \"ePAD stores annotations and quantitative imaging features in AIM XML and exposes RESTful web services allowing plugins and external tools to retrieve AIM annotations and associated images for downstream analysis. Plugins compute biomarkers and save results back into AIM format, while integrations with external pipelines such as QIFP and pyRadiomics enable feature extraction with outputs persisted in AIM. This AIM-centric architecture facilitates standardized training data management, radiomic feature extraction, and programmatic access for machine learning workflows, cohort analyses, and interoperable analytics environments for quantitative imaging research.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.18383/j.tom.2018.00055\"]}]	[PACS-to-AIM Conversion for ML-Ready Label Generation|ePAD AIM-Based Radiomics and ML Data Platform]	[B2AI_APP:2|B2AI_APP:104]	[['https://doi.org/10.1007/s10278-019-00191-6']|['https://doi.org/10.18383/j.tom.2018.00055']]	[A deployed workflow converts proprietary vendor DICOM presentation state annotations from commercial PACS systems into AIM XML using the AIM API, enabling standardization of legacy radiologist annotations for machine learning. A Python module matches lesions across longitudinal studies via 3D coordinates, producing AIM files that are imported into ePAD where lesions are linked over time and quantitative metrics are computed. This AIM conversion pipeline unlocks historical radiologist annotations from PACS for supervised training, radiomics pipelines, and generation of high-quality labeled datasets for deep learning, while ensuring interoperability and enabling large-scale analysis across institutions.|ePAD stores annotations and quantitative imaging features in AIM XML and exposes RESTful web services allowing plugins and external tools to retrieve AIM annotations and associated images for downstream analysis. Plugins compute biomarkers and save results back into AIM format, while integrations with external pipelines such as QIFP and pyRadiomics enable feature extraction with outputs persisted in AIM. This AIM-centric architecture facilitates standardized training data management, radiomic feature extraction, and programmatic access for machine learning workflows, cohort analyses, and interoperable analytics environments for quantitative imaging research.]	[B2AI:Application|B2AI:Application]	[False|False]		[B2AI_STANDARD:98]	[B2AI_STANDARD:849]	[B2AI_SUBSTRATE:11]			
B2AI_STANDARD:9	B2AI_STANDARD:BiomedicalStandard	ANSI/CTA-2090	ANSI/CTA Standard - The Use of Artificial Intelligence in Health Care Trustworthiness	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:5]	This standard outlines key principles for ensuring trustworthiness in AI applications within healthcare, focusing on human trust, technical reliability, and regulatory compliance. It provides a framework for evaluating AI systems in clinical settings, emphasizing transparency, accountability, and ethical considerations.	False	False	https://shop.cta.tech/products/cta-2090			[B2AI_ORG:4|B2AI_ORG:122]	[{\"id\": \"B2AI_APP:3\", \"category\": \"B2AI:Application\", \"name\": \"Pre-Development ML Scoping and Data Provenance Documentation\", \"description\": \"CTA-2090 recommends concrete developer actions for AI/ML workflows including listing potential use cases before development to properly scope algorithm functionality and limits, documenting whether datasets are raw or pre-processed and what preprocessing was performed, and understanding how original data were collected to identify potential biases. These practices operationalize the standard's emphasis on clear intended use, data lineage, traceability, and documentation requirements for trustworthy ML pipelines in healthcare, ensuring teams establish proper foundations during planning and data preparation stages.\", \"used_in_bridge2ai\": false, \"references\": [\"https://www.fdli.org/wp-content/uploads/2023/01/9-Ross.pdf\"]}, {\"id\": \"B2AI_APP:105\", \"category\": \"B2AI:Application\", \"name\": \"Bias Assessment and Post-Deployment Monitoring\", \"description\": \"CTA-2090 guidance is applied through explicit testing for racial and other biases including testing on vulnerable populations, contractual diversity and representation benchmarks for training data, and inventories to review, screen, retrain, and prevent bias during development. Post-deployment operationalization includes ongoing real-world testing after approval and continuous review of algorithm results during clinical use. These practices implement the standard's expectations for bias identification, mitigation, representative data, lifecycle monitoring, and real-world performance auditing in healthcare AI systems.\", \"used_in_bridge2ai\": false, \"references\": [\"https://www.fdli.org/wp-content/uploads/2023/01/9-Ross.pdf\"]}, {\"id\": \"B2AI_APP:106\", \"category\": \"B2AI:Application\", \"name\": \"Z-Inspection Stakeholder Co-Design and Trust Assessment\", \"description\": \"Healthcare AI systems apply CTA-2090 trustworthiness principles through Z-Inspection, an ethically aligned co-design methodology involving interdisciplinary stakeholders to examine ethical, technical, medical, and legal implications during development and deployment. Assessments uncover dataset bias risks, deployment shortcomings such as accent-related accuracy degradation, and protocol effects on ML output accuracy. Recommended practices include building explainability into models, documenting how decisions are generated, and conducting real-world validation that translates the standard's transparency, fairness, and reproducibility dimensions into concrete governance and evaluation activities.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.48550/arxiv.2206.09887\"]}]	[Pre-Development ML Scoping and Data Provenance Documentation|Bias Assessment and Post-Deployment Monitoring|Z-Inspection Stakeholder Co-Design and Trust Assessment]	[B2AI_APP:3|B2AI_APP:105|B2AI_APP:106]	[['https://www.fdli.org/wp-content/uploads/2023/01/9-Ross.pdf']|['https://www.fdli.org/wp-content/uploads/2023/01/9-Ross.pdf']|['https://doi.org/10.48550/arxiv.2206.09887']]	[CTA-2090 recommends concrete developer actions for AI/ML workflows including listing potential use cases before development to properly scope algorithm functionality and limits, documenting whether datasets are raw or pre-processed and what preprocessing was performed, and understanding how original data were collected to identify potential biases. These practices operationalize the standard's emphasis on clear intended use, data lineage, traceability, and documentation requirements for trustworthy ML pipelines in healthcare, ensuring teams establish proper foundations during planning and data preparation stages.|CTA-2090 guidance is applied through explicit testing for racial and other biases including testing on vulnerable populations, contractual diversity and representation benchmarks for training data, and inventories to review, screen, retrain, and prevent bias during development. Post-deployment operationalization includes ongoing real-world testing after approval and continuous review of algorithm results during clinical use. These practices implement the standard's expectations for bias identification, mitigation, representative data, lifecycle monitoring, and real-world performance auditing in healthcare AI systems.|Healthcare AI systems apply CTA-2090 trustworthiness principles through Z-Inspection, an ethically aligned co-design methodology involving interdisciplinary stakeholders to examine ethical, technical, medical, and legal implications during development and deployment. Assessments uncover dataset bias risks, deployment shortcomings such as accent-related accuracy degradation, and protocol effects on ML output accuracy. Recommended practices include building explainability into models, documenting how decisions are generated, and conducting real-world validation that translates the standard's transparency, fairness, and reproducibility dimensions into concrete governance and evaluation activities.]	[B2AI:Application|B2AI:Application|B2AI:Application]	[False|False|False]							
B2AI_STANDARD:10	B2AI_STANDARD:BiomedicalStandard	AB1	Applied Biosystems sequence read binary format file	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12|B2AI_TOPIC:13]	A binary version of raw DNA sequence reads from Applied Biosystems sequencing analysis software. Also known as ABIF.	False	False	https://www.thermofisher.com/us/en/home/life-science/sequencing/sanger-sequencing.html			[B2AI_ORG:123]														
B2AI_STANDARD:11	B2AI_STANDARD:BiomedicalStandard	ABI	Applied Biosystems sequence read binary format file	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12|B2AI_TOPIC:13]	A binary version of raw DNA sequence reads from Applied Biosystems sequencing analysis software.	False	False	https://tools.thermofisher.com/content/sfs/manuals/4346366_DNA_Sequenc_Analysis_5_1_UG.pdf	https://tools.thermofisher.com/content/sfs/manuals/4346366_DNA_Sequenc_Analysis_5_1_UG.pdf		[B2AI_ORG:123]														
B2AI_STANDARD:12	B2AI_STANDARD:BiomedicalStandard	ARB	ARB software binary alignment format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12|B2AI_TOPIC:13]	A binary alignment format used by the ARB package.	True	False	http://www.arb-home.de/documentation.html											doi:10.1093/nar/gkh293						
B2AI_STANDARD:13	B2AI_STANDARD:BiomedicalStandard	ADL	Archetype Definition Language	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:9]	ADL (Archetype Definition Language) provides a formal, human-readable syntax for expressing constraint-based models of clinical information structures. It enables the definition of reusable archetypes that constrain generic reference models (such as openEHR's) to represent specific clinical concepts like blood pressure measurements or problem lists. The language consists of cADL (constraint ADL) for structural definitions, dADL (data ADL) for metadata and terminology bindings, and an assertion language for business rules. ADL archetypes support multi-lingual terminology, specialization hierarchies, and versioning, making them suitable for creating maintainable, semantically interoperable health information systems. Tools exist for authoring, compiling, and validating ADL archetypes, with XML exchange formats also supported. The syntax is designed to be accessible to both clinical domain experts and technical implementers.	True	False	https://specifications.openehr.org/releases/AM/latest/ADL1.4.html	https://specifications.openehr.org/releases/AM/latest/ADL1.4.html		[B2AI_ORG:79]														
B2AI_STANDARD:14	B2AI_STANDARD:BiomedicalStandard	StructureDefinition-argo-careplan	Argonaut Data Query Implementation Guide	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datamodel]	[B2AI_TOPIC:4]	Specifications for sharing single sets of patient care plans. Based on FHIR R2.	True	False	http://www.fhir.org/guides/argonaut/r2/StructureDefinition-argo-careplan.html			[B2AI_ORG:40]									[B2AI_STANDARD:109]	[B2AI_STANDARD:845|B2AI_STANDARD:846|B2AI_STANDARD:847|B2AI_STANDARD:850]				
B2AI_STANDARD:15	B2AI_STANDARD:BiomedicalStandard	AMIS	Article Minimum Information Standard	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[minimuminformationschema]	[B2AI_TOPIC:16]	The curation process is significantly slowed down by missing information in the articles analyzed. The identity of the clones used to generate ISH probes and the precise sequences tested in reporter assays constituted the most frequent omissions. To help authors ensure in the future that necessary information is present in their article, the Article Minimum Information Standard (AMIS) guidelines have been defined. The guideline describes the mandatory (and useful) information that should be mentioned in literature articles to facilitate the curation process. These guidelines extend the minimal information defined by the MISFISHIE format (Deutsch at al. 2008, Nature Biotechnology).	True	False	https://www.aniseed.fr/aniseed/default/submit_data?module=aniseed&action=default:submit_data#tab-4																	
B2AI_STANDARD:16	B2AI_STANDARD:BiomedicalStandard	Axt	Axt Alignment Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12|B2AI_TOPIC:13]	The Axt (Alignment eXTended) format is a simple text-based format developed by UCSC Genome Browser for storing pairwise DNA sequence alignments between two species or sequences. Each alignment block in an Axt file consists of four lines - a summary line containing alignment number, chromosome names, alignment start and end positions for both sequences, strand information, and alignment score, followed by the aligned sequence from the first genome, the aligned sequence from the second genome, and a blank separator line. The format uses zero-based coordinates for the first sequence and supports alignments on either strand, with sequences from the negative strand reverse-complemented. Axt files are particularly useful for representing whole-genome alignments between species (such as human-mouse or human-chimp comparisons) generated by alignment tools like BLASTZ or LASTZ. The format's straightforward structure makes it easy to parse programmatically and convert to other formats. UCSC provides utilities including axtToMaf for converting to MAF (Multiple Alignment Format), axtChain for chaining together alignments, and axtBest for selecting the best alignment for each position. While Axt efficiently represents pairwise alignments, it has been largely superseded by MAF and chain/net formats for more complex multi-way alignments and hierarchical alignment representations in comparative genomics workflows.	True	False	https://genome.ucsc.edu/goldenPath/help/axt.html			[B2AI_ORG:119]														
B2AI_STANDARD:17	B2AI_STANDARD:BiomedicalStandard	BAI	BAM indexing format file	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12|B2AI_TOPIC:13]	A file containing the index for a Binary Alignment Map (BAM) file.	True	False	https://www.ncbi.nlm.nih.gov/tools/gbench/tutorial6/																	
B2AI_STANDARD:18	B2AI_STANDARD:BiomedicalStandard	BEDgraph	BEDgraph format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12|B2AI_TOPIC:13]	The bedGraph format allows display of continuous-valued data in track format.	True	False	http://genome.ucsc.edu/goldenPath/help/bedgraph.html			[B2AI_ORG:119]									[B2AI_STANDARD:19|B2AI_STANDARD:20]					
B2AI_STANDARD:19	B2AI_STANDARD:BiomedicalStandard	bigBed	Big Browser Extensible Data Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12|B2AI_TOPIC:13]	The bigBed format is an indexed binary format for storing genome annotation data, developed by UCSC Genome Browser as a high-performance alternative to text-based BED files for large datasets. BigBed files store annotation items representing either simple genomic features or linked collections of exons, maintaining BED semantics while enabling efficient region-specific data transfer. Files are created using the bedToBigBed utility which converts sorted BED files into compressed binary format with built-in indexing, requiring chromosome sizes files and optionally AutoSql (.as) format definitions for describing standard and custom fields. The format supports BED3 through BED12 plus additional user-defined fields, with features including itemRgb color specification, extra searchable indices via the -extraIndex parameter for track hub item searches, and trackDb settings for mouseOver labels, field filtering, and URL transformations. Only the portions of bigBed files needed for the currently displayed chromosomal region are transferred to the browser, dramatically improving performance compared to full BED file loading. The format supports web-accessible hosting via HTTP, HTTPS, or FTP with sparse file caching, and includes utilities (bigBedToBed, bigBedInfo, bigBedSummary) for extracting and querying data. BigBed files are widely used in UCSC Genome Browser custom tracks, track hubs for consortia data sharing, and integrated with genome analysis workflows requiring scalable annotation storage and rapid genomic region queries.	True	False	https://genome.ucsc.edu/goldenPath/help/bigBed.html			[B2AI_ORG:119]									[B2AI_STANDARD:18|B2AI_STANDARD:20]					
B2AI_STANDARD:20	B2AI_STANDARD:BiomedicalStandard	bigWig	Big Wiggle Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12|B2AI_TOPIC:13]	The bigWig format is an indexed binary file format developed by UCSC for efficient storage and visualization of dense, continuous genomic data displayed as graphs in genome browsers. Created from wiggle (wig) or bedGraph files using the wigToBigWig or bedGraphToBigWig utilities, bigWig files enable rapid data access by transferring only the portions needed to display specific genomic regions, rather than entire datasets. This sparse file caching mechanism provides dramatically faster performance than text-based formats when working with large-scale datasets such as ChIP-seq, RNA-seq coverage, methylation levels, or conservation scores. The format supports various visualization options including customizable graph types (bar or points), scaling parameters, smoothing windows, logarithmic transformations, and color schemes. BigWig files can also display sequence logos using the dynseq feature, which scales nucleotide characters by base-resolution scores. The format includes utilities for data extraction (bigWigToBedGraph, bigWigToWig, bigWigSummary, bigWigAverageOverBed) and file inspection (bigWigInfo), making it suitable for both visualization and downstream computational analysis.	True	False	https://genome.ucsc.edu/goldenPath/help/bigWig.html			[B2AI_ORG:119]									[B2AI_STANDARD:18|B2AI_STANDARD:19]					
B2AI_STANDARD:21	B2AI_STANDARD:BiomedicalStandard	BAM/CRAM	Binary Alignment Map / Compressed Reference-oriented Alignment Map	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat|standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:12|B2AI_TOPIC:13]	BAM (Binary Alignment/Map) and CRAM (Compressed Reference-oriented Alignment Map) are binary formats for storing DNA sequence alignments to reference genomes, maintained by the GA4GH Large Scale Genomics work stream. BAM is the binary equivalent of the text-based SAM (Sequence Alignment/Map) format, providing efficient storage and retrieval of aligned sequencing reads with quality scores, alignment positions, CIGAR strings, and optional tags defined in the SAMtags specification. CRAM (currently version 3.x) achieves superior compression ratios by storing differences from a reference sequence rather than full sequence data, using custom compression codecs detailed in the CRAMcodecs specification. Both formats support indexing (BAI for BAM, CRAI for CRAM, and CSI as a more scalable successor) enabling rapid random access to genomic regions. The formats are widely supported by genomics toolkits including samtools, htslib, htsjdk, and GATK, with standard operations for sorting, merging, filtering, and format conversion. CRAM offers significant storage savings particularly important for large-scale projects, while maintaining full compatibility with SAM/BAM workflows. Both formats support the htsget protocol for parallel streaming access and can be wrapped with crypt4gh encryption for secure data sharing.	True	False	https://samtools.github.io/hts-specs/		True	[B2AI_ORG:34]	[{\"id\": \"B2AI_APP:5\", \"category\": \"B2AI:Application\", \"name\": \"DeepVariant Deep Learning Variant Calling\", \"description\": \"DeepVariant uses BAM/CRAM alignment files as input to train convolutional neural networks that classify candidate genomic variants with high accuracy across multiple sequencing technologies. The model transforms aligned reads into image-like pileup tensors that capture base qualities, mapping qualities, and strand information, enabling the CNN to learn complex patterns that distinguish true genetic variants from sequencing artifacts and alignment errors. This approach achieves state-of-the-art performance on benchmark datasets and generalizes well across Illumina, PacBio, and Oxford Nanopore sequencing platforms without technology-specific parameter tuning.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1038/nbt.4235\", \"https://doi.org/10.1038/s41587-021-01108-x\"]}, {\"id\": \"B2AI_APP:101\", \"category\": \"B2AI:Application\", \"name\": \"Deep Learning for Structural Variant Detection\", \"description\": \"Deep learning models trained on BAM/CRAM files enable detection of complex structural variants including deletions, duplications, inversions, and translocations that are challenging for traditional callers. Neural networks analyze read depth, split reads, discordant read pairs, and local assembly features from aligned data to identify structural rearrangements, with applications in cancer genomics where somatic structural variants drive tumorigenesis and treatment resistance. These models improve sensitivity for detecting variants in repetitive regions and provide better breakpoint resolution than conventional methods.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1038/s41467-022-28289-w\", \"https://doi.org/10.1093/bioinformatics/btab732\"]}]	[DeepVariant Deep Learning Variant Calling|Deep Learning for Structural Variant Detection]	[B2AI_APP:5|B2AI_APP:101]	[['https://doi.org/10.1038/nbt.4235', 'https://doi.org/10.1038/s41587-021-01108-x']|['https://doi.org/10.1038/s41467-022-28289-w', 'https://doi.org/10.1093/bioinformatics/btab732']]	[DeepVariant uses BAM/CRAM alignment files as input to train convolutional neural networks that classify candidate genomic variants with high accuracy across multiple sequencing technologies. The model transforms aligned reads into image-like pileup tensors that capture base qualities, mapping qualities, and strand information, enabling the CNN to learn complex patterns that distinguish true genetic variants from sequencing artifacts and alignment errors. This approach achieves state-of-the-art performance on benchmark datasets and generalizes well across Illumina, PacBio, and Oxford Nanopore sequencing platforms without technology-specific parameter tuning.|Deep learning models trained on BAM/CRAM files enable detection of complex structural variants including deletions, duplications, inversions, and translocations that are challenging for traditional callers. Neural networks analyze read depth, split reads, discordant read pairs, and local assembly features from aligned data to identify structural rearrangements, with applications in cancer genomics where somatic structural variants drive tumorigenesis and treatment resistance. These models improve sensitivity for detecting variants in repetitive regions and provide better breakpoint resolution than conventional methods.]	[B2AI:Application|B2AI:Application]	[False|False]		[B2AI_STANDARD:22]			[B2AI_ORG:117|B2AI_ORG:116]		
B2AI_STANDARD:22	B2AI_STANDARD:BiomedicalStandard	BAM	Binary Alignment Map format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12|B2AI_TOPIC:13]	A BAM file (.bam) is the binary version of a SAM file.	True	False	https://en.wikipedia.org/wiki/Binary_Alignment_Map	https://samtools.github.io/hts-specs/SAMv1.pdf	True	[B2AI_ORG:34]									[B2AI_STANDARD:21]					
B2AI_STANDARD:23	B2AI_STANDARD:BiomedicalStandard	2bit	Binary sequence information Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12|B2AI_TOPIC:13]	A .2bit file stores multiple DNA sequences (up to 4 Gb total) in a compact randomly-accessible format. The file contains masking information as well as the DNA itself.	True	False	http://genome.ucsc.edu/FAQ/FAQformat.html#format7			[B2AI_ORG:119]														
B2AI_STANDARD:24	B2AI_STANDARD:BiomedicalStandard	BCF	Binary variant call format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12|B2AI_TOPIC:13]	A binary version of the variant call format (VCF).	True	False	https://samtools.github.io/bcftools/bcftools.html																	
B2AI_STANDARD:25	B2AI_STANDARD:BiomedicalStandard	BioCompute	BioCompute Object standard	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:1]	BioCompute Objects (BCOs) are a formal standard for representing bioinformatics computational workflows and analyses, designed to facilitate communication and reproducibility in high-throughput sequencing research. Developed through a collaborative effort and formally recognized as IEEE Standard 2791-2020, BCOs structure critical workflow information into standardized domains including provenance, usability, extension, description, execution, parametric, input/output, and error domains. BCOs are represented in JSON format adhering to JSON schema draft-07, making them both human and machine readable. The standard addresses the challenge of documenting complex bioinformatics methods by providing predictable structure and stability for workflow communication. Upon assignment of a digital etag, three domains become immutable to ensure integrity: the Parametric Domain, Execution Domain, and I/O Domain. BCOs support regulatory compliance including FDA Title 21 CFR Part 11 considerations for electronic records and digital signatures, with hash values and encryption keys generated from execution and parametric domains for validation purposes.	True	False	https://docs.biocomputeobject.org/user_guide/											doi:10.5731/pdajpst.2016.006734				[B2AI_ORG:44]		
B2AI_STANDARD:26	B2AI_STANDARD:BiomedicalStandard	Biolink	Biolink Model	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datamodel]	[B2AI_TOPIC:20]	Biolink Model is a comprehensive, high-level data model designed to standardize types and relationships in biological knowledge graphs. It provides a consistent framework for representing biological knowledge across various databases and formats, covering entities such as genes, diseases, chemical substances, organisms, genomics, phenotypes, and pathways. The model incorporates object-oriented classification and graph-oriented features, with a core set of hierarchical, interconnected classes (categories) and relationships (predicates). Biolink Model includes over 400 classes ranging from molecular entities like genes and proteins to clinical concepts like diseases and treatments, plus comprehensive predicates for describing relationships such as \"treats,\" \"causes,\" \"associated_with,\" and \"regulates.\" It supports advanced features like qualifiers for contextualizing relationships, evidence attribution, and knowledge provenance tracking. The model is particularly valuable for translational science applications, enabling integration of data from diverse sources including clinical databases, molecular biology repositories, and literature mining systems. It serves as the foundational schema for knowledge graphs in the Biomedical Data Translator project and other large-scale biomedical data integration efforts.	True	False	https://biolink.github.io/biolink-model/		True		[{\"id\": \"B2AI_APP:6\", \"category\": \"B2AI:Application\", \"name\": \"Translator Knowledge Graph for Drug Repurposing\", \"description\": \"NCATS Biomedical Data Translator uses Biolink Model to integrate diverse biomedical knowledge sources into a unified knowledge graph supporting AI-driven drug repurposing and mechanism discovery. The standardized Biolink schema enables graph neural networks to perform multi-hop reasoning across chemical-protein-disease relationships, identifying candidate therapeutics by connecting drugs to diseases through intermediate biological entities. The Translator system's reasoning agents leverage Biolink predicates to score and rank hypothesized drug-disease associations based on mechanistic evidence paths learned from integrated data.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1111/cts.13302\"]}]	[Translator Knowledge Graph for Drug Repurposing]	[B2AI_APP:6]	[['https://doi.org/10.1111/cts.13302']]	[NCATS Biomedical Data Translator uses Biolink Model to integrate diverse biomedical knowledge sources into a unified knowledge graph supporting AI-driven drug repurposing and mechanism discovery. The standardized Biolink schema enables graph neural networks to perform multi-hop reasoning across chemical-protein-disease relationships, identifying candidate therapeutics by connecting drugs to diseases through intermediate biological entities. The Translator system's reasoning agents leverage Biolink predicates to score and rank hypothesized drug-disease associations based on mechanistic evidence paths learned from integrated data.]	[B2AI:Application]	[False]	doi:10.1111/cts.13302				[B2AI_ORG:70]		
B2AI_STANDARD:27	B2AI_STANDARD:BiomedicalStandard	BEL	Biological Expression Language	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:5]	The Biological Expression Language (BEL) is a domain-specific language for representing scientific findings from life sciences literature in a computable, structured format. BEL captures causal and correlative relationships between biological entities (genes, proteins, complexes, biological processes, pathways) along with their experimental and publication context, enabling systematic knowledge representation and integration. BEL statements are expressed as triples (subject-relationship-object) that can be combined into biological networks and knowledge graphs. Each BEL assertion is packaged as a \"nanopub\" that includes provenance information, experimental conditions, and citations, allowing relationships to be properly evaluated in context. The language supports standard biological nomenclatures (Gene Ontology, HGNC, ChEBI, etc.) through namespace integration and provides a simplified syntax that is more accessible than traditional chemical notation. BEL's computable format enables applications in reverse causal reasoning, heat diffusion algorithms, prior knowledge for machine learning models, and detection of contradictory findings across literature. The standard is maintained by the BEL Language Committee with updates managed through BEL Enhancement Proposals (BEPs).	True	False	https://bel.bio/	https://language.bel.bio/																
B2AI_STANDARD:28	B2AI_STANDARD:BiomedicalStandard	BRIDG Model	Biomedical Research Integrated Domain Group Model	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	The Biomedical Research Integrated Domain Group (BRIDG) Model is a collaborative effort engaging stakeholders from the Clinical Data Interchange Standards Consortium (CDISC), the HL7 BRIDG Work Group, the US National Cancer Institute (NCI), and the US Food and Drug Administration (FDA). The goal of the BRIDG Model is to produce a shared view of the dynamic and static semantics for the domain of basic, pre-clinical, clinical, and translational research and its associated regulatory artifacts.	True	False	https://bridgmodel.nci.nih.gov/	https://github.com/CBIIT/bridg-model/										doi:10.1093/jamia/ocx004				[B2AI_ORG:15|B2AI_ORG:31|B2AI_ORG:40|B2AI_ORG:71]		
B2AI_STANDARD:29	B2AI_STANDARD:BiomedicalStandard	BioPAX	BioPAX standard	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:21]	BioPAX is a standard language that aims to enable integration, exchange, visualization and analysis of biological pathway data. Specifically, BioPAX supports data exchange between pathway data groups and thus reduces the complexity of interchange between data formats by providing an accepted standard format for pathway data. By offering a standard, with well-defined semantics for pathway representation, BioPAX allows pathway databases and software to interact more efficiently. In addition, BioPAX enables the development of pathway visualization from databases and facilitates analysis of experimentally generated data through combination with prior knowledge. The BioPAX effort is coordinated closely with that of other pathway related standards initiatives namely; PSI-MI, SBML, CellML, and SBGN in order to deliver a compatible standard in the areas where they overlap.	True	False	http://www.biopax.org/											doi:10.1038/nbt.1666						
B2AI_STANDARD:30	B2AI_STANDARD:BiomedicalStandard	Bioschemas	Bioschemas	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:1]	Bioschemas is a community-driven initiative that extends Schema.org to improve the findability and discoverability of life sciences resources on the web through structured markup. The project makes two main contributions to the life sciences community - proposing new types and properties to Schema.org specifically for life science resources, and defining usage profiles over existing Schema.org types that specify essential, recommended, and optional properties for consistent markup. Bioschemas profiles significantly simplify the markup process by reducing complex Schema.org types to manageable subsets while ensuring compatibility with search engines like Google Dataset Search. The initiative has achieved major recognition with six Bioschemas types (BioChemEntity, ChemicalSubstance, Gene, MolecularEntity, Protein, Taxon) officially included in Schema.org version 13.0. Endorsed by the European Research Council and serving as a flagship policy of ELIXIR, Bioschemas supports FAIR data principles by enabling automated discovery, collation, and analysis of distributed life sciences resources including datasets, software applications, training materials, and biological entities across the web ecosystem.	True	False	https://bioschemas.org/	https://github.com/BioSchemas/specifications														[B2AI_ORG:28]		
B2AI_STANDARD:31	B2AI_STANDARD:BiomedicalStandard	BRISQ	Biospecimen Reporting for Improved Study Quality	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:1]	Human biospecimens are subject to a number of different collection, processing, and storage factors that can significantly alter their molecular composition and consistency. These biospecimen preanalytical factors, in turn, influence experimental outcomes and the ability to reproduce scientific results. Currently, the extent and type of information specific to the biospecimen preanalytical conditions reported in scientific publications and regulatory submissions varies widely. To improve the quality of research utilizing human tissues, it is critical that information regarding the handling of biospecimens be reported in a thorough, accurate, and standardized manner. The Biospecimen Reporting for Improved Study Quality (BRISQ) recommendations outlined herein are intended to apply to any study in which human biospecimens are used. The purpose of reporting these details is to supply others, from researchers to regulators, with more consistent and standardized information to better evaluate, interpret, compare, and reproduce the experimental results. The BRISQ guidelines are proposed as an important and timely resource tool to strengthen communication and publications around biospecimen-related research and help reassure patient contributors and the advocacy community that the contributions are valued and respected	True	False	https://doi.org/10.1002/cncy.20147											doi:10.1002/cncy.20147				[B2AI_ORG:72]		
B2AI_STANDARD:32	B2AI_STANDARD:BiomedicalStandard	BioXSD	BioXSD	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datamodel]	[B2AI_TOPIC:20]	Data model and exchange formats for basic bioinformatics types of data - sequences, alignments, feature records with associated data and metadata.	True	False	http://bioxsd.org/	https://github.com/bioxsd/bioxsd			[{\"id\": \"B2AI_APP:7\", \"category\": \"B2AI:Application\", \"name\": \"FreeContact Protein Contact Prediction ML Output Format\", \"description\": \"FreeContact implements statistical learning-based approaches for protein residue-residue contact prediction using mean-field Direct Coupling Analysis and PSICOV sparse inverse covariance methods. The tool explicitly supports BioXSD as an output format to facilitate integration into bioinformatics workflows and web services, with planned BioXSD input support. This demonstrates BioXSD serving as the standardized exchange format for machine learning tool outputs, enabling incorporation of ML-based contact predictions into interoperable analysis pipelines.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1186/1471-2105-15-85\"]}, {\"id\": \"B2AI_APP:107\", \"category\": \"B2AI:Application\", \"name\": \"OntoDT Datatype Ontology for ML Dataset Repositories\", \"description\": \"The OntoDT generic ontology of datatypes uses BioXSD to improve representation of basic bioinformatics datatypes in exchange formats, enabling construction of taxonomies for datasets, data mining tasks, generalizations, and algorithms. OntoDT can be used for annotation and querying of machine learning dataset repositories and for constructing data mining workflows. This links BioXSD datatype representations to ML and data mining ontologies, supporting standardized workflow assembly through consistent bioinformatics datatype semantics.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1016/j.ins.2015.08.006\"]}]	[FreeContact Protein Contact Prediction ML Output Format|OntoDT Datatype Ontology for ML Dataset Repositories]	[B2AI_APP:7|B2AI_APP:107]	[['https://doi.org/10.1186/1471-2105-15-85']|['https://doi.org/10.1016/j.ins.2015.08.006']]	[FreeContact implements statistical learning-based approaches for protein residue-residue contact prediction using mean-field Direct Coupling Analysis and PSICOV sparse inverse covariance methods. The tool explicitly supports BioXSD as an output format to facilitate integration into bioinformatics workflows and web services, with planned BioXSD input support. This demonstrates BioXSD serving as the standardized exchange format for machine learning tool outputs, enabling incorporation of ML-based contact predictions into interoperable analysis pipelines.|The OntoDT generic ontology of datatypes uses BioXSD to improve representation of basic bioinformatics datatypes in exchange formats, enabling construction of taxonomies for datasets, data mining tasks, generalizations, and algorithms. OntoDT can be used for annotation and querying of machine learning dataset repositories and for constructing data mining workflows. This links BioXSD datatype representations to ML and data mining ontologies, supporting standardized workflow assembly through consistent bioinformatics datatype semantics.]	[B2AI:Application|B2AI:Application]	[False|False]	doi:10.1093/bioinformatics/btq391				[B2AI_ORG:28]		
B2AI_STANDARD:33	B2AI_STANDARD:BiomedicalStandard	BIDS	Brain Imaging Data Structure	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datamodel]	[B2AI_TOPIC:22]	The Brain Imaging Data Structure (BIDS) is a simple and intuitive way to organize and describe data. This document defines the BIDS specification, which provides many details to help implement the standard. It includes the core specification as well as many extensions to specific brain imaging modalities, and increasingly also to other kinds of data.	True	False	https://bids-specification.readthedocs.io/en/stable/	https://github.com/bids-standard/bids-specification			[{\"id\": \"B2AI_APP:8\", \"category\": \"B2AI:Application\", \"name\": \"BIDS Apps for ML-Ready Preprocessing and Feature Extraction\", \"description\": \"Containerized BIDS Apps such as fMRIPrep, MRIQC, and MRtrix3 Connectome accept BIDS datasets and emit standardized derivatives including preprocessed time series, image quality metrics, and connectivity matrices that function directly as inputs or labels for ML workflows. The BIDS Apps ecosystem uses containerization to provide reproducible preprocessing pipelines across systems including HPC environments via Singularity, enabling consistent feature extraction for training machine learning models on neuroimaging data.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1371/journal.pcbi.1005209\"]}, {\"id\": \"B2AI_APP:108\", \"category\": \"B2AI:Application\", \"name\": \"PyBIDS and MNE-BIDS Programmatic Dataset Assembly\", \"description\": \"PyBIDS provides programmatic access to query BIDS datasets and their metadata and to organize derivatives in BIDS-Derivatives format, facilitating reproducible train-validation splits and feature assembly for ML pipelines. MNE-BIDS provides an integration layer for MNE-Python enabling standardized ingestion of EEG, MEG, and iEEG BIDS data into analysis pipelines that feed ML methods. These tools enable automated dataset selection, metadata-driven curation, and scalable learning across neuroimaging studies.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1162/imag_a_00103\"]}, {\"id\": \"B2AI_APP:109\", \"category\": \"B2AI:Application\", \"name\": \"BIDS Derivatives and Connectivity Specifications for ML Features\", \"description\": \"The BIDS Connectivity extension and Derivatives specifications define interoperable formats for structural and functional connectivity matrices, seed-based maps, tractograms, and tractometry across modalities including sMRI, fMRI, DWI, PET, EEG, iEEG, and MEG. These standardized derivative schemas enable common ML features to be shared and allow benchmarking of ML models on consistent feature representations, supporting reproducible radiomics and connectomics analyses.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1162/imag_a_00103\"]}, {\"id\": \"B2AI_APP:110\", \"category\": \"B2AI:Application\", \"name\": \"XGBoost-Based DICOM to BIDS Automated Conversion\", \"description\": \"An XGBoost classifier trained on DICOM metadata achieves 99.5% accuracy in classifying MRI acquisition types and automatically transforms unstructured clinical imaging into BIDS datasets with little to no user intervention. This ML-enabled conversion tool reduces manual curation burden, accelerates creation of standardized training corpora, and enables clinical imaging data to be rapidly prepared for downstream machine learning analyses in BIDS format.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1007/s12021-024-09659-5\"]}]	[BIDS Apps for ML-Ready Preprocessing and Feature Extraction|PyBIDS and MNE-BIDS Programmatic Dataset Assembly|BIDS Derivatives and Connectivity Specifications for ML Features|XGBoost-Based DICOM to BIDS Automated Conversion]	[B2AI_APP:8|B2AI_APP:108|B2AI_APP:109|B2AI_APP:110]	[['https://doi.org/10.1371/journal.pcbi.1005209']|['https://doi.org/10.1162/imag_a_00103']|['https://doi.org/10.1162/imag_a_00103']|['https://doi.org/10.1007/s12021-024-09659-5']]	[Containerized BIDS Apps such as fMRIPrep, MRIQC, and MRtrix3 Connectome accept BIDS datasets and emit standardized derivatives including preprocessed time series, image quality metrics, and connectivity matrices that function directly as inputs or labels for ML workflows. The BIDS Apps ecosystem uses containerization to provide reproducible preprocessing pipelines across systems including HPC environments via Singularity, enabling consistent feature extraction for training machine learning models on neuroimaging data.|PyBIDS provides programmatic access to query BIDS datasets and their metadata and to organize derivatives in BIDS-Derivatives format, facilitating reproducible train-validation splits and feature assembly for ML pipelines. MNE-BIDS provides an integration layer for MNE-Python enabling standardized ingestion of EEG, MEG, and iEEG BIDS data into analysis pipelines that feed ML methods. These tools enable automated dataset selection, metadata-driven curation, and scalable learning across neuroimaging studies.|The BIDS Connectivity extension and Derivatives specifications define interoperable formats for structural and functional connectivity matrices, seed-based maps, tractograms, and tractometry across modalities including sMRI, fMRI, DWI, PET, EEG, iEEG, and MEG. These standardized derivative schemas enable common ML features to be shared and allow benchmarking of ML models on consistent feature representations, supporting reproducible radiomics and connectomics analyses.|An XGBoost classifier trained on DICOM metadata achieves 99.5% accuracy in classifying MRI acquisition types and automatically transforms unstructured clinical imaging into BIDS datasets with little to no user intervention. This ML-enabled conversion tool reduces manual curation burden, accelerates creation of standardized training corpora, and enables clinical imaging data to be rapidly prepared for downstream machine learning analyses in BIDS format.]	[B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application]	[False|False|False|False]	doi:10.1038/sdata.2016.44			[B2AI_SUBSTRATE:3]			
B2AI_STANDARD:34	B2AI_STANDARD:BiomedicalStandard	BFI	Brief Fatigue Inventory	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[diagnosticinstrument]	[B2AI_TOPIC:9]	The Brief Fatigue Inventory (BFI) is used to rapidly assess the severity and impact of cancer-related fatigue. An increasing focus on cancer-related fatigue emphasized the need for sensitive tools to assess this most frequently reported symptom. The six interference items correlate with standard quality-of-life measures.	True	False	https://www.mdanderson.org/research/departments-labs-institutes/departments-divisions/symptom-research/symptom-assessment-tools/brief-fatigue-inventory.html	http://www.npcrc.org/files/news/brief_fatigue_inventory.pdf																
B2AI_STANDARD:35	B2AI_STANDARD:BiomedicalStandard	BPI	Brief Pain Inventory	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[diagnosticinstrument]	[B2AI_TOPIC:9]	The Brief Pain Inventory (BPI) rapidly assesses the severity of pain and its impact on functioning. The BPI has been translated into dozens of languages, and it is widely used in both research and clinical settings.	True	False	https://www.mdanderson.org/research/departments-labs-institutes/departments-divisions/symptom-research/symptom-assessment-tools/brief-pain-inventory.html	http://www.npcrc.org/files/news/briefpain_short.pdf																
B2AI_STANDARD:36	B2AI_STANDARD:BiomedicalStandard	BED	Browser Extensible Data Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12|B2AI_TOPIC:13]	The Browser Extensible Data (BED) format is a flexible, tab-delimited text format designed for defining annotation tracks in genome browsers, particularly the UCSC Genome Browser. BED format provides a standardized way to represent genomic features with positional information, supporting both simple coordinate-based annotations and complex multi-exon structures. The format consists of 12 possible fields, with the first three (chromosome, start position, end position) being mandatory and nine additional optional fields providing detailed feature information including name, score, strand orientation, thick start/end coordinates for coding regions, RGB color values for visualization, and block structures for representing discontinuous features like exons and introns. BED files use zero-based, half-open coordinate system where the start position is inclusive and the end position is exclusive, enabling precise genomic interval representation. The format supports various annotation types from simple genomic intervals to complex gene models, making it essential for genomic data visualization, comparative genomics, and functional annotation workflows in bioinformatics research.	True	False	https://genome.ucsc.edu/FAQ/FAQformat.html#format1	https://github.com/samtools/hts-specs/blob/master/BEDv1.pdf		[B2AI_ORG:119]											[B2AI_SUBSTRATE:53]			
B2AI_STANDARD:37	B2AI_STANDARD:BiomedicalStandard	BPMN	Business Process Modeling Notation	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A graphical notation that depicts the steps in a business process. BPMN depicts the end-to-end flow of a business process. The notation has been specifically designed to coordinate the sequence of processes and the messages that flow between different process participants in a related set of activities.	True	False	https://www.omg.org/bpmn/index.htm															[B2AI_ORG:10]		
B2AI_STANDARD:38	B2AI_STANDARD:BiomedicalStandard	CMMN	Case Management Model and Notation	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4]	A common meta- model and notation for modeling and graphically expressing a case as well as an interchange format for exchanging case models among different tools.	True	False	https://www.omg.org/cmmn/															[B2AI_ORG:10]		
B2AI_STANDARD:39	B2AI_STANDARD:BiomedicalStandard	CARE	Case Report guidelines	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:4]	The CARE guidelines provide a framework that supports transparency and accuracy in the publication of case reports and the reporting of information from patient encounters.	True	False	https://www.care-statement.org/																	
B2AI_STANDARD:40	B2AI_STANDARD:BiomedicalStandard	Define	CDISC Case Report Tabulation Data Definition Specification	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4]	The CDISC Case Report Tabulation Data Definition Specification (define.xml) Version 1.0 reflects changes from a comment period through the Health Level 7 (HL7) Regulated Clinical Research Information Management Technical Committee (RCRIM) in December 2003 (www.hl7.org) and CDISC's website in September 2004 as well as the work done by the define.xml team in conjunction with the CDISC ODM team to add functionality, features, and additional documentation. This document specifies the standard for providing Case Report Tabulations Data Definitions in an XML format for submission to regulatory authorities (e.g., FDA). The XML schema used to define the expected structure for these XML files is based on an extension to the CDISC Operational Data Model (ODM). The 1999 FDA electronic submission (eSub) guidance and the electronic Common Technical Document (eCTD) documents specify that a document describing the content and structure of the included data should be provided within a submission. This document is known as the Data Definition Document (e.g., define.pdf in the 1999 guidance). The Data Definition Document provides a list of the datasets included in the submission along with a detailed description of the contents of each dataset. To increase the level of automation and improve the efficiency of the Regulatory Review process, define.xml can be used to provide the Data Definition Document in a machine-readable format.	True	True	https://www.cdisc.org/standards/data-exchange/define-xml			[B2AI_ORG:15]														
B2AI_STANDARD:41	B2AI_STANDARD:BiomedicalStandard	CDISC ADaM	CDISC Controlled Terminology for Analysis Dataset Model	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Controlled terminology for data analysis and assessments.	True	False	https://evs.nci.nih.gov/ftp1/CDISC/ADaM/ADaM%20Terminology.html	https://evs.nci.nih.gov/ftp1/CDISC/ADaM/		[B2AI_ORG:15]									[B2AI_STANDARD:42|B2AI_STANDARD:43]					
B2AI_STANDARD:42	B2AI_STANDARD:BiomedicalStandard	CDISC Protocol	CDISC Controlled Terminology for Data Collection for Protocol	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Controlled terminology for biomedical protocols.	True	False	https://evs.nci.nih.gov/ftp1/CDISC/Protocol/Protocol%20Terminology.html	https://evs.nci.nih.gov/ftp1/CDISC/Protocol/		[B2AI_ORG:15]									[B2AI_STANDARD:41|B2AI_STANDARD:43]					
B2AI_STANDARD:43	B2AI_STANDARD:BiomedicalStandard	CDISC TAUGs	CDISC Controlled Terminology for Therapeutic Area Standards	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4|B2AI_TOPIC:7]	Disease-specific metadata, examples and guidance on implementing CDISC standards.	True	False	https://www.cdisc.org/standards/therapeutic-areas			[B2AI_ORG:15]									[B2AI_STANDARD:41|B2AI_STANDARD:42]					
B2AI_STANDARD:44	B2AI_STANDARD:BiomedicalStandard	SDTM	CDISC Controlled Terminology Standards for Data Aggregation through Study Data Tabulation Model (including QRS, Medical Device and Pharmacogenomics Data)	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4]	A standard for organizing and formatting data to streamline processes in collection, management, analysis and reporting.	True	False	https://www.cdisc.org/standards/foundational/sdtm	https://evs.nci.nih.gov/ftp1/CDISC/SDTM/		[B2AI_ORG:15]														
B2AI_STANDARD:45	B2AI_STANDARD:BiomedicalStandard	CDASH	CDISC Controlled Terminology Standards for Data Collection through Clinical Data Acquisition Standards Harmonization	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4]	The Clinical Data Acquisition Standards Harmonization (CDASH) is a CDISC foundational standard that establishes consistent data collection practices across clinical studies and sponsors. CDASH provides a standardized framework for designing case report forms (CRFs) and electronic data capture (eCRF) systems by defining a common set of data collection fields and structures. The standard specifies field names, labels, prompt questions, and controlled terminology for capturing clinical observations, ensuring that data can be directly traced to the Study Data Tabulation Model (SDTM) without extensive post-collection transformation. By harmonizing data collection at the source, CDASH reduces data cleaning efforts, improves data quality, and accelerates the transition from collection to analysis. The standard covers various therapeutic areas and data domains including demographics, vital signs, adverse events, concomitant medications, and laboratory results. CDISC provides a library of ready-to-use, CDASH-compliant annotated eCRFs in multiple formats (PDF, HTML, XML) that can be implemented as-is or customized for specific study needs, facilitating regulatory submission and data review processes.	True	False	https://www.cdisc.org/standards/foundational/cdash	https://evs.nci.nih.gov/ftp1/CDISC/SDTM/		[B2AI_ORG:15]														
B2AI_STANDARD:46	B2AI_STANDARD:BiomedicalStandard	CDISC Dataset	CDISC Dataset-XML	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:4]	CDISC Dataset-XML, which was released for comment under the name StudyDataSet-XML but was renamed to avoid confusion with the CDISC SDS team, is a new standard used to exchange study datasets in an XML format. The purpose of Dataset-XML is to support the interchange of tabular data for clinical research applications using ODM-based XML technologies. The Dataset-XML model is based on the CDISC Operational Data Model (ODM) standard and should follow the metadata structure defined in the CDISC Define-XML standard. Dataset-XML can represent any tabular dataset including SDTM, ADaM, SEND, or non-standard legacy datasets. Some noteworthy items relating to Dataset-XML v1.0 include alternative to SAS Version 5 Transport (XPT) format for datasets ODM-based model for representation of SEND, SDTM, ADaM or legacy datasets Capable of supporting CDISC regulatory data submissions Based on Define-XML v2 or v1 metadata, easy to reference Dataset-XML supports all language encodings supported by XML.	True	True	https://www.cdisc.org/standards/data-exchange/dataset-xml			[B2AI_ORG:15]														
B2AI_STANDARD:47	B2AI_STANDARD:BiomedicalStandard	CDISC LAB	CDISC Laboratory Data Model	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datamodel]	[B2AI_TOPIC:4]	LAB provides a standard model for the acquisition and exchange of laboratory data, primarily between labs and sponsors or CROs. The LAB standard was specifically designed for the interchange of lab data acquired in clinical trials.	True	True	https://www.cdisc.org/standards/data-exchange/lab			[B2AI_ORG:15]	[{\"id\": \"B2AI_APP:9\", \"category\": \"B2AI:Application\", \"name\": \"HL7 FHIR to CDISC Laboratory Mapping for ML-Ready EHR Data\", \"description\": \"The HL7 FHIR to CDISC Joint Mapping Implementation Guide includes mappings for the Laboratory domain that translate FHIR lab resources into CDISC variables, linked with CDISC LB-to-LOINC mapping guidance to leverage real-world data for clinical trials. This harmonization enables EHR laboratory observations to be transformed into standardized, ML-ready lab features with consistent semantic coding via LOINC, supporting downstream machine learning analyses, pooled studies, and data mining across clinical trial and real-world datasets.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.47912/jscdm.162\"]}, {\"id\": \"B2AI_APP:111\", \"category\": \"B2AI:Application\", \"name\": \"Metadata-Driven ETL for Scalable Lab Dataset Construction\", \"description\": \"CDISC Laboratory Model implementations use metadata-driven ETL approaches to produce standards-compliant lab datasets from source systems, reducing manual data preparation through automated transformation logic that operates at the metadata level rather than requiring code changes. This metadata-driven approach is foundational for building scalable, ML-ready dataset construction pipelines that can consistently process laboratory data across multiple studies and sites, enabling reproducible preprocessing for machine learning workflows.\", \"used_in_bridge2ai\": false, \"references\": [\"https://www.lexjansen.com/pharmasug/2002/proceed/DM/dm01.pdf\"]}, {\"id\": \"B2AI_APP:112\", \"category\": \"B2AI:Application\", \"name\": \"Integrated SDTM Laboratory Data for Pooled ML Analytics\", \"description\": \"Integration of clinical trial and real-world data using CDISC standards including the Laboratory domain produces harmonized SDTM and ADaM datasets that enable pooled analyses and data mining across multiple studies and sponsors. Standardized laboratory variables with CDISC Controlled Terminology facilitate dataset aggregation, warehousing, and reuse, supporting pooled machine learning analyses and real-world evidence research by providing consistent lab feature representations that serve as inputs for predictive modeling and downstream analytics.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.47912/jscdm.128\"]}]	[HL7 FHIR to CDISC Laboratory Mapping for ML-Ready EHR Data|Metadata-Driven ETL for Scalable Lab Dataset Construction|Integrated SDTM Laboratory Data for Pooled ML Analytics]	[B2AI_APP:9|B2AI_APP:111|B2AI_APP:112]	[['https://doi.org/10.47912/jscdm.162']|['https://www.lexjansen.com/pharmasug/2002/proceed/DM/dm01.pdf']|['https://doi.org/10.47912/jscdm.128']]	[The HL7 FHIR to CDISC Joint Mapping Implementation Guide includes mappings for the Laboratory domain that translate FHIR lab resources into CDISC variables, linked with CDISC LB-to-LOINC mapping guidance to leverage real-world data for clinical trials. This harmonization enables EHR laboratory observations to be transformed into standardized, ML-ready lab features with consistent semantic coding via LOINC, supporting downstream machine learning analyses, pooled studies, and data mining across clinical trial and real-world datasets.|CDISC Laboratory Model implementations use metadata-driven ETL approaches to produce standards-compliant lab datasets from source systems, reducing manual data preparation through automated transformation logic that operates at the metadata level rather than requiring code changes. This metadata-driven approach is foundational for building scalable, ML-ready dataset construction pipelines that can consistently process laboratory data across multiple studies and sites, enabling reproducible preprocessing for machine learning workflows.|Integration of clinical trial and real-world data using CDISC standards including the Laboratory domain produces harmonized SDTM and ADaM datasets that enable pooled analyses and data mining across multiple studies and sponsors. Standardized laboratory variables with CDISC Controlled Terminology facilitate dataset aggregation, warehousing, and reuse, supporting pooled machine learning analyses and real-world evidence research by providing consistent lab feature representations that serve as inputs for predictive modeling and downstream analytics.]	[B2AI:Application|B2AI:Application|B2AI:Application]	[False|False|False]							
B2AI_STANDARD:48	B2AI_STANDARD:BiomedicalStandard	CDISC ODM	CDISC Operational Data Model	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datamodel]	[B2AI_TOPIC:4]	The CSDIC ODM is a vendor-neutral, platform-independent format for exchanging and archiving clinical and translational research data, along with their associated metadata, administrative data, reference data, and audit information. ODM facilitates the regulatory-compliant acquisition, archival and exchange of metadata and data.	True	False	https://www.cdisc.org/standards/data-exchange/odm			[B2AI_ORG:15]	[{\"id\": \"B2AI_APP:10\", \"category\": \"B2AI:Application\", \"name\": \"Clinical Trial Data Integration and Predictive Modeling\", \"description\": \"CDISC ODM (Operational Data Model) is used in AI applications for standardizing clinical trial data exchange, enabling machine learning models to train on multi-study datasets and predict trial outcomes, patient enrollment, and safety events. AI systems leverage ODM's XML-based representation of study metadata, case report forms, and clinical data to automatically harmonize data from different trials, perform cross-study analyses, and develop predictive models for trial design optimization. The standard enables AI applications that forecast patient dropout rates, identify optimal sites for recruitment based on historical data, and detect protocol deviations through anomaly detection. ODM's structured format facilitates automated quality control and regulatory submission preparation through AI-driven validation.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1016/j.cct.2019.105820\"]}]	[Clinical Trial Data Integration and Predictive Modeling]	[B2AI_APP:10]	[['https://doi.org/10.1016/j.cct.2019.105820']]	[CDISC ODM (Operational Data Model) is used in AI applications for standardizing clinical trial data exchange, enabling machine learning models to train on multi-study datasets and predict trial outcomes, patient enrollment, and safety events. AI systems leverage ODM's XML-based representation of study metadata, case report forms, and clinical data to automatically harmonize data from different trials, perform cross-study analyses, and develop predictive models for trial design optimization. The standard enables AI applications that forecast patient dropout rates, identify optimal sites for recruitment based on historical data, and detect protocol deviations through anomaly detection. ODM's structured format facilitates automated quality control and regulatory submission preparation through AI-driven validation.]	[B2AI:Application]	[False]		[B2AI_STANDARD:689]					
B2AI_STANDARD:49	B2AI_STANDARD:BiomedicalStandard	CDISC PRM	CDISC Protocol Representation Model	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4]	The CDISC Protocol Representation Model Version 1.0 (PRM V1.0) is intended for those involved in the planning and design of a research protocol. The model focuses on the characteristics of a study and the definition and association of activities within the protocols, including arms and epochs. PRM V1.0 also includes the definitions of the roles that participate in those activities. The scope of this model includes protocol content including Study Design, Eligibility Criteria, and the requirements from the ClinicalTrials.gov and World Health Organization (WHO) registries. The majority of business requirements were provided by subject matter experts in clinical trial protocols. PRM V1.0 is based on the BRIDG Release 3.0 Protocol Representation sub-domain. It includes all classes in the BRIDG Protocol Representation sub-domain plus some classes from other BRIDG sub-domains, generally classes required for ClinicalTrials.gov and the WHO registries.	True	True	https://www.cdisc.org/standards/foundational/protocol			[B2AI_ORG:15]														
B2AI_STANDARD:50	B2AI_STANDARD:BiomedicalStandard	SEND	CDISC Standard for the Exchange of Nonclinical Data	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4]	The CDISC SEND is intended to guide the organization, structure, and format of standard nonclinical tabulation datasets for interchange between organizations such as sponsors and CROs and for submission to the US Food and Drug Administration (FDA)	True	True	https://www.cdisc.org/standards/foundational/send			[B2AI_ORG:15]														
B2AI_STANDARD:51	B2AI_STANDARD:BiomedicalStandard	CDISC SDM	CDISC Study Design Model in XML	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4]	The CDISC Study Design Model in XML (SDM-XML) version 1.0 allows organizations to provide rigorous, machine-readable, interchangeable descriptions of the designs of their clinical studies, including treatment plans, eligibility and times and events. As an extension to the existing CDISC Operational Data Model (ODM) specification, SDM-XML affords implementers the ease of leveraging existing ODM concepts and re-using existing ODM definitions. SDM-XML defines three key sub-modules  Structure, Workflow, and Timing  permitting various levels of detail in any representation of a clinical studys design, while allowing a high degree of authoring flexibility. The specification document is available for download as a PDF file. A ZIP file containing the XML Schemas, several examples, and an SDM-XML element and attribute reference also is available.	True	True	https://www.cdisc.org/standards/data-exchange/sdm-xml			[B2AI_ORG:15]														
B2AI_STANDARD:52	B2AI_STANDARD:BiomedicalStandard	CHADO	CHADO XML interchange Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:5]	Chado is a modular schema covering many aspects of biology, not just sequence data. Chado-XML has exactly the same scope as the Chado schema.	True	False	https://github.com/GMOD/Chado	https://github.com/GMOD/Chado										doi:10.1093/bioinformatics/btm189				[B2AI_ORG:125]		
B2AI_STANDARD:53	B2AI_STANDARD:BiomedicalStandard	chain	Chain Format for pairwise alignment	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12|B2AI_TOPIC:13]	The Chain format is a compact file format developed by UCSC for representing pairwise genomic sequence alignments that permit gaps in both the reference and query sequences simultaneously. Each chain alignment consists of a header line specifying score, chromosome identifiers, sizes, strands, and coordinates for both sequences, followed by alignment data lines describing ungapped blocks and the gaps between them. The format uses zero-based half-open intervals for coordinates and includes support for reverse-complement alignments through strand indicators. Chain files are particularly useful for comparing whole genomes or large genomic regions, representing syntenic relationships, structural variations, and evolutionary rearrangements. They are commonly used in genome browsers and liftOver tools for mapping genomic coordinates between different genome assemblies or between species. The format supports \"snake\" or rearrangement display modes that visualize complex structural variants including inversions, duplications, and translocations. Chain alignments can be produced by tools like BLASTZ and are widely used in comparative genomics workflows.	True	False	http://genome.ucsc.edu/goldenPath/help/chain.html			[B2AI_ORG:119]														
B2AI_STANDARD:54	B2AI_STANDARD:BiomedicalStandard	CARD	CHARMM Card File Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:27]	The CARD file format is the standard means in CHARMM for providing a human readable and writable coordinate file.	True	False	https://charmm-gui.org/charmmdoc/io.html																	
B2AI_STANDARD:55	B2AI_STANDARD:BiomedicalStandard	CML	Chemical Markup Language	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:3]	CML (Chemical Markup Language) is an XML language designed to hold most of the central concepts in chemistry. It was the first language to be developed and plays the same role for chemistry as MathML for mathematics and GML for geographical systems. CML covers most mainstream chemistry and especially molecules, reactions, solid-state, computation and spectroscopy. Since it has a special flexible approach to numeric science it also covers a very wide range of chemical properties, parameters and experimental observation. It is particularly concerned with the communication between machines and humans, and machines to machines. It has been heavily informed by the current chemical scholarly literature and chemical databases. XML is a mainstream approach providing semantics for science, such as MathML, SBML/BIOPAX (biology), GML and KML (geo) SVG (graphics) and NLM-DTD, ODT and OOXML (documents). CML provides support for most chemistry, especially molecules, compounds, reactions, spectra, crystals and computational chemistry (compchem). CML has been developed by Peter Murray-Rust and Henry Rzepa since 1995. It is the de facto XML for chemistry, accepted by publishers and with more than 1 million lines of Open Source code supporting it. CML can be validated and built into authoring tools (for example the Chemistry Add-in for Microsoft Word). A list of CML-compliant and CML-aware software can be found on the software page. The infrastructure includes legacy converters, dictionaries and conventions, Semantic Web and Linked Open Data. There are several versions of the CML schema. The most recent schema is schema 3. This essentially relaxes many of the constraints imposed in the previous stable release (schema 2.4), allowing users to put together the elements and attributes in a more flexible manner to fit the data that they want to represent more easily.	True	False	https://www.xml-cml.org/																	
B2AI_STANDARD:56	B2AI_STANDARD:BiomedicalStandard	ClinGen Interpretation	ClinGen Interpretation Model	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datamodel]	[B2AI_TOPIC:4|B2AI_TOPIC:35]	The ClinGen Interpretation Model is a comprehensive data model and exchange format designed to capture, structure, and communicate the clinical interpretation of genetic variants. It supports the representation of evidence, reasoning, provenance, and contextual information underlying variant pathogenicity assessments, aligning with ACMG/AMP guidelines and community standards. The model enables detailed documentation of what evidence was used, how it was applied, who performed each interpretive act, and how interpretations build upon prior knowledge. It is flexible enough to represent both simple assertions and fully evidence-based interpretations, and is implemented using industry-standard JSON-LD for interoperability. The ClinGen Interpretation Model is closely aligned with the Monarch Initiative's SEPIO ontology, supporting integration with other biomedical data models and facilitating automated reasoning, reproducibility, and data sharing in clinical genomics.	True	False	https://dataexchange.clinicalgenome.org/interpretation/	https://github.com/clingen-data-model/clingen-interpretation			[{\"id\": \"B2AI_APP:11\", \"category\": \"B2AI:Application\", \"name\": \"LitGen Semi-Supervised Literature Recommendation and Evidence Classification\", \"description\": \"LitGen uses semi-supervised deep learning trained on ClinGen VCI-curated papers annotated with ACMG/AMP-aligned evidence types, incorporating curator explanations and unlabeled ClinGen-linked papers to improve proxy labels for literature recommendation and evidence-type classification. The system leverages ClinGen Variant Curation Interface annotations to predict which papers support specific ACMG/AMP evidence criteria, achieving 7.9-12.6% relative performance improvement over supervised-only models and reducing manual curation burden for variant interpretation workflows.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1142/9789811215636_0007\"]}, {\"id\": \"B2AI_APP:113\", \"category\": \"B2AI:Application\", \"name\": \"ACMG/AMP Feature-Based Variant Pathogenicity Classification\", \"description\": \"Machine learning models explicitly encode ACMG/AMP evidence criteria as features in penalized logistic regression to produce probabilistic pathogenicity scores and rank variants of uncertain significance for prioritization. The approach treats ClinGen/ACMG guideline-based evidence levels as structured ML features, enabling guidelines-informed classification that slightly outperforms some in silico scores on certain VUS datasets and improves prioritization for clinical decision support in hereditary disease gene panels.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1038/s41598-022-06547-3\"]}, {\"id\": \"B2AI_APP:114\", \"category\": \"B2AI:Application\", \"name\": \"ClinGen SVI Calibration of Computational Predictors to PP3/BP4\", \"description\": \"A probabilistic framework maps continuous scores from missense variant prediction tools to ACMG/AMP PP3 and BP4 evidence strength levels using ClinGen/ACMG standards, establishing score intervals for Supporting, Moderate, and Strong evidence. This calibration enables automated, standardized assignment of computational evidence to ClinGen-aligned interpretation workflows, with most tools achieving Supporting level and several reaching Moderate or Strong evidence thresholds for pathogenic or benign classification.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1016/j.ajhg.2022.10.013\"]}, {\"id\": \"B2AI_APP:115\", \"category\": \"B2AI:Application\", \"name\": \"CGBench LLM Benchmarking for ClinGen Evidence Reasoning\", \"description\": \"CGBench uses ClinGen VCI and GCI entries with expert explanations as ground truth to evaluate large language models on evidence extraction, strength scoring, and explanation concordance for variant interpretation. The benchmark shows moderate extraction performance with GPT-4o achieving 49% precision and 79% recall, improved explanation concordance with few-shot prompting reaching 70%, but limited strength-change prediction accuracy, revealing both capabilities and gaps in LLM reasoning for ClinGen-aligned genomic curation tasks.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.48550/arxiv.2510.11985\"]}]	[LitGen Semi-Supervised Literature Recommendation and Evidence Classification|ACMG/AMP Feature-Based Variant Pathogenicity Classification|ClinGen SVI Calibration of Computational Predictors to PP3/BP4|CGBench LLM Benchmarking for ClinGen Evidence Reasoning]	[B2AI_APP:11|B2AI_APP:113|B2AI_APP:114|B2AI_APP:115]	[['https://doi.org/10.1142/9789811215636_0007']|['https://doi.org/10.1038/s41598-022-06547-3']|['https://doi.org/10.1016/j.ajhg.2022.10.013']|['https://doi.org/10.48550/arxiv.2510.11985']]	[LitGen uses semi-supervised deep learning trained on ClinGen VCI-curated papers annotated with ACMG/AMP-aligned evidence types, incorporating curator explanations and unlabeled ClinGen-linked papers to improve proxy labels for literature recommendation and evidence-type classification. The system leverages ClinGen Variant Curation Interface annotations to predict which papers support specific ACMG/AMP evidence criteria, achieving 7.9-12.6% relative performance improvement over supervised-only models and reducing manual curation burden for variant interpretation workflows.|Machine learning models explicitly encode ACMG/AMP evidence criteria as features in penalized logistic regression to produce probabilistic pathogenicity scores and rank variants of uncertain significance for prioritization. The approach treats ClinGen/ACMG guideline-based evidence levels as structured ML features, enabling guidelines-informed classification that slightly outperforms some in silico scores on certain VUS datasets and improves prioritization for clinical decision support in hereditary disease gene panels.|A probabilistic framework maps continuous scores from missense variant prediction tools to ACMG/AMP PP3 and BP4 evidence strength levels using ClinGen/ACMG standards, establishing score intervals for Supporting, Moderate, and Strong evidence. This calibration enables automated, standardized assignment of computational evidence to ClinGen-aligned interpretation workflows, with most tools achieving Supporting level and several reaching Moderate or Strong evidence thresholds for pathogenic or benign classification.|CGBench uses ClinGen VCI and GCI entries with expert explanations as ground truth to evaluate large language models on evidence extraction, strength scoring, and explanation concordance for variant interpretation. The benchmark shows moderate extraction performance with GPT-4o achieving 49% precision and 79% recall, improved explanation concordance with few-shot prompting reaching 70%, but limited strength-change prediction accuracy, revealing both capabilities and gaps in LLM reasoning for ClinGen-aligned genomic curation tasks.]	[B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application]	[False|False|False|False]							
B2AI_STANDARD:57	B2AI_STANDARD:BiomedicalStandard	CLSI AUTO16	CLSI Next-Generation In Vitro Diagnostic Interface	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Exchange of data about and produced by in vitro diagnostic tests.	False	True	https://clsi.org/standards/products/automation-and-informatics/documents/auto16/			[B2AI_ORG:16]														
B2AI_STANDARD:58	B2AI_STANDARD:BiomedicalStandard	MSF	CLUSTAL-W Alignment Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12|B2AI_TOPIC:13|B2AI_TOPIC:26|B2AI_TOPIC:28]	The MSF (Multiple Sequence Format) is a standardized file format produced by CLUSTAL-W for representing multiple sequence alignments of proteins or nucleic acids. It includes sequence metadata, alignment length, checksums for data integrity verification, and displays aligned sequences in blocks with position numbering. The format supports gap characters and ambiguity codes, making it widely compatible with bioinformatics tools for phylogenetic analysis, sequence conservation studies, and comparative genomics. MSF files preserve alignment quality information and are human-readable while maintaining machine-parseable structure for automated processing.	True	False	https://bioinfo.nhri.edu.tw/gcg/doc/11.0/clustalw+.html																	
B2AI_STANDARD:59	B2AI_STANDARD:BiomedicalStandard	DND	CLUSTAL-W Dendrogram Guide File Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12|B2AI_TOPIC:13|B2AI_TOPIC:26|B2AI_TOPIC:28]	Format for the tree (or dendrogram) used to guide the a multiple sequence alignment process.	True	False	https://bioinfo.nhri.edu.tw/gcg/doc/11.0/clustalw+.html																	
B2AI_STANDARD:60	B2AI_STANDARD:BiomedicalStandard	CODE-EHR	CODE-EHR best-practice framework for the use of structured electronic health-care records in clinical research	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:9]	...we propose the CODE-EHR minimum standards framework to be used by researchers and clinicians to improve the design of studies and enhance transparency of study methods. The CODE-EHR framework aims to develop robust and effective utilisation of health-care data for research purposes.	True	False	https://www.bigdata4betterhearts.eu/News/ID/146/More-about-the-CODE-EHR-approach-and-framework											doi:10.1016/S2589-7500(22)00151-0						
B2AI_STANDARD:61	B2AI_STANDARD:BiomedicalStandard	CXI	Coherent X-ray Imaging Data Bank format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:15]	The CXI format was created as common format for all the data in the Coherent X-ray Imaging Data Bank (CXIDB). Naturally its scope is all experimental data collected during Coherent X-ray Imaging experiments as well as all data generated during the analysis of the experimental data.	True	False	https://www.cxidb.org/cxi.html																	
B2AI_STANDARD:62	B2AI_STANDARD:BiomedicalStandard	CCPN	Collaborative Computing Project for the NMR community data model	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datamodel]	[B2AI_TOPIC:5]	The CCPN Data Model for macromolecular NMR is intended to cover all data needed for macromolecular NMR spectroscopy from the initial experimental data to the final validation. It serves for exchange of data between programs, for storage, data harvesting, and database deposition. The data model proper is an abstract description of the relevant data and their relationships - it is implemented in the modeling language UML. From this CCPN autogenerates interfaces (APIs) for various languages, format description and I/O routines, and documentation.	True	False	https://sites.google.com/site/ccpnwiki/home/documentation/ccpnmr-analysis/core-concepts/the-ccpn-data-model				[{\"id\": \"B2AI_APP:12\", \"category\": \"B2AI:Application\", \"name\": \"CcpNmr-Integrated Deep Learning for HNCA Backbone Assignment\", \"description\": \"Deep neural networks integrated with the CcpNmr AnalysisAssign software analyze HNCA spectrum line shapes to yield amino acid type probabilities for automated protein backbone assignment. The networks are trained on synthetic databases with realistic instrumental artifacts and noise, taking 2D slices of pyruvate-patterned HNCA spectra as input and outputting probability tables that combine with primary sequence information for rapid assignment. This ML integration within the CCPN software framework accelerates NMR analysis workflows, though networks require retraining when experimental parameters change.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1126/sciadv.ado0403\"]}]	[CcpNmr-Integrated Deep Learning for HNCA Backbone Assignment]	[B2AI_APP:12]	[['https://doi.org/10.1126/sciadv.ado0403']]	[Deep neural networks integrated with the CcpNmr AnalysisAssign software analyze HNCA spectrum line shapes to yield amino acid type probabilities for automated protein backbone assignment. The networks are trained on synthetic databases with realistic instrumental artifacts and noise, taking 2D slices of pyruvate-patterned HNCA spectra as input and outputting probability tables that combine with primary sequence information for rapid assignment. This ML integration within the CCPN software framework accelerates NMR analysis workflows, though networks require retraining when experimental parameters change.]	[B2AI:Application]	[False]	doi:10.1002/prot.20449						
B2AI_STANDARD:63	B2AI_STANDARD:BiomedicalStandard	CFDE C2M2	Common Fund Data Ecosystem Crosscut Metadata Model	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datamodel]	[B2AI_TOPIC:5]	The Common Fund Data Ecosystem (CFDE) Crosscut Metadata Model (C2M2) is a flexible, extensible data model designed to harmonize metadata across NIH Common Fund data coordinating centers (DCCs) to enable integrated search, discovery, and analysis. C2M2 provides a standardized framework for describing biomedical research assets including files, biosamples, subjects, and collections with their associated metadata using controlled vocabularies and ontologies. The model consists of core entities (file, biosample, subject, project, collection) connected through relationships that capture data provenance, experimental context, and biological associations. C2M2 submissions include data tables describing these entities along with controlled vocabulary term usage tables that link to standard ontologies for enhanced semantic interoperability. The model supports supra-schematic constraints validated by dedicated tools, and includes ontology reference files for adding display names, descriptions, and synonyms to controlled terms. C2M2 enables the CFDE Workbench portal to provide unified search and browsing across heterogeneous Common Fund datasets, facilitating cross-program data discovery and promoting FAIR (Findable, Accessible, Interoperable, Reusable) principles for biomedical research data.	True	False	https://docs.nih-cfde.org/en/latest/c2m2/draft-C2M2_specification/	https://osf.io/bq6k9/	True	[B2AI_ORG:68]	[{\"id\": \"B2AI_APP:13\", \"category\": \"B2AI:Application\", \"name\": \"CFDE Workbench ML-Ready Data Harmonization and Knowledge Graph Integration\", \"description\": \"The CFDE Workbench uses C2M2 to harmonize metadata and host processed data in standardized, AI-ready formats that can be loaded as data frames and integrated into ML workflows. C2M2-derived metadata feeds PostgreSQL databases supporting complex queries for ML dataset preparation, while planned conversion to Neo4j knowledge graphs will align database structure with the metadata model to enhance search performance and facilitate ML applications. This harmonization across Common Fund programs reduces barriers to assembling training corpora and interoperable inputs for cross-program AI/ML analyses.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1101/2025.02.04.636535\"]}, {\"id\": \"B2AI_APP:116\", \"category\": \"B2AI:Application\", \"name\": \"LLM-Powered Chatbot with C2M2-Grounded Workflow Automation\", \"description\": \"The CFDE Workbench integrates an LLM chatbot using OpenAI assistants API that answers Common Fund program and data questions while executing API calls based on standardized workflow specifications. The chatbot leverages C2M2-harmonized resources and codified ETL workflows through the Playbook Workflow Builder to automate discovery and retrieval, using function calling to ensure comprehensive and reproducible results. This direct AI application is enabled by C2M2-backed infrastructure that provides the structured metadata and APIs necessary for the LLM to interact with heterogeneous datasets.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1101/2025.02.04.636535\"]}, {\"id\": \"B2AI_APP:117\", \"category\": \"B2AI:Application\", \"name\": \"FAIRshake Assessment of C2M2-Derived Assets for AI-Readiness\", \"description\": \"The CFDE Workbench subjects metadata, processed data, and code assets derived from C2M2 submissions to automated FAIR assessments via FAIRshake. These assessments evaluate findability, accessibility, interoperability, and reusability of C2M2-derived assets that support AI/ML tools including the Playbook Workflow Builder, GeneSetCart, DD-KG-UI, and CFDE-GSE. By operationalizing AI-readiness through systematic FAIRness evaluation, this application ensures that C2M2-structured datasets meet the accessibility and interoperability requirements necessary for downstream ML consumption.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1101/2025.02.04.636535\"]}, {\"id\": \"B2AI_APP:118\", \"category\": \"B2AI:Application\", \"name\": \"Ecosystem-Level Metadata Harmonization for Cross-Program ML Dataset Discovery\", \"description\": \"The CFDE catalog uses C2M2 to ingest diverse Data Coordinating Center metadata into a unified model supporting centralized indexing and search across NIH Common Fund programs. This harmonization with controlled vocabularies and ontologies minimizes barriers to cross-program dataset discovery and reuse, which is prerequisite for assembling training corpora and interoperable inputs for AI/ML analyses. C2M2's evolving model adapts to community standards and emerging AI/ML technologies, enabling effective search and retrieval mechanisms essential for training AI models on biomedical data while promoting FAIR principles.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1101/2021.11.05.467504\"]}]	[CFDE Workbench ML-Ready Data Harmonization and Knowledge Graph Integration|LLM-Powered Chatbot with C2M2-Grounded Workflow Automation|FAIRshake Assessment of C2M2-Derived Assets for AI-Readiness|Ecosystem-Level Metadata Harmonization for Cross-Program ML Dataset Discovery]	[B2AI_APP:13|B2AI_APP:116|B2AI_APP:117|B2AI_APP:118]	[['https://doi.org/10.1101/2025.02.04.636535']|['https://doi.org/10.1101/2025.02.04.636535']|['https://doi.org/10.1101/2025.02.04.636535']|['https://doi.org/10.1101/2021.11.05.467504']]	[The CFDE Workbench uses C2M2 to harmonize metadata and host processed data in standardized, AI-ready formats that can be loaded as data frames and integrated into ML workflows. C2M2-derived metadata feeds PostgreSQL databases supporting complex queries for ML dataset preparation, while planned conversion to Neo4j knowledge graphs will align database structure with the metadata model to enhance search performance and facilitate ML applications. This harmonization across Common Fund programs reduces barriers to assembling training corpora and interoperable inputs for cross-program AI/ML analyses.|The CFDE Workbench integrates an LLM chatbot using OpenAI assistants API that answers Common Fund program and data questions while executing API calls based on standardized workflow specifications. The chatbot leverages C2M2-harmonized resources and codified ETL workflows through the Playbook Workflow Builder to automate discovery and retrieval, using function calling to ensure comprehensive and reproducible results. This direct AI application is enabled by C2M2-backed infrastructure that provides the structured metadata and APIs necessary for the LLM to interact with heterogeneous datasets.|The CFDE Workbench subjects metadata, processed data, and code assets derived from C2M2 submissions to automated FAIR assessments via FAIRshake. These assessments evaluate findability, accessibility, interoperability, and reusability of C2M2-derived assets that support AI/ML tools including the Playbook Workflow Builder, GeneSetCart, DD-KG-UI, and CFDE-GSE. By operationalizing AI-readiness through systematic FAIRness evaluation, this application ensures that C2M2-structured datasets meet the accessibility and interoperability requirements necessary for downstream ML consumption.|The CFDE catalog uses C2M2 to ingest diverse Data Coordinating Center metadata into a unified model supporting centralized indexing and search across NIH Common Fund programs. This harmonization with controlled vocabularies and ontologies minimizes barriers to cross-program dataset discovery and reuse, which is prerequisite for assembling training corpora and interoperable inputs for AI/ML analyses. C2M2's evolving model adapts to community standards and emerging AI/ML technologies, enabling effective search and retrieval mechanisms essential for training AI models on biomedical data while promoting FAIR principles.]	[B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application]	[False|False|False|False]							
B2AI_STANDARD:64	B2AI_STANDARD:BiomedicalStandard	Common Metadata	Common Metadata Elements for Cataloging Biomedical Datasets	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:5]	This dataset outlines a proposed set of core, minimal metadata elements that can be used to describe biomedical datasets, such as those resulting from research funded by the National Institutes of Health. It can inform efforts to better catalog or index such data to improve discoverability. The proposed metadata elements are based on an analysis of the metadata schemas used in a set of NIH-supported data sharing repositories. Common elements from these data repositories were identified, mapped to existing data-specific metadata standards from to existing multidisciplinary data repositories, DataCite and Dryad, and compared with metadata used in MEDLINE records to establish a sustainable and integrated metadata schema.	True	False	https://figshare.com/articles/dataset/Common_Metadata_Elements_for_Cataloging_Biomedical_Datasets/1496573	https://figshare.com/articles/dataset/Common_Metadata_Elements_for_Cataloging_Biomedical_Datasets/1496573?file=3377663																
B2AI_STANDARD:65	B2AI_STANDARD:BiomedicalStandard	FACIT-COST	COmprehensive Score for financial Toxicity A FACIT Measure of Financial Toxicity	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[diagnosticinstrument]	[B2AI_TOPIC:5]	Developed in conjunction with the University of Chicago, the COST is a patient-reported outcome measure that describes the financial distress experienced by cancer patients. Since its initial publication, an additional item from the FACIT System has been included to screen for financial toxicity and to provide a good global summary item for financial toxicity.	True	False	https://www.facit.org/measures/FACIT-COST											doi:10.1002/cncr.30369				[B2AI_ORG:30]		
B2AI_STANDARD:66	B2AI_STANDARD:BiomedicalStandard	C-CDA	Consolidated Clinical Document Architecture	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:9]	A widely-used, XML-based format for electronic health records. Superceded by FHIR document standards.	True	True	https://www.healthit.gov/topic/standards-technology/consolidated-cda-overview	http://www.hl7.org/implement/standards/product_brief.cfm?product_id=492		[B2AI_ORG:40]														
B2AI_STANDARD:67	B2AI_STANDARD:BiomedicalStandard	COREQ	Consolidated criteria for reporting qualitative research	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:5]	COREQ (Consolidated Criteria for Reporting Qualitative Research) is a comprehensive 32-item reporting checklist specifically designed to improve the transparency, rigor, and completeness of qualitative research reports, particularly those involving interviews and focus groups. This evidence-based guideline addresses three major domains essential for qualitative research reporting: research team and reflexivity (covering the researcher's credentials, occupation, experience, gender, and relationship with participants), study design (including theoretical framework, participant selection, setting, and data collection methods), and analysis and findings (encompassing data analysis approaches, verification procedures, and presentation of findings). COREQ serves as a critical quality assessment tool for researchers, editors, reviewers, and readers, helping ensure that qualitative studies provide sufficient detail for proper evaluation and potential replication. The checklist promotes methodological transparency and supports the credibility and trustworthiness of qualitative health research by standardizing reporting expectations across the research community.	True	False	http://www.cnfs.net/modules/module2/story_content/external_files/13_COREQ_checklist_000017.pdf	http://www.cnfs.net/modules/module2/story_content/external_files/13_COREQ_checklist_000017.pdf										doi:10.1093/intqhc/mzm042						
B2AI_STANDARD:68	B2AI_STANDARD:BiomedicalStandard	CHEERS	Consolidated Health Economic Evaluation Reporting Standards	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:5]	The Consolidated Health Economic Evaluation Reporting Standards (CHEERS) statement is an attempt to consolidate and update previous health economic evaluation guidelines efforts into one current, useful reporting guidance. The primary audiences for the CHEERS statement are researchers reporting economic evaluations and the editors and peer reviewers assessing them for publication.	True	True	https://www.ispor.org/heor-resources/good-practices/article/consolidated-health-economic-evaluation-reporting-standards-2022-cheers-2022-statement-updated-reporting-guidance-for-health-economic-evaluations	https://www.ispor.org/heor-resources/good-practices/article/consolidated-health-economic-evaluation-reporting-standards-2022-cheers-2022-statement-updated-reporting-guidance-for-health-economic-evaluations										doi:10.1136/bmj.f1049						
B2AI_STANDARD:69	B2AI_STANDARD:BiomedicalStandard	CONSORT	Consolidated Standards of Reporting Trials	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:4]	CONSORT (Consolidated Standards of Reporting Trials) is an evidence-based minimum set of recommendations for reporting randomized controlled trial (RCT) results, developed to address inadequate reporting that prevents proper assessment of trial quality and generalizability. The CONSORT 2025 Statement consists of a 30-item checklist and flow diagram covering how trials are designed, analyzed, and interpreted, with detailed recommendations for reporting participant recruitment, randomization procedures, interventions, outcomes, sample size calculations, statistical methods, baseline characteristics, results for each outcome, harms, interpretation, and generalizability. The flow diagram tracks progress of all participants through each trial stage (enrollment, allocation, follow-up, analysis) with numbers and reasons for exclusions and losses. CONSORT is companion to SPIRIT (Standard Protocol Items - Recommendations for Interventional Trials) which provides a 34-item checklist for protocol reporting. Both guidelines emphasize transparent and complete reporting to enable readers to critically appraise, interpret, and apply research findings. CONSORT includes numerous extensions for specific trial designs (cluster randomized, non-inferiority, pragmatic trials), interventions (herbal, acupuncture, non-pharmacological treatment), and data types. The guidelines are supported by Explanation and Elaboration documents with detailed rationale and examples, online tools (COBWEB for report writing, SEPTRE for protocol management), translations in multiple languages, and endorsement by hundreds of medical journals worldwide as part of the EQUATOR Network research reporting standards.	True	False	https://www.consort-statement.org/											doi:10.1136/bmj.c332						
B2AI_STANDARD:70	B2AI_STANDARD:BiomedicalStandard	Continua	Continua Design Guidelines	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:5]	A suite of open industry standards and specifications that provide several means to end-to-end interoperability between personal medical devices and health information systems.	True	False	https://www.pchalliance.org/continua-design-guidelines	https://members.pchalliance.org/document/dl/2148																
B2AI_STANDARD:71	B2AI_STANDARD:BiomedicalStandard	CCDEF	Critical care data exchange format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:9]	HDF5-based critical care data exchange format which stores multiparametric data in an efficient, self-describing, hierarchical structure and supports real-time streaming and compression. In addition to cardiorespiratory and laboratory data, the format can, in future, accommodate other large datasets such as imaging and genomics.	True	False	https://ccdef.org/	https://github.com/autonlab/auviewer										doi:10.1088/1361-6579/abfc9b	[B2AI_STANDARD:339]		[B2AI_SUBSTRATE:16]			
B2AI_STANDARD:72	B2AI_STANDARD:BiomedicalStandard	Crypt4GH	Crypt4GH format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:5]	Crypt4GH is a file format specification developed by the Global Alliance for Genomics and Health (GA4GH) for secure storage and transmission of genomic and health-related data. The format provides end-to-end encryption and authentication, allowing sensitive genomic data to be stored in public repositories while maintaining confidentiality. It uses modern cryptographic standards including Curve25519 for key agreement, ChaCha20 for encryption, and Poly1305 for authentication. The format supports multiple recipients through per-recipient encrypted headers, enabling controlled data sharing where different users can decrypt the same file using their own private keys. Crypt4GH is designed for streaming access, allowing applications to decrypt and process data incrementally without loading entire files into memory. The format includes metadata protection and supports selective decryption of file segments, making it suitable for large-scale genomic datasets. Existing bioinformatics tools can be adapted with minimal modifications to read and write Crypt4GH-encrypted data, facilitating adoption in genomics workflows.	True	False	https://samtools.github.io/hts-specs/crypt4gh.pdf	https://github.com/samtools/hts-specs														[B2AI_ORG:34]		
B2AI_STANDARD:73	B2AI_STANDARD:BiomedicalStandard	CIF	Crystallographic Information File format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:27]	The acronym CIF is used both for the Crystallographic Information File, the data exchange standard file format of Hall, Allen & Brown (1991) (see Documentation), and for the Crystallographic Information Framework, a broader system of exchange protocols based on data dictionaries and relational rules expressible in different machine-readable manifestations, including, but not restricted to, Crystallographic Information File and XML.	True	False	https://en.wikipedia.org/wiki/Crystallographic_Information_File																	
B2AI_STANDARD:74	B2AI_STANDARD:OntologyOrVocabulary	CPT	Current Procedural Terminology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[codesystem|standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:9]	Code set used to bill outpatient & office procedures.	False	True	https://www.ama-assn.org/amaone/cpt-current-procedural-terminology	https://www.cms.gov/Medicare/Fraud-and-Abuse/PhysicianSelfReferral	True	[B2AI_ORG:3]														
B2AI_STANDARD:75	B2AI_STANDARD:BiomedicalStandard	DDI-Lifecycle	Data Documentation Initiative Lifecycle	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	The freely available international DDI standard describes data that result from observational methods in the social, behavioral, economic, and health sciences. DDI is used to document data in over 60 countries of the world.	True	False	https://ddialliance.org/Specification/DDI-Lifecycle/3.3/			[B2AI_ORG:23]														
B2AI_STANDARD:76	B2AI_STANDARD:BiomedicalStandard	DOS-DP	Dead simple owl design pattern exchange format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A simple design pattern system that can easily be consumed, whatever your code base, for OWL ontologies.	True	False	https://oboacademy.github.io/obook/tutorial/dosdp-template/	https://github.com/INCATools/dead_simple_owl_design_patterns										doi:10.1186/s13326-017-0126-0				[B2AI_ORG:58]		
B2AI_STANDARD:77	B2AI_STANDARD:BiomedicalStandard	DMN	Decision Model and Notation	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A modeling language and notation for the precise specification of business decisions and business rules.	True	False	https://www.omg.org/dmn/															[B2AI_ORG:10]		
B2AI_STANDARD:78	B2AI_STANDARD:BiomedicalStandard	DELTA	Description Language for Taxonomy	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:5]	When taxonomic descriptions are prepared for input to computer programs, the form of the coding is usually dictated by the requirements of a particular program or set of programs. This restricts the type of data that can be represented, and the number of other programs that can use the data. Even when working with a particular program, it is frequently necessary to set up different versions of the same basic data, for example, when using restricted sets of taxa or characters to make special-purpose keys. The potential advantages of automation, especially in connexion with large groups, cannot be realized if the data have to be restructured by hand for every operation. The DELTA (DEscription Language for TAxonomy) system was developed to overcome these problems. It was designed primarily for easy use by people rather than for convenience in computer programming, and is versatile enough to replace the written description as the primary means of recording data. Consequently, it can be used as a shorthand method of recording data, even if computer processing of the data is not envisaged.	True	False	https://www.delta-intkey.com/																	
B2AI_STANDARD:79	B2AI_STANDARD:BiomedicalStandard	DICOMDIR	DICOM Part 10 Media Storage and File Format for Media Interchange	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[audiovisual|fileformat|standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:15]	This Part of the DICOM Standard specifies a general model for the storage of Medical Imaging information on removable media. The purpose of this Part is to provide a framework allowing the interchange of various types of medical images and related information on a broad range of physical storage media.	True	False	https://www.dicomstandard.org/current	http://dicom.nema.org/medical/dicom/current/output/html/part10.html	True	[B2AI_ORG:25]										[B2AI_STANDARD:849]	[B2AI_SUBSTRATE:11]		[B2AI_STANDARD:98]	
B2AI_STANDARD:80	B2AI_STANDARD:BiomedicalStandard	DICOM Part 11	DICOM Part 11 Media Storage Application Profiles	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:15]	This Part of the DICOM Standard specifies application specific subsets of the DICOM Standard to which an implementation may claim conformance. Such a conformance statement applies to the interoperable interchange of medical images and related information on storage media for specific clinical uses.	True	False	https://www.dicomstandard.org/current	http://dicom.nema.org/medical/dicom/current/output/html/part11.html	True	[B2AI_ORG:25]										[B2AI_STANDARD:849]	[B2AI_SUBSTRATE:11]		[B2AI_STANDARD:98]	
B2AI_STANDARD:81	B2AI_STANDARD:BiomedicalStandard	DICOM Part 12	DICOM Part 12 Media Formats and Physical Media for Media Interchange	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:15]	This Part of the DICOM Standard facilitates the interchange of information between digital imaging computer systems in medical environments. This interchange will enhance diagnostic imaging and potentially other clinical applications. The multi-part DICOM Standard defines the services and data that shall be supplied to achieve this interchange of information.	True	False	https://www.dicomstandard.org/current	http://dicom.nema.org/medical/dicom/current/output/html/part12.html	True	[B2AI_ORG:25]										[B2AI_STANDARD:849]	[B2AI_SUBSTRATE:11]		[B2AI_STANDARD:98]	
B2AI_STANDARD:82	B2AI_STANDARD:BiomedicalStandard	DICOM Part 14	DICOM Part 14 Grayscale Standard Display Function	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:15]	PS3.14 specifies a standardized Display Function for display of grayscale images. It provides examples of methods for measuring the Characteristic Curve of a particular Display System for the purpose of either altering the Display System to match the Grayscale Standard Display Function, or for measuring the conformance of a Display System to the Grayscale Standard Display Function. Display Systems include such things as monitors with their associated driving electronics and printers producing films that are placed on light-boxes or alternators.	True	False	https://www.dicomstandard.org/current	http://dicom.nema.org/medical/dicom/current/output/html/part14.html	True	[B2AI_ORG:25]										[B2AI_STANDARD:849]	[B2AI_SUBSTRATE:11]		[B2AI_STANDARD:98]	
B2AI_STANDARD:83	B2AI_STANDARD:BiomedicalStandard	DICOM Part 15	DICOM Part 15 Security and System Management Profiles	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:15]	This Part of the DICOM Standard specifies Security and System Management Profiles to which implementations may claim conformance. Security and System Management Profiles are defined by referencing externally developed standard protocols, such as TLS, ISCL, DHCP, and LDAP, with attention to their use in a system that uses DICOM Standard protocols for information interchange.	True	False	https://www.dicomstandard.org/current	http://dicom.nema.org/medical/dicom/current/output/html/part15.html	True	[B2AI_ORG:25]										[B2AI_STANDARD:849]	[B2AI_SUBSTRATE:11]		[B2AI_STANDARD:98]	
B2AI_STANDARD:84	B2AI_STANDARD:BiomedicalStandard	DCMR	DICOM Part 16 Content Mapping Resource	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:15]	DICOM Part 16 Content Mapping Resource (DCMR) is a comprehensive component of the DICOM standard that provides the foundational vocabulary and semantic structures used throughout medical imaging systems worldwide. DCMR defines the Templates and Context Groups that standardize the representation of clinical concepts, measurements, and observations within DICOM objects, ensuring consistent interpretation of medical imaging data across different systems, vendors, and healthcare institutions. The Content Mapping Resource includes structured templates for various types of medical imaging reports, measurements, and annotations, covering specialties such as radiology, cardiology, ophthalmology, and oncology. These templates define the specific data elements, their relationships, and the controlled terminologies that should be used when creating structured reports and enhanced imaging objects. DCMR plays a crucial role in enabling interoperability, supporting artificial intelligence applications in medical imaging, and facilitating the exchange of semantically rich imaging information in clinical practice and research environments.	True	False	https://www.dicomstandard.org/current	http://dicom.nema.org/medical/dicom/current/output/html/part16.html	True	[B2AI_ORG:25]										[B2AI_STANDARD:849]	[B2AI_SUBSTRATE:11]			
B2AI_STANDARD:85	B2AI_STANDARD:BiomedicalStandard	DICOM Part 17	DICOM Part 17 Explanatory Information	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:15]	This Part of the DICOM Standard contains explanatory information in the form of Normative and Informative Annexes.	True	False	https://www.dicomstandard.org/current	http://dicom.nema.org/medical/dicom/current/output/html/part17.html	True	[B2AI_ORG:25]										[B2AI_STANDARD:849]	[B2AI_SUBSTRATE:11]		[B2AI_STANDARD:98]	
B2AI_STANDARD:86	B2AI_STANDARD:BiomedicalStandard	DICOMweb	DICOM Part 18 Web Services	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:15]	PS3.18 specifies web services (using the HTTP family of protocols) for managing and distributing DICOM (Digital Imaging and Communications in Medicine) Information Objects, such as medical images, annotations, reports, etc. to healthcare organizations, providers, and patients. The term DICOMweb is used to designate the RESTful Web Services described here.	True	False	https://www.dicomstandard.org/current	http://dicom.nema.org/medical/dicom/current/output/html/part18.html	True	[B2AI_ORG:25]										[B2AI_STANDARD:849]	[B2AI_SUBSTRATE:11]		[B2AI_STANDARD:98]	
B2AI_STANDARD:87	B2AI_STANDARD:BiomedicalStandard	DICOM Part 19	DICOM Part 19 Application Hosting	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:15]	This Part of the DICOM Standard defines an interface between two software applications. One application, the Hosting System, provides the second application with data, such as a set of images and related data. The second application, the Hosted Application, analyzes that data, potentially returning the results of that analysis, for example in the form of another set of images and/or structured reports, to the first application. Such an Application Program Interface (API) differs in scope from other portions of the DICOM Standard in that it standardizes the data interchange between software components on the same system, instead of data interchange between different systems.	True	False	https://www.dicomstandard.org/current	http://dicom.nema.org/medical/dicom/current/output/html/part19.html	True	[B2AI_ORG:25]										[B2AI_STANDARD:849]	[B2AI_SUBSTRATE:11]		[B2AI_STANDARD:98]	
B2AI_STANDARD:88	B2AI_STANDARD:BiomedicalStandard	DICOM Part 2	DICOM Part 2 Conformance	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:15]	An implementation need not employ all the optional components of the DICOM Standard. After meeting the minimum general requirements, a conformant DICOM implementation may utilize whatever SOP Classes, communications protocols, Media Storage Application Profiles, optional (Type 3) Attributes, codes and controlled terminology, etc., needed to accomplish its designed task.	True	False	https://www.dicomstandard.org/current	http://dicom.nema.org/medical/dicom/current/output/html/part02.html	True	[B2AI_ORG:25]										[B2AI_STANDARD:849]	[B2AI_SUBSTRATE:11]		[B2AI_STANDARD:98]	
B2AI_STANDARD:89	B2AI_STANDARD:BiomedicalStandard	DICOM Part 20	DICOM Part 20 Imaging Reports using HL7 Clinical Document Architecture	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:15]	This Part of the DICOM Standard specifies templates for the encoding of imaging reports using the HL7 Clinical Document Architecture Release 2 (CDA R2, or simply CDA) Standard. Within this scope are clinical procedure reports for specialties that use imaging for screening, diagnostic, or therapeutic purposes.	True	False	https://www.dicomstandard.org/current	http://dicom.nema.org/medical/dicom/current/output/html/part20.html	True	[B2AI_ORG:25]										[B2AI_STANDARD:849]	[B2AI_SUBSTRATE:11]	[B2AI_ORG:40]	[B2AI_STANDARD:98]	
B2AI_STANDARD:90	B2AI_STANDARD:BiomedicalStandard	DICOM Part 21	DICOM Part 21 Transformations between DICOM and other Representations	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:15|standards_process_maturity_final|implementation_maturity_production]	This Part of the DICOM Standard specifies the transformations between DICOM and other representations of the same information.	True	False	https://www.dicomstandard.org/current	http://dicom.nema.org/medical/dicom/current/output/html/part21.html	True	[B2AI_ORG:25]										[B2AI_STANDARD:849]	[B2AI_SUBSTRATE:11]		[B2AI_STANDARD:98]	
B2AI_STANDARD:91	B2AI_STANDARD:BiomedicalStandard	DICOM Part 22	DICOM Part 22 Real-Time Communication	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:15]	This Part of the DICOM Standard specifies an SMPTE ST 2110-10 based service, relying on RTP, for the real-time transport of DICOM metadata. It provides a mechanism for the transport of DICOM metadata associated with a video or an audio flow based on the SMPTE ST 2110-20 and SMPTE ST 2110-30, respectively.	True	False	https://www.dicomstandard.org/current	http://dicom.nema.org/medical/dicom/current/output/html/part22.html	True	[B2AI_ORG:25]										[B2AI_STANDARD:849]	[B2AI_SUBSTRATE:11]		[B2AI_STANDARD:98]	
B2AI_STANDARD:92	B2AI_STANDARD:BiomedicalStandard	DICOM Part 3	DICOM Part 3 Information Object Definitions	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:15]	This Part of the DICOM Standard specifies the set of Information Object Definitions (IODs) that provide an abstract definition of real-world objects applicable to communication of digital medical information. For each IOD, this Part specifies any necessary information for the semantic description of the IOD, relationships to associated real-world objects relevant to the IOD, Attributes that describe the characteristics of the IOD.	True	False	https://www.dicomstandard.org/current	http://dicom.nema.org/medical/dicom/current/output/html/part03.html	True	[B2AI_ORG:25]										[B2AI_STANDARD:849]	[B2AI_SUBSTRATE:11]		[B2AI_STANDARD:98]	
B2AI_STANDARD:93	B2AI_STANDARD:BiomedicalStandard	DICOM Part 4	DICOM Part 4 Service Class Specifications	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:15]	This Part of the DICOM Standard specifies the set of Service Class Definitions that provide an abstract definition of real-world activities applicable to communication of digital medical information.	True	False	https://www.dicomstandard.org/current	http://dicom.nema.org/medical/dicom/current/output/html/part04.html	True	[B2AI_ORG:25]										[B2AI_STANDARD:849]	[B2AI_SUBSTRATE:11]		[B2AI_STANDARD:98]	
B2AI_STANDARD:94	B2AI_STANDARD:BiomedicalStandard	DICOM Part 5	DICOM Part 5 Data Structures and Encoding	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:15]	In this Part of the Standard the structure and encoding of the Data Set is specified. In the context of Application Entities communicating over a network, a Data Set is that portion of a DICOM Message that conveys information about real world objects being managed over the network. A Data Set may have other contexts in other applications of this Standard; e.g., in media exchange the Data Set translates to file content structure.	True	False	https://www.dicomstandard.org/current	http://dicom.nema.org/medical/dicom/current/output/html/part05.html	True	[B2AI_ORG:25]										[B2AI_STANDARD:849]	[B2AI_SUBSTRATE:11]		[B2AI_STANDARD:98]	
B2AI_STANDARD:95	B2AI_STANDARD:BiomedicalStandard	DICOM Part 6	DICOM Part 6 Data Dictionary	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:15]	This Part of the DICOM Standard is PS 3.6 of a multi-part standard produced to facilitate the interchange of information between digital imaging computer systems in medical environments. This interchange will enhance diagnostic imaging and potentially other clinical applications. The multi-part DICOM Standard covers the protocols and data that shall be supplied to achieve this interchange of information.	True	False	https://www.dicomstandard.org/current	http://dicom.nema.org/medical/dicom/current/output/html/part06.html	True	[B2AI_ORG:25]										[B2AI_STANDARD:849]	[B2AI_SUBSTRATE:11]		[B2AI_STANDARD:98]	
B2AI_STANDARD:96	B2AI_STANDARD:BiomedicalStandard	DIMSE	DICOM Part 7 Message Exchange	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:15]	This Part of the DICOM Standard specifies the DICOM Message Service Element (DIMSE). The DIMSE defines an Application Service Element (both the service and protocol) used by peer DICOM Application Entities for the purpose of exchanging medical images and related information.	True	False	https://www.dicomstandard.org/current	http://dicom.nema.org/medical/dicom/current/output/html/part07.html	True	[B2AI_ORG:25]										[B2AI_STANDARD:849]	[B2AI_SUBSTRATE:11]		[B2AI_STANDARD:98]	
B2AI_STANDARD:97	B2AI_STANDARD:BiomedicalStandard	DICOM Part 8	DICOM Part 8 Network Communication Support for Message Exchange	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:15]	The Communication Protocols specified in this Part of PS3 closely fit the ISO Open Systems Interconnection Basic Reference Model (ISO 7498-1, see Figure 1-1). They relate to the following layers - Physical, Data Link, Network, Transport, Session, Presentation and the Association Control Services (ACSE) of the Application layer. The communication protocols specified by this Part are general purpose communication protocols (TCP/IP) and not specific to this Standard.	True	False	https://www.dicomstandard.org/current	http://dicom.nema.org/medical/dicom/current/output/html/part08.html	True	[B2AI_ORG:25]										[B2AI_STANDARD:849]	[B2AI_SUBSTRATE:11]		[B2AI_STANDARD:98]	
B2AI_STANDARD:98	B2AI_STANDARD:BiomedicalStandard	DICOM	Digital Imaging And Communications In Medicine	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:15]	Digital Imaging and Communications in Medicine (DICOM) is the international standard for medical imaging information and related data. DICOM defines the formats and communication protocols for medical images (CT, MRI, ultrasound, X-ray, etc.) and associated metadata including patient demographics, study information, equipment parameters, and image acquisition details. The standard encompasses file formats, network protocols, and information models that enable interoperability across imaging devices, PACS (Picture Archiving and Communication Systems), workstations, and clinical information systems. DICOM supports structured reporting through DICOM-SR for encoding measurements, annotations, and AI-generated findings in a standardized machine-readable format. The standard includes specialized modules for radiation dose tracking, workflow management, and integration of AI inference results. DICOM's comprehensive metadata structure and pixel data encoding make it essential for training and deploying AI models in medical imaging, while its widespread clinical adoption ensures that AI tools can integrate seamlessly into existing radiological workflows and health IT infrastructure.	True	False	https://www.dicomstandard.org/	https://www.dicomstandard.org/current	True	[B2AI_ORG:25]	[{\"id\": \"B2AI_APP:33\", \"category\": \"B2AI:Application\", \"name\": \"NCI Imaging Data Commons FAIR Dataset Harmonization for ML Training\", \"description\": \"The NCI Imaging Data Commons uses DICOM to harmonize large-scale public imaging collections with AI-derived annotations, supporting transparent cohort selection and reproducible ML development. IDC encodes images and derived artifacts (volumetric segmentations, slice-level annotations, radiomics features) consistently using DICOM Segmentation objects and Structured Reports, enabling FAIR (findable, accessible, interoperable, reusable) datasets. Cloud-ready tools and libraries including highdicom lower barriers for ML developers to produce and consume DICOM-compliant training labels and outputs directly from Python workflows, facilitating dataset quality control and reproducibility for AI model development.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1038/s41597-023-02864-y\"]}, {\"id\": \"B2AI_APP:119\", \"category\": \"B2AI:Application\", \"name\": \"DICOM Segmentation and Structured Report Encoding for Supervised Learning Labels\", \"description\": \"Research datasets are re-encoded into DICOM annotation objects to make labels directly consumable by ML pipelines. LIDC-IDRI lung nodule annotations were converted to DICOM Segmentation (SEG) objects and DICOM Structured Reports following template TID1500, providing interoperable nodular masks and qualitative/quantitative assessments. Disease-specific datasets like HCC-TACE distribute DICOM-SEG tumor/liver labels for algorithm training and evaluation. DICOM-SEG metadata tags capture algorithm provenance (Segment algorithm type, Segmentation type) essential for data curation, reproducibility, and supervised ML workflows, while software stacks (DCMTK, ITK, dcmqi, pydicom-seg) enable conversion and pipeline integration.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1186/s13244-021-01081-8\"]}, {\"id\": \"B2AI_APP:120\", \"category\": \"B2AI:Application\", \"name\": \"Vendor-Agnostic PACS-Integrated Shadow Testing Pipeline for Near-Real-Time Segmentation\", \"description\": \"A containerized, DICOM-compatible pipeline integrates PACS with ML inference for near-real-time clinical shadow testing of segmentation algorithms. PACS sends studies via classic DICOM services (C-STORE/C-FIND/C-MOVE) to an on-premises GPU host that performs nnU-Net inference and converts results to DICOM Segmentation objects and Structured Reports encoding volumetry. Results are stored in a DCM4CHEE archive and visualized in OHIF web viewer, enabling clinical evaluation, human-in-the-loop corrections, and dataset curation. This demonstrates concrete PACS-to-inference-to-reporting workflows using established DICOM network protocols for AI integration into radiology operations.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.3389/fmed.2023.1241570\"]}, {\"id\": \"B2AI_APP:121\", \"category\": \"B2AI:Application\", \"name\": \"DICOM Structured Report TID1500 for Machine-Readable AI Results and Radiomics\", \"description\": \"Quantitative AI outputs including radiomics features and imaging biomarkers are standardized via DICOM Structured Report template TID1500 measurement reports, providing machine-readable, auditable AI results that facilitate downstream research and clinical decision support. DICOM SR encodes semantic measurements with standardized format and lexicon, acting as a big-data container for multimodal patient data integration. DICOM Parametric Map IODs preserve derived pixelwise feature maps (kurtosis, texture) as DICOM objects for reproducible feature extraction. Integration guidance from standards bodies emphasizes encoding AI outputs into SR to scale deployment across vendors and sites.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1186/s13244-021-01081-8\", \"https://doi.org/10.1117/1.jmi.7.1.016502\"]}, {\"id\": \"B2AI_APP:122\", \"category\": \"B2AI:Application\", \"name\": \"DICOMweb RESTful Services for Scalable Cloud and Web-Based ML Pipelines\", \"description\": \"DICOMweb RESTful services (WADO-RS, QIDO-RS, STOW-RS) connect external ML applications and viewers to enterprise archives over HTTP, enabling scalable web and cloud AI pipelines. DICOMweb client APIs allow external apps to retrieve, query, and store DICOM objects without classic DIMSE protocols, facilitating integration with modern web architectures. Frameworks stream DICOM and metadata from hospital PACS into research compute clusters to power real-time ML and operational analytics. Examples include Niffler for PACS-to-research pipelines with metadata extraction and ML analytics, and EMPAIA decentralized platform for running third-party AI on DICOM data at scale.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1016/j.cmpb.2024.108113\"]}, {\"id\": \"B2AI_APP:123\", \"category\": \"B2AI:Application\", \"name\": \"DICOM-WSI and DICOMweb for Computational Pathology ML Development\", \"description\": \"DICOM Whole Slide Imaging (DICOM-WSI) and DICOMweb enable interoperable storage, visualization, and annotation of whole-slide images in web viewers for computational pathology. These standards support collecting standardized annotations and displaying AI inference outputs including segmentation masks, heat maps, and image-derived measurements within the DICOM ecosystem. DICOM-WSI brings pathology images into uniform DICOM workflows to facilitate ML model development and validation, while web viewers (e.g., Visilab Viewer) provide platforms for annotating training data and reviewing AI results in digital pathology contexts.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1038/s41597-023-02864-y\", \"https://doi.org/10.1016/j.cmpb.2024.108113\"]}, {\"id\": \"B2AI_APP:124\", \"category\": \"B2AI:Application\", \"name\": \"IODeep Proposed IOD for DNN Model Management and Federated Learning in PACS\", \"description\": \"The proposed IODeep DICOM Information Object Definition stores deep neural network architectures and weights inside PACS as non-patient objects, enabling model selection via DICOM tag metadata (modality, anatomical region, disease). Inference runs client-side or on dedicated servers proximal to viewers, with predicted ROIs saved as RT Structure Sets for physician validation and retraining. IODeep supports model lifecycle management including fine-tuning with hospital data and federated learning scenarios by sharing model metadata without moving patient data. The design uses Unlimited Text VR for JSON architecture descriptions and weight URIs, with parsers for TensorFlow/PyTorch instantiation, demonstrating an emerging pattern for managing AI models within DICOM-compliant workflows.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1016/j.cmpb.2024.108113\"]}, {\"id\": \"B2AI_APP:125\", \"category\": \"B2AI:Application\", \"name\": \"DICOM GSPS and RT Structure Sets for AI Annotation Presentation and Clinical Feedback\", \"description\": \"DICOM Grayscale Softcopy Presentation State (GSPS) ensures consistent overlay and display of AI annotations across workstations, while RT Structure Set (RTSS) objects store physician-validated contours after AI suggestion, supporting clinical quality assurance and retraining workflows. RTSS is widely supported for storing ROI contours and labels for model training, enabling human-in-the-loop correction loops where radiologists review AI predictions, validate or correct them, and create ground-truth datasets. Integration of CAD results into PACS using DICOM SR and GSPS, combined with DICOM routers for communication and correction, supports maturity levels from investigational AI display through continuously updated deployed models driven by radiologist interactions.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1117/1.jmi.7.1.016502\", \"https://doi.org/10.1016/j.cmpb.2024.108113\"]}]	[NCI Imaging Data Commons FAIR Dataset Harmonization for ML Training|DICOM Segmentation and Structured Report Encoding for Supervised Learning Labels|Vendor-Agnostic PACS-Integrated Shadow Testing Pipeline for Near-Real-Time Segmentation|DICOM Structured Report TID1500 for Machine-Readable AI Results and Radiomics|DICOMweb RESTful Services for Scalable Cloud and Web-Based ML Pipelines|DICOM-WSI and DICOMweb for Computational Pathology ML Development|IODeep Proposed IOD for DNN Model Management and Federated Learning in PACS|DICOM GSPS and RT Structure Sets for AI Annotation Presentation and Clinical Feedback]	[B2AI_APP:33|B2AI_APP:119|B2AI_APP:120|B2AI_APP:121|B2AI_APP:122|B2AI_APP:123|B2AI_APP:124|B2AI_APP:125]	[['https://doi.org/10.1038/s41597-023-02864-y']|['https://doi.org/10.1186/s13244-021-01081-8']|['https://doi.org/10.3389/fmed.2023.1241570']|['https://doi.org/10.1186/s13244-021-01081-8', 'https://doi.org/10.1117/1.jmi.7.1.016502']|['https://doi.org/10.1016/j.cmpb.2024.108113']|['https://doi.org/10.1038/s41597-023-02864-y', 'https://doi.org/10.1016/j.cmpb.2024.108113']|['https://doi.org/10.1016/j.cmpb.2024.108113']|['https://doi.org/10.1117/1.jmi.7.1.016502', 'https://doi.org/10.1016/j.cmpb.2024.108113']]	[The NCI Imaging Data Commons uses DICOM to harmonize large-scale public imaging collections with AI-derived annotations, supporting transparent cohort selection and reproducible ML development. IDC encodes images and derived artifacts (volumetric segmentations, slice-level annotations, radiomics features) consistently using DICOM Segmentation objects and Structured Reports, enabling FAIR (findable, accessible, interoperable, reusable) datasets. Cloud-ready tools and libraries including highdicom lower barriers for ML developers to produce and consume DICOM-compliant training labels and outputs directly from Python workflows, facilitating dataset quality control and reproducibility for AI model development.|Research datasets are re-encoded into DICOM annotation objects to make labels directly consumable by ML pipelines. LIDC-IDRI lung nodule annotations were converted to DICOM Segmentation (SEG) objects and DICOM Structured Reports following template TID1500, providing interoperable nodular masks and qualitative/quantitative assessments. Disease-specific datasets like HCC-TACE distribute DICOM-SEG tumor/liver labels for algorithm training and evaluation. DICOM-SEG metadata tags capture algorithm provenance (Segment algorithm type, Segmentation type) essential for data curation, reproducibility, and supervised ML workflows, while software stacks (DCMTK, ITK, dcmqi, pydicom-seg) enable conversion and pipeline integration.|A containerized, DICOM-compatible pipeline integrates PACS with ML inference for near-real-time clinical shadow testing of segmentation algorithms. PACS sends studies via classic DICOM services (C-STORE/C-FIND/C-MOVE) to an on-premises GPU host that performs nnU-Net inference and converts results to DICOM Segmentation objects and Structured Reports encoding volumetry. Results are stored in a DCM4CHEE archive and visualized in OHIF web viewer, enabling clinical evaluation, human-in-the-loop corrections, and dataset curation. This demonstrates concrete PACS-to-inference-to-reporting workflows using established DICOM network protocols for AI integration into radiology operations.|Quantitative AI outputs including radiomics features and imaging biomarkers are standardized via DICOM Structured Report template TID1500 measurement reports, providing machine-readable, auditable AI results that facilitate downstream research and clinical decision support. DICOM SR encodes semantic measurements with standardized format and lexicon, acting as a big-data container for multimodal patient data integration. DICOM Parametric Map IODs preserve derived pixelwise feature maps (kurtosis, texture) as DICOM objects for reproducible feature extraction. Integration guidance from standards bodies emphasizes encoding AI outputs into SR to scale deployment across vendors and sites.|DICOMweb RESTful services (WADO-RS, QIDO-RS, STOW-RS) connect external ML applications and viewers to enterprise archives over HTTP, enabling scalable web and cloud AI pipelines. DICOMweb client APIs allow external apps to retrieve, query, and store DICOM objects without classic DIMSE protocols, facilitating integration with modern web architectures. Frameworks stream DICOM and metadata from hospital PACS into research compute clusters to power real-time ML and operational analytics. Examples include Niffler for PACS-to-research pipelines with metadata extraction and ML analytics, and EMPAIA decentralized platform for running third-party AI on DICOM data at scale.|DICOM Whole Slide Imaging (DICOM-WSI) and DICOMweb enable interoperable storage, visualization, and annotation of whole-slide images in web viewers for computational pathology. These standards support collecting standardized annotations and displaying AI inference outputs including segmentation masks, heat maps, and image-derived measurements within the DICOM ecosystem. DICOM-WSI brings pathology images into uniform DICOM workflows to facilitate ML model development and validation, while web viewers (e.g., Visilab Viewer) provide platforms for annotating training data and reviewing AI results in digital pathology contexts.|The proposed IODeep DICOM Information Object Definition stores deep neural network architectures and weights inside PACS as non-patient objects, enabling model selection via DICOM tag metadata (modality, anatomical region, disease). Inference runs client-side or on dedicated servers proximal to viewers, with predicted ROIs saved as RT Structure Sets for physician validation and retraining. IODeep supports model lifecycle management including fine-tuning with hospital data and federated learning scenarios by sharing model metadata without moving patient data. The design uses Unlimited Text VR for JSON architecture descriptions and weight URIs, with parsers for TensorFlow/PyTorch instantiation, demonstrating an emerging pattern for managing AI models within DICOM-compliant workflows.|DICOM Grayscale Softcopy Presentation State (GSPS) ensures consistent overlay and display of AI annotations across workstations, while RT Structure Set (RTSS) objects store physician-validated contours after AI suggestion, supporting clinical quality assurance and retraining workflows. RTSS is widely supported for storing ROI contours and labels for model training, enabling human-in-the-loop correction loops where radiologists review AI predictions, validate or correct them, and create ground-truth datasets. Integration of CAD results into PACS using DICOM SR and GSPS, combined with DICOM routers for communication and correction, supports maturity levels from investigational AI display through continuously updated deployed models driven by radiologist interactions.]	[B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application]	[False|False|False|False|False|False|False|False]		[B2AI_STANDARD:691]	[B2AI_STANDARD:849]	[B2AI_SUBSTRATE:11|B2AI_SUBSTRATE:19]	[B2AI_ORG:115|B2AI_ORG:114|B2AI_ORG:117]		
B2AI_STANDARD:99	B2AI_STANDARD:BiomedicalStandard	Direct Standard	Direct Applicability Statement for Secure Health Transport	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Describes how to use SMTP, S/MIME, and X.509 certificates to securely transport health information over the Internet.	True	False	https://wiki.directproject.org/w/images/e/e6/Applicability_Statement_for_Secure_Health_Transport_v1.2.pdf	https://wiki.directproject.org/w/images/e/e6/Applicability_Statement_for_Secure_Health_Transport_v1.2.pdf														[B2AI_ORG:26]		
B2AI_STANDARD:100	B2AI_STANDARD:BiomedicalStandard	DAS	Distributed Sequence Annotation System	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:12|B2AI_TOPIC:13]	Allows sequence annotations to be decentralized among multiple third-party annotators and integrated on an as-needed basis by client-side software.	True	False	https://static-content.springer.com/esm/art%3A10.1186%2F1471-2105-2-7/MediaObjects/12859_2001_8_MOESM1_ESM.pdf	https://static-content.springer.com/esm/art%3A10.1186%2F1471-2105-2-7/MediaObjects/12859_2001_8_MOESM1_ESM.pdf										doi:10.1186/1471-2105-2-7						
B2AI_STANDARD:101	B2AI_STANDARD:BiomedicalStandard	EML	Ecological Metadata Language	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:5]	A metadata standard developed for the earth, environmental and ecological sciences.	True	False	https://eml.ecoinformatics.org/											doi:10.5063/F11834T2						
B2AI_STANDARD:102	B2AI_STANDARD:BiomedicalStandard	EBDCS	Economic Botany Data Collection Standard	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:1]	This standard provides a system whereby uses of plants (in their cultural context) can be described, using standardised descriptors and terms, and attached to taxonomic data sets. It resulted from discussions at the International Working Group on Taxonomic Databases for Plant Sciences (TDWG) between 1989 and 1992. Users and potential users of the standard include economic botanists and ethnobotanists whose purpose is to record all known information about the uses of a taxon; educationalists, taxonomists, biochemists, anatomists etc. who wish to record plant use, often at a broad level; economic botany collection curators who need to describe accurately the uses and values of specimens in their collections; bibliographers who need to describe plant uses referred to in publications and to apply keywords consistently for ease of data retrieval. While this standard is still in use, it is no longer actively maintained (labelled as prior on the TDWG website).	True	False	https://www.tdwg.org/standards/economic-botany/			[B2AI_ORG:93]														
B2AI_STANDARD:103	B2AI_STANDARD:BiomedicalStandard	EC	Enzyme Commission Number	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[codesystem]	[B2AI_TOPIC:26]	A system for classification of enzymes that also serves as a basis for assigning code numbers to them.	True	False	https://iubmb.qmul.ac.uk/enzyme/rules.html			[B2AI_ORG:61]								doi:10.1126/science.150.3697.719						
B2AI_STANDARD:104	B2AI_STANDARD:BiomedicalStandard	EORTC QLQ	EORTC Quality of Life questionnaires	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[diagnosticinstrument]	[B2AI_TOPIC:9]	Instruments designed to assess (some of) the different aspects that define the QoL of (a specific group of) cancer patients.	True	True	https://qol.eortc.org/questionnaires/																	
B2AI_STANDARD:105	B2AI_STANDARD:BiomedicalStandard	EDF	European Data Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:37]	The European Data Format (EDF) is a simple and flexible format for exchange and storage of multichannel biological and physical signals, particularly in clinical neurophysiology, sleep medicine, and cardiology. Originally developed in 1990 by European biomedical engineers through the \"Methodology for the Analysis of the Sleep-Wakefulness Continuum\" project funded by the European Community, EDF became the de-facto standard for EEG and PSG recordings. The format is hardware and software independent, supporting various montages, transducers, prefiltering options, and sampling frequencies. EDF enables creation of common databases for sleep records, comparative analysis of algorithms across laboratories, and standardized evaluation of manual and automatic analysis methods. The format has been extended to EDF+, which maintains backward compatibility while adding support for interrupted recordings, annotations, stimuli, events, and analysis results such as deltaplots, QRS parameters, and sleep stages. EDF+ can store any medical recording including EMG, evoked potentials, and ECG data, with stricter specifications that enable automatic electrode localization and calibration. Both formats are freely available and widely adopted in commercial equipment and multicenter research projects.	True	False	https://www.edfplus.info/																	
B2AI_STANDARD:106	B2AI_STANDARD:BiomedicalStandard	ELS	European Laryngological Society guidelines	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[diagnosticinstrument]	[B2AI_TOPIC:9]	A multidimensional set of minimal basic measurements suitable for all common dysphonias is proposed. It includes five different approaches - perception (grade, roughness, breathiness), videostroboscopy (closure, regularity, mucosal wave and symmetry), acoustics (jitter, shimmer, Fo-range and softest intensity), aerodynamics (phonation quotient), and subjective rating by the patient.	True	False	https://doi.org/10.1007/s004050000299											doi:10.1007/s004050000299						
B2AI_STANDARD:107	B2AI_STANDARD:BiomedicalStandard	EQ-5D-3L	EuroQol Five Dimension Three Level descriptive system	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[diagnosticinstrument]	[B2AI_TOPIC:9]	The EQ-5D-3L descriptive system comprises the following five dimensions - mobility, self-care, usual activities, pain/discomfort and anxiety/depression. Each dimension has 3 levels - no problems, some problems, and extreme problems. The patient is asked to indicate his/her health state by ticking the box next to the most appropriate statement in each of the five dimensions. This decision results into a 1-digit number that expresses the level selected for that dimension. The digits for the five dimensions can be combined into a 5-digit number that describes the patients health state.	True	True	https://euroqol.org/eq-5d-instruments/eq-5d-3l-about/																	
B2AI_STANDARD:108	B2AI_STANDARD:BiomedicalStandard	FAIR Cookbook	FAIR Cookbook	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:5]	An online, open and live resource for the Life Sciences with recipes that help you to make and keep data Findable, Accessible, Interoperable and Reusable; in one word FAIR.	True	False	https://faircookbook.elixir-europe.org/											doi:10.5281/zenodo.7156792				[B2AI_ORG:28]		
B2AI_STANDARD:109	B2AI_STANDARD:BiomedicalStandard	FHIR	Fast Healthcare Interoperability Resources	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:5]	Fast Healthcare Interoperability Resources (FHIR) is a standard for exchanging healthcare information electronically, developed by HL7. FHIR defines a set of modular components called \"resources\" that represent discrete clinical and administrative concepts (patients, observations, medications, procedures, diagnostic reports, etc.) with well-defined data structures, terminology bindings, and RESTful API interactions. Each resource can be retrieved, created, updated, and searched via standard HTTP operations, enabling flexible system integration and mobile/web application development. FHIR supports multiple exchange paradigms including RESTful APIs, messaging, documents, and services, with extensibility mechanisms (profiles, extensions) to adapt to local requirements while maintaining interoperability. The standard leverages modern web technologies (JSON, XML), common terminologies (SNOMED CT, LOINC, RxNorm), and implementation guides to facilitate rapid deployment across EHR systems, clinical decision support tools, patient-facing applications, and research platforms. FHIR's structured, granular data representation and standardized APIs make it particularly suitable for AI/ML applications that require consistent access to longitudinal patient data, clinical observations, and healthcare workflows for training predictive models and deploying real-time decision support at the point of care.	True	True	https://www.hl7.org/fhir/overview.html		True	[B2AI_ORG:40]	[{\"id\": \"B2AI_APP:34\", \"category\": \"B2AI:Application\", \"name\": \"Cumulus Federated EHR Learning with Bulk FHIR and AI/NLP\", \"description\": \"The Cumulus platform operationalizes SMART/HL7 Bulk FHIR Access API for standardized data export across multiple healthcare institutions, then applies AI and natural language processing for computable phenotyping to define cohorts and outcomes from both structured and unstructured EHR data. The SMART Text2FHIR pipeline extracts insights from clinical texts and converts them into structured FHIR data elements for analysis. Only aggregate outputs leave each institution, enabling privacy-preserving federated learning across sites for public health monitoring and research while maintaining data sovereignty and interoperability through standardized FHIR exchange.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1101/2024.02.02.24301940\"]}, {\"id\": \"B2AI_APP:126\", \"category\": \"B2AI:Application\", \"name\": \"FHIR Observation Encoding of Radiology AI Findings for Interoperable Integration\", \"description\": \"A framework models radiology AI outputs (such as pulmonary nodule characteristics from automated detection algorithms) as FHIR Observation resources embedded within DiagnosticReport objects, standardizing AI-generated findings alongside radiologist reports. This FHIR-based encoding enables interoperable downstream use, long-term tracking of imaging findings, and integration with electronic health records for clinical decision-making. The structured FHIR representation facilitates reproducible data management, cohort identification, and longitudinal analysis of AI-detected imaging biomarkers across healthcare systems.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1093/jamia/ocae134\"]}, {\"id\": \"B2AI_APP:127\", \"category\": \"B2AI:Application\", \"name\": \"MyDigiTwin FHIR Harmonization for Federated Learning Cohort Studies\", \"description\": \"The MyDigiTwin federated learning research infrastructure uses a FHIR-based data harmonization framework to standardize cohort study variables across multiple sites for cardiovascular risk prediction modeling. The pipeline successfully generated approximately 150,000 FHIR bundles from Lifelines cohort variable data, demonstrating practical large-scale FHIR use for multi-center ML. This FHIR harmonization ensures interoperability and facilitates the structuring, standardization, and exchange of healthcare data across federated learning nodes, enabling distributed model training while data remains at local institutions.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.3233/shti240735\"]}, {\"id\": \"B2AI_APP:128\", \"category\": \"B2AI:Application\", \"name\": \"SMART on FHIR Integration for AI-Driven Clinical Decision Support Systems\", \"description\": \"SMART on FHIR provides a modular framework enabling third-party AI-driven clinical decision support applications to integrate seamlessly into health information systems. Implementations include automated breast cancer diagnosis via ultrasound with FHIR-standardized diagnostic metadata, ECG stream analysis frameworks for cloud-native AI processing, and machine learning-enhanced architectures that use FHIR to standardize clinical and imaging data flow for AI-based risk assessment. FHIR's standardized APIs and resource models enable deployment of predictive models across clinical domains, improving adherence to guidelines and enhancing diagnostic accessibility through interoperable, scalable CDSS implementations.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.20944/preprints202509.0818.v1\"]}, {\"id\": \"B2AI_APP:129\", \"category\": \"B2AI:Application\", \"name\": \"FHIR-Based Analytics Platforms for Clinical Predictive Model Deployment\", \"description\": \"FHIR data models and APIs support analytics frameworks and deployment of clinical predictive models including sepsis prediction, distributed phenotyping analytics platforms, and personalized medicine applications. Implementations provide patterns for converting FHIR resources to analysis-ready formats, hosting predictive models as web services, and automating management of audit files and structured medical data. Platforms like doc.ai leverage FHIR analytical capabilities for personalized medicine, while distributed phenotyping systems use FHIR to enable clinical decision-making across multiple healthcare sites with standardized data access and model integration.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.3390/healthcare11121729\"]}]	[Cumulus Federated EHR Learning with Bulk FHIR and AI/NLP|FHIR Observation Encoding of Radiology AI Findings for Interoperable Integration|MyDigiTwin FHIR Harmonization for Federated Learning Cohort Studies|SMART on FHIR Integration for AI-Driven Clinical Decision Support Systems|FHIR-Based Analytics Platforms for Clinical Predictive Model Deployment]	[B2AI_APP:34|B2AI_APP:126|B2AI_APP:127|B2AI_APP:128|B2AI_APP:129]	[['https://doi.org/10.1101/2024.02.02.24301940']|['https://doi.org/10.1093/jamia/ocae134']|['https://doi.org/10.3233/shti240735']|['https://doi.org/10.20944/preprints202509.0818.v1']|['https://doi.org/10.3390/healthcare11121729']]	[The Cumulus platform operationalizes SMART/HL7 Bulk FHIR Access API for standardized data export across multiple healthcare institutions, then applies AI and natural language processing for computable phenotyping to define cohorts and outcomes from both structured and unstructured EHR data. The SMART Text2FHIR pipeline extracts insights from clinical texts and converts them into structured FHIR data elements for analysis. Only aggregate outputs leave each institution, enabling privacy-preserving federated learning across sites for public health monitoring and research while maintaining data sovereignty and interoperability through standardized FHIR exchange.|A framework models radiology AI outputs (such as pulmonary nodule characteristics from automated detection algorithms) as FHIR Observation resources embedded within DiagnosticReport objects, standardizing AI-generated findings alongside radiologist reports. This FHIR-based encoding enables interoperable downstream use, long-term tracking of imaging findings, and integration with electronic health records for clinical decision-making. The structured FHIR representation facilitates reproducible data management, cohort identification, and longitudinal analysis of AI-detected imaging biomarkers across healthcare systems.|The MyDigiTwin federated learning research infrastructure uses a FHIR-based data harmonization framework to standardize cohort study variables across multiple sites for cardiovascular risk prediction modeling. The pipeline successfully generated approximately 150,000 FHIR bundles from Lifelines cohort variable data, demonstrating practical large-scale FHIR use for multi-center ML. This FHIR harmonization ensures interoperability and facilitates the structuring, standardization, and exchange of healthcare data across federated learning nodes, enabling distributed model training while data remains at local institutions.|SMART on FHIR provides a modular framework enabling third-party AI-driven clinical decision support applications to integrate seamlessly into health information systems. Implementations include automated breast cancer diagnosis via ultrasound with FHIR-standardized diagnostic metadata, ECG stream analysis frameworks for cloud-native AI processing, and machine learning-enhanced architectures that use FHIR to standardize clinical and imaging data flow for AI-based risk assessment. FHIR's standardized APIs and resource models enable deployment of predictive models across clinical domains, improving adherence to guidelines and enhancing diagnostic accessibility through interoperable, scalable CDSS implementations.|FHIR data models and APIs support analytics frameworks and deployment of clinical predictive models including sepsis prediction, distributed phenotyping analytics platforms, and personalized medicine applications. Implementations provide patterns for converting FHIR resources to analysis-ready formats, hosting predictive models as web services, and automating management of audit files and structured medical data. Platforms like doc.ai leverage FHIR analytical capabilities for personalized medicine, while distributed phenotyping systems use FHIR to enable clinical decision-making across multiple healthcare sites with standardized data access and model integration.]	[B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application]	[False|False|False|False|False]		[B2AI_STANDARD:720|B2AI_STANDARD:688|B2AI_STANDARD:693|B2AI_STANDARD:694|B2AI_STANDARD:697]	[B2AI_STANDARD:845|B2AI_STANDARD:846|B2AI_STANDARD:847|B2AI_STANDARD:850]		[B2AI_ORG:114|B2AI_ORG:117]		
B2AI_STANDARD:110	B2AI_STANDARD:BiomedicalStandard	FASTA	FASTA Sequence Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:13]	A text-based data format for nucleotide and amino acid sequences.	True	False	https://en.wikipedia.org/wiki/FASTA_format		True															
B2AI_STANDARD:111	B2AI_STANDARD:BiomedicalStandard	FASTQ	FASTQ Sequence and Sequence Quality Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:13]	FASTQ is a text-based file format for storing nucleotide sequences along with their corresponding per-base quality scores, making it the de facto standard for raw sequencing data from high-throughput sequencing platforms (Illumina, PacBio, Oxford Nanopore, etc.). Each sequence record consists of four lines containing a sequence identifier, the raw nucleotide sequence, a separator line, and ASCII-encoded quality scores (Phred scores) representing the confidence of each base call. The quality scores enable downstream tools to weight bases appropriately during alignment, variant calling, and assembly operations. FASTQ supports various quality encoding schemes (Phred+33, Phred+64) and accommodates reads of variable length from different sequencing technologies. The format's simultaneous representation of sequence and quality information is essential for quality control, error correction, read filtering, and AI/ML applications that require confidence metrics for training models on sequencing data quality assessment, base calling improvement, contamination detection, and metagenomic classification tasks.	True	False	https://en.wikipedia.org/wiki/FASTQ_format		True		[{\"id\": \"B2AI_APP:35\", \"category\": \"B2AI:Application\", \"name\": \"seqQscorer Automated FASTQ Quality Control Using Machine Learning\", \"description\": \"seqQscorer computes features from FASTQ files including per-base and per-read quality metrics, overrepresented sequences, k-mer counts, and mapping-derived statistics, then trains tree-based (Random Forest) and deep learning classifiers to automatically flag low- versus high-quality NGS files. The system uses FASTQ-derived quality scores as core predictive features with labels from ENCODE curation, creating generalizable models across assays (RNA-seq, ChIP-seq, DNase-seq, ATAC-seq) and species. This ML-based automated QC provides decision support for curators and researchers, saving resources by identifying poor-quality experiments early in analysis pipelines.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1186/s13059-021-02294-2\"]}, {\"id\": \"B2AI_APP:130\", \"category\": \"B2AI:Application\", \"name\": \"MiniScrub CNN-Based Nanopore Read Error Scrubbing Using FASTQ Quality Pileups\", \"description\": \"MiniScrub encodes Oxford Nanopore read-to-read overlaps and per-base qualities from FASTQ into image-like pileup channels that capture minimizer matches, base quality scores, and inter-minimizer distances. A convolutional neural network trained on these FASTQ-derived representations identifies and removes low-quality read segments de novo, without requiring reference alignment or hybrid error correction. This CNN-based scrubbing improves downstream assembly quality by reducing misassemblies and indel errors, demonstrating how FASTQ sequence and quality information can be transformed into ML-friendly image representations for quality classification tasks.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1186/s12859-019-3103-z\"]}, {\"id\": \"B2AI_APP:131\", \"category\": \"B2AI:Application\", \"name\": \"MAC-ErrorReads Supervised Classification for Filtering Erroneous NGS Reads\", \"description\": \"MAC-ErrorReads formulates erroneous-read filtration as supervised binary classification using features extracted from FASTQ sequences via TF-IDF encoding (treating sequence tokens/k-mers like text). Multiple supervised algorithms including Naive Bayes models are trained to distinguish erroneous versus accurate reads, retaining higher-quality reads and improving downstream assembly quality and genomic coverage. This ML approach addresses increasing sequencing throughput by providing automated, scalable read filtering that complements traditional k-mer-based and alignment-based error-handling methods across both short-read and long-read NGS platforms.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1186/s12859-024-05681-1\"]}, {\"id\": \"B2AI_APP:132\", \"category\": \"B2AI:Application\", \"name\": \"SeqTagger CTC-CRF Basecaller for Nanopore Barcode Demultiplexing with Quality Filtering\", \"description\": \"SeqTagger demultiplexes direct RNA nanopore datasets by training a CTC-CRF DNA basecalling model (using Bonito software) to decode barcode sequences from signal data, producing FASTQ basecalls that are then mapped to reference barcodes. Per-read median base quality from FASTQ is used to filter out potential misassignments, achieving high precision (approximately 99%) and recall (approximately 95%) for demultiplexing. This workflow demonstrates how FASTQ basecalled sequences and their quality scores enable accurate barcode assignment and quality-based filtering in direct RNA sequencing, contrasting with image-based CNN approaches by operating in sequence space.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1101/2024.10.29.620808\"]}, {\"id\": \"B2AI_APP:133\", \"category\": \"B2AI:Application\", \"name\": \"Deep Learning Variant Calling from FASTQ-Aligned Read Pileups\", \"description\": \"Deep learning variant callers including DeepVariant, Clair3, and DNAscope operate on pileup tensors built from FASTQ-aligned reads, incorporating per-base quality scores as input features alongside base identity. These models transform FASTQ-derived alignments into image-like or tensor representations that preserve sequence context and quality information, then apply CNNs or specialized architectures for genotype prediction. Benchmarks demonstrate that these deep learning approaches outperform traditional algorithms for SNP and indel calling across platforms (Illumina, PacBio HiFi, Oxford Nanopore), with quality scores explicitly used to weight evidence and calibrate predictions through methods like GATK's variant quality score recalibration (VQSR).\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1101/2024.03.15.585313\", \"https://doi.org/10.1002/0471250953.bi1110s43\"]}, {\"id\": \"B2AI_APP:134\", \"category\": \"B2AI:Application\", \"name\": \"CNN-Based eDNA Read Classification and rRNA Detection from FASTQ\", \"description\": \"Deep learning models for metagenomics and environmental DNA analysis map FASTQ reads directly to taxonomic labels using k-mer embeddings or sequence representations, enabling fast and scalable classification with comparable accuracy to conventional pipelines. CNN-based workflows process raw FASTQ at high throughput for taxonomic assignment, while tools like RiboDetector use bidirectional LSTM models to identify ribosomal RNA reads in short-read data for contaminant removal. These approaches transform FASTQ sequences (and optionally quality-derived features) into token/k-mer embeddings or sequence inputs for classification tasks, demonstrating rapid and accurate read-level identification that reduces misclassification in downstream RNA-seq and metagenomic workflows.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1101/2024.03.15.585313\"]}, {\"id\": \"B2AI_APP:135\", \"category\": \"B2AI:Application\", \"name\": \"fastp FASTQ Preprocessing for ML-Ready Feature Generation\", \"description\": \"fastp performs ultra-fast all-in-one FASTQ preprocessing including quality control, adapter trimming, quality filtering, UMI processing, and base correction in a single scan, generating standardized outputs used as inputs or targets for ML workflows. The tool produces per-base quality profiles, k-mer occurrence tables, overrepresented-sequence positions, adapter detection results, and paired-end overlap corrections that serve as features for ML quality prediction models, denoising/error-correction training data, and metadata for classifiers. UMI-processed consensus reads reduce noise for ML training, while comprehensive pre- and post-filtering metrics enable automated quality assessment and artifact detection across NGS workflows, facilitating large-scale dataset generation for ML model development.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1093/bioinformatics/bty560\"]}]	[seqQscorer Automated FASTQ Quality Control Using Machine Learning|MiniScrub CNN-Based Nanopore Read Error Scrubbing Using FASTQ Quality Pileups|MAC-ErrorReads Supervised Classification for Filtering Erroneous NGS Reads|SeqTagger CTC-CRF Basecaller for Nanopore Barcode Demultiplexing with Quality Filtering|Deep Learning Variant Calling from FASTQ-Aligned Read Pileups|CNN-Based eDNA Read Classification and rRNA Detection from FASTQ|fastp FASTQ Preprocessing for ML-Ready Feature Generation]	[B2AI_APP:35|B2AI_APP:130|B2AI_APP:131|B2AI_APP:132|B2AI_APP:133|B2AI_APP:134|B2AI_APP:135]	[['https://doi.org/10.1186/s13059-021-02294-2']|['https://doi.org/10.1186/s12859-019-3103-z']|['https://doi.org/10.1186/s12859-024-05681-1']|['https://doi.org/10.1101/2024.10.29.620808']|['https://doi.org/10.1101/2024.03.15.585313', 'https://doi.org/10.1002/0471250953.bi1110s43']|['https://doi.org/10.1101/2024.03.15.585313']|['https://doi.org/10.1093/bioinformatics/bty560']]	[seqQscorer computes features from FASTQ files including per-base and per-read quality metrics, overrepresented sequences, k-mer counts, and mapping-derived statistics, then trains tree-based (Random Forest) and deep learning classifiers to automatically flag low- versus high-quality NGS files. The system uses FASTQ-derived quality scores as core predictive features with labels from ENCODE curation, creating generalizable models across assays (RNA-seq, ChIP-seq, DNase-seq, ATAC-seq) and species. This ML-based automated QC provides decision support for curators and researchers, saving resources by identifying poor-quality experiments early in analysis pipelines.|MiniScrub encodes Oxford Nanopore read-to-read overlaps and per-base qualities from FASTQ into image-like pileup channels that capture minimizer matches, base quality scores, and inter-minimizer distances. A convolutional neural network trained on these FASTQ-derived representations identifies and removes low-quality read segments de novo, without requiring reference alignment or hybrid error correction. This CNN-based scrubbing improves downstream assembly quality by reducing misassemblies and indel errors, demonstrating how FASTQ sequence and quality information can be transformed into ML-friendly image representations for quality classification tasks.|MAC-ErrorReads formulates erroneous-read filtration as supervised binary classification using features extracted from FASTQ sequences via TF-IDF encoding (treating sequence tokens/k-mers like text). Multiple supervised algorithms including Naive Bayes models are trained to distinguish erroneous versus accurate reads, retaining higher-quality reads and improving downstream assembly quality and genomic coverage. This ML approach addresses increasing sequencing throughput by providing automated, scalable read filtering that complements traditional k-mer-based and alignment-based error-handling methods across both short-read and long-read NGS platforms.|SeqTagger demultiplexes direct RNA nanopore datasets by training a CTC-CRF DNA basecalling model (using Bonito software) to decode barcode sequences from signal data, producing FASTQ basecalls that are then mapped to reference barcodes. Per-read median base quality from FASTQ is used to filter out potential misassignments, achieving high precision (approximately 99%) and recall (approximately 95%) for demultiplexing. This workflow demonstrates how FASTQ basecalled sequences and their quality scores enable accurate barcode assignment and quality-based filtering in direct RNA sequencing, contrasting with image-based CNN approaches by operating in sequence space.|Deep learning variant callers including DeepVariant, Clair3, and DNAscope operate on pileup tensors built from FASTQ-aligned reads, incorporating per-base quality scores as input features alongside base identity. These models transform FASTQ-derived alignments into image-like or tensor representations that preserve sequence context and quality information, then apply CNNs or specialized architectures for genotype prediction. Benchmarks demonstrate that these deep learning approaches outperform traditional algorithms for SNP and indel calling across platforms (Illumina, PacBio HiFi, Oxford Nanopore), with quality scores explicitly used to weight evidence and calibrate predictions through methods like GATK's variant quality score recalibration (VQSR).|Deep learning models for metagenomics and environmental DNA analysis map FASTQ reads directly to taxonomic labels using k-mer embeddings or sequence representations, enabling fast and scalable classification with comparable accuracy to conventional pipelines. CNN-based workflows process raw FASTQ at high throughput for taxonomic assignment, while tools like RiboDetector use bidirectional LSTM models to identify ribosomal RNA reads in short-read data for contaminant removal. These approaches transform FASTQ sequences (and optionally quality-derived features) into token/k-mer embeddings or sequence inputs for classification tasks, demonstrating rapid and accurate read-level identification that reduces misclassification in downstream RNA-seq and metagenomic workflows.|fastp performs ultra-fast all-in-one FASTQ preprocessing including quality control, adapter trimming, quality filtering, UMI processing, and base correction in a single scan, generating standardized outputs used as inputs or targets for ML workflows. The tool produces per-base quality profiles, k-mer occurrence tables, overrepresented-sequence positions, adapter detection results, and paired-end overlap corrections that serve as features for ML quality prediction models, denoising/error-correction training data, and metadata for classifiers. UMI-processed consensus reads reduce noise for ML training, while comprehensive pre- and post-filtering metrics enable automated quality assessment and artifact detection across NGS workflows, facilitating large-scale dataset generation for ML model development.]	[B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application]	[False|False|False|False|False|False|False]					[B2AI_ORG:116|B2AI_ORG:114]		
B2AI_STANDARD:112	B2AI_STANDARD:BiomedicalStandard	Common Rule	Federal Policy for the Protection of Human Subjects	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[policy]	[B2AI_TOPIC:5]	The Federal Policy for the Protection of Human Subjects or the Common Rule was published in 1991 and codified in separate regulations by 15 Federal departments and agencies, as listed below. The HHS regulations, 45 CFR part 46, include four subparts - subpart A, also known as the Federal Policy or the Common Rule; subpart B, additional protections for pregnant women, human fetuses, and neonates; subpart C, additional protections for prisoners; and subpart D, additional protections for children. Each agency includes in its chapter of the Code of Federal Regulations [CFR] section numbers and language that are identical to those of the HHS codification at 45 CFR part 46, subpart A. For all participating departments and agencies the Common Rule outlines the basic provisions for IRBs, informed consent, and Assurances of Compliance. Human subject research conducted or supported by each federal department/agency is governed by the regulations of that department/agency.	True	False	https://www.hhs.gov/ohrp/regulations-and-policy/regulations/common-rule/index.html		True													[B2AI_ORG:39]		
B2AI_STANDARD:113	B2AI_STANDARD:BiomedicalStandard	CDMH	FHIR Common Data Models Harmonization	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4]	The Common Data Models Harmonization (CDMH) FHIR Implementation Guide (IG) will focus on mapping and translating observational data extracted for Patient Centered Outcome Research (PCOR) and other purposes into FHIR format. Data is extracted from the different networks each of which may use a different data model for their data representation. The project focuses on the Common Data Models (CDMs) from the following four networks:Patient Centered Outcome Research Network (PCORNet), Informatics for Integrating Biology and Bedside (i2b2) Accrual to Clinical Trials (ACT), also known as i2b2/ACT., Observational Medical Outcomes Partnership (OMOP), and Food and Drug Administrations Sentinel	True	False	https://build.fhir.org/ig/HL7/cdmh/	https://build.fhir.org/ig/HL7/cdmh/profiles.html		[B2AI_ORG:40]									[B2AI_STANDARD:109]	[B2AI_STANDARD:845|B2AI_STANDARD:846|B2AI_STANDARD:847|B2AI_STANDARD:850]				
B2AI_STANDARD:114	B2AI_STANDARD:BiomedicalStandard	Genomics Operations	FHIR Genomics Operations	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:13|B2AI_TOPIC:35]	A standardized suite of genomics operations in FHIR designed to support a wide range of clinical scenarios, such as variant discovery; clinical trial matching; hereditary condition and pharmacogenomic screening; and variant reanalysis.	True	False	http://build.fhir.org/ig/HL7/genomics-reporting/operations.html	https://github.com/FHIR/genomics-operations		[B2AI_ORG:40]								doi:10.1093/jamia/ocac246	[B2AI_STANDARD:109]	[B2AI_STANDARD:845|B2AI_STANDARD:846|B2AI_STANDARD:847|B2AI_STANDARD:850]				
B2AI_STANDARD:115	B2AI_STANDARD:BiomedicalStandard	Provenance	FHIR Provenance Resource	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:5]	\"The FHIR Provenance resource provides comprehensive tracking of information about activities that create, revise, delete, or sign healthcare resources, describing the entities and agents involved in these processes. Based on the W3C Provenance specification, it establishes a critical foundation for assessing authenticity, enabling trust, and allowing reproducibility in healthcare data systems. The resource supports three key components from W3C Provenance: Entity (physical, digital, or conceptual things with fixed aspects), Agent (persons, devices, systems, organizations, or care teams bearing responsibility for activities), and Activity (time-bound processes that act upon entities). Provenance resources serve as record-keeping assertions that capture context information for quality, reliability, and trustworthiness assessments. The standard supports multiple use cases including tracking data lineage in clinical workflows, enabling audit trails for regulatory compliance, supporting clinical decision-making through data origin verification, and facilitating data integration from multiple healthcare systems. It includes digital signature capabilities for integrity verification and non-repudiation, supports versioning and unique identification requirements, and provides mechanisms for recording import and transformation activities in healthcare interoperability scenarios.\"	True	False	https://www.hl7.org/fhir/provenance.html	https://www.hl7.org/fhir/provenance-definitions.html		[B2AI_ORG:40]									[B2AI_STANDARD:109]	[B2AI_STANDARD:845|B2AI_STANDARD:846|B2AI_STANDARD:847|B2AI_STANDARD:850]				
B2AI_STANDARD:116	B2AI_STANDARD:BiomedicalStandard	CodeSystem	FHIR Resource CodeSystem	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	The CodeSystem resource is used to declare the existence of and describe a code system or code system supplement and its key properties, and optionally define a part or all of its content.	True	False	http://hl7.org/fhir/codesystem.html		True	[B2AI_ORG:40]										[B2AI_STANDARD:845|B2AI_STANDARD:846|B2AI_STANDARD:847|B2AI_STANDARD:850]				
B2AI_STANDARD:117	B2AI_STANDARD:BiomedicalStandard	ConceptMap	FHIR Resource ConceptMap	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4|B2AI_TOPIC:32]	A statement of relationships from one set of concepts to one or more other concepts - either concepts in code systems, or data element/data element concepts, or classes in class models.	True	False	http://hl7.org/fhir/conceptmap.html			[B2AI_ORG:40]										[B2AI_STANDARD:845|B2AI_STANDARD:846|B2AI_STANDARD:847|B2AI_STANDARD:850]				
B2AI_STANDARD:118	B2AI_STANDARD:BiomedicalStandard	NamingSystem	FHIR Resource NamingSystem	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A curated namespace that issues unique symbols within that namespace for the identification of concepts, people, devices, etc. Represents a System used within the Identifier and Coding data types.	True	False	http://hl7.org/fhir/namingsystem.html			[B2AI_ORG:40]										[B2AI_STANDARD:845|B2AI_STANDARD:846|B2AI_STANDARD:847|B2AI_STANDARD:850]				
B2AI_STANDARD:119	B2AI_STANDARD:BiomedicalStandard	TerminologyCapabilities	FHIR Resource TerminologyCapabilities	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A TerminologyCapabilities resource documents a set of capabilities (behaviors) of a FHIR Terminology Server that may be used as a statement of actual server functionality or a statement of required or desired server implementation.	True	False	http://hl7.org/fhir/terminologycapabilities.html			[B2AI_ORG:40]										[B2AI_STANDARD:845|B2AI_STANDARD:846|B2AI_STANDARD:847|B2AI_STANDARD:850]				
B2AI_STANDARD:120	B2AI_STANDARD:BiomedicalStandard	ValueSet	FHIR Resource ValueSet	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A ValueSet resource instance specifies a set of codes drawn from one or more code systems, intended for use in a particular context. Value sets link between CodeSystem definitions and their use in coded elements.	True	False	http://hl7.org/fhir/valueset.html			[B2AI_ORG:40]										[B2AI_STANDARD:845|B2AI_STANDARD:846|B2AI_STANDARD:847|B2AI_STANDARD:850]				
B2AI_STANDARD:121	B2AI_STANDARD:BiomedicalStandard	FCS	Flow Cytometry Standard format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:2]	The flow cytometry data file standard provides the specifications needed to completely describe flow cytometry data sets within the confines of the file containing the experimental data. In 1984, the first Flow Cytometry Standard format for data files was adopted as FCS 1.0. This standard was modified in 1990 as FCS 2.0 and again in 1997 as FCS 3.0. We report here on the next generation Flow Cytometry Standard data file format. FCS 3.1 is a minor revision based on suggested improvements from the community. The unchanged goal of the Standard is to provide a uniform file format that allows files created by one type of acquisition hardware and software to be analyzed by any other type.	True	False	https://en.wikipedia.org/wiki/Flow_Cytometry_Standard											doi:10.1002/cyto.a.20825						
B2AI_STANDARD:122	B2AI_STANDARD:BiomedicalStandard	FORCE11 DC	FORCE11 Data Citation Principles	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:5]	The Data Citation Principles cover the purpose, function and attributes of citations. These principles recognize the dual necessity of creating citation practices that are both human understandable and machine-actionable.	True	False	https://force11.org/info/joint-declaration-of-data-citation-principles-final/			[B2AI_ORG:32]														
B2AI_STANDARD:123	B2AI_STANDARD:BiomedicalStandard	FACT-G	Functional Assessment of Cancer Therapy - General	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[diagnosticinstrument]	[B2AI_TOPIC:9]	The Functional Assessment of Cancer Therapy - General (FACT-G) is a 27-item questionnaire designed to measure four domains of HRQOL in cancer patients - Physical, social, emotional, and functional well-being. Original development and validation involved 854 patients with cancer and 15 oncology specialists. An initial pool of 370 overlapping items for breast, lung, and colorectal cancer was generated by open-ended interviews with patients experienced with the symptoms of cancer and oncology professionals. Using preselected criteria, items were reduced to a 38-item general version. Factor and scaling analyses of these 38 items on 545 patients with mixed cancer diagnoses resulted in the 27-item FACT-General (FACT-G). Coefficients of reliability and validity were uniformly high. The scale's ability to discriminate patients on the basis of stage of disease, performance status rating (PSR), and hospitalization status supports its sensitivity. It has also demonstrated sensitivity to change over time.	True	False	https://www.facit.org/measures/FACT-G			[B2AI_ORG:30]														
B2AI_STANDARD:124	B2AI_STANDARD:BiomedicalStandard	FACIT-Dyspnea	Functional Assessment of Chronic Illness Therapy - Dyspnea	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[diagnosticinstrument]	[B2AI_TOPIC:9]	Functional Assessment of Chronic Illness Therapy - Dyspnea-10 item	True	False	https://www.facit.org/measures/facit-dyspnea			[B2AI_ORG:30]								doi:10.1016/j.jval.2010.06.001						
B2AI_STANDARD:125	B2AI_STANDARD:BiomedicalStandard	FANLTC	Functional Assessment of Non-Life Threatening Conditions	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[diagnosticinstrument]	[B2AI_TOPIC:9]	The Functional Assessment of Non-Life Threatening Conditions (FANLTC) is a 26-item version of the FACT-G designed to be administered to patients with non-life threatening conditions. The item from the FACT-G making reference to anxiety about death has been removed, and the instrument measures four domains of HRQOL - Physical, social/family, emotional and functional well-being.	True	False	https://www.facit.org/measures/FANLTC			[B2AI_ORG:30]														
B2AI_STANDARD:126	B2AI_STANDARD:BiomedicalStandard	Regier2018	Functional equivalence of genome sequencing analysis pipelines enables harmonized variant calling across human genetics projects	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:13|B2AI_TOPIC:35]	Regier2018 defines whole genome sequencing (WGS) data processing standards that enable functional equivalence (FE) across different analysis pipelines used by various research groups. These standards establish best practices for alignment, variant calling, and quality control steps in genomic analysis workflows, allowing different computational implementations to produce concordant results while still permitting innovation in pipeline optimization. The framework addresses key challenges in large-scale genomic studies by defining expected outputs, quality metrics, and validation procedures that ensure harmonized variant calling across projects. This standardization is particularly important for collaborative efforts like the Centers for Common Disease Genomics (CCDG), where multiple centers must produce comparable data that can be combined for downstream analysis. The standards cover reference genome usage, read alignment parameters, variant detection algorithms, and filtering criteria, providing a foundation for reproducible genomics research.	True	False	https://github.com/CCDG/Pipeline-Standardization	https://github.com/CCDG/Pipeline-Standardization										doi:10.1038/s41467-018-06159-4						
B2AI_STANDARD:127	B2AI_STANDARD:BiomedicalStandard	FuGE-ML	Functional Genomics Experiment Markup Language	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:13]	Functional genomics experiments present many challenges in data archiving, sharing and querying. As the size and complexity of data generated from such experiments grows, so does the requirement for standard data formats. To address these needs, the Functional Genomics Experiment [Object Model / Markup-Language] (FuGE-OM, FuGE-ML) has been created to facilitate the development of data standards.FuGE is a model of the shared components in different functional genomics domains. FuGE facilitates the development of data standards in functional genomics in two ways. 1. FuGE provides a model of common components in functional genomics investigations, such as materials, data, protocols, equipment and software. These models can be extended to develop modular data formats with consistent structure. 2. FuGE provides a framework for capturing complete laboratory workflows, enabling the integration of pre-existing data formats. In this context, FuGE allows the capture of additional metadata that gives formats a context within the complete workflow. FuGE is available as a UML model and an XML Schema	True	False	https://fuge.sourceforge.net/index.php																	
B2AI_STANDARD:128	B2AI_STANDARD:BiomedicalStandard	FuGEFlow	Functional Genomics Experiment model for flow cytometry	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datamodel]	[B2AI_TOPIC:13]	FuGEFlow represents a collaborative effort to develop of flow cytometry experimental workflow description based on the FuGE model. The Functional Genomics Experiment data model (FuGE) describes common aspects of comprehensive, high-throughput experiments. FuGE is an extendable model that provides a basis for creation of new technology-specific data formats, such as FuGEFlow for flow cytometry.	True	False	https://doi.org/10.1186/1471-2105-10-184				[{\"id\": \"B2AI_APP:36\", \"category\": \"B2AI:Application\", \"name\": \"Functional Genomics Workflow Modeling and Reproducibility\", \"description\": \"FuGEFlow is used in AI applications for standardizing functional genomics experimental workflows, enabling automated workflow optimization, experimental design prediction, and quality control through machine learning. AI systems leverage FuGEFlow's formal representation of experimental protocols, data transformations, and sample tracking to train models that predict optimal experimental conditions, identify protocol deviations that affect data quality, and automate experimental planning. The standard supports AI-driven meta-analysis across functional genomics studies by ensuring computational reproducibility and enabling models to learn from both successful and failed experiments. Applications include automated protocol generation, prediction of experimental outcomes based on design parameters, and workflow recommendation systems for genomics laboratories.\", \"used_in_bridge2ai\": false}]	[Functional Genomics Workflow Modeling and Reproducibility]	[B2AI_APP:36]		[FuGEFlow is used in AI applications for standardizing functional genomics experimental workflows, enabling automated workflow optimization, experimental design prediction, and quality control through machine learning. AI systems leverage FuGEFlow's formal representation of experimental protocols, data transformations, and sample tracking to train models that predict optimal experimental conditions, identify protocol deviations that affect data quality, and automate experimental planning. The standard supports AI-driven meta-analysis across functional genomics studies by ensuring computational reproducibility and enabling models to learn from both successful and failed experiments. Applications include automated protocol generation, prediction of experimental outcomes based on design parameters, and workflow recommendation systems for genomics laboratories.]	[B2AI:Application]	[False]	doi:10.1186/1471-2105-10-184						
B2AI_STANDARD:129	B2AI_STANDARD:BiomedicalStandard	FAF	Functionality Assessment Flowchart	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[diagnosticinstrument]	[B2AI_TOPIC:9]	The Functionality Assessment Flowchart (FAF) is a structured clinical assessment tool designed to evaluate cancer patients' performance status through a standardized flowchart-based methodology. FAF provides a systematic approach to measuring functional capacity and activities of daily living in oncology patients, yielding performance status scores with high inter-observer agreement. The flowchart format guides clinicians through a series of binary decision points based on patient capabilities (ability to work, walk, self-care, etc.), resulting in consistent and reproducible assessments. FAF addresses limitations of traditional performance status scales by reducing subjectivity and improving reliability across different observers and clinical settings. The standardized scoring enables comparison of patient functional status over time, supports clinical decision-making for treatment planning, facilitates stratification in clinical trials, and provides structured data suitable for integration into electronic health records and outcomes research databases.	True	False	https://doi.org/10.1186/s12885-015-1526-0											doi:10.1186/s12885-015-1526-0						
B2AI_STANDARD:130	B2AI_STANDARD:BiomedicalStandard	GA4GH	GA4GH metadata model	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[deprecated]	[B2AI_TOPIC:13]	The Global Alliance for Genomics and Health (GA4GH) metadata model provides a standardized framework for representing and sharing genomic and clinical data across institutions and international borders, developed by a coalition of public and private stakeholders including research institutions, healthcare organizations, and technology companies. The model defines schemas for core genomic data types including variants, reads, references, annotations, and associated clinical phenotypes, using protocol buffers (protobuf) and JSON for interoperable data exchange. GA4GH schemas cover reference genomes and sequences, variant calls and annotations (compatible with VCF), sequencing reads and alignments (supporting BAM/CRAM), RNA quantification, continuous-valued genomic signals, genomic features and annotations, metadata about biosamples and individuals, phenotypic information using ontologies, and provenance tracking. The framework enables federated data access through APIs including hts-get for streaming genomic data, Beacon for discovery queries across datasets, Data Repository Service (DRS) for accessing data objects, Task Execution Service (TES) for running analysis workflows, and Workflow Execution Service (WES) for managing computational pipelines. GA4GH standards facilitate large-scale collaborative research initiatives, support FAIR principles (Findable, Accessible, Interoperable, Reusable), enable secure data sharing with privacy-preserving technologies, and provide the foundation for international genomics research networks, clinical genomics implementations, and precision medicine programs.	True	False	https://github.com/ga4gh-metadata/metadata-schemas?tab=readme-ov-file	https://github.com/ga4gh-metadata/metadata-schemas		[B2AI_ORG:34]														
B2AI_STANDARD:131	B2AI_STANDARD:BiomedicalStandard	Gating-ML	Gating-ML specification	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:5]	The Gating-ML specification represents a proposal on how to form unambiguous XML-based gate definitions that may be used independently as well as included as one of the components of ACS. Such a description of gates can facilitate the interchange and validation of data between different software packages with the potential of significant increase of hardware and software interoperability. The specification supports rectangular gates in n dimensions (i.e., from one-dimensional range gates up to n-dimensional hyper-rectangular regions), polygon gates in two (and more) dimensions, ellipsoid gates in n dimensions, decision tree structures, and Boolean collections of any of the types of gates. Gates can be uniquely identified and may be ordered into a hierarchical structure to describe a gating strategy. Gates may be applied on parameters as in list mode data files (e.g., FCS files) or on transformed parameters as described by any explicit parameter transformation. Therefore, since version 1.5, parameter transformation and compensation description are included as part of the Gating-ML specification.	True	False	http://flowcyt.sourceforge.net/gating/																	
B2AI_STANDARD:132	B2AI_STANDARD:BiomedicalStandard	GB	GenBank Sequence Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12|B2AI_TOPIC:26|B2AI_TOPIC:33]	The GenBank format is a comprehensive text-based annotation format for nucleotide and protein sequences maintained by NCBI as part of the GenBank sequence database, one of the three components of the International Nucleotide Sequence Database Collaboration (INSDC) alongside EMBL and DDBJ. Each GenBank record contains a sequence along with extensive structured metadata organized into standardized fields including locus name and length, definition line, accession and version numbers, keywords, source organism with taxonomic classification, literature references with PubMed links, and feature table annotations. The feature table uses a hierarchical key-value structure to annotate biological features such as genes, coding sequences (CDS) with translation, regulatory elements, binding sites, variations, and experimental evidence, with controlled qualifiers and ontology terms. GenBank format supports protein translations, codon usage information, cross-references to other databases (UniProt, PDB, Gene, etc.), and provenance tracking of sequence submissions and updates. The rich annotation structure enables integration of genomic context, functional annotations, phylogenetic information, and experimental validation data in a single standardized record, making GenBank essential for comparative genomics, gene annotation pipelines, sequence analysis tools, and machine learning applications that leverage both sequence content and biological metadata for training predictive models of gene structure, function, and regulation.	True	False	https://en.wikipedia.org/wiki/GenBank																	
B2AI_STANDARD:133	B2AI_STANDARD:BiomedicalStandard	GenePred	Gene Prediction File Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12]	The genePred (Gene Prediction) format is a flexible tab-delimited table format developed by UCSC Genome Browser for representing gene structure predictions and annotations, widely used for storing transcript models, gene predictions from ab initio tools, and curated gene annotations. The basic genePred format contains 10 required fields - gene name, chromosome, strand, transcription start/end positions, coding sequence (CDS) start/end positions, exon count, comma-separated exon start positions, and comma-separated exon end positions. Extended versions (genePredExt) add fields for unique identifiers, CDS start/end status (complete/incomplete), exon frame information for each coding exon, and gene symbols or descriptions. The format efficiently represents complex transcript structures including alternative splicing isoforms, UTRs (untranslated regions), introns, and partial gene models. GenePred files support various gene prediction algorithms (GeneMark, Augustus, SNAP), RefSeq annotations, Ensembl gene builds, and GENCODE comprehensive gene sets. UCSC provides utilities including genePredToBed for BED format conversion, genePredToGtf for GTF conversion, and genePredToPsl for PSL format output. The format's compact structure and explicit exon coordinates make it ideal for genome browsers, annotation pipelines, and comparative genomics analyses requiring efficient storage and rapid retrieval of gene structure information across whole genomes.	True	False	https://genome.ucsc.edu/FAQ/FAQformat.html#format9			[B2AI_ORG:119]														
B2AI_STANDARD:134	B2AI_STANDARD:BiomedicalStandard	GPAD	Gene Product Annotation Data format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12|B2AI_TOPIC:26|B2AI_TOPIC:33]	The Gene Ontology project provides annotations describing attributes of biological entities such as genes and gene products. The Gene Ontology has historically provided annotations via Gene Association Format (GAF), including GAF-1 and GAF-2. Ontologies are distributed separately, using an OWL serialization or OBO format. The use of GAF has some drawbacks. Combined representation of gene/gene product data and annotations leads to redundancy/repetition No way to represent gene/gene product metadata for unannotated genes Requirement to maintain backward compatibility makes it harder to introduce enhancements such as use of an ontology for evidence types GAF formats will continue to be supported, but the need for a way to represent genes/gene products separately from annotations, as well as the need to use the evidence ontology has lead to the creation of the GPAD (Gene Product Annotation Data) and GPI (Gene Product Information) formats, defined here. Whilst GPAD and GPI have been defined for use within the Gene Ontology Consortium for GO annotation, this specification is designed to be reusable for analagous ontology-based annotation - for example, gene phenotype annotation.	True	False	http://geneontology.org/docs/gene-product-association-data-gpad-format/			[B2AI_ORG:36]														
B2AI_STANDARD:135	B2AI_STANDARD:BiomedicalStandard	GTF	Gene Transfer Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12]	GTF (Gene Transfer Format, GTF2.2) is an extension to, and backward compatible with, GFF2. The first eight GTF fields are the same as GFF. The feature field is the same as GFF, with the exception that it also includes the following optional values, 5UTR, 3UTR, inter, inter_CNS, and intron_CNS.	True	False	http://genome.ucsc.edu/FAQ/FAQformat.html#format4			[B2AI_ORG:119]														
B2AI_STANDARD:136	B2AI_STANDARD:BiomedicalStandard	GMT	GenePattern GeneSet Table Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12]	The GMT (Gene Matrix Transposed) format is a tab-delimited text file format developed by the Broad Institute for representing gene sets used in gene set enrichment analysis (GSEA) and related computational biology applications. Each line in a GMT file represents a single gene set with a simple structure containing the gene set name, description (or URL), and a tab-separated list of gene identifiers (typically gene symbols, Entrez IDs, or other standard identifiers). The format's simplicity and human readability make it ideal for storing and sharing curated gene set collections such as pathway databases (KEGG, Reactome), Gene Ontology term associations, disease gene signatures, transcription factor targets, and experimentally derived co-expression modules. GMT files are widely used by enrichment analysis tools (GSEA, Enrichr, WebGestalt), enable integration of custom gene sets into analysis workflows, support reproducible research through standardized gene set definitions, and facilitate machine learning applications that use gene set membership as features for classification, dimension reduction, and biological interpretation of high-throughput genomic data.	True	False	https://www.genepattern.org/file-formats-guide#GMT																	
B2AI_STANDARD:137	B2AI_STANDARD:BiomedicalStandard	GFF	General Feature Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:13]	The General Feature Format (GFF) is a tab-delimited text file format developed by the Sequence Ontology Consortium for representing genomic features and annotations. GFF3, the current version, provides a flexible structure for describing genes, transcripts, exons, regulatory elements, and other sequence features with their genomic coordinates, strand orientation, phase information, and hierarchical relationships. Each line represents a single feature with nine standardized fields including sequence ID, source, feature type (from Sequence Ontology terms), start/end positions, score, strand, and attributes. The format supports parent-child relationships through ID and Parent attributes, enabling representation of complex gene structures with multiple transcripts and alternative splicing. GFF3 includes directives for metadata and embedded FASTA sequences. The format is widely used by genome browsers (UCSC, Ensembl, JBrowse), annotation pipelines, and analysis tools for visualizing and processing genomic data. Its human-readable structure and extensive tool support make it a standard choice for genome annotation exchange and long-term data archiving.	True	False	https://github.com/The-Sequence-Ontology/Specifications	https://github.com/The-Sequence-Ontology/Specifications																
B2AI_STANDARD:138	B2AI_STANDARD:BiomedicalStandard	GAF	Genome Annotation File	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:13]	The Gene Ontology Annotation File (GAF) format is a standardized, tab-delimited text file specification developed by the Gene Ontology Consortium for representing gene product annotations using GO terms. GAF 2.1 defines 17 fields per line, capturing essential information such as database identifiers, gene symbols, GO terms, evidence codes, references, taxonomic information, and annotation extensions. Each line encodes a single association between a gene product and a GO term, supported by evidence and references, enabling precise tracking of functional, process, and cellular component annotations. The format supports both required and optional fields, allowing for rich annotation detail and interoperability across databases. GAF is widely used for large-scale functional genomics, comparative biology, and integrative bioinformatics, providing a foundation for computational analysis, data sharing, and automated reasoning about gene function across species.	True	False	http://geneontology.org/docs/go-annotation-file-gaf-format-2.1/			[B2AI_ORG:36]														
B2AI_STANDARD:139	B2AI_STANDARD:BiomedicalStandard	Beacon	Genome Beacons	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:13]	Beacon v2 is a protocol/specification established by the Global Alliance for Genomics and Health initiative (GA4GH) that defines an open standard for federated discovery of genomic (and phenoclinic) data in biomedical research and clinical applications.	True	False	http://docs.genomebeacons.org/	https://github.com/ga4gh-beacon/beacon-v2		[B2AI_ORG:34]								doi:10.1002/humu.24369						
B2AI_STANDARD:140	B2AI_STANDARD:BiomedicalStandard	GVF	Genome Variation Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:13|B2AI_TOPIC:35]	The Genome Variation Format (GVF) is a very simple file format for describing sequence alteration features at nucleotide resolution relative to a reference genome.	True	False	https://github.com/The-Sequence-Ontology/Specifications/blob/master/gvf.md	https://github.com/The-Sequence-Ontology/Specifications/blob/master/gvf.md																
B2AI_STANDARD:141	B2AI_STANDARD:BiomedicalStandard	GCDML	Genomic Contextual Data Markup Language	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:13]	The Genomic Contextual Data Markup Language (GCDML) is an XML Schema developed by the Genomic Standards Consortium (GSC) to implement the Minimum Information about a Genome Sequence (MIGS), Minimum Information about a Metagenome Sequence (MIMS), and Minimum Information about a MARKer gene Sequence (MIMARKS) specifications. This sample-centric, strongly-typed schema provides a comprehensive set of descriptors for documenting the complete provenance of biological samples, from initial collection and environmental context through sequencing and subsequent analysis. GCDML enables standardized reporting of genomic and metagenomic data by defining required metadata fields that capture sampling conditions, laboratory processing methods, sequencing parameters, and analytical workflows. The schema facilitates data exchange between research groups, repositories, and databases, ensuring that essential contextual information needed to interpret sequence data is preserved and communicated effectively.	True	False	https://doi.org/10.1089/omi.2008.0A10			[B2AI_ORG:38]								doi:10.1089/omi.2008.0A10						
B2AI_STANDARD:142	B2AI_STANDARD:BiomedicalStandard	GGBN	Global Genome Biodiversity Network Data Standard	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:13]	In order to facilitate exchange of information on genomic samples and their derived data, the Global Genome Biodiversity Network (GGBN) Data Standard is intended to provide a platform based on a documented agreement to promote the efficient sharing and usage of genomic sample material and associated specimen information in a consistent way.	True	False	https://www.tdwg.org/standards/ggbn/			[B2AI_ORG:93]								doi:10.1093/database/baw125						
B2AI_STANDARD:143	B2AI_STANDARD:BiomedicalStandard	GlycoCT	GlycoCT encoding scheme	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:20]	\"GlycoCT (Glyco Connection Table) format is an advanced, comprehensive encoding scheme specifically designed to describe complex carbohydrate sequences with unprecedented precision and consistency in glycobioinformatics. Developed as version 4 (KIROI) by Stephan Herget and Rene Ranzinger in 2007, this format employs a controlled vocabulary based on IUPAC nomenclature rules to systematically name monosaccharides and utilizes a connection table approach rather than linear encoding, enabling accurate representation of branched and complex glycan structures. GlycoCT incorporates a sophisticated block concept to efficiently handle frequently occurring structural features such as repeating units, which are common in biological carbohydrates. The format exists in two complementary variants: a condensed form optimized for database applications with strict sorting rules ensuring uniqueness for use as primary keys, and a more verbose XML syntax that provides enhanced readability and machine processing capabilities. Released under Creative Commons Attribution 4.0 International License, GlycoCT represents a significant advancement toward achieving a unified and broadly accepted sequence format in glycobioinformatics, encompassing the capabilities of the heterogeneous landscape of existing digital encoding schemata while providing the foundation for developing ontological relationships between glycan structural terms and supporting automated glycan structure analysis.\"	True	False	https://github.com/glycoinfo/GlycoCT	https://github.com/glycoinfo/GlycoCT										doi:10.1016/j.carres.2008.03.011						
B2AI_STANDARD:144	B2AI_STANDARD:BiomedicalStandard	GTrack	GTrack genomic data format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:13]	GTrack is a tabular format that was developed as part of the Genomic HyperBrowser system to provide a uniform representation of most types of genomic datasets. GTrack is able to replace common formats such as WIG, GFF, BED, FASTA, in addition to represent chromatin capture datasets, such as Hi-C and ChIA-PET.	True	False	https://github.com/gtrack/gtrack	https://github.com/gtrack/gtrack										doi:10.1186/1471-2105-12-494						
B2AI_STANDARD:145	B2AI_STANDARD:BiomedicalStandard	GUID-AS	GUID and Life Sciences Identifiers Applicability Statements	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:20]	GUIDs are Globally Unique Identifiers which should be referentially consistent and resolvable in order to support tests of uniqueness and the acquisition of associated metadata. Further, permanent and robust resolution services need to be available. The TDWG Globally Unique Identifiers Task Group (TDWG GUID), after meeting twice in 2006, recommended the use of the Life Sciences Identifiers (LSID) to uniquely identify shared data objects in the biodiversity domain.	True	False	http://www.tdwg.org/standards/150	https://github.com/tdwg/guid-as		[B2AI_ORG:93]														
B2AI_STANDARD:146	B2AI_STANDARD:BiomedicalStandard	GIATE	Guidelines for Information About Therapy Experiments	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[minimuminformationschema]	[B2AI_TOPIC:4]	The Guidelines for Information About Therapy Experiments (GIATE) is a minimum information checklist developed to establish a consistent framework for transparently reporting the purpose, methods, and results of therapeutic experiments, particularly in preclinical and translational research contexts. GIATE provides structured reporting requirements covering experimental design, intervention details (therapeutic agents, dosing, administration routes, timing), subject characteristics, outcome measures, statistical methods, and results presentation. The guidelines aim to improve reproducibility, enable meta-analyses, facilitate comparison across studies, and enhance the quality of preclinical therapeutic research by ensuring complete documentation of experimental parameters that affect interpretation and translatability. GIATE addresses the need for standardized reporting of therapy experiments to support evidence synthesis, reduce publication bias, and improve the rigor and transparency of preclinical studies that inform clinical trial design and therapeutic development pipelines.	True	False	https://doi.org/10.1186/1756-0500-5-10											doi:10.1186/1756-0500-5-10						
B2AI_STANDARD:147	B2AI_STANDARD:BiomedicalStandard	HIPAA	Health Insurance Portability and Accountability Act of 1996	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[policy]	[B2AI_TOPIC:9]	The Health Insurance Portability and Accountability Act of 1996 (HIPAA) is a federal law that required the creation of national standards to protect sensitive patient health information from being disclosed without the patient's consent or knowledge. The US Department of Health and Human Services (HHS) issued the HIPAA Privacy Rule to implement the requirements of HIPAA. The HIPAA Security Rule protects a subset of information covered by the Privacy Rule.	True	False	https://www.hhs.gov/hipaa/index.html	https://aspe.hhs.gov/reports/health-insurance-portability-accountability-act-1996	True	[B2AI_ORG:39]														
B2AI_STANDARD:148	B2AI_STANDARD:OntologyOrVocabulary	HCPCS	Healthcare Common Procedure Coding System	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[codesystem|standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:9]	The Healthcare Common Procedure Coding System (HCPCS) is a standardized code set maintained by the Centers for Medicare & Medicaid Services (CMS) for reporting medical procedures, products, supplies, and services to Medicare, Medicaid, and private health insurers for claims processing and reimbursement. HCPCS consists of two levels - Level I comprises CPT (Current Procedural Terminology) codes maintained by the American Medical Association for physician services and procedures, while Level II contains alphanumeric codes (A through V series) for items and services not covered by CPT including durable medical equipment (DME), prosthetics, orthotics, supplies, ambulance services, drugs administered via methods other than oral, and temporary procedures. Each HCPCS code is accompanied by modifiers that provide additional information about the service or item (laterality, level of service, unusual circumstances). The code set is updated quarterly to reflect new technologies, procedures, and supplies, and includes detailed descriptions, Medicare coverage policies, and pricing information. HCPCS enables standardized billing, facilitates claims adjudication, supports healthcare cost analysis and utilization research, and provides the foundation for healthcare payment systems and revenue cycle management across payers.	True	False	https://www.cms.gov/Medicare/Coding/HCPCSReleaseCodeSets		True	[B2AI_ORG:17]														
B2AI_STANDARD:149	B2AI_STANDARD:BiomedicalStandard	HISPID3	Herbarium Information Standards and Protocols for Interchange of Data	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:1]	The 'Herbarium Information Standards and Protocols for Interchange of Data' (HISPID) is a standard format for the interchange of electronic herbarium specimen information. HISPID has been developed by a committee of representatives from all major Australian herbaria. This interchange standard was first published in 1989, with a revised version published in 1993./nHISPID3 (version 3) is an accession-based interchange standard. Although many fields refer to attributes of the taxon they should be construed as applying to the specimen represented by the record, not to the taxon per se. The interchange of taxonomic, nomenclatural, bibliographic, typification, rare and endangered plant conservation, and other related information is not dealt with in this standard, unless it specifically refers to a particular accession (record). While this standard is still in use, it is no longer actively maintained (labelled as prior on the TDWG website).	True	False	https://www.tdwg.org/standards/hispid3/			[B2AI_ORG:93]														
B2AI_STANDARD:150	B2AI_STANDARD:BiomedicalStandard	HML	Histoimmunogenetics Markup Language	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:5]	Histoimmunogenetics Markup Language (HML) is intended as a potentially general-purpose XML format for exchanging genetic typing data. This format supports NGS based genotyping methods, raw sequence reads, registered methodologies, reference data, complete reporting of allele and genotype ambiguity and MIRING compliant reporting.	True	False	https://bioinformatics.bethematchclinical.org/hla-resources/hml/	https://github.com/nmdp-bioinformatics/hml																
B2AI_STANDARD:151	B2AI_STANDARD:BiomedicalStandard	Arden	HL7 Arden Syntax	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	The Arden Syntax for Medical Logic Systems Version 2.10 is the latest version of a formalism for clinical knowledge representation that can be used by clinicians, knowledge engineers, administrators and others to implement clinical decision support (CDS) solutions to help improve the quality and safety of care. Arden Syntax can be used to create a knowledge base for CDS systems that, when coupled with patient data, can generate patient-specific CDS interventions for improving patient care. The key change in Version 2.10 over Version 2.9 is inclusion of a normative XML representation for Arden Syntax. This was done because the use of XML facilitates the development of tools such as syntax checkers and editors that can help increase the correctness of executable knowledge modules, and this in turn will foster the augmentation of the development and production environments for Arden, thereby increasing its utility.	True	True	http://www.hl7.org/implement/standards/product_brief.cfm?product_id=372	https://www.hl7.org/login/index.cfm?next=/implement/standards/product_brief.cfm?product_id=2		[B2AI_ORG:40]								doi:10.1016/j.jbi.2012.02.001						
B2AI_STANDARD:152	B2AI_STANDARD:BiomedicalStandard	HL7 CDA Data Provenance	HL7 Clinical Document Architecture Implementation Guide - Data Provenance	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:9]	The HL7 Clinical Document Architecture (CDA) Data Provenance Implementation Guide provides standardized guidance and templates for capturing and exchanging comprehensive provenance information about clinical and care-related information within CDA documents. The guide addresses the fundamental provenance questions - who created the information (author, organization, device), when it was created (timestamps, versioning), where it was created (facility, location), how it was created (method, procedure, source system), and why it was created (purpose, context). The implementation guide defines structured elements and coded values for documenting data transformations, aggregations, derivations, and chains of custody that are critical for clinical decision-making, regulatory compliance, and legal accountability. Provenance metadata supports trust and transparency in health information exchange, enables auditability and traceability of clinical data across systems and care settings, facilitates detection of data quality issues, and provides essential context for interpreting clinical information in research, quality improvement, and AI/ML applications where understanding data lineage and reliability is paramount.	True	True	http://www.hl7.org/implement/standards/product_brief.cfm?product_id=420			[B2AI_ORG:40]														
B2AI_STANDARD:153	B2AI_STANDARD:BiomedicalStandard	DiagnosticReport	HL7 FHIR Resource DiagnosticReport	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:9]	Resource for findings and interpretation of diagnostic tests performed on patients, groups of patients, devices, and locations, and/or specimens derived from these. The report includes clinical context such as requesting and provider information, and some mix of atomic results, images, textual and coded interpretations, and formatted representation of diagnostic reports.	True	False	http://hl7.org/fhir/diagnosticreport.html	http://hl7.org/fhir/diagnosticreport-definitions.html		[B2AI_ORG:40]										[B2AI_STANDARD:845|B2AI_STANDARD:846|B2AI_STANDARD:847|B2AI_STANDARD:850]				
B2AI_STANDARD:154	B2AI_STANDARD:BiomedicalStandard	GenomicStudy	HL7 FHIR Resource GenomicStudy	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:9|B2AI_TOPIC:13]	GenomicStudy resource aims at delineating relevant information of a genomic study. A genomic study might comprise one or more analyses, each serving a specific purpose. These analyses may vary in method (e.g., karyotyping, CNV, or SNV detection), performer, software, devices used, or regions targeted.	True	False	https://build.fhir.org/genomicstudy.html	https://build.fhir.org/genomicstudy-definitions.html		[B2AI_ORG:40]										[B2AI_STANDARD:845|B2AI_STANDARD:846|B2AI_STANDARD:847|B2AI_STANDARD:850]				
B2AI_STANDARD:155	B2AI_STANDARD:BiomedicalStandard	MolecularSequence	HL7 FHIR Resource MolecularSequence	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4|B2AI_TOPIC:20]	The FHIR MolecularSequence resource provides a standardized structure for representing raw and processed biological sequence data including DNA, RNA, and amino acid sequences within the FHIR ecosystem, enabling integration of genomic information with clinical data. The resource captures observed sequences, reference sequences, sequence variations (SNPs, indels, structural variants), quality scores, read coverage information, and repository links to external sequence databases (GenBank, EMBL, RefSeq). MolecularSequence supports representation of sequence coordinates using zero-based or one-based numbering systems, strand orientation, and relationships between sequences (such as transcript-to-protein mappings). The resource can reference associated specimens, patients, or other subjects, and links to variant calling results, expression data, and functional annotations. MolecularSequence enables clinical genomics workflows including variant interpretation, pharmacogenomics decision support, molecular diagnostics reporting, and precision medicine applications by providing a FHIR-native way to exchange sequence data alongside phenotypic, diagnostic, and treatment information in electronic health records and clinical information systems.	True	False	http://hl7.org/implement/standards/fhir/molecularsequence.html	http://hl7.org/implement/standards/fhir/molecularsequence-definitions.html		[B2AI_ORG:40]										[B2AI_STANDARD:845|B2AI_STANDARD:846|B2AI_STANDARD:847|B2AI_STANDARD:850]				
B2AI_STANDARD:156	B2AI_STANDARD:BiomedicalStandard	Observation	HL7 FHIR Resource Observation	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:9]	The FHIR Observation resource is a central, versatile structure for representing measurements, assessments, and assertions made about a patient, device, location, or other subject in healthcare contexts. Observations encompass a wide range of clinical and research data including vital signs (blood pressure, heart rate, temperature), laboratory results (chemistry, hematology, microbiology), imaging measurements, clinical assessments (pain scores, functional status), device readings (glucose monitors, pulse oximeters), social determinants of health, genomic findings, and quality metrics. The resource uses coded values from standard terminologies (LOINC, SNOMED CT) to identify what was measured, supports numeric values with units (UCUM), coded results, textual findings, and multimedia attachments. Observation includes metadata for effective time, status, performer, method, specimen reference, reference ranges, and interpretation flags. The resource supports panels and components for organizing related observations (complete blood count, metabolic panel), enables longitudinal tracking through sequences and trends, and links to supporting evidence and derivations. Observations are fundamental to AI/ML applications requiring structured clinical phenotypes, time-series analytics, predictive modeling, and integration of diverse data types for clinical decision support and population health analysis.	True	False	http://hl7.org/implement/standards/fhir/observation.html	http://hl7.org/implement/standards/fhir/observation-definitions.html		[B2AI_ORG:40]										[B2AI_STANDARD:845|B2AI_STANDARD:846|B2AI_STANDARD:847|B2AI_STANDARD:850]				
B2AI_STANDARD:157	B2AI_STANDARD:BiomedicalStandard	Patient	HL7 FHIR Resource Patient	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:6|B2AI_TOPIC:9]	The FHIR Patient resource provides a standardized structure for representing demographics and administrative information about individuals or animals receiving care or health-related services. The resource captures core demographic data including name (with multiple name types and use contexts), birth date, gender, address (with structured components and period of validity), telecom contact information (phone, email, with use and priority indicators), and multiple identifiers (medical record numbers, social security numbers, insurance IDs) with system and type codes. Patient supports marital status, multiple languages with proficiency levels, photo attachments, contact persons and their relationships, communication preferences, general practitioners, managing organization, and links to related patients (merged records, see-also relationships). The resource includes deceased indicator, multiple birth information, and animal-specific extensions for species and breed. Patient serves as a central hub linking to all other clinical resources (observations, conditions, procedures, medications) and enables patient matching, record linkage, demographic analytics, cohort identification, and population health management. The standardized demographics are essential for AI/ML applications requiring patient stratification, bias detection, social determinants analysis, and fair representation across diverse populations in clinical prediction models and decision support systems.	True	False	http://hl7.org/implement/standards/fhir/patient.html	http://hl7.org/implement/standards/fhir/patient-definitions.html		[B2AI_ORG:40]										[B2AI_STANDARD:845|B2AI_STANDARD:846|B2AI_STANDARD:847|B2AI_STANDARD:850]				
B2AI_STANDARD:158	B2AI_STANDARD:BiomedicalStandard	Specimen	HL7 FHIR Resource Specimen	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:9]	The FHIR Specimen resource provides a standardized structure for representing any material sample taken from a biological entity (living or dead) or from a physical object or the environment, essential for laboratory testing, biobanking, and clinical diagnostics. The resource captures comprehensive specimen metadata including type (blood, tissue, urine, etc.), subject/patient association, collection details (method, body site, quantity, date/time), identifiers (accession number, container ID), processing and handling procedures (centrifugation, fixation, storage), parent-child specimen relationships for aliquots and derivatives, storage conditions (temperature, humidity), and current status (available, unavailable, unsatisfactory, entered-in-error). The resource supports complex laboratory workflows by tracking specimen provenance through collection, transport, processing, and analysis stages, with fields for collection procedure, additive substances, container types, and special handling requirements. Specimen integrates with other FHIR resources including Patient, Practitioner, ServiceRequest, DiagnosticReport, and Observation to create complete clinical laboratory information workflows. The resource enables biobank specimen catalogs, clinical trial sample tracking, public health surveillance specimen management, and research biorepository operations. FHIR Specimen supports interoperability between laboratory information systems (LIS), electronic health records (EHR), biobank management systems, and research databases, facilitating standardized specimen data exchange across healthcare and research ecosystems while maintaining compliance with regulations for specimen handling, consent, and data privacy.	True	False	http://hl7.org/fhir/specimen.html	http://hl7.org/fhir/specimen-definitions.html		[B2AI_ORG:40]										[B2AI_STANDARD:845|B2AI_STANDARD:846|B2AI_STANDARD:847|B2AI_STANDARD:850]				
B2AI_STANDARD:159	B2AI_STANDARD:BiomedicalStandard	GHP	HL7 Gender Harmony Project	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:6]	Currently, it is common that a single data element is used to capture both sex and gender information, often assuming these two items are one unified idea. This specification challenges that notion and proposes that independent consideration of sex and gender, and the assessment of their differences promotes the health of women, men, and people of diverse gender identities of all ages, avoiding systematic errors that generate results with a low validity (if any) in clinical studies. The Gender Harmony model describes an approach that can improve data accuracy for sex and gender information in health care systems.	True	True	http://www.hl7.org/implement/standards/product_brief.cfm?product_id=564			[B2AI_ORG:40]								doi:10.1093/jamia/ocab196						
B2AI_STANDARD:160	B2AI_STANDARD:BiomedicalStandard	TraML	HUPO-PSI TraML format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:28]	The HUPO PSI Mass Spectrometry Standards Working Group (MSS WG) has developed a specification for a standardized format for the exchange and transmission of transition lists for selected reaction monitoring (SRM) experiments.	True	False	https://www.psidev.info/traml			[B2AI_ORG:41]														
B2AI_STANDARD:161	B2AI_STANDARD:BiomedicalStandard	IEEE P360	IEEE 360-2022 IEEE Standard for Wearable Consumer Electronic Devices	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:18]	An overview, terminology, and categorization for Wearable Consumer Electronic Devices (Wearables). It further outlines an architecture for a series of standard specifications that define technical requirements and testing methods for different aspects of Wearables, from basic security and suitableness of wearing to various functional areas like health, fitness, and infotainment, etc.	False	True	https://standards.ieee.org/ieee/360/6244/			[B2AI_ORG:44]														
B2AI_STANDARD:162	B2AI_STANDARD:BiomedicalStandard	BPPC	IHE Basic Patient Privacy Consents	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:9]	A mechanism to record the patient privacy consent(s) and a method for Content Consumers to use to enforce the privacy consent appropriate to the use.	True	False	https://profiles.ihe.net/ITI/TF/Volume1/ch-19.html	https://profiles.ihe.net/ITI/TF/Volume1/ch-19.html		[B2AI_ORG:46]														
B2AI_STANDARD:163	B2AI_STANDARD:BiomedicalStandard	IHE CDS-OAT	IHE Clinical Decision Support Order Appropriateness Tracking	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Profile for Clinical Decision Support and Appropriate Use Criteria (AUC) information as received from the CDS Mechanism 145 (CDSM) on the order and charge transaction to the revenue cycle application that is responsible to create a claim.	True	False	https://www.ihe.net/uploadedFiles/Documents/Radiology/IHE_Rad_Suppl_CDS-OAT.pdf			[B2AI_ORG:46]														
B2AI_STANDARD:164	B2AI_STANDARD:BiomedicalStandard	CRD	IHE Clinical Research Document standard	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	The Clinical Research Document Profile (CRD) specifies a standard way to generate a clinical research document from EHR data provided in the CDA standard.	True	False	https://www.ihe.net/uploadedFiles/Documents/QRPH/IHE_QRPH_Suppl_CRD.pdf	https://www.ihe.net/uploadedFiles/Documents/QRPH/IHE_QRPH_Suppl_CRD.pdf		[B2AI_ORG:46]														
B2AI_STANDARD:165	B2AI_STANDARD:BiomedicalStandard	IHE XCPD	IHE Cross-Community Patient Discovery Profile	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Standardized means to locate communities that hold patient relevant health data and the translation of patient identifiers across communities holding the same patients data.	True	False	https://profiles.ihe.net/ITI/TF/Volume1/ch-27.html			[B2AI_ORG:46]														
B2AI_STANDARD:166	B2AI_STANDARD:BiomedicalStandard	IHE ITI	IHE IT Infrastructure Technical Framework	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Technical profiles and definitions for IT use cases, transactions, content, and metadata.	True	False	https://profiles.ihe.net/ITI/index.html	https://profiles.ihe.net/ITI/TF/index.html		[B2AI_ORG:46]														
B2AI_STANDARD:167	B2AI_STANDARD:BiomedicalStandard	LAW	IHE Laboratory Analytical Workflow	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Profile to support the analytical workflow between analyzers of the clinical laboratory and the IT systems managing their work	True	False	https://www.ihe.net/resources/technical_frameworks/#PaLM	https://www.ihe.net/uploadedFiles/Documents/PaLM/IHE_PaLM_TF_Vol1.pdf		[B2AI_ORG:46]														
B2AI_STANDARD:168	B2AI_STANDARD:BiomedicalStandard	IHE PCD	IHE Patient Care Device Profiles	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	IHE Patient Care Device (PCD) Profiles are standards-based interoperability specifications developed by Integrating the Healthcare Enterprise (IHE) to address medical device communication challenges in clinical settings. The profiles follow IHE's technical framework development process including proposal, supplement development, public comment, trial implementation at Connectathon events, and finalization. Key PCD profiles include ACM (Alert Communication Management) for standardized alarm handling, DEC (Device Enterprise Communication) for device-to-enterprise integration, IDCO (Implantable Device Cardiac Observations) for cardiac device data, IPEC (Infusion Pump Event Communication) for infusion pump safety, PIV (Point of Care Infusion Verification) for medication verification, PCIM (Point of Care Identity Management) for device authentication, RDQ (Retrospective Data Query) for historical data retrieval, RTM (Rosetta Terminology Mapping) for standardized device terminology, and WCM (Waveform Communication Module) for physiological waveform data. These profiles leverage existing standards like HL7, DICOM, and IEEE 11073 to enable vendor-neutral device interoperability, supporting critical use cases such as automated vital signs documentation in electronic health records, integrated alarm management across monitoring systems, and safe medication administration workflows.	True	False	https://wiki.ihe.net/index.php?title=PCD_Profiles			[B2AI_ORG:46]														
B2AI_STANDARD:169	B2AI_STANDARD:BiomedicalStandard	IHE PDQ	IHE Patient Demographics Query Integration Profile	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Standardized ways for multiple distributed applications to query a patient information server for a list of patients, based on user-defined search criteria, and retrieve a patients demographic (and, optionally, visit or visit-related) information directly into the application.	True	False	https://profiles.ihe.net/ITI/TF/Volume1/ch-8.html			[B2AI_ORG:46]														
B2AI_STANDARD:170	B2AI_STANDARD:BiomedicalStandard	RFD	IHE Retrieve Form for Data Capture	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4]	The RFD Profile provides a generic polling mechanism to allow an external agency to indicate issues with data that have been captured and enable the healthcare provider to correct the data. The profile does not dictate the mechanism employed or content required to achieve such corrections.	True	False	https://profiles.ihe.net/ITI/TF/Volume1/ch-17.html			[B2AI_ORG:46]														
B2AI_STANDARD:171	B2AI_STANDARD:BiomedicalStandard	LIVD	IICC Digital Format for Publication of LOINC to Vendor IVD Test Results	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Industry-defined format to facilitate the publication and exchange of LOINC codes for vendor IVD test results.	True	False	https://ivdconnectivity.org/livd/													[B2AI_STANDARD:848]		[B2AI_ORG:53]		
B2AI_STANDARD:172	B2AI_STANDARD:BiomedicalStandard	ICS	Image Cytometry Standard	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:19]	The Image Cytometry Standard (ICS) is a digital multidimensional image file format used in life sciences microscopy. It stores not only the image data, but also the microscopic parameters describing the optics during the acquisition.	True	False	https://en.wikipedia.org/wiki/Image_Cytometry_Standard											doi:10.1002/cyto.990110502						
B2AI_STANDARD:173	B2AI_STANDARD:BiomedicalStandard	imzML	imzML format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:15|B2AI_TOPIC:28]	The purpose of imzML is to facilitate the exchange and processing of mass spectrometry imaging data. This website is intended to provide all information neccesary to implement imzML.imzML was developed in the framework of the EU funded project COMPUTIS. The main goals during the development were complete description of MS imaging experiments and efficient storage of (very large) data sets. imzML is it not limited to MS imaging, but is also useful for other MS applications generating large data sets such as LC-FTMS. The current version is mzML 1.1.0. The metadata part of imzML is based on the mzML format by HUPO-PSI	True	False	https://ms-imaging.org/imzml/											doi:10.1016/j.jprot.2012.07.026				[B2AI_ORG:20]		
B2AI_STANDARD:174	B2AI_STANDARD:OntologyOrVocabulary	ICD-10-CM	International Classification of Diseases 10th Revision Clinical Modification	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[codesystem|standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:9]	ICD-10-CM (International Classification of Diseases, 10th Revision, Clinical Modification) is the United States' clinical modification of the WHO's ICD-10 classification system, mandated for reporting diagnoses and health conditions in all healthcare settings for billing, epidemiology, quality measurement, and public health surveillance since October 2015. Maintained by the National Center for Health Statistics (NCHS) in collaboration with CMS, ICD-10-CM provides over 70,000 diagnosis codes with significantly expanded granularity compared to ICD-9-CM, enabling more precise documentation of disease severity, anatomical location, episode of care (initial encounter, subsequent encounter, sequela), and laterality (left, right, bilateral). The coding structure uses alphanumeric format with 3-7 characters where the first character is alphabetic, second is numeric, third through seventh provide increasing specificity of body site, etiology, severity, and clinical details, with optional seventh characters as extensions for encounter context or injury staging. Major chapters include infectious diseases (A00-B99), neoplasms (C00-D49), endocrine and metabolic disorders (E00-E89), mental and behavioral disorders (F01-F99), nervous system (G00-G99), circulatory (I00-I99), respiratory (J00-J99), digestive (K00-K95), musculoskeletal (M00-M99), and external causes of morbidity (V00-Y99). Critical features include combination codes that capture multiple conditions or manifestations in single codes, placeholder 'x' characters to allow for future expansion, and seventh character extensions for obstetric outcomes, fracture healing status, and diabetes complications. ICD-10-CM drives healthcare reimbursement through diagnosis-related groups (DRGs), enables clinical decision support systems through structured diagnosis data, supports population health analytics identifying disease trends and risk factors, powers epidemiological surveillance for CDC reportable conditions tracking disease outbreaks, and provides standardized terminology for electronic health record (EHR) systems ensuring interoperability. Annual updates released October 1st incorporate new diseases, refined definitions, and code revisions reflecting medical advances, essential for medical billing specialists, clinical coders, health informaticians, public health researchers, and healthcare quality officers.	True	False	https://www.cdc.gov/nchs/icd/icd-10-cm.htm		True	[B2AI_ORG:14]														
B2AI_STANDARD:175	B2AI_STANDARD:OntologyOrVocabulary	ICD-11	International Classification of Diseases 11th Revision	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[codesystem|standards_process_maturity_final]	[B2AI_TOPIC:9]	ICD-11 (International Classification of Diseases, 11th Revision) is the World Health Organization's latest global standard for diagnostic health information, officially adopted by the World Health Assembly in 2019 and effective from January 2022, representing the most comprehensive revision in over three decades with fully digital-native architecture designed for electronic health systems and multilingual implementation. Developed through extensive international collaboration involving thousands of health professionals across 90 countries, ICD-11 contains over 17,000 diagnostic categories with significantly enhanced granularity, clinical detail, and scientific accuracy compared to ICD-10, incorporating advances in medical knowledge including genomic medicine, patient safety, traditional medicine integration, and sexual health classifications. The classification uses a multi-hierarchical structure with foundation component enabling flexible content management, allowing multiple parent categories and post-coordination where complex conditions are composed by combining codes for anatomy, etiology, severity, temporality, and other clinical dimensions. Major structural improvements include entirely new chapters for traditional medicine (validated practices from Chinese, Ayurvedic, and other systems), extension codes for functional assessment using WHO Disability Assessment Schedule 2.0 (WHODAS), detailed antimicrobial resistance documentation, gaming disorder and other emerging conditions, and significantly expanded mental health classifications with dimensional assessments. ICD-11 features built-in multilingual support covering Arabic, Chinese, English, French, Russian, Spanish with machine translation capabilities, linearizations tailored for different use cases (mortality reporting, morbidity statistics, primary care, clinical documentation), and modern URI-based coding enabling semantic web integration and FHIR compatibility. Critical technical features include embedded logical definitions using description logic enabling automated classification and consistency checking, standardized application programming interfaces (APIs) for EHR integration, and continuous online updating mechanism replacing decennial revision cycles with regular incremental updates. ICD-11 supports global health surveillance enabling real-time disease outbreak tracking, international health statistics comparability across countries and regions, research data standardization for epidemiological studies and clinical trials, health service planning through accurate disease burden measurement, and AI/machine learning applications through structured, semantically rich diagnostic data. Adoption varies globally with some countries implementing immediately while others transition gradually, essential for medical informaticians, public health authorities, clinical coders, WHO collaborating centers, and healthcare system planners.	True	False	https://icd.who.int/en			[B2AI_ORG:100]														
B2AI_STANDARD:176	B2AI_STANDARD:OntologyOrVocabulary	ICD-9-CM	International Classification of Diseases 9th Revision Clinical Modification	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[codesystem]	[B2AI_TOPIC:9]	ICD-9-CM (International Classification of Diseases, 9th Revision, Clinical Modification) was the official medical classification system used in the United States from 1979 until September 2015 for coding diagnoses and procedures in healthcare billing, epidemiological research, and health statistics, representing the U.S. clinical modification of WHO's ICD-9 with additional detail for morbidity classification. Developed by the National Center for Health Statistics (NCHS) and used by CMS for Medicare/Medicaid reimbursement, ICD-9-CM contained approximately 13,000 diagnosis codes and 3,000 procedure codes, structured with 3-5 digit numeric format where first three digits represent disease category and subsequent digits add specificity for anatomical location, etiology, or manifestations. Major diagnostic chapters included infectious and parasitic diseases (001-139), neoplasms (140-239), endocrine and metabolic diseases (240-279), mental disorders (290-319), nervous system (320-389), circulatory system (390-459), respiratory system (460-519), digestive system (520-579), genitourinary system (580-629), and external causes (E codes E800-E999) documenting injury circumstances. The procedure classification (Volume 3) used 2-4 digit numeric codes organized by anatomical system, covering surgical operations, diagnostic procedures, and therapeutic interventions primarily for inpatient hospital settings. Despite decades of widespread adoption creating massive legacy datasets and establishing institutional coding workflows, ICD-9-CM became outdated due to limited specificity insufficient for modern medicine's diagnostic precision, exhausted code capacity with no room for new diseases or procedures, lack of laterality designation, and outdated medical terminology inconsistent with current clinical practice. The mandated transition to ICD-10-CM/PCS in October 2015 required extensive healthcare industry preparation including EHR system updates, coder retraining, and billing system modifications, yet ICD-9-CM remains critically important for historical medical records analysis, longitudinal epidemiological studies spanning pre-2015 data, retrospective cohort studies, trend analysis requiring crosswalk mappings between ICD-9 and ICD-10, and legacy system maintenance. Applications include historical disease surveillance tracking public health trends, health services research analyzing treatment patterns, medical informatics developing code translation algorithms, and archival medical data management, essential for health informaticians managing legacy data, epidemiologists conducting temporal studies, medical archivists, and researchers working with pre-2015 healthcare datasets.	True	False	https://www.cdc.gov/nchs/icd/icd9cm.htm		True	[B2AI_ORG:14]														
B2AI_STANDARD:177	B2AI_STANDARD:BiomedicalStandard	INTRPRT	INTRPRT guidelines for transparent machine learning for medical image analysis	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:15]	A design directive for transparent ML systems in medical image analysis. The INTRPRT guideline suggests human-centered design principles, recommending formative user research as the first step to understand user needs and domain requirements.	True	False	https://doi.org/10.1038/s41746-022-00699-2											doi:10.1038/s41746-022-00699-2						
B2AI_STANDARD:178	B2AI_STANDARD:BiomedicalStandard	IPSM-AF	IPSM Alignment Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:21]	Alignment can be interpreted as a set of uni-directional mappings for transforming input RDF graph into output RDF graph. It is persisted in IPSM Alignment Format (IPSM-AF) that is based on a well known Alignment API Format.	True	False	https://inter-iot.readthedocs.io/projects/ipsm/en/latest/Configuration/Alignment-format/IPSM-alignment-format/	https://github.com/INTER-IoT/ipsm-alignments																
B2AI_STANDARD:179	B2AI_STANDARD:BiomedicalStandard	ISA-TAB-Nano	ISA-Tab-Nano format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:5]	ISA-TAB-Nano specifies the format for representing and sharing information about nanomaterials, small molecules and biological specimens along with their assay characterization data (including metadata, and summary data) using spreadsheet or TAB-delimited files.	True	False	https://wiki.nci.nih.gov/display/icr/isa-tab-nano											doi:10.1186/1472-6750-13-2				[B2AI_ORG:47]		
B2AI_STANDARD:180	B2AI_STANDARD:BiomedicalStandard	EN13606	ISO 13606 standard for Electronic Health Record Communication	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:9]	A means for communicating part or all of the electronic health record (EHR) of one or more identified subjects of care between EHR systems, or between EHR systems and a centralised EHR data repository. It can also be used for EHR communication between an EHR system or repository and clinical applications or middleware components (such as decision support components), or personal health applications and devices, that need to access or provide EHR data, or as the representation of EHR data within a distributed (federated) record system.	False	True	http://www.en13606.org/	https://www.iso.org/standard/67868.html														[B2AI_ORG:49]		
B2AI_STANDARD:181	B2AI_STANDARD:BiomedicalStandard	ITU-T E-health	ITU H.810, H.811, H.812, H.812.5, and H.813	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Standards for medical device communication.	True	False	https://www.itu.int/en/ITU-T/studygroups/2013-2016/16/Pages/rm/ehealth.aspx			[B2AI_ORG:50]														
B2AI_STANDARD:182	B2AI_STANDARD:BiomedicalStandard	JCAMP-DX	Joint Committee on Atomic and Molecular Physical Data standard	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:3|B2AI_TOPIC:28]	The JCAMP-DX was one of the earliest specifications providing a standard file format for data exchange in mass spectrometry. It was initially developed for infrared spectrometry and related chemical and physical information between spectrometer data systems of different manufacture. It was also used later for nuclear magnetic resonance spectroscopy. JCAMP-DX is an ASCII based format and therefore not very compact even though it includes standards for file compression. All data are stored as labeled fields of variable length using printable ASCII characters. JCAMP-DX was officially released in 1988. JCAMP-DX was found impractical for today's large MS data sets, but it is still used for exchanging moderate numbers of spectra. IUPAC is currently in charge of its maintenance and the latest protocol is from 2005.	True	False	https://iupac.org/what-we-do/digital-standards/jcamp-dx/			[B2AI_ORG:51]														
B2AI_STANDARD:183	B2AI_STANDARD:BiomedicalStandard	Countgraph	K-mer countgraph	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:5]	A format used by the khmer tool to represent k-mer counts and their occurences.	True	False	https://khmer.readthedocs.io/en/v2.0/dev/binary-file-formats.html#countgraph											doi:10.12688/f1000research.6924.1	[B2AI_STANDARD:782]					
B2AI_STANDARD:184	B2AI_STANDARD:BiomedicalStandard	KPS	Karnofsky Performance Scale	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[diagnosticinstrument]	[B2AI_TOPIC:9]	A standard way of measuring the ability of cancer patients to perform ordinary tasks. The Karnofsky Performance Status scores range from 0 to 100. A higher score means the patient is better able to carry out daily activities. Karnofsky Performance Status may be used to determine a patient's prognosis, to measure changes in a patients ability to function, or to decide if a patient could be included in a clinical trial. Also called KPS.	True	False	http://www.npcrc.org/files/news/karnofsky_performance_scale.pdf											doi:10.1200/JCO.1984.2.3.187						
B2AI_STANDARD:185	B2AI_STANDARD:BiomedicalStandard	KGML	KEGG Markup Language	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:21]	The KEGG Markup Language (KGML) is an exchange format of the KEGG pathway maps, which is converted from internally used KGML+ (KGML+SVG) format.	True	False	https://www.kegg.jp/kegg/xml/			[B2AI_ORG:52]														
B2AI_STANDARD:186	B2AI_STANDARD:BiomedicalStandard	LS-DAM	Life sciences domain analysis model	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[deprecated]	[B2AI_TOPIC:20]	The LS DAM v2.2.1 is comprised of 130 classes and covers several core areas including Experiment, Molecular Biology, Molecular Databases and Specimen. Nearly half of these classes originate from the BRIDG model, emphasizing the semantic harmonization between these models. Validation of the LS DAM against independently derived information models, research scenarios and reference databases supports its general applicability to represent life sciences research.	True	False	https://doi.org/10.1136/amiajnl-2011-000763											doi:10.1136/amiajnl-2011-000763						
B2AI_STANDARD:187	B2AI_STANDARD:OntologyOrVocabulary	LOINC	Logical Observation Identifier Names and Codes	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[codesystem|standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:9]	Tests, observations, diagnostics, and other clinical procedures.	True	True	loinc.org		True	[B2AI_ORG:53]	[{\"id\": \"B2AI_APP:37\", \"category\": \"B2AI:Application\", \"name\": \"Automated ML-Based Mapping of Free-Text Laboratory Codes to LOINC\", \"description\": \"Machine learning classifiers including Random Forest, logistic regression, and ensemble methods automatically map free-text laboratory descriptors (test names, units, panels) from local EHR systems to standardized LOINC codes. Novel encoding methods vectorize heterogeneous lab terminology, and trained models achieve high accuracy (Random Forest approximately 94.5% top-1 accuracy; ensemble approaches up to 99% accuracy) on nationwide oncology EHR data from approximately 280 US clinics. This automation dramatically reduces manual terminologist effort (which typically requires 6-8 hours per 1000 terms) and enables research-ready datasets by harmonizing laboratory data across institutions for downstream ML applications.\", \"used_in_bridge2ai\": false, \"references\": [\"https://pmc.ncbi.nlm.nih.gov/articles/PMC8861721/\"]}, {\"id\": \"B2AI_APP:136\", \"category\": \"B2AI:Application\", \"name\": \"LLM-Based LOINC Standardization Using Pre-Trained T5 Embeddings\", \"description\": \"Pre-trained T5 large language models with contrastive learning and few-shot fine-tuning retrieve and suggest top-k LOINC code candidates from local laboratory descriptions, supporting human-in-the-loop curation workflows. The approach uses contextual embeddings and data augmentation to handle acronyms, synonyms, misspellings, and missing metadata across LOINC's six dimensions (component, property, time, system, scale, method), generalizing to unseen LOINC targets without retraining. Training and evaluation use source-target pairs from MIMIC-III EHR data, demonstrating improved top-k retrieval performance for scalable cross-site laboratory harmonization given LOINC's large code space (over 80,000 codes).\", \"used_in_bridge2ai\": false, \"references\": [\"https://proceedings.mlr.press/v193/tu22a/tu22a.pdf\"]}, {\"id\": \"B2AI_APP:137\", \"category\": \"B2AI:Application\", \"name\": \"Value-Based Laboratory Code Mapping to LOINC Using Result Distributions\", \"description\": \"Statistical feature engineering on laboratory test result distributions (mean, median, quartiles, variance, skewness) combined with unit normalization enables AI mapping of in-house codes to LOINC via intermediate standards (JLAC10) without relying on test-name NLP. Applied to the J-DREAMS diabetes database (955,011 entries across 15 facilities, 51 analytes), classifiers achieved at least 70% mapping accuracy for 80.4% of analytes. This value-centric approach is particularly useful where NLP resources or medical corpora are limited, demonstrating that unit and value harmonization tied to LOINC groupings is critical for accurate automated mapping and international data sharing.\", \"used_in_bridge2ai\": false, \"references\": [\"https://pmc.ncbi.nlm.nih.gov/articles/PMC12150744/\"]}, {\"id\": \"B2AI_APP:138\", \"category\": \"B2AI:Application\", \"name\": \"LOINC-Coded Laboratory Features for Improved Multi-Site Predictive Modeling\", \"description\": \"Using LOINC-standardized laboratory features in predictive models significantly improves performance and cross-site transferability compared to unmapped local codes. In a 13-hospital UPMC heart failure cohort (2008-2012) predicting 30-day readmission, models trained with manually LOINC-mapped lab features consistently achieved significantly higher AUCs despite modest overall performance. LOINC aggregation and grouping procedures materially affect model reproducibility and external validation across institutions, demonstrating that explicit LOINC standardization should be reported as a critical data preprocessing step in multi-site ML studies to ensure interpretability and portability.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1093/jamiaopen/ooy063\"]}, {\"id\": \"B2AI_APP:139\", \"category\": \"B2AI:Application\", \"name\": \"LOINC-to-HPO Semantic Integration for Deep Phenotyping and Biomarker Discovery\", \"description\": \"A widely adopted pipeline maps LOINC-coded laboratory test results transmitted via FHIR to Human Phenotype Ontology (HPO) terms, enabling computational deep phenotyping and biomarker discovery. The system manually biocurated mappings for 2,923 commonly used LOINC tests, handling numeric, ordinal, and nominal results using standardized FHIR interpretation codes, and leveraging HPO's hierarchical structure to aggregate heterogeneous tests with comparable interpretations. Validated on 15,681 UNC EHR patients with respiratory complaints and identifying known asthma biomarkers, the approach supports association studies, cohort analysis, and was released as a SMART on FHIR application for EHR integration and downstream ML applications requiring ontology-based phenotype embeddings.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1038/s41746-019-0110-4\"]}, {\"id\": \"B2AI_APP:140\", \"category\": \"B2AI:Application\", \"name\": \"N3C Unit and Value Harmonization Using LOINC Concept Grouping for ML-Ready Datasets\", \"description\": \"The National COVID Cohort Collaborative (N3C) unit harmonization pipeline groups laboratory measurements by LOINC concepts, selects canonical units with UCUM conventions and conversion formulas, and infers missing units using Kolmogorov-Smirnov distributional comparisons across pooled multi-site data. Applied to billions of OMOP-mapped EHR lab records, the pipeline harmonized 88.1% of values and imputed units for 78.2% of records missing units (41% of contributors' records), reclaiming large data fractions otherwise unavailable for analysis. This LOINC-based harmonization directly enables creation of ML-ready datasets for predictive modeling, phenotyping, and analytics across heterogeneous EHR sources.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1093/jamia/ocac054\"]}, {\"id\": \"B2AI_APP:141\", \"category\": \"B2AI:Application\", \"name\": \"High-Throughput LOINC Document Ontology Mapping Using Metadata-Based Methods\", \"description\": \"A scalable pipeline maps clinical document metadata to LOINC Document Ontology (LOINC DO) codes at large scale using bag-of-words and vector distance methods applied to structured EHR fields (event titles, tags, encounter types) rather than full-text NLP. Applied to University of Missouri Cerner database (over 130 million decompressed notes), the metadata-driven approach achieved 73.4% document coverage and mapped 132 million notes in under 2 hours, claimed to be an order of magnitude more efficient than NLP-based methods. This LOINC DO standardization supports downstream computable phenotyping, documentation quality assessment, and potential ML applications requiring standardized clinical document classification.\", \"used_in_bridge2ai\": false, \"references\": [\"https://pmc.ncbi.nlm.nih.gov/articles/PMC10785913/pdf/1116.pdf\"]}]	[Automated ML-Based Mapping of Free-Text Laboratory Codes to LOINC|LLM-Based LOINC Standardization Using Pre-Trained T5 Embeddings|Value-Based Laboratory Code Mapping to LOINC Using Result Distributions|LOINC-Coded Laboratory Features for Improved Multi-Site Predictive Modeling|LOINC-to-HPO Semantic Integration for Deep Phenotyping and Biomarker Discovery|N3C Unit and Value Harmonization Using LOINC Concept Grouping for ML-Ready Datasets|High-Throughput LOINC Document Ontology Mapping Using Metadata-Based Methods]	[B2AI_APP:37|B2AI_APP:136|B2AI_APP:137|B2AI_APP:138|B2AI_APP:139|B2AI_APP:140|B2AI_APP:141]	[['https://pmc.ncbi.nlm.nih.gov/articles/PMC8861721/']|['https://proceedings.mlr.press/v193/tu22a/tu22a.pdf']|['https://pmc.ncbi.nlm.nih.gov/articles/PMC12150744/']|['https://doi.org/10.1093/jamiaopen/ooy063']|['https://doi.org/10.1038/s41746-019-0110-4']|['https://doi.org/10.1093/jamia/ocac054']|['https://pmc.ncbi.nlm.nih.gov/articles/PMC10785913/pdf/1116.pdf']]	[Machine learning classifiers including Random Forest, logistic regression, and ensemble methods automatically map free-text laboratory descriptors (test names, units, panels) from local EHR systems to standardized LOINC codes. Novel encoding methods vectorize heterogeneous lab terminology, and trained models achieve high accuracy (Random Forest approximately 94.5% top-1 accuracy; ensemble approaches up to 99% accuracy) on nationwide oncology EHR data from approximately 280 US clinics. This automation dramatically reduces manual terminologist effort (which typically requires 6-8 hours per 1000 terms) and enables research-ready datasets by harmonizing laboratory data across institutions for downstream ML applications.|Pre-trained T5 large language models with contrastive learning and few-shot fine-tuning retrieve and suggest top-k LOINC code candidates from local laboratory descriptions, supporting human-in-the-loop curation workflows. The approach uses contextual embeddings and data augmentation to handle acronyms, synonyms, misspellings, and missing metadata across LOINC's six dimensions (component, property, time, system, scale, method), generalizing to unseen LOINC targets without retraining. Training and evaluation use source-target pairs from MIMIC-III EHR data, demonstrating improved top-k retrieval performance for scalable cross-site laboratory harmonization given LOINC's large code space (over 80,000 codes).|Statistical feature engineering on laboratory test result distributions (mean, median, quartiles, variance, skewness) combined with unit normalization enables AI mapping of in-house codes to LOINC via intermediate standards (JLAC10) without relying on test-name NLP. Applied to the J-DREAMS diabetes database (955,011 entries across 15 facilities, 51 analytes), classifiers achieved at least 70% mapping accuracy for 80.4% of analytes. This value-centric approach is particularly useful where NLP resources or medical corpora are limited, demonstrating that unit and value harmonization tied to LOINC groupings is critical for accurate automated mapping and international data sharing.|Using LOINC-standardized laboratory features in predictive models significantly improves performance and cross-site transferability compared to unmapped local codes. In a 13-hospital UPMC heart failure cohort (2008-2012) predicting 30-day readmission, models trained with manually LOINC-mapped lab features consistently achieved significantly higher AUCs despite modest overall performance. LOINC aggregation and grouping procedures materially affect model reproducibility and external validation across institutions, demonstrating that explicit LOINC standardization should be reported as a critical data preprocessing step in multi-site ML studies to ensure interpretability and portability.|A widely adopted pipeline maps LOINC-coded laboratory test results transmitted via FHIR to Human Phenotype Ontology (HPO) terms, enabling computational deep phenotyping and biomarker discovery. The system manually biocurated mappings for 2,923 commonly used LOINC tests, handling numeric, ordinal, and nominal results using standardized FHIR interpretation codes, and leveraging HPO's hierarchical structure to aggregate heterogeneous tests with comparable interpretations. Validated on 15,681 UNC EHR patients with respiratory complaints and identifying known asthma biomarkers, the approach supports association studies, cohort analysis, and was released as a SMART on FHIR application for EHR integration and downstream ML applications requiring ontology-based phenotype embeddings.|The National COVID Cohort Collaborative (N3C) unit harmonization pipeline groups laboratory measurements by LOINC concepts, selects canonical units with UCUM conventions and conversion formulas, and infers missing units using Kolmogorov-Smirnov distributional comparisons across pooled multi-site data. Applied to billions of OMOP-mapped EHR lab records, the pipeline harmonized 88.1% of values and imputed units for 78.2% of records missing units (41% of contributors' records), reclaiming large data fractions otherwise unavailable for analysis. This LOINC-based harmonization directly enables creation of ML-ready datasets for predictive modeling, phenotyping, and analytics across heterogeneous EHR sources.|A scalable pipeline maps clinical document metadata to LOINC Document Ontology (LOINC DO) codes at large scale using bag-of-words and vector distance methods applied to structured EHR fields (event titles, tags, encounter types) rather than full-text NLP. Applied to University of Missouri Cerner database (over 130 million decompressed notes), the metadata-driven approach achieved 73.4% document coverage and mapped 132 million notes in under 2 hours, claimed to be an order of magnitude more efficient than NLP-based methods. This LOINC DO standardization supports downstream computable phenotyping, documentation quality assessment, and potential ML applications requiring standardized clinical document classification.]	[B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application]	[False|False|False|False|False|False|False]			[B2AI_STANDARD:848]		[B2AI_ORG:114|B2AI_ORG:115]		
B2AI_STANDARD:188	B2AI_STANDARD:BiomedicalStandard	mmCIF	Macromolecular Crystallographic Information File	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:27]	PDBx/mmCIF became the standard PDB archive format in 2014. All PDB data processing and annotation will be performed using PDBx/mmCIF at all wwPDB sites. PDBx/mmCIF consists of categories of information represented as tables and keyword value pairs.	True	False	https://mmcif.wwpdb.org/			[B2AI_ORG:82]														
B2AI_STANDARD:189	B2AI_STANDARD:BiomedicalStandard	MSAML	Markup Components for Describing Multiple Sequence Alignments	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:23]	MSAML was formulated to make manipulation and extraction of multiple sequence alignment information easier by logically defining the parts of an alignment for use in an XML conformant application.	True	False	http://xml.coverpages.org/msaml-desc-dec.html																	
B2AI_STANDARD:190	B2AI_STANDARD:BiomedicalStandard	MDL	MDL molfile Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:3|B2AI_TOPIC:27]	An MDL Molfile is a file format for holding information about the atoms, bonds, connectivity and coordinates of a molecule.	True	False	https://en.wikipedia.org/wiki/Chemical_table_file																	
B2AI_STANDARD:191	B2AI_STANDARD:BiomedicalStandard	RXN	MDL reaction Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:3]	The MDL reaction format is used to store information on chemical reactions.	True	False	https://open-babel.readthedocs.io/en/latest/FileFormats/MDL_RXN_format.html																	
B2AI_STANDARD:192	B2AI_STANDARD:BiomedicalStandard	MSGISO	Measuring Sex, Gender Identity, and Sexual Orientation	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:6|B2AI_TOPIC:31]	Measuring Sex, Gender Identity, and Sexual Orientation recommends that the National Institutes of Health (NIH) adopt new practices for collecting data on sex, gender, and sexual orientation - including collecting gender data by default, and not conflating gender with sex as a biological variable. The report recommends standardized language to be used in survey questions that ask about a respondent's sex, gender identity, and sexual orientation. Better measurements will improve data quality, as well as the NIH's ability to identify LGBTQI+ populations and understand the challenges they face.	True	False	https://nap.nationalacademies.org/catalog/26424/measuring-sex-gender-identity-and-sexual-orientation			[B2AI_ORG:60]								doi:10.17226/26424						
B2AI_STANDARD:193	B2AI_STANDARD:BiomedicalStandard	MNC	Medical Imaging NetCDF (Minc) format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:22]	A system for flexible, self-documenting representation of neuroscientific imaging data with arbitrary orientation and dimensionality.	True	False	https://en.wikibooks.org/wiki/MINC/SoftwareDevelopment/MINC2.0_File_Format_Reference											doi:10.3389/fninf.2016.00035						
B2AI_STANDARD:194	B2AI_STANDARD:BiomedicalStandard	MS-DRG	Medicare Severity Diagnosis Related Groups codes	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[codesystem]	[B2AI_TOPIC:9]	Medical cases in the US are classified into Medicare Severity Diagnosis Related Groups (MS-DRGs) for payment based on the following information reported by the hospital - the principal diagnosis, up to 24 additional diagnoses, and up to 25 procedures performed during the stay. In a small number of MS-DRGs, classification is also based on the age, sex, and discharge status of the patient. Effective October 1, 2015, the diagnosis and procedure information is reported by the hospital using codes from the International Classification of Diseases, Tenth Revision, Clinical Modification (ICD-10-CM) and the International Classification of Diseases, Tenth Revision, Procedure Coding System (ICD-10-PCS).	True	False	https://www.cms.gov/Medicare/Medicare-Fee-for-Service-Payment/AcuteInpatientPPS/MS-DRG-Classifications-and-Software			[B2AI_ORG:17]														
B2AI_STANDARD:195	B2AI_STANDARD:BiomedicalStandard	MEDIN	MEDIN Discovery Metadata Standard	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datamodel]	[B2AI_TOPIC:11]	MEDIN Discovery Metadata Standard is a UK marine profile of the UK GEMINI (GEo-spatial Metadata Interoperability iNItiative) standard that also complies with the EU INSPIRE Directive and ISO19115. It provides comprehensive guidance for creating discovery metadata that accompanies marine datasets, describing what the data contains, where it was collected, and how to access it. The standard supports both geospatial (v3.1.2) and non-spatial marine data types. MEDIN provides multiple tools including the web-based Discovery Metadata Editor for creating, validating, exporting, and publishing records to the MEDIN Data Discovery Portal, and the standalone Metadata Maestro desktop application with user-friendly interface and offline capability. The standard includes XML Schema Definition (XSD) and Schematron files for validation, enabling systematic quality control of metadata records within organizational workflows.	True	False	https://www.medin.org.uk/medin-discovery-metadata-standard	https://github.com/medin-marine/Discovery-Standard-public-content																
B2AI_STANDARD:196	B2AI_STANDARD:BiomedicalStandard	MSAS	Memorial Symptom Assessment Scale	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[diagnosticinstrument]	[B2AI_TOPIC:9]	The Memorial Symptom Assessment Scale (MSAS) is a new patient-rated instrument that was developed to provide multidimensional information about a diverse group of common symptoms.	True	False	http://www.npcrc.org/files/news/memorial_symptom_assessment_scale.pdf											doi:10.1016/0959-8049(94)90182-1						
B2AI_STANDARD:197	B2AI_STANDARD:BiomedicalStandard	MOD-CO	Meta-omics Data and Collection Objects	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:13|B2AI_TOPIC:28|B2AI_TOPIC:34]	The conceptual and procedural meta-omics schema is developed as part of the MOD-CO project. It is entitled with MOD-CO schema, a conceptual schema for processing sample data in meta-omics research and published in various kind of schema representations. With that, the MOD-CO schema is a generic and comprehensive schema providing specifications useful for later software implementation and facilitates international standardisation processes.	True	False	https://www.mod-co.net/wiki/Schema_Representations																	
B2AI_STANDARD:198	B2AI_STANDARD:BiomedicalStandard	MINiML	MIAME Notation in Markup Language	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831'	[fileformat]	[B2AI_TOPIC:33]	MINiML (MIAME Notation in Markup Language, pronounced 'minimal') is a data exchange format optimized for microarray gene expression data, as well as many other types of high-throughput molecular abundance data. MINiML assumes only very basic relations between objects - Platform (e.g., array), Sample (e.g., hybridization), and Series (experiment). MINiML captures all components of the MIAME checklist, as well as any additional information that the submitter wants to provide. MINiML uses XML Schema as syntax.	True	False	https://www.ncbi.nlm.nih.gov/geo/info/MINiML.html															[B2AI_ORG:74]		
B2AI_STANDARD:199	B2AI_STANDARD:BiomedicalStandard	MAGE-ML	MicroArray Gene Expression Markup Language	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:33|B2AI_TOPIC:34]	This document is a standard that addresses the representation of gene expression data and relevant annotations, as well as mechanisms for exchanging these data. The field of gene expression experiments has several distinct technologies that a standard must include (e.g., single vs. dual channel experiments, cDNA vs. oligonucleotides). Because of these different technologies and different types of gene expression experiments, it is not expected that all aspects of the standard will be used by all organizations. With the acceptance of XML Metadata Interchange as an OMG standard it is possible to specify a normative UML model using a tool such as Rational Rose that describes the data structures for Gene Expression	True	False	http://scgap.systemsbiology.net/standards/mage_miame.php																	
B2AI_STANDARD:200	B2AI_STANDARD:BiomedicalStandard	MAGE-TAB	MicroArray Gene Expression Markup Language Tab format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:33|B2AI_TOPIC:34]	Sharing of microarray data within the research community has been greatly facilitated by the development of the disclosure and communication standards MIAME and MAGE-ML by the FGED Society. However, the complexity of the MAGE-ML format has made its use impractical for laboratories lacking dedicated bioinformatics support. We propose a simple tab-delimited, spreadsheet-based format, MAGE-TAB, which will become a part of the MAGE microarray data standard and can be used for annotating and communicating microarray data in a MIAME compliant fashion. MAGE-TAB will enable laboratories without bioinformatics experience or support to manage, exchange and submit well-annotated microarray data in a standard format using a spreadsheet. The MAGE-TAB format is self-contained, and does not require an understanding of MAGE-ML or XML	True	False	http://scgap.systemsbiology.net/standards/mage_miame.php																	
B2AI_STANDARD:201	B2AI_STANDARD:BiomedicalStandard	MCL	Microbiological Common Language	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:1]	MCL is a data exchange standard for microbiological information. In short, MCL defines terms which can be used to reference and describe microorganisms. It is designed to form a simple and generic framework leveraging the electronical exchange of information about microorganisms. MCL is loosely coupled from its actual representation technologies and is currently used to structure XML and RDF files (see examples).	True	False	https://doi.org/10.1016/j.resmic.2010.02.005											doi:10.1016/j.resmic.2010.02.005						
B2AI_STANDARD:202	B2AI_STANDARD:BiomedicalStandard	WFDB Format	MIMIC Waveform Database Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat|implementation_maturity_production]	[B2AI_TOPIC:37]	Format for MIMIC Waveform Database records.	True	False	https://wfdb.io/mimic_wfdb_tutorials/mimic/formatting.html		True		[{\"id\": \"B2AI_APP:39\", \"category\": \"B2AI:Application\", \"name\": \"MIMIC-BP Curated Dataset for Blood Pressure Estimation Benchmarking\", \"description\": \"The Python wfdb package reads MIMIC-III waveform signals directly, converts samples to double precision, and preserves signals exactly as recorded for ML workflows. MIMIC-BP organizes 1,524 patients into fixed 30-second segments (30 segments per patient) at 125 Hz with ABP, ECG, PPG, and RESP waveforms stored as NumPy files, providing per-segment median SBP/DBP labels for supervised learning. Pre-defined per-subject train/validation/test splits prevent data leakage under calibration-free protocols. This WFDB-enabled standardized segmentation, labeling, and splitting supports reproducible benchmarking of deep learning (ResNet, LSTM, Transformer) and classical ML (SVR, Random Forest) models for cuff-less blood pressure estimation across approximately 380 hours of ICU data.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1038/s41597-024-04041-1\"]}, {\"id\": \"B2AI_APP:142\", \"category\": \"B2AI:Application\", \"name\": \"PulseDB Large-Scale BP Estimation Corpus Using WFDB Toolbox\", \"description\": \"The WFDB toolbox retrieves and validates MIMIC-III matched subset records, filtering for simultaneous ECG (lead II), PPG, and ABP channels while removing NaN samples by extracting the longest valid intervals per record (10 seconds to 10 hours). Signals are resampled to 125 Hz and segmented into standardized 10-second windows with beat-level annotations and per-segment SBP/DBP labels, yielding 5.2 million segments. This WFDB-enabled quality control, multi-channel synchronization, and standardized segmentation provides a large benchmarking corpus for training and evaluating ML/DL cuff-less blood pressure estimators with cross-study comparability and reproducible preprocessing pipelines.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.3389/fdgth.2022.1090854\"]}, {\"id\": \"B2AI_APP:143\", \"category\": \"B2AI:Application\", \"name\": \"Intracranial Hypertension Forecasting Using Multi-Scale WFDB Waveforms\", \"description\": \"MIMIC-III WFDB provides high-frequency waveforms (125 Hz) and derived 1 Hz time series for ML early-warning systems. Specific channels (ICP, CPP, ABP variants, HR, PLETH, RESP, ECG) are selected via WFDB, with cohort filtering enforcing minimum 24-hour recording length and maximum 25% per-channel missing values, reducing to 123 usable segments. Preprocessing produces 1-minute blocks with artifact deletion, and supervised labels are derived from WFDB ICP time series (median ICP greater than 20 mmHg for five consecutive minutes defines intracranial hypertension events). This WFDB-based channel selection, quality filtering, segmentation, and labeling enables training of logistic regression and feature-based predictive models for forecasting intracranial hypertension up to 8 hours ahead.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1088/1361-6579/ab6360\"]}, {\"id\": \"B2AI_APP:144\", \"category\": \"B2AI:Application\", \"name\": \"Deep Learning ABP Waveform Imputation from WFDB Multi-Channel Signals\", \"description\": \"MIMIC-III waveform records accessed via WFDB provide synchronized ECG (lead II), PPG, and invasive ABP for training deep learning models to impute continuous arterial blood pressure waveforms from non-invasive signals. Preprocessing includes downsampling to 100 Hz, low-pass filtering (16 Hz cutoff), robust per-window scaling, and cross-correlation-based clock drift correction (up to plus or minus 4 seconds). Sliding 32-second windows with 16-second steps are quality-filtered (variance and peak-count thresholds), and labels are derived from invasive ABP medians over 4-second windows. This WFDB-enabled preprocessing, segmentation, inter-signal alignment, and label derivation supports training U-Net-like segmentation models with transfer learning to external (UCLA) cohorts for generalizable waveform-to-waveform prediction.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1038/s41598-021-94913-y\"]}, {\"id\": \"B2AI_APP:145\", \"category\": \"B2AI:Application\", \"name\": \"Comparative Deep Learning for ABP Prediction from WFDB Physiological Signals\", \"description\": \"MIMIC waveform records provide co-recorded ECG (lead V), PPG, and ABP for comparative evaluation of deep learning architectures including ResNet, WaveNet, and LSTM for blood pressure prediction. WFDB-sourced data undergoes artifact detection and removal (flat lines, missing heartbeats, negative BP values), signal filtering (8th-order Chebyshev bandpass for ECG at 2-59 Hz), and segmentation into 10-minute recordings. Target ABP is normalized with physiological bounds (15-300 mmHg) for model training using RMSE and Huber loss, requiring de-normalization for evaluation. This WFDB-based data selection, preprocessing, segmentation, and target normalization pipeline supports systematic comparison of CNN and RNN architectures for continuous blood pressure estimation from ICU waveforms.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1007/s12559-021-09910-0\"]}]	[MIMIC-BP Curated Dataset for Blood Pressure Estimation Benchmarking|PulseDB Large-Scale BP Estimation Corpus Using WFDB Toolbox|Intracranial Hypertension Forecasting Using Multi-Scale WFDB Waveforms|Deep Learning ABP Waveform Imputation from WFDB Multi-Channel Signals|Comparative Deep Learning for ABP Prediction from WFDB Physiological Signals]	[B2AI_APP:39|B2AI_APP:142|B2AI_APP:143|B2AI_APP:144|B2AI_APP:145]	[['https://doi.org/10.1038/s41597-024-04041-1']|['https://doi.org/10.3389/fdgth.2022.1090854']|['https://doi.org/10.1088/1361-6579/ab6360']|['https://doi.org/10.1038/s41598-021-94913-y']|['https://doi.org/10.1007/s12559-021-09910-0']]	[The Python wfdb package reads MIMIC-III waveform signals directly, converts samples to double precision, and preserves signals exactly as recorded for ML workflows. MIMIC-BP organizes 1,524 patients into fixed 30-second segments (30 segments per patient) at 125 Hz with ABP, ECG, PPG, and RESP waveforms stored as NumPy files, providing per-segment median SBP/DBP labels for supervised learning. Pre-defined per-subject train/validation/test splits prevent data leakage under calibration-free protocols. This WFDB-enabled standardized segmentation, labeling, and splitting supports reproducible benchmarking of deep learning (ResNet, LSTM, Transformer) and classical ML (SVR, Random Forest) models for cuff-less blood pressure estimation across approximately 380 hours of ICU data.|The WFDB toolbox retrieves and validates MIMIC-III matched subset records, filtering for simultaneous ECG (lead II), PPG, and ABP channels while removing NaN samples by extracting the longest valid intervals per record (10 seconds to 10 hours). Signals are resampled to 125 Hz and segmented into standardized 10-second windows with beat-level annotations and per-segment SBP/DBP labels, yielding 5.2 million segments. This WFDB-enabled quality control, multi-channel synchronization, and standardized segmentation provides a large benchmarking corpus for training and evaluating ML/DL cuff-less blood pressure estimators with cross-study comparability and reproducible preprocessing pipelines.|MIMIC-III WFDB provides high-frequency waveforms (125 Hz) and derived 1 Hz time series for ML early-warning systems. Specific channels (ICP, CPP, ABP variants, HR, PLETH, RESP, ECG) are selected via WFDB, with cohort filtering enforcing minimum 24-hour recording length and maximum 25% per-channel missing values, reducing to 123 usable segments. Preprocessing produces 1-minute blocks with artifact deletion, and supervised labels are derived from WFDB ICP time series (median ICP greater than 20 mmHg for five consecutive minutes defines intracranial hypertension events). This WFDB-based channel selection, quality filtering, segmentation, and labeling enables training of logistic regression and feature-based predictive models for forecasting intracranial hypertension up to 8 hours ahead.|MIMIC-III waveform records accessed via WFDB provide synchronized ECG (lead II), PPG, and invasive ABP for training deep learning models to impute continuous arterial blood pressure waveforms from non-invasive signals. Preprocessing includes downsampling to 100 Hz, low-pass filtering (16 Hz cutoff), robust per-window scaling, and cross-correlation-based clock drift correction (up to plus or minus 4 seconds). Sliding 32-second windows with 16-second steps are quality-filtered (variance and peak-count thresholds), and labels are derived from invasive ABP medians over 4-second windows. This WFDB-enabled preprocessing, segmentation, inter-signal alignment, and label derivation supports training U-Net-like segmentation models with transfer learning to external (UCLA) cohorts for generalizable waveform-to-waveform prediction.|MIMIC waveform records provide co-recorded ECG (lead V), PPG, and ABP for comparative evaluation of deep learning architectures including ResNet, WaveNet, and LSTM for blood pressure prediction. WFDB-sourced data undergoes artifact detection and removal (flat lines, missing heartbeats, negative BP values), signal filtering (8th-order Chebyshev bandpass for ECG at 2-59 Hz), and segmentation into 10-minute recordings. Target ABP is normalized with physiological bounds (15-300 mmHg) for model training using RMSE and Huber loss, requiring de-normalization for evaluation. This WFDB-based data selection, preprocessing, segmentation, and target normalization pipeline supports systematic comparison of CNN and RNN architectures for continuous blood pressure estimation from ICU waveforms.]	[B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application]	[False|False|False|False|False]				[B2AI_SUBSTRATE:9]	[B2AI_ORG:57|B2AI_ORG:115]		
B2AI_STANDARD:203	B2AI_STANDARD:BiomedicalStandard	MINSEQE	Minimal Information about a high throughput SEQuencing Experiment	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[minimuminformationschema]	[B2AI_TOPIC:12|B2AI_TOPIC:13]	Information needed to enable the unambiguous interpretation and facilitate reproduction of the results of a high throughput sequencing experiment.	True	False	https://zenodo.org/record/5706412	https://drive.google.com/file/d/1YyvWT02puzMG_UgNmfAEJwVr60-pMvIE/view?usp=sharing																
B2AI_STANDARD:204	B2AI_STANDARD:BiomedicalStandard	MISME	Minimal Information about a Self-Monitoring Experiment	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[minimuminformationschema]	[B2AI_TOPIC:5]	This is a reporting guideline for self-monitoring and quantified-self experiments and their use for research purposes.	False	False	https://doi.org/10.3233/978-1-61499-423-7-79											doi:10.3233/978-1-61499-423-7-79						
B2AI_STANDARD:205	B2AI_STANDARD:BiomedicalStandard	MIRIAM	Minimal Information Required In the Annotation of Models	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[minimuminformationschema]	[B2AI_TOPIC:5]	MIRIAM (Minimum Information Required In the Annotation of biochemical Models) is a set of guidelines and standards developed by the computational systems biology community to ensure consistent annotation and curation of computational models in biology, particularly those encoded in SBML (Systems Biology Markup Language) and CellML formats. MIRIAM defines requirements for model attribution including reference publications, creator information, creation dates, and modification history to ensure proper provenance tracking and reproducibility. The guidelines mandate the use of controlled vocabularies and standardized identifiers from MIRIAM Resources (identifiers.org) to annotate biological entities such as genes, proteins, metabolites, reactions, and pathways with unambiguous references to external databases like UniProt, ChEBI, Gene Ontology, KEGG, and Reactome. MIRIAM specifies the use of RDF (Resource Description Framework) and qualified annotations to encode biological semantics and relationships between model components and biological knowledge. The standard promotes model reusability by requiring clear licensing information (Creative Commons, etc.), encoded parameter units using standardized ontologies, and comprehensive documentation of model assumptions and limitations. MIRIAM compliance is supported by tools including the MIRIAM annotation editor in systems biology software, libSBML annotation functions, and BioModels Database validation workflows. The guidelines have been widely adopted by model repositories (BioModels, CellML Model Repository), enhancing model discoverability, integration into larger modeling frameworks, and facilitating quantitative comparison of alternative models representing the same biological system.	True	False	http://co.mbine.org/standards/miriam			[B2AI_ORG:19]														
B2AI_STANDARD:206	B2AI_STANDARD:BiomedicalStandard	MIFlowCyt	Minimum Information about a Flow Cytometry Experiment	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[minimuminformationschema]	[B2AI_TOPIC:2]	The fundamental tenet of scientific research is that the published results of any study have to be open to independent validation or refutation. The Minimum Information about a Flow Cytometry Experiment (MIFlowCyt) establishes criteria for recording and reporting information about the flow cytometry experiment overview, samples, instrumentation and data analysis. It promotes consistent annotation of clinical, biological and technical issues surrounding a flow cytometry experiment by specifying the requirements for data content and by providing a structured framework for capturing information.	True	False	https://isac-net.org/page/MIFlowCyt											doi:10.1002/cyto.a.20623						
B2AI_STANDARD:207	B2AI_STANDARD:BiomedicalStandard	MIAPE	Minimum Information About a Proteomics Experiment	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[minimuminformationschema]	[B2AI_TOPIC:28]	Information from whole proteomics experiments; where samples came from, and how analyses of them were performed.	True	False	https://www.psidev.info/miape			[B2AI_ORG:41]														
B2AI_STANDARD:208	B2AI_STANDARD:BiomedicalStandard	MIARE	Minimum Information About a RNAi Experiment	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[minimuminformationschema]	[B2AI_TOPIC:33]	Minimum Information About an RNAi Experiment (MIARE) is a set of reporting guidelines that describes the minimum information that should be reported about an RNAi experiment to enable the unambiguous interpretation and reproduction of the results. MIARE forms part of a larger effort to develop RNAi data standards that include a data model, data exchange format, controlled vocabulary and supporting software tools.	True	False	http://miare.sourceforge.net/HomePage																	
B2AI_STANDARD:209	B2AI_STANDARD:BiomedicalStandard	MIxS	Minimum information about any sequence	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[minimuminformationschema]	[B2AI_TOPIC:5]	The Minimum Information about any (x) Sequence (MIxS) is a unified standard developed by the Genomic Standards Consortium (GSC) for describing sequence data from diverse environments and organisms. MIxS provides a single point of entry for the scientific community to access and use GSC checklists, which include 11 distinct sequence types covering bacterial/archaeal genomes (MigsBa), eukaryotes (MigsEu), organelles (MigsOrg), plasmids (MigsPl), viruses (MigsVi), marker sequences (MimarksC/S), metagenomes (Mims), metagenome-assembled genomes (Mimag), single amplified genomes (Misag), and uncultivated virus genomes (Miuvig). The standard is complemented by 21 environmental extensions tailored to specific sampling contexts including agriculture, air, built environment, food production, host-associated, human-associated (with specific gut, oral, skin, and vaginal extensions), hydrocarbon resources, microbial mat/biofilm, plant-associated, sediment, soil, symbiont-associated, wastewater/sludge, and water environments. The specification is maintained as a YAML-formatted LinkML schema serving as the authoritative source for generating downstream GSC artifacts.	True	False	https://genomicsstandardsconsortium.github.io/mixs/	https://github.com/GenomicsStandardsConsortium/mixs/		[B2AI_ORG:38]								doi:10.1038/nbt.1823						
B2AI_STANDARD:210	B2AI_STANDARD:BiomedicalStandard	MIAPPE	Minimum Information About Plant Phenotyping Experiments	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[minimuminformationschema]	[B2AI_TOPIC:5]	A reporting guideline for plant phenotyping experiments. Comprises a checklist, i.e., a list of attributes that may be necessary to fully describe an experiment so that it is understandable and replicable. Should be consulted by people recording and depositing the data. Covers description of the following aspects of plant phenotyping experiment - study, environment, experimental design, sample management, biosource, treatment and phenotype. To read more, please visit http://cropnet.pl/phenotypes	True	False	https://www.miappe.org/																	
B2AI_STANDARD:211	B2AI_STANDARD:BiomedicalStandard	MIASM	Minimum Information About Somatic Mutation	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[minimuminformationschema]	[B2AI_TOPIC:35]	MIASM was developed for the collection of somatic variations to promote standards for annotations of somatic variation data, and to promote data integration with other data resources.	True	False	http://structure.bmc.lu.se/MIASM/miasm.html											doi:10.1002/humu.20832						
B2AI_STANDARD:212	B2AI_STANDARD:BiomedicalStandard	MIQAS	Minimum Information for QTLs and Association Studies	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[minimuminformationschema]	[B2AI_TOPIC:35]	The MIQAS set of rules accompanied with the standardized XML and tab-delimited file formats will serve two goals - to encourage research groups that wish to publish a QTL paper to provide and submit the necessary information that would make meta-analysis possible and to allow easy interchange of data between different QTL and association analysis databases. Databases that implement the standardized XML format will typically write an import and an export filter to read data from and dump data into that an XML file. This is the same approach as used for the exchange of sequences between NCBI, Ensembl and DDBJ at the early stages of the Human Genome Project.	True	False	http://miqas.sourceforge.net/																	
B2AI_STANDARD:213	B2AI_STANDARD:BiomedicalStandard	MIABIS	Minimum information required to initiate collaborations between biobanks	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[minimuminformationschema]	[B2AI_TOPIC:5]	MIABIS represents the minimum information required to initiate collaborations between biobanks and to enable the exchange of biological samples and data. The aim is to facilitate the reuse of bio-resources and associated data by harmonizing biobanking and biomedical research.	True	False	https://doi.org/10.1089/bio.2015.0070											doi:10.1089/bio.2015.0070						
B2AI_STANDARD:214	B2AI_STANDARD:BiomedicalStandard	MIAPE-Quant	Minimum information required to report the use of quantification techniques in a proteomics experiment	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[minimuminformationschema]	[B2AI_TOPIC:28]	This module identifies the minimum information required to report the use of quantification techniques in a proteomics experiment, sufficient to support both the effective interpretation and assessment of the data and the potential recreation of the results of the data analysis.	True	False	https://www.psidev.info/attachments/miape-quant-091-documents			[B2AI_ORG:41]														
B2AI_STANDARD:215	B2AI_STANDARD:BiomedicalStandard	ModelCIF	ModelCIF	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:27]	An extension of the Protein Data Bank Exchange / macromolecular Crystallographic Information Framework (PDBx/mmCIF); provides an extensible data representation for deposition, archiving, and public dissemination of predicted 3D models of proteins.	True	False	http://github.com/ihmwg/ModelCIF	http://github.com/ihmwg/ModelCIF										doi:10.1101/2022.12.06.518550						
B2AI_STANDARD:216	B2AI_STANDARD:BiomedicalStandard	MLHIM	Multilevel Healthcare Information Modeling specifications	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[multimodal]	[B2AI_TOPIC:5]	The Multilevel Healthcare Information Modeling (MLHIM) specifications enables the exchange of syntactically and semantically interoperable data extracts between distributed, independently developed, biomedical databases and clinical applications, promoting syntactic and semantic integration of Translational Research data. The Semantic MedWeb is an implementation of a MLHIM-based database development platform (open source code available at https://github.com/mlhim/SemanticMedWeb)	True	False	https://mlhim-specifications.readthedocs.io/en/master/																	
B2AI_STANDARD:217	B2AI_STANDARD:BiomedicalStandard	MAF	Multiple Alignment Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:23]	The multiple alignment format stores a series of multiple alignments in a format that is easy to parse and relatively easy to read. This format stores multiple alignments at the DNA level between entire genomes. Previously used formats are suitable for multiple alignments of single proteins or regions of DNA without rearrangements, but would require considerable extension to cope with genomic issues such as forward and reverse strand directions, multiple pieces to the alignment, and so forth.	True	False	http://genome.ucsc.edu/FAQ/FAQformat.html#format5			[B2AI_ORG:119]														
B2AI_STANDARD:218	B2AI_STANDARD:BiomedicalStandard	mz5	mz5 format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:5]	The mz5 format is a complete reimplementation of the mzML data model and ontology built on the HDF5 (Hierarchical Data Format 5) storage backend, designed to address performance and scalability limitations of XML-based mass spectrometry formats. mz5 preserves the semantic structure and controlled vocabulary terms of mzML while leveraging HDF5's binary format, chunked storage, and native compression capabilities to achieve significantly faster read/write operations and reduced file sizes compared to mzML. The format organizes mass spectrometry data into hierarchical HDF5 groups and datasets representing spectra, chromatograms, instrument configurations, and metadata, with support for efficient random access to individual spectra and parallel I/O operations. mz5 maintains compatibility with the Proteomics Standards Initiative controlled vocabularies and supports the same rich metadata and data provenance as mzML, while providing superior performance for large-scale proteomics and metabolomics datasets. The HDF5 foundation enables integration with high-performance computing workflows, supports multiple programming languages (C, Python, Java, R), and facilitates efficient processing of high-resolution and high-throughput mass spectrometry data for downstream analysis pipelines and machine learning applications.	True	False	https://doi.org/10.1074/mcp.O111.011379											doi:10.1074/mcp.O111.011379	[B2AI_STANDARD:339]		[B2AI_SUBSTRATE:16]			
B2AI_STANDARD:219	B2AI_STANDARD:BiomedicalStandard	mzData	mzData format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:28]	mzData is an XML format for representing mass spectrometry data in such a way as to completely describe the instrumental aspects of the experiment. This format is deprecated and has been superseded by mzML.	True	False	https://psidev.info/mass-spectrometry-workgroup#mzdata			[B2AI_ORG:41]														
B2AI_STANDARD:220	B2AI_STANDARD:BiomedicalStandard	mzIndentML	mzIndentML format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:28]	A large number of different proteomics search engines are available that produce output in a variety of different formats. It is intended that mzIdentML will provide a common format for the export of identification results from any search engine. The format was originally developed under the name AnalysisXML as a format for several types of computational analyses performed over mass spectra in the proteomics context.	True	False	https://www.psidev.info/mzidentml			[B2AI_ORG:41]														
B2AI_STANDARD:221	B2AI_STANDARD:BiomedicalStandard	mzML	mzML format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:28]	From 2005-2008 there has existed two separate XML formats for encoding raw spectrometer output, mzData developed by the PSI and mzXML developed at the Seattle Proteome Center at the Institute for Systems Biology. It was recognized that the existence of two separate formats for essentially the same thing generated confusion and required extra programming effort. Therefore the PSI, with full participation by ISB, has developed a new format by taking the best aspects of each of the precursor formats to form a single one. It is intended to replace the previous two formats. This new format was originally given a working name of dataXML. The final name is mzML.	True	False	https://psidev.info/mzML	http://www.peptideatlas.org/tmp/mzML1.1.0.html		[B2AI_ORG:41]								doi:10.1007/978-1-60761-444-9_22						
B2AI_STANDARD:222	B2AI_STANDARD:BiomedicalStandard	mzQuantML	mzQuantML format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:28]	The mzQuantML standard format is intended to store the systematic description of workflows quantifying molecules (principly peptides and proteins) by mass spectrometry. A large number of different software packages are available that produce output in a variety of different formats. It is intended that mzQuantML will provide a common format for the export of identification results from any software package. The format was originally developed under the name AnalysisXML as a format for several types of computational analyses performed over mass spectra in the proteomics context. It has been decided to split development into two formats, mzIdentML for peptide and protein identification and mzQuantML (described here), covering quantitative proteomic data derived from MS. The development of mzQuantML is driven by some general principles, specific use cases and the goal of supporting specific techniques, as listed below. These were discussed and agreed at the development meeting in Tubingen in July 2011.	True	False	https://www.psidev.info/mzquantml			[B2AI_ORG:41]														
B2AI_STANDARD:223	B2AI_STANDARD:BiomedicalStandard	mzXML	mzXML format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[deprecated|fileformat]	[B2AI_TOPIC:28]	mzXML was a pioneering open, generic XML (extensible markup language) representation specifically developed for mass spectrometry (MS) data exchange and storage in proteomics research. As one of the first standardized formats for MS data, mzXML provided a vendor-neutral way to represent complex mass spectrometry information including mass-to-charge ratios, intensity values, scan parameters, and instrument metadata in a structured, machine-readable format. The format supported both profile and centroid data representation modes and could accommodate various types of MS experiments including MS/MS fragmentation spectra. Despite its historical significance in establishing data standardization practices in proteomics, mzXML is now deprecated and has been superseded by the more comprehensive mzML format, which offers enhanced features, better compression, controlled vocabularies, and broader community support. While legacy systems may still encounter mzXML files, current best practices recommend migration to mzML for new data processing workflows and long-term data preservation in mass spectrometry applications.	True	False	https://doi.org/10.1038/nbt1031											doi:10.1038/nbt1031						
B2AI_STANDARD:224	B2AI_STANDARD:BiomedicalStandard	NCI EVS	National Cancer Institute Enterprise Vocabulary Service	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Terminology content, tools, and services to accurately code, analyze and share cancer and biomedical research, clinical and public health information. Includes NCI Thesaurus and Metathesaurus.	True	True	https://evs.nci.nih.gov/			[B2AI_ORG:71]														
B2AI_STANDARD:225	B2AI_STANDARD:BiomedicalStandard	NEMSIS	National Emergency Medical Services Information System	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:5]	Standard for the collection and transmission of emergency medical services (EMS) operations and patient care data.	True	False	https://nemsis.org/technical-resources/															[B2AI_ORG:66]		
B2AI_STANDARD:226	B2AI_STANDARD:BiomedicalStandard	PCORNet CDM	National Patient-Centered Clinical Research Network Common Data Model	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4]	This guiding principle is expressed in the CDM design through prioritization of analytic functionality, and a parsimonious approach based upon analytic utility. At times, this results in decisions that are not based in relational database modeling principles such as normalization. The model is designed to facilitate routine and rapid execution of distributed complex analytics. To meet this design requirement, some fields are duplicated across multiple tables to support faster analytic operations for distributed querying. The PCORnet CDM is based on the FDA Mini-Sentinel CDM. This allows PCORnet to more easily leverage the large array of analytic tools and expertise developed for the MSCDM v4.0, including data characterization approaches and the various tools for complex distributed analytics.	True	False	http://pcornet.org/pcornet-common-data-model/	https://pcornet.org/wp-content/uploads/2022/01/PCORnet-Common-Data-Model-v60-2020_10_221.pdf		[B2AI_ORG:81]														
B2AI_STANDARD:227	B2AI_STANDARD:BiomedicalStandard	NCD	Natural Collections Description standard	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:1]	Natural Collections Description (NCD) (A data standard for exchanging data describing natural history collections) is a proposed data standard for describing collections of natural history materials at the collection level; one NCD record describes one entire collection. Collection descriptions are electronic records that document the holdings of an organisation as groups of items, which complement the more traditional item-level records such as are produced for a single specimen or a library book. NCD is tailored to natural history. It lies between general resource discovery standards such as Dublin Core (DC) and rich collection description standards such as the Encoded Archival Description (EAD). The NCD standard covers all types of natural history collections, such as specimens, original artwork, archives, observations, library materials, datasets, photographs or mixed collections such as those that result from expeditions and voyages of discovery.	True	False	https://www.tdwg.org/standards/ncd/			[B2AI_ORG:93]														
B2AI_STANDARD:228	B2AI_STANDARD:BiomedicalStandard	BioProject Schema	NCBI BioProject XML Schema	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datamodel]	[B2AI_TOPIC:1]	This is a XML Schema specification of BioProject data. A BioProject is a collection of biological data related to a single initiative, originating from a single organization or from a consortium. A BioProject record provides users a single place to find links to the diverse data types generated for that project.	True	False	https://www.ncbi.nlm.nih.gov/data_specs/schema/other/bioproject/	https://www.ncbi.nlm.nih.gov/data_specs/schema/other/bioproject/			[{\"id\": \"B2AI_APP:40\", \"category\": \"B2AI:Application\", \"name\": \"Research Project Metadata Mining and Dataset Discovery\", \"description\": \"BioProject schema is used in AI applications for mining metadata about biological research projects, enabling automated discovery of relevant datasets, prediction of experimental outcomes, and analysis of research trends. Machine learning systems parse BioProject records to identify related studies for meta-analyses, recommend similar projects to researchers, and predict which experimental approaches are likely to succeed based on project descriptions. AI applications leverage structured project metadata to train models that classify research projects by methodology, extract experimental design features, and identify collaborative networks in biological research. The schema enables large-scale scientometric analyses and AI-driven research prioritization based on historical project outcomes and resource allocation.\", \"used_in_bridge2ai\": false}]	[Research Project Metadata Mining and Dataset Discovery]	[B2AI_APP:40]		[BioProject schema is used in AI applications for mining metadata about biological research projects, enabling automated discovery of relevant datasets, prediction of experimental outcomes, and analysis of research trends. Machine learning systems parse BioProject records to identify related studies for meta-analyses, recommend similar projects to researchers, and predict which experimental approaches are likely to succeed based on project descriptions. AI applications leverage structured project metadata to train models that classify research projects by methodology, extract experimental design features, and identify collaborative networks in biological research. The schema enables large-scale scientometric analyses and AI-driven research prioritization based on historical project outcomes and resource allocation.]	[B2AI:Application]	[False]	doi:10.1093/nar/gkr1163				[B2AI_ORG:74]		
B2AI_STANDARD:229	B2AI_STANDARD:BiomedicalStandard	NCPDP F&B	NCPDP Formulary and Benefit Standard	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	The NCPDP Formulary and Benefit Standard provides a standardized means for pharmacy benefit payers, including health plans and Pharmacy Benefit Managers (PBMs), to electronically communicate formulary and benefit information to prescribers via technology vendor systems. This standard enables real-time access to critical medication coverage information at the point of prescribing, including patient eligibility verification, product coverage status, benefit financial details (copayments, deductibles, out-of-pocket costs), coverage restrictions such as prior authorization requirements, step therapy protocols, quantity limits, and therapeutic alternatives when restrictions exist. By standardizing this data exchange, the standard reduces prescription abandonment, improves medication adherence, decreases prior authorization processing time, and enhances clinical decision-making by providing prescribers with transparent, actionable benefit information. The standard supports integration with Electronic Health Record (EHR) systems and e-prescribing platforms in accordance with HIPAA, MMA, HITECH, and Meaningful Use requirements, ultimately contributing to reduced healthcare costs and improved patient safety through more informed prescribing decisions.	False	True	https://standards.ncpdp.org/Access-to-Standards.aspx			[B2AI_ORG:63]														
B2AI_STANDARD:230	B2AI_STANDARD:BiomedicalStandard	NESTcc DQF	NESTcc Data Quality Framework	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:4]	Guiding principles and a foundation for the capture and use of high-quality data for post-market evaluation of medical devices	True	True	https://nestcc.org/data-quality-and-methods/	https://mdic.org/wp-content/uploads/2020/02/NESTcc-Data-Quality-Framework.pdf		[B2AI_ORG:64]														
B2AI_STANDARD:231	B2AI_STANDARD:BiomedicalStandard	NWB	Neurodata Without Borders	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Neurodata Without Borders (NWB) is a comprehensive data standard and software ecosystem for neurophysiology and behavioral data, developed by a collaborative team of neuroscientists and software developers to break down barriers to data sharing in neuroscience. NWB supports diverse neurophysiology data types including intracellular and extracellular electrophysiology (single-unit recordings, local field potentials, patch-clamp), optical physiology (calcium imaging, two-photon microscopy, optogenetics), behavioral tracking, stimulus presentation metadata, and experimental trial structures. The format is built on HDF5, providing efficient storage and access to large-scale datasets while maintaining human-readable metadata. NWB's extensibility mechanism through neurodata extensions allows researchers to adapt the standard for novel recording modalities and experimental paradigms without breaking compatibility. The ecosystem includes PyNWB and MatNWB APIs for data creation and access, validation tools, and integration with the DANDI Archive for public data sharing. NWB adoption is growing across major neuroscience initiatives with support from Allen Institute, HHMI, Kavli Foundation, Simons Foundation, and INCF, facilitating reproducible research, cross-laboratory data integration, and development of standardized analysis pipelines across cellular, systems, and computational neuroscience.	True	False	https://www.nwb.org/	https://github.com/NeurodataWithoutBorders										doi:10.1101/523035						
B2AI_STANDARD:232	B2AI_STANDARD:BiomedicalStandard	NIDM	Neuroimaging Data Model	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datamodel]	[B2AI_TOPIC:22]	The Neuroimaging Data Model (NIDM) is a collection of specification documents and examples that outline a domain specific extension to the W3C Provenance Data Model (PROV-DM) for the exchange and sharing of human brain imaging data. The goal of the data model is to capture data, information about the data and processes that generated the data (i.e. provenance). This information can be converted to RDF and therefore queried using SPARQL. This representation allows machine accessible representations of brain imaging data and will provide links to related resources such as publications, virtual machines, people and funding agencies.	True	False	http://nidm.nidash.org/				[{\"id\": \"B2AI_APP:41\", \"category\": \"B2AI:Application\", \"name\": \"Neuroimaging Results Sharing and Meta-Analytic AI\", \"description\": \"NIDM (NeuroImaging Data Model) is used in AI applications for sharing and aggregating neuroimaging analysis results across studies, enabling meta-analytic machine learning and improving reproducibility in computational neuroscience. AI systems leverage NIDM's semantic representation of imaging workflows, statistical maps, and analysis provenance to train models that learn from aggregated results rather than raw images, respecting data sharing constraints while enabling large-scale analyses. The model supports AI applications that perform automated quality assessment of imaging studies, detect inconsistencies in reported results, and synthesize findings across diverse analysis pipelines. NIDM enables machine learning systems to understand the complete analytical context of neuroimaging results, improving reproducibility and meta-analytic power.\", \"used_in_bridge2ai\": false}]	[Neuroimaging Results Sharing and Meta-Analytic AI]	[B2AI_APP:41]		[NIDM (NeuroImaging Data Model) is used in AI applications for sharing and aggregating neuroimaging analysis results across studies, enabling meta-analytic machine learning and improving reproducibility in computational neuroscience. AI systems leverage NIDM's semantic representation of imaging workflows, statistical maps, and analysis provenance to train models that learn from aggregated results rather than raw images, respecting data sharing constraints while enabling large-scale analyses. The model supports AI applications that perform automated quality assessment of imaging studies, detect inconsistencies in reported results, and synthesize findings across diverse analysis pipelines. NIDM enables machine learning systems to understand the complete analytical context of neuroimaging results, improving reproducibility and meta-analytic power.]	[B2AI:Application]	[False]					[B2AI_ORG:99]		
B2AI_STANDARD:233	B2AI_STANDARD:BiomedicalStandard	NIFTI	Neuroimaging Informatics Technology Initiative file format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:22]	The Neuroimaging Informatics Technology Initiative (nifti) file format was envisioned about a decade ago as a replacement to the then widespread, yet problematic, analyze 7.5 file format. The main problem with the previous format was perhaps the lack of adequate information about orientation in space, such that the stored data could not be unambiguously interpreted. Although the file was used by many different imaging software, the lack of adequate information on orientation obliged some, most notably spm, to include, for every analyze file, an accompanying file describing the orientation, such as a file with extension .mat.	True	False	https://nifti.nimh.nih.gov/	https://nifti.nimh.nih.gov/nifti-2																
B2AI_STANDARD:234	B2AI_STANDARD:BiomedicalStandard	NeuroML	NeuroML	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:5]	NeuroML is a model description language developed in XML (extensible Markup Language) that was created to facilitate data archiving, data and model exchange, database creation, and model publication in the neurosciences. One of the goals of the NeuroML project is to develop standards for model specification that will allow for greater simulator interoperability and model exchange.	True	False	https://neuroml.org/																	
B2AI_STANDARD:235	B2AI_STANDARD:BiomedicalStandard	NDF	Neurophysiology Data Translation Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:48]	The purpose of the Neurophysiology Data Translation Format (NDF) is to provide a means of sharing neurophysiology experimental data and derived data between services and tools developed within the CARMEN project (www.carmen.org.uk). This document specifes the NDF. The specification supports the types of data that are currently used by members of the CARMEN consortium and provides a capability to support future data types. It is capable of accommodating external data file formats as well as metadata such as user defined experimental descriptions and the history (provenance) of derived data.	True	False	https://doi.org/10.3389/conf.fnins.2010.13.00118											doi:10.3389/conf.fnins.2010.13.00118				[B2AI_ORG:11]		
B2AI_STANDARD:236	B2AI_STANDARD:BiomedicalStandard	NHX	New Hampshire eXtended Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12|B2AI_TOPIC:26]	NHX is based on the New Hampshire (NH) standard (also called Newick tree format).	True	False	http://www.phylosoft.org/NHX/																	
B2AI_STANDARD:237	B2AI_STANDARD:BiomedicalStandard	Newick	Newick tree Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12|B2AI_TOPIC:26]	The Newick Standard for representing trees in computer-readable form makes use of the correspondence between trees and nested parentheses, noticed in 1857 by the famous English mathematician Arthur Cayley.	True	False	http://evolution.genetics.washington.edu/phylip/newicktree.html																	
B2AI_STANDARD:238	B2AI_STANDARD:BiomedicalStandard	NeML	NeXML format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12|B2AI_TOPIC:25|B2AI_TOPIC:26]	To facilitate interoperability in evolutionary comparative analysis, we present NeXML, an XML standard (inspired by the current standard, NEXUS) that supports exchange of richly annotated comparative data. NeXML defines syntax for operational taxonomic units, character-state matrices, and phylogenetic trees and networks. Documents can be validated unambiguously. Importantly, any data element can be annotated, to an arbitrary degree of richness, using a system that is both flexible and rigorous. We describe how the use of NeXML by the TreeBASE and Phenoscape projects satisfies user needs that cannot be satisfied with other available file formats	True	False	https://github.com/nexml/nexml	https://github.com/nexml/nexml																
B2AI_STANDARD:239	B2AI_STANDARD:BiomedicalStandard	nib	Nibble sequence format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12]	The nibble (.nib) format is a compact binary file format for storing DNA sequences, originally developed for the UCSC Genome Browser and BLAT alignment tool. The format achieves 4-fold compression by packing two nucleotide bases per byte using 2-bit encoding (A=00, C=01, G=10, T=11), significantly reducing storage requirements compared to text-based FASTA format. Each .nib file contains a single sequence record with a simple header specifying sequence length and format version, followed by the packed sequence data. The format stores sequences in a form optimized for rapid access by genome browsers and alignment algorithms, supporting efficient memory mapping for large-scale genomic analyses. While .nib files provide space-efficient storage, they lack the flexibility of indexed formats like 2bit (which can store multiple sequences) and have been largely superseded by more modern compressed formats. The format remains in use for legacy applications and continues to be supported by UCSC Genome Browser utilities including nibFrag for sequence extraction and faToNib/nibToFa for format conversion, primarily for maintaining compatibility with older genome browser implementations and analysis pipelines.	True	False	https://genomebrowser.wustl.edu/goldenPath/help/blatSpec.html																	
B2AI_STANDARD:240	B2AI_STANDARD:BiomedicalStandard	NMR-STAR	NMR Self-defining Text Archive and Retrieval format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:3]	Format and ontology used to represent experiments, spectral and derived data, and supporting metadata.	True	False	https://bmrb.io/standards/			[B2AI_ORG:9]								doi:10.1007/s10858-018-0220-3						
B2AI_STANDARD:241	B2AI_STANDARD:BiomedicalStandard	nmrML	nmrML	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:17]	nmrML is an open mark-up language for NMR data. It is currently under heavy development and is not yet ready for public use. The development of this standard is coordinated by Workpackage 2 of the COSMOS - COordination Of Standards In MetabOlomicS Project. COSMOS is a global effort to enable free and open sharing of metabolomics data. Coordinated by Dr Christoph Steinbeck of the EMBL-European Bioinformatics Institute, COSMOS brings together European data providers to set and promote community standards that will make it easier to disseminate metabolomics data through life science e-infrastructures. This Coordination Action has been financed with 2 million by the European Commission's Seventh Framework Programme. The nmrML data standard will be approved by the Metabolomics Standards Initiative and was derived from an earlier nmrML that was developed by the Metabolomics Innovation Centre (TMIC).	True	False	https://nmrml.org/															[B2AI_ORG:21]		
B2AI_STANDARD:242	B2AI_STANDARD:BiomedicalStandard	Observ-Tab	Observ-Tab format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:25]	Observ-Tab is a simple spreadsheet format to represent and exchange phenotype data.	True	False	https://doi.org/10.1002/humu.22070											doi:10.1002/humu.22070						
B2AI_STANDARD:243	B2AI_STANDARD:BiomedicalStandard	OMOP CDM	Observational Medical Outcomes Partnership Common Data Model	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datamodel|standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:4]	The Observational Medical Outcomes Partnership (OMOP) Common Data Model is an open community data standard designed to standardize the structure and content of observational healthcare data from electronic health records, administrative claims, registries, and other sources to enable efficient, large-scale analyses that produce reliable evidence. OMOP CDM organizes patient-level data into standardized tables including person, visit, condition occurrence, drug exposure, procedure occurrence, measurement, observation, and device exposure, with relationships linking events to patients and care episodes. The model uses standardized vocabularies (SNOMED CT, RxNorm, LOINC, etc.) mapped through the OHDSI Vocabulary for semantic interoperability, ensuring consistent representation of clinical concepts across diverse healthcare systems. OMOP CDM supports longitudinal patient histories, enables reproducible network studies where the same analytic code runs on data from multiple institutions without sharing patient-level information, and provides the foundation for the OHDSI open-science community's analytical tools including cohort definition, characterization, population-level effect estimation, and patient-level prediction. The standardized schema facilitates federated learning, multi-site clinical research, comparative effectiveness studies, pharmacovigilance, and machine learning applications requiring harmonized features across heterogeneous EHR systems for training predictive models, phenotyping algorithms, and decision support tools.	True	False	https://ohdsi.github.io/CommonDataModel/	https://github.com/OHDSI/CommonDataModel	True	[B2AI_ORG:76]	[{\"id\": \"B2AI_APP:42\", \"category\": \"B2AI:Application\", \"name\": \"Multi-Database Patient-Level Risk Prediction with External Validation\", \"description\": \"OMOP-standardized features and the OHDSI Patient-Level Prediction (PLP) pipeline enabled development and extensive external validation of clinical risk models across international sites. A Lasso logistic regression model predicting symptomatic hemorrhagic transformation after ischemic stroke was developed on OMOP-mapped EHR and externally validated across 10 databases spanning the US, Europe, and Asia (internal AUC 0.75; mean external AUC approximately 0.71, range 0.60-0.78). OMOP's standardized covariate definitions and PLP tooling enabled identical feature extraction and cross-site code portability for reproducible, multi-database risk modeling.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1371/journal.pone.0226718\"]}, {\"id\": \"B2AI_APP:146\", \"category\": \"B2AI:Application\", \"name\": \"Federated Diabetes Heart Failure Risk Modeling\", \"description\": \"OMOP CDM enabled federated patient-level prediction for 1-year incident heart failure risk in type 2 diabetes patients across five US databases. Shared cohort and covariate definitions in OMOP, combined with OHDSI tools, enabled reproducible distributed model development and validation using multiple classifiers (Lasso logistic regression, Random Forest, Gradient Boosting, XGBoost), achieving external AUCs of approximately 0.72-0.80 across validation sites. OMOP's standardization facilitated consistent model portability without patient-level data sharing.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1371/journal.pone.0226718\"]}, {\"id\": \"B2AI_APP:147\", \"category\": \"B2AI:Application\", \"name\": \"Hospital Length-of-Stay Prediction with Explainable AI\", \"description\": \"OMOP v5.3-standardized features from condition, drug, procedure, and measurement tables supported operational length-of-stay prediction for planned hospital admissions using gradient-boosting methods (XGBoost, LightGBM) with SHAP explainability. A single-site OMOP implementation (South Korea) achieved internal AUROC up to 0.891, with external validation at a separate OMOP-mapped hospital yielding AUROC approximately 0.804. OMOP's standardized feature representation allowed reproducible training and external testing for operational forecasting.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1101/2024.08.23.24311950\"]}, {\"id\": \"B2AI_APP:148\", \"category\": \"B2AI:Application\", \"name\": \"Portable NLP-Based Clinical Phenotyping\", \"description\": \"A portable NLP phenotyping system stored NLP outputs and rule artifacts in OMOP tables (notes and annotation mapping) to enable cross-institutional reuse. The system combined rule-based extractions with statistical machine learning classifiers for phenotype identification (e.g., obesity and comorbidities) and demonstrated competitive performance on i2b2 challenge discharge summaries. OMOP's common schema for text-derived concepts enabled portable NLP pipelines and downstream machine learning for cohort discovery and trial recruitment across multiple sites.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1016/j.jbi.2023.104343\"]}, {\"id\": \"B2AI_APP:149\", \"category\": \"B2AI:Application\", \"name\": \"Clinical Knowledge Graphs for Explainable AI\", \"description\": \"OMOP CDM served as the data source for constructing clinical knowledge graphs in FHIR RDF format (FHIR-Ontop-OMOP) to support explainable AI workflows. OMOP's standardized vocabularies and relational structure enabled consistent semantic linking and query over patient-level data, transforming OMOP tables into RDF knowledge graphs that provide semantic features and enable explainable AI by linking standardized clinical concepts through ontological relationships.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1101/2024.08.23.24311950\"]}, {\"id\": \"B2AI_APP:150\", \"category\": \"B2AI:Application\", \"name\": \"Process Mining of Clinical Workflows\", \"description\": \"Methods to derive event logs from OMOP tables (visit, procedure, measurement) enabled process mining of inpatient, outpatient, and emergency workflows and patient care pathways. Real-world surgical cases at a tertiary hospital were analyzed to construct clinical pathway models, and artificial neural networks were demonstrated for pathway variance prediction. OMOP CDM provided a reproducible source for process-aware analytics and downstream predictive tasks from standardized event sequences.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1101/2024.08.23.24311950\"]}, {\"id\": \"B2AI_APP:151\", \"category\": \"B2AI:Application\", \"name\": \"Imaging AI Enablement via MI-CDM Extension\", \"description\": \"The Medical Imaging CDM (MI-CDM) extends OMOP with imaging metadata and feature-provenance tables to link DICOM data and imaging-derived biomarkers with clinical OMOP data, enabling multimodal phenotyping and imaging AI workflows. A prototype use case demonstrated longitudinal CT lung nodule tracking, and implementations for prostate cancer research (ProCAncer-I) captured imaging metadata and curation processes. MI-CDM makes image-derived features computable within OMOP, supporting reproducible imaging AI pipelines and phenotype definitions that include imaging biomarkers.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1007/s10278-024-00982-6\"]}, {\"id\": \"B2AI_APP:152\", \"category\": \"B2AI:Application\", \"name\": \"Cross-Country Model Generalizability and Feature Selection\", \"description\": \"OMOP-standardized features from EHRs mapped across the US, UK, Finland, and Korea enabled cross-site feature evaluation to improve external validity of prolonged opioid use prediction after surgery. Independent cross-site feature selection workflows using Lasso logistic regression improved generalizability, with local AUROC approximately 0.75 and averaged external AUROC approximately 0.69 after cross-site feature selection. OMOP's consistent feature representation enabled generalizable machine learning across countries and healthcare systems.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1101/2024.08.23.24311950\"]}, {\"id\": \"B2AI_APP:153\", \"category\": \"B2AI:Application\", \"name\": \"Oncology-Specific AI with Genomic and Imaging Vocabularies\", \"description\": \"OMOP oncology extensions incorporating genomic vocabularies (ClinVar, CIVic, OncoKB), HemOnc chemotherapy regimen vocabularies, and radiology CDM (R-CDM) with RadLex-mapped imaging tables enable cancer-specific AI applications. Use cases include case identification from clinical notes using support vector machines and tree-based models, predictive modeling with the ATLAS Patient-Level Prediction module on genomically-enriched OMOP data, and standardized imaging-AI workflows. OMOP's oncology-specific vocabularies and modules facilitate AI model development for precision oncology across harmonized multicenter datasets.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.3390/ijms231911834\", \"https://doi.org/10.1101/2024.08.23.24311950\"]}]	[Multi-Database Patient-Level Risk Prediction with External Validation|Federated Diabetes Heart Failure Risk Modeling|Hospital Length-of-Stay Prediction with Explainable AI|Portable NLP-Based Clinical Phenotyping|Clinical Knowledge Graphs for Explainable AI|Process Mining of Clinical Workflows|Imaging AI Enablement via MI-CDM Extension|Cross-Country Model Generalizability and Feature Selection|Oncology-Specific AI with Genomic and Imaging Vocabularies]	[B2AI_APP:42|B2AI_APP:146|B2AI_APP:147|B2AI_APP:148|B2AI_APP:149|B2AI_APP:150|B2AI_APP:151|B2AI_APP:152|B2AI_APP:153]	[['https://doi.org/10.1371/journal.pone.0226718']|['https://doi.org/10.1371/journal.pone.0226718']|['https://doi.org/10.1101/2024.08.23.24311950']|['https://doi.org/10.1016/j.jbi.2023.104343']|['https://doi.org/10.1101/2024.08.23.24311950']|['https://doi.org/10.1101/2024.08.23.24311950']|['https://doi.org/10.1007/s10278-024-00982-6']|['https://doi.org/10.1101/2024.08.23.24311950']|['https://doi.org/10.3390/ijms231911834', 'https://doi.org/10.1101/2024.08.23.24311950']]	[OMOP-standardized features and the OHDSI Patient-Level Prediction (PLP) pipeline enabled development and extensive external validation of clinical risk models across international sites. A Lasso logistic regression model predicting symptomatic hemorrhagic transformation after ischemic stroke was developed on OMOP-mapped EHR and externally validated across 10 databases spanning the US, Europe, and Asia (internal AUC 0.75; mean external AUC approximately 0.71, range 0.60-0.78). OMOP's standardized covariate definitions and PLP tooling enabled identical feature extraction and cross-site code portability for reproducible, multi-database risk modeling.|OMOP CDM enabled federated patient-level prediction for 1-year incident heart failure risk in type 2 diabetes patients across five US databases. Shared cohort and covariate definitions in OMOP, combined with OHDSI tools, enabled reproducible distributed model development and validation using multiple classifiers (Lasso logistic regression, Random Forest, Gradient Boosting, XGBoost), achieving external AUCs of approximately 0.72-0.80 across validation sites. OMOP's standardization facilitated consistent model portability without patient-level data sharing.|OMOP v5.3-standardized features from condition, drug, procedure, and measurement tables supported operational length-of-stay prediction for planned hospital admissions using gradient-boosting methods (XGBoost, LightGBM) with SHAP explainability. A single-site OMOP implementation (South Korea) achieved internal AUROC up to 0.891, with external validation at a separate OMOP-mapped hospital yielding AUROC approximately 0.804. OMOP's standardized feature representation allowed reproducible training and external testing for operational forecasting.|A portable NLP phenotyping system stored NLP outputs and rule artifacts in OMOP tables (notes and annotation mapping) to enable cross-institutional reuse. The system combined rule-based extractions with statistical machine learning classifiers for phenotype identification (e.g., obesity and comorbidities) and demonstrated competitive performance on i2b2 challenge discharge summaries. OMOP's common schema for text-derived concepts enabled portable NLP pipelines and downstream machine learning for cohort discovery and trial recruitment across multiple sites.|OMOP CDM served as the data source for constructing clinical knowledge graphs in FHIR RDF format (FHIR-Ontop-OMOP) to support explainable AI workflows. OMOP's standardized vocabularies and relational structure enabled consistent semantic linking and query over patient-level data, transforming OMOP tables into RDF knowledge graphs that provide semantic features and enable explainable AI by linking standardized clinical concepts through ontological relationships.|Methods to derive event logs from OMOP tables (visit, procedure, measurement) enabled process mining of inpatient, outpatient, and emergency workflows and patient care pathways. Real-world surgical cases at a tertiary hospital were analyzed to construct clinical pathway models, and artificial neural networks were demonstrated for pathway variance prediction. OMOP CDM provided a reproducible source for process-aware analytics and downstream predictive tasks from standardized event sequences.|The Medical Imaging CDM (MI-CDM) extends OMOP with imaging metadata and feature-provenance tables to link DICOM data and imaging-derived biomarkers with clinical OMOP data, enabling multimodal phenotyping and imaging AI workflows. A prototype use case demonstrated longitudinal CT lung nodule tracking, and implementations for prostate cancer research (ProCAncer-I) captured imaging metadata and curation processes. MI-CDM makes image-derived features computable within OMOP, supporting reproducible imaging AI pipelines and phenotype definitions that include imaging biomarkers.|OMOP-standardized features from EHRs mapped across the US, UK, Finland, and Korea enabled cross-site feature evaluation to improve external validity of prolonged opioid use prediction after surgery. Independent cross-site feature selection workflows using Lasso logistic regression improved generalizability, with local AUROC approximately 0.75 and averaged external AUROC approximately 0.69 after cross-site feature selection. OMOP's consistent feature representation enabled generalizable machine learning across countries and healthcare systems.|OMOP oncology extensions incorporating genomic vocabularies (ClinVar, CIVic, OncoKB), HemOnc chemotherapy regimen vocabularies, and radiology CDM (R-CDM) with RadLex-mapped imaging tables enable cancer-specific AI applications. Use cases include case identification from clinical notes using support vector machines and tree-based models, predictive modeling with the ATLAS Patient-Level Prediction module on genomically-enriched OMOP data, and standardized imaging-AI workflows. OMOP's oncology-specific vocabularies and modules facilitate AI model development for precision oncology across harmonized multicenter datasets.]	[B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application]	[False|False|False|False|False|False|False|False|False]	doi:10.3233/978-1-61499-564-7-574	[B2AI_STANDARD:733|B2AI_STANDARD:692|B2AI_STANDARD:695|B2AI_STANDARD:698|B2AI_STANDARD:703]	[B2AI_STANDARD:844]		[B2AI_ORG:115|B2AI_ORG:114]		
B2AI_STANDARD:244	B2AI_STANDARD:BiomedicalStandard	OMOP CEM	Observational Medical Outcomes Partnership Common Evidence Model	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datamodel]	[B2AI_TOPIC:4]	An improved replacement to the previously reported LAERTES system. One of the initial uses of CEM has been its use in generating lists of negative control concepts to be used in empirical calibration.	True	False	https://github.com/OHDSI/CommonEvidenceModel/wiki/Postprocessing-Negative-Controls	https://github.com/OHDSI/CommonEvidenceModel		[B2AI_ORG:76]	[{\"id\": \"B2AI_APP:43\", \"category\": \"B2AI:Application\", \"name\": \"Causal Inference and Observational Study AI\", \"description\": \"OMOP Common Evidence Model is used in AI applications for standardizing the representation of evidence from observational studies, enabling machine learning models to learn causal relationships from real-world data and generate reliable evidence for treatment effectiveness. AI systems leverage CEM's structured representation of study designs, populations, exposures, and outcomes to train models that estimate treatment effects from observational data, adjust for confounding using propensity score methods, and validate predictions through negative controls. The model enables AI applications in comparative effectiveness research, pharmacovigilance, and evidence synthesis where distinguishing correlation from causation is critical. Machine learning approaches use CEM to automate evidence quality assessment and synthesize findings across heterogeneous observational studies.\", \"used_in_bridge2ai\": false}]	[Causal Inference and Observational Study AI]	[B2AI_APP:43]		[OMOP Common Evidence Model is used in AI applications for standardizing the representation of evidence from observational studies, enabling machine learning models to learn causal relationships from real-world data and generate reliable evidence for treatment effectiveness. AI systems leverage CEM's structured representation of study designs, populations, exposures, and outcomes to train models that estimate treatment effects from observational data, adjust for confounding using propensity score methods, and validate predictions through negative controls. The model enables AI applications in comparative effectiveness research, pharmacovigilance, and evidence synthesis where distinguishing correlation from causation is critical. Machine learning approaches use CEM to automate evidence quality assessment and synthesize findings across heterogeneous observational studies.]	[B2AI:Application]	[False]	doi:10.1007/s40264-014-0189-0		[B2AI_STANDARD:844]				
B2AI_STANDARD:245	B2AI_STANDARD:BiomedicalStandard	OBO	Open Biomedical Ontology Flat File Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:5]	The Open Biomedical Ontology (OBO) Flat File Format is a human-readable, line-oriented syntax for representing biomedical ontologies that originated with the Gene Ontology and has become a widely adopted standard within the OBO Foundry community. OBO format provides a simplified, tag-value structure for defining ontology terms, their hierarchical relationships (is_a), logical definitions, synonyms, cross-references, and metadata, with each term enclosed in a [Term] stanza containing fields like id, name, namespace, def (text definition with citations), and relationship tags. The format has a defined mapping to OWL (Web Ontology Language), allowing OBO files to be converted to OWL/RDF for semantic web applications while maintaining a more accessible syntax for biologists and curators. OBO format supports rich relationship types beyond simple hierarchies (part_of, regulates, develops_from), structured synonym types (exact, broad, narrow, related), obsoletion workflows with replaced_by and consider tags, and cross-references to external databases. The format is designed for version control systems (line-oriented changes), supports modular ontology development through import statements, and enables community-driven collaborative ontology construction. OBO format files are processed by standard tools (ROBOT, Owltools, OWL API with OBO parser) and underpin hundreds of biomedical ontologies including GO, Uberon, ChEBI, Disease Ontology, and Cell Ontology. The format balances human readability for manual curation with machine-parseability for computational workflows, semantic reasoning, data annotation, and integration into knowledge graphs supporting AI/ML applications in biomedicine.	True	False	https://owlcollab.github.io/oboformat/doc/GO.format.obo-1_4.html	https://owlcollab.github.io/oboformat/doc/obo-syntax.html																
B2AI_STANDARD:246	B2AI_STANDARD:BiomedicalStandard	Open mHealth	Open mHealth	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[implementation_maturity_production]	[B2AI_TOPIC:18]	Open mHealth (OMH) is a comprehensive ecosystem of data standards, schemas, and tools designed to make patient-generated health data from mobile devices, wearables, and sensors interoperable, meaningful, and actionable across healthcare and research applications. The OMH data standards provide JSON-based schemas for over 75 common health data types including physical activity (steps, distance, calories burned), vital signs (heart rate, blood pressure, body temperature), sleep metrics, body composition, medications, survey responses, and environmental data. Each schema includes metadata for data provenance (acquisition source, temporal relationship, user notes) enabling proper contextualization and interpretation. OMH tools include Shimmer for integrating data from diverse sources (Fitbit, Apple HealthKit, Google Fit, Withings, iHealth), libraries for data validation and storage, visualization components for detecting health patterns, and FHIR integration capabilities for aligning mobile health data with electronic health records. The platform supports diverse use cases including randomized controlled trials with standardized mobile data collection, remote patient monitoring programs, n-of-1 clinical trial analyses, machine learning algorithm development on normalized health datasets, and patient-reported outcomes integrated with biometric data. OMH is used by over 6,000 developers and health organizations including Cornell Tech, Kaiser Permanente, Stanford Medicine, UCSF, and Copenhagen Center for Health Technology, enabling research reproducibility and clinical care integration by transforming heterogeneous mobile health data into a unified, queryable format.	True	False	https://www.openmhealth.org/	https://github.com/openmhealth	True		[{\"id\": \"B2AI_APP:44\", \"category\": \"B2AI:Application\", \"name\": \"Mobile Health Data Integration for Behavioral AI\", \"description\": \"Open mHealth schemas are used in AI applications for standardizing and integrating data from wearable devices, mobile health apps, and patient-generated health data streams for behavioral pattern recognition, activity classification, and health prediction models. Machine learning systems leverage Open mHealth's JSON-based data schemas to process heterogeneous data from fitness trackers, sleep monitors, medication adherence apps, and symptom tracking tools, enabling AI models for real-time health monitoring, early disease detection, and personalized intervention recommendations. The standardized format facilitates training of deep learning models on multi-modal time-series data including heart rate, physical activity, sleep patterns, and self-reported symptoms, supporting applications in chronic disease management, mental health monitoring, and precision behavioral medicine.\", \"used_in_bridge2ai\": false}]	[Mobile Health Data Integration for Behavioral AI]	[B2AI_APP:44]		[Open mHealth schemas are used in AI applications for standardizing and integrating data from wearable devices, mobile health apps, and patient-generated health data streams for behavioral pattern recognition, activity classification, and health prediction models. Machine learning systems leverage Open mHealth's JSON-based data schemas to process heterogeneous data from fitness trackers, sleep monitors, medication adherence apps, and symptom tracking tools, enabling AI models for real-time health monitoring, early disease detection, and personalized intervention recommendations. The standardized format facilitates training of deep learning models on multi-modal time-series data including heart rate, physical activity, sleep patterns, and self-reported symptoms, supporting applications in chronic disease management, mental health monitoring, and precision behavioral medicine.]	[B2AI:Application]	[False]			[B2AI_STANDARD:845|B2AI_STANDARD:846|B2AI_STANDARD:847|B2AI_STANDARD:850]		[B2AI_ORG:114]		
B2AI_STANDARD:247	B2AI_STANDARD:BiomedicalStandard	OME-TIFF	Open Microscopy Environment TIFF specification	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:19]	OME-TIFF (Open Microscopy Environment TIFF) is a file format specification that combines standard TIFF image files with OME-XML metadata embedded in the TIFF header, enabling storage of complex multi-dimensional microscopy datasets including multi-channel fluorescence, z-stacks, time series, and multi-position acquisitions. Each OME-TIFF file contains one or more standard TIFF images with rich microscopy metadata encoded as OME-XML in the ImageDescription tag of the first IFD (Image File Directory), describing acquisition parameters, instrument settings, channel information, dimensions, physical pixel sizes, timestamps, and experimental context. The format supports large datasets by allowing data to span multiple TIFF files while maintaining metadata consistency through the OME-XML master file that references all constituent files. OME-TIFF balances the advantages of widespread TIFF support in image processing software with the semantic richness of OME-XML metadata, making microscopy data accessible to both OME-aware applications (Bio-Formats, OMERO, ImageJ/Fiji) and standard image viewers. This dual compatibility facilitates data sharing, long-term archiving, interoperability across microscopy platforms, and integration into computational workflows including machine learning pipelines for image analysis, segmentation, and feature extraction in biological imaging applications.	True	False	https://docs.openmicroscopy.org/ome-model/5.6.3/ome-tiff/#			[B2AI_ORG:77]														
B2AI_STANDARD:248	B2AI_STANDARD:BiomedicalStandard	OME-XML	Open Microscopy Environment XML format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:19]	OME-XML (Open Microscopy Environment XML) is a comprehensive XML-based file format for storing both microscopy image pixels and associated metadata using the OME Data Model, a rich schema that describes biological imaging experiments with semantic precision. The format encodes multi-dimensional image data (x, y, z, channel, time) as Base64-encoded binary pixel arrays within XML elements, alongside extensive metadata including instrument configuration (microscope, objectives, detectors, light sources), acquisition parameters (exposure times, wavelengths, filters), experimental context (annotations, regions of interest, overlays), specimen information, and structured annotations. OME-XML uses a controlled vocabulary and hierarchical schema (defined by XSD) that ensures consistent representation of microscopy concepts across diverse imaging modalities including widefield, confocal, super-resolution, high-content screening, and light-sheet microscopy. The format serves as the foundation for OME-TIFF and is the native format for Bio-Formats library, enabling interoperability across proprietary microscope file formats through standardized conversion. OME-XML supports FAIR principles by providing rich, machine-readable metadata essential for data sharing, long-term preservation, reproducible analysis, and integration into computational workflows including image processing pipelines and machine learning applications requiring comprehensive contextual information about imaging experiments.	True	False	https://docs.openmicroscopy.org/ome-model/5.6.3/ome-xml/			[B2AI_ORG:77]								doi:10.1186/gb-2005-6-5-r47						
B2AI_STANDARD:249	B2AI_STANDARD:BiomedicalStandard	OMEX	Open Modeling EXchange format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:27]	OMEX (Open Modeling EXchange format) is a ZIP-based container file format developed by the COMBINE (Computational Modeling in Biology Network) community for bundling all components of a computational modeling and simulation experiment into a single, self-contained, exchangeable archive. An OMEX archive contains model descriptions (SBML, CellML, NeuroML, etc.), simulation experiment specifications (SED-ML for defining simulation protocols, analysis steps, and visualization), associated data files (initial conditions, parameters, experimental observations), metadata (OMEX Metadata describing provenance, authorship, annotations using RDF), and a manifest file that catalogs all contents with their roles and formats. The format ensures reproducibility by packaging model structure, simulation configuration, analysis workflows, and contextual information together, enabling complete recreation of computational experiments across different simulation tools and platforms. OMEX supports FAIR principles for computational models through standardized packaging, facilitates model exchange and reuse across the systems biology and computational physiology communities, enables archiving in model repositories (BioModels, PMR), and provides the foundation for reproducible in silico experiments essential for model validation, parameter estimation, and integration into larger computational workflows including systems biology pipelines and machine learning applications for biological modeling.	True	False	https://github.com/combine-org/combine-specifications/blob/main/specifications/omex.md	http://co.mbine.org/specifications/omex.version-1.pdf		[B2AI_ORG:19]														
B2AI_STANDARD:250	B2AI_STANDARD:BiomedicalStandard	OpenICE	Open-Source Integrated Clinical Environment Standard	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4]	OpenICE is an initiative to create a community implementation of an Integrated Clinical Environment. The initiative encompasses not only software implementation but also an architecture for a wider clinical ecosystem to enable new avenues of clinical research. OpenICE seeks to integrate an inclusive framework of healthcare devices and clinical applications to existing Healthcare IT ecosystems.	True	False	https://www.openice.info/	https://github.com/mdpnp/mdpnp		[B2AI_ORG:54]														
B2AI_STANDARD:251	B2AI_STANDARD:BiomedicalStandard	AOM14	openEHR Archetype Object Model	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datamodel]	[B2AI_TOPIC:9]	This document contains the definitive statement of archetype semantics, in the form of an object model for archetypes. The AOM provides a comprehensive formal specification enabling software to process archetypes independent of their persistent representation (ADL, XML, or other formats). The model defines constraint-based domain entities through hierarchical structures alternating between object and attribute constraints (C_COMPLEX_OBJECT, C_ATTRIBUTE, C_PRIMITIVE_OBJECT classes). It supports archetype specialization with explicit depth tracking, composition through archetype slots with assertion-based constraints, and domain-specific extensions via C_DOMAIN_TYPE classes. The AOM includes comprehensive primitive type constraints (C_DATE, C_TIME, C_STRING, C_INTEGER), reference mechanisms (ARCHETYPE_INTERNAL_REF, CONSTRAINT_REF), and assertion capabilities using first-order predicate logic. It provides the semantic foundation for archetype-enabled kernels, ADL parsers, and archetype validation systems in clinical information modeling, enabling programmatic manipulation of constraint structures and serving as the API specification for archetype-based software applications.	True	False	https://specifications.openehr.org/releases/AM/latest/AOM1.4.html	https://specifications.openehr.org/releases/AM/latest/AOM1.4.html		[B2AI_ORG:79]	[{\"id\": \"B2AI_APP:45\", \"category\": \"B2AI:Application\", \"name\": \"Clinical Archetypes and Semantic Interoperability for AI\", \"description\": \"AOM14 (Archetype Object Model) is used in AI applications for defining reusable clinical information models that enable semantic interoperability of health data across systems and support consistent feature extraction for machine learning. AI systems leverage archetypes to understand the clinical meaning and constraints of health data elements, enabling models to learn from data collected using openEHR archetypes across different implementations. The standardized clinical models support AI applications that require portable feature definitions, enable transfer learning across healthcare systems using archetypes, and ensure that AI models interpret clinical concepts consistently regardless of underlying database structures. Archetypes provide machine-readable clinical semantics that improve AI model interpretability and facilitate automated validation of model inputs.\", \"used_in_bridge2ai\": false}]	[Clinical Archetypes and Semantic Interoperability for AI]	[B2AI_APP:45]		[AOM14 (Archetype Object Model) is used in AI applications for defining reusable clinical information models that enable semantic interoperability of health data across systems and support consistent feature extraction for machine learning. AI systems leverage archetypes to understand the clinical meaning and constraints of health data elements, enabling models to learn from data collected using openEHR archetypes across different implementations. The standardized clinical models support AI applications that require portable feature definitions, enable transfer learning across healthcare systems using archetypes, and ensure that AI models interpret clinical concepts consistently regardless of underlying database structures. Archetypes provide machine-readable clinical semantics that improve AI model interpretability and facilitate automated validation of model inputs.]	[B2AI:Application]	[False]							
B2AI_STANDARD:252	B2AI_STANDARD:BiomedicalStandard	openEHR	openEHR Architecture	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:9]	openEHR is the name of a technology for e-health, consisting of open specifications, clinical models and software that can be used to create standards, and build information and interoperability solutions for healthcare. The various artefacts of openEHR are produced by the openEHR community and managed by openEHR International, an international non-profit organisation originally established in 2003 and previously managed by the openEHR Foundation.	True	False	https://www.openehr.org/about/what_is_openehr	https://specifications.openehr.org/releases/BASE/latest/architecture_overview.html		[B2AI_ORG:79]														
B2AI_STANDARD:253	B2AI_STANDARD:BiomedicalStandard	ORION	Outbreak Reports and Intervention Studies Of Nosocomial infection	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:5]	The quality of research in hospital epidemiology (infection control) must be improved to be robust enough to influence policy and practice. In order to raise the standards of research and publication, a CONSORT equivalent for these largely quasi-experimental studies has been prepared by the authors of two relevant systematic reviews undertaken for the HTA and the Cochrane Collaboration. The statement was revised following widespread consultation with learned societies, editors of journals and researchers. It consists of a 22 item checklist, and a summary table. The emphasis is on transparency to improve the quality of reporting and on the use of appropriate statistical techniques.The statement has been endorsed and welcomed by a number of professional special interest groups and societies including the Association of Medical Microbiologists (AMM), Bristish Society for Antimicrobial Chemotherapy (BSAC) and the Infection Control Nurses' Association (ICNA) Research and Development Group. Like CONSORT, ORION considers itself a work in progress, which requires ongoing dialogue for successful promotion and dissemination. The statement is therefore offered for further public discussion and journals are encouraged to trial it as part of their reviewing and editing process and feedback to the authors.	True	False	https://www.ucl.ac.uk/antimicrobial-resistance/reporting-guidelines/orion-statement-consort-equivalent-infection-control-intervention-studies	https://www.ucl.ac.uk/drupal/site_antimicrobial-resistance/sites/antimicrobial-resistance/files/checklist_authors.pdf										doi:10.1016/S1473-3099(07)70082-8						
B2AI_STANDARD:254	B2AI_STANDARD:BiomedicalStandard	ICE	Patient-Centric Integrated Clinical Environment Standard	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4]	This standard specifies the characteristics necessary for the safe integration of MEDICAL DEVICES and other equipment, via an electronic interface, from different MANUFACTURERS into a single medical system for the care of a single high acuity PATIENT. This standard establishes requirements for a medical system that is intended to have greater error resistance and improved PATIENT safety, treatment efficacy and workflow efficiency than can be achieved with independently used MEDICAL DEVICES.	False	False	https://mdpnp.org/mdice.html	https://www.astm.org/f2761-09r13.html		[B2AI_ORG:54]														
B2AI_STANDARD:255	B2AI_STANDARD:BiomedicalStandard	HMMER Format	Pfam / HMMER Profile file format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:26]	The profile HMM calculated from multiple sequnce alignment data in this service is stored in Profile HMM save format (usually with .hmm extension). It is an ASCII file containing a lot of header and descriptive records followed by large numerical matrix which holds probabilistic model of the motif. The file of this format is useful to search against sequnce databases to find out other proteins which share the same motif. This HMM file should not be edited manually (especially the matrix part) because it contains consistent numerical model as a whole.	True	False	http://hmmer.org/	https://www.genome.jp/tools/motif/hmmformat.htm																
B2AI_STANDARD:256	B2AI_STANDARD:BiomedicalStandard	Phenopackets	Phenopackets schema	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datamodel]	[B2AI_TOPIC:4]	Phenopackets is a schema for exchanging computable representations of patient clinical phenotypes, diseases, genomic information, and associated metadata in a standardized, machine-readable format. The schema uses Protocol Buffers (protobuf) and JSON to encode clinical observations as Human Phenotype Ontology (HPO) terms, diseases as OMIM/MONDO identifiers, age and temporal information, biosample details, genomic interpretations (variants, genes), pedigree information, measurements, medical actions, and provenance metadata. Phenopackets supports multiple use cases including individual patient records, family pedigrees, and cohort descriptions, enabling seamless exchange of phenotypic data between clinical systems, research databases, and diagnostic platforms. The format bridges clinical phenotyping with genomics by linking patient phenotypes to genetic variants, facilitating variant interpretation, genotype-phenotype correlation studies, and rare disease diagnosis. Phenopackets promotes FAIR principles for clinical data through standardized vocabularies (HPO, LOINC, UCUM), supports federated analysis across rare disease registries without sharing raw patient data, enables AI/ML applications for automated phenotyping and diagnostic support, and provides the foundation for international data sharing initiatives including matchmaking services that connect patients with similar phenotypes for clinical research and trial recruitment.	True	False	http://phenopackets.org/	https://github.com/phenopackets/phenopacket-schema		[B2AI_ORG:34]	[{\"id\": \"B2AI_APP:46\", \"category\": \"B2AI:Application\", \"name\": \"ML-Ready Rare Disease Corpus for Benchmarking and Training\", \"description\": \"A curated corpus of 4,916 case-level phenopackets spanning 277 Mendelian and chromosomal diseases was released explicitly as an analysis-ready, AI-ready dataset to enable machine learning analyses of clinical phenotype data. The corpus supports gene and disease prioritization pipelines, patient stratification studies, and genotype-phenotype correlation analyses by providing standardized HPO-encoded phenotypes linked to genomic diagnoses. Phenopackets' uniform format enables benchmarking of diagnostic software performance, testing and tuning of algorithms on standardized inputs, and evaluation of ML models for rare disease genomic diagnostics across diverse disease presentations.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1101/2024.05.29.24308104\"]}, {\"id\": \"B2AI_APP:154\", \"category\": \"B2AI:Application\", \"name\": \"Phenotype-Driven Diagnostic Tools Integration\", \"description\": \"Widely used phenotype-driven diagnostic and gene/variant prioritization tools including Exomiser, LIRICAL, Phen2Gene, and CADA accept Phenopackets as standardized input files, enabling integration of patient-level HPO-encoded phenotypes with genomic data for AI-enabled diagnostics. Phenopackets facilitate computational pipeline integration by providing a consistent format for representing clinical observations, supporting aggregation across sites, and enabling interoperability with electronic health record (EHR) systems and rare disease registries. This standardization allows diagnostic AI tools to process patient data from diverse sources using a common schema, improving diagnostic accuracy and enabling federated diagnostic workflows.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1101/2021.11.27.21266944\", \"https://doi.org/10.1038/s41587-022-01357-4\"]}, {\"id\": \"B2AI_APP:155\", \"category\": \"B2AI:Application\", \"name\": \"Clinical Annotation System with Enhanced ML Performance\", \"description\": \"SAMS (Symptom Annotation Made Simple), an HPO-integrated clinical annotation system that imports and exports Phenopackets, demonstrated measurable improvements in data quality that benefit downstream AI/ML pipelines. SAMS reported a 10% increase in recall for scientific publication annotations and a 20% increase in recall for EHR-derived annotations through improved entity linking and standardized symptom representation. These quality improvements in structured phenotype data directly enhance the performance of machine learning models that rely on high-quality, standardized clinical phenotypes for training and inference in diagnostic and research applications.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1093/nar/gkad1005\"]}, {\"id\": \"B2AI_APP:156\", \"category\": \"B2AI:Application\", \"name\": \"Large-Scale Federated Rare Disease Data Sharing\", \"description\": \"Within the Solve-RD consortium, Phenopackets support standardized clinical data sharing for large federated cohorts with 11,349+ individuals (growing to over 19,000), facilitating AI/ML-ready data integration for rare disease diagnostics and research. The European Joint Programme on Rare Diseases (EJP RD) utilizes Phenopackets for federated data discovery across rare disease resources while adhering to FAIR principles. Phenopackets enable distributed analyses, patient matchmaking services, and cohort identification across international sites without requiring patient-level data sharing, providing a computable substrate for training and validating machine learning models on multi-institutional rare disease datasets.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1101/2021.11.27.21266944\"]}, {\"id\": \"B2AI_APP:157\", \"category\": \"B2AI:Application\", \"name\": \"Genotype-Phenotype Correlation Analytics\", \"description\": \"The GPSEA (GenoPheno Statistical Evidence Assessment) framework uses Phenopackets to represent adverse phenotype data and compute genotype-phenotype correlations across 6,613 individuals spanning 85 cohorts. Phenopackets' standardized representation of patient-level phenotypes, variants, and biosample information enables systematic computational analysis of genotype-phenotype relationships, supporting downstream AI/ML analytics for variant interpretation, phenotype prediction from genotypes, and discovery of novel genotype-phenotype associations. The format facilitates aggregation of case-level data from diverse sources into analysis-ready datasets that machine learning models can use to learn patterns between genetic variants and clinical presentations.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1101/2024.05.29.24308104\"]}, {\"id\": \"B2AI_APP:158\", \"category\": \"B2AI:Application\", \"name\": \"Patient Matchmaking and Cohort Discovery Systems\", \"description\": \"Phenopackets serve as the computable representation underlying data-driven patient matchmaking services that connect individuals with similar phenotypes across international rare disease networks for clinical research, trial recruitment, and collaborative diagnosis. The standardized format enables AI-powered differential diagnosis systems and automated cohort identification tools that match patient phenotypes to disease profiles, identify suitable clinical trial candidates, and discover similar cases in distributed databases. Phenopackets' consistent encoding of clinical observations using HPO terms allows machine learning algorithms to compute phenotypic similarity scores, cluster patients by presentation, and support precision medicine initiatives through data-driven patient stratification across global rare disease registries.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.17863/cam.87963\", \"https://doi.org/10.1038/s41587-022-01357-4\"]}]	[ML-Ready Rare Disease Corpus for Benchmarking and Training|Phenotype-Driven Diagnostic Tools Integration|Clinical Annotation System with Enhanced ML Performance|Large-Scale Federated Rare Disease Data Sharing|Genotype-Phenotype Correlation Analytics|Patient Matchmaking and Cohort Discovery Systems]	[B2AI_APP:46|B2AI_APP:154|B2AI_APP:155|B2AI_APP:156|B2AI_APP:157|B2AI_APP:158]	[['https://doi.org/10.1101/2024.05.29.24308104']|['https://doi.org/10.1101/2021.11.27.21266944', 'https://doi.org/10.1038/s41587-022-01357-4']|['https://doi.org/10.1093/nar/gkad1005']|['https://doi.org/10.1101/2021.11.27.21266944']|['https://doi.org/10.1101/2024.05.29.24308104']|['https://doi.org/10.17863/cam.87963', 'https://doi.org/10.1038/s41587-022-01357-4']]	[A curated corpus of 4,916 case-level phenopackets spanning 277 Mendelian and chromosomal diseases was released explicitly as an analysis-ready, AI-ready dataset to enable machine learning analyses of clinical phenotype data. The corpus supports gene and disease prioritization pipelines, patient stratification studies, and genotype-phenotype correlation analyses by providing standardized HPO-encoded phenotypes linked to genomic diagnoses. Phenopackets' uniform format enables benchmarking of diagnostic software performance, testing and tuning of algorithms on standardized inputs, and evaluation of ML models for rare disease genomic diagnostics across diverse disease presentations.|Widely used phenotype-driven diagnostic and gene/variant prioritization tools including Exomiser, LIRICAL, Phen2Gene, and CADA accept Phenopackets as standardized input files, enabling integration of patient-level HPO-encoded phenotypes with genomic data for AI-enabled diagnostics. Phenopackets facilitate computational pipeline integration by providing a consistent format for representing clinical observations, supporting aggregation across sites, and enabling interoperability with electronic health record (EHR) systems and rare disease registries. This standardization allows diagnostic AI tools to process patient data from diverse sources using a common schema, improving diagnostic accuracy and enabling federated diagnostic workflows.|SAMS (Symptom Annotation Made Simple), an HPO-integrated clinical annotation system that imports and exports Phenopackets, demonstrated measurable improvements in data quality that benefit downstream AI/ML pipelines. SAMS reported a 10% increase in recall for scientific publication annotations and a 20% increase in recall for EHR-derived annotations through improved entity linking and standardized symptom representation. These quality improvements in structured phenotype data directly enhance the performance of machine learning models that rely on high-quality, standardized clinical phenotypes for training and inference in diagnostic and research applications.|Within the Solve-RD consortium, Phenopackets support standardized clinical data sharing for large federated cohorts with 11,349+ individuals (growing to over 19,000), facilitating AI/ML-ready data integration for rare disease diagnostics and research. The European Joint Programme on Rare Diseases (EJP RD) utilizes Phenopackets for federated data discovery across rare disease resources while adhering to FAIR principles. Phenopackets enable distributed analyses, patient matchmaking services, and cohort identification across international sites without requiring patient-level data sharing, providing a computable substrate for training and validating machine learning models on multi-institutional rare disease datasets.|The GPSEA (GenoPheno Statistical Evidence Assessment) framework uses Phenopackets to represent adverse phenotype data and compute genotype-phenotype correlations across 6,613 individuals spanning 85 cohorts. Phenopackets' standardized representation of patient-level phenotypes, variants, and biosample information enables systematic computational analysis of genotype-phenotype relationships, supporting downstream AI/ML analytics for variant interpretation, phenotype prediction from genotypes, and discovery of novel genotype-phenotype associations. The format facilitates aggregation of case-level data from diverse sources into analysis-ready datasets that machine learning models can use to learn patterns between genetic variants and clinical presentations.|Phenopackets serve as the computable representation underlying data-driven patient matchmaking services that connect individuals with similar phenotypes across international rare disease networks for clinical research, trial recruitment, and collaborative diagnosis. The standardized format enables AI-powered differential diagnosis systems and automated cohort identification tools that match patient phenotypes to disease profiles, identify suitable clinical trial candidates, and discover similar cases in distributed databases. Phenopackets' consistent encoding of clinical observations using HPO terms allows machine learning algorithms to compute phenotypic similarity scores, cluster patients by presentation, and support precision medicine initiatives through data-driven patient stratification across global rare disease registries.]	[B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application|B2AI:Application]	[False|False|False|False|False|False]					[B2AI_ORG:58]		
B2AI_STANDARD:257	B2AI_STANDARD:BiomedicalStandard	PHIN Guide	PHIN Messaging Guide for Syndromic Surveillance Emergency Department, Urgent Care, Inpatient and Ambulatory Care Settings	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	An HL7 messaging and content reference standard for national, syndromic surveillance electronic health record technology certification; A basis for local and state syndromic surveillance messaging implementation guides; A resource for planning for the increasing use of electronic health record technology and for providing details on health data elements that may become a part of future public health syndromic surveillance messaging requirements; Optional elements of interest for adding laboratory results to syndromic surveillance messages using ORU^R01 message structure (see details in the PHIN messaging Standard, National Condition Reporting case Notification, ORU^R01 message Structure Specification profile, Version 2.1, 2014)	True	False	https://knowledgerepository.syndromicsurveillance.org/hl7-version-251-phin-messaging-guide-syndromic-surveillance-emergency-department-urgent-care-and	https://www.cdc.gov/nssp/documents/guides/syndrsurvmessagguide2_messagingguide_phn.pdf														[B2AI_ORG:40]		
B2AI_STANDARD:258	B2AI_STANDARD:BiomedicalStandard	phyloXML	phyloXML	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:5|B2AI_TOPIC:12]	phyloXML is a specialized XML-based language for representing phylogenetic trees and networks, along with rich associated metadata. It provides standardized elements for encoding taxonomic information, gene names, sequence identifiers, branch lengths, support values, gene duplication and speciation events, and other evolutionary attributes. The extensible structure of phyloXML enables interoperability between evolutionary biology and comparative genomics tools, supporting both simple and complex tree annotations. Its schema allows for domain-specific extensions and integration with other bioinformatics resources, making it a widely adopted standard for sharing, visualizing, and analyzing phylogenetic data in research and database applications.	True	False	http://www.phyloxml.org/											doi:10.1186/1471-2105-10-356						
B2AI_STANDARD:259	B2AI_STANDARD:BiomedicalStandard	ANSI/CTA-2056	Physical Activity Monitoring for Fitness Wearables Step Counting	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:18]	ANSI/CTA-2056 is a voluntary consensus standard developed by the Consumer Technology Association (CTA) that establishes standardized definitions, testing methodologies, and minimum performance criteria for measuring step counting accuracy on consumer wearable devices and smartphone applications used for physical activity monitoring and health tracking. The standard defines a \"step\" as a single stride during human locomotion and specifies validation protocols using controlled laboratory testing on treadmills at various speeds (slow walking 2.0 mph, normal walking 3.0 mph, brisk walking 4.0 mph) as well as free-living wear tests in real-world conditions to evaluate device performance across diverse user populations and activity patterns. ANSI/CTA-2056 requires manufacturers to report step counting accuracy as mean absolute percentage error (MAPE) and specifies acceptable error thresholds - devices should achieve less than 10% error for controlled walking tests and less than 20% error for free-living conditions to meet the standard's performance benchmarks. The standard addresses measurement challenges including false positive step detection from upper body movements, vehicle vibrations, and daily living activities, as well as false negative errors from slow shuffling gaits or irregular walking patterns. ANSI/CTA-2056 promotes transparency by requiring clear disclosure of test conditions, participant demographics, reference measurement systems (ActiGraph accelerometers, manual observation), and statistical analysis methods used for validation. The standard facilitates comparability across commercial fitness trackers, smartwatches, and pedometer applications from manufacturers like Fitbit, Apple Watch, Garmin, and Samsung, supporting consumer informed decision-making, clinical research applications requiring validated activity metrics, and workplace wellness programs utilizing step count goals for employee health initiatives.	True	True	https://webstore.ansi.org/standards/ansi/cta20562016ansi			[B2AI_ORG:4]														
B2AI_STANDARD:260	B2AI_STANDARD:BiomedicalStandard	PEP	Portable Encapsulated Project specification	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:1]	The Portable Encapsulated Project (PEP) specification is a standardized, machine-readable format for organizing and describing biological sample metadata and associated computational analysis configurations in data-intensive bioinformatics projects. PEP uses a YAML-based configuration file (project_config.yaml) that points to a sample annotation table (typically CSV or TSV) containing sample attributes, along with optional subsample tables for complex hierarchical relationships. The specification defines a formal structure for representing sample metadata with arbitrary attributes, derived attributes computed from other columns, sample modifiers for conditional processing, and implied attributes inferred from organizational context. PEP supports amendments for alternative project configurations, subanotations for linking multiple data files to single samples, and project-level metadata including descriptions, keywords, and namespace information. The format is designed to make projects portable across compute environments, reproducible through version-controlled configurations, and interoperable across analysis tools through a growing ecosystem of PEP-compatible software (looper, pypiper, pepr, geofetch, pephub). PEP's structured metadata representation enables systematic queries across sample collections, facilitates batch processing and parallelization of analyses, supports complex experimental designs with multiple data types per sample, and provides a foundation for FAIR data practices in genomics and multi-omics research. The specification is particularly valuable for managing large-scale projects with hundreds or thousands of samples, enabling metadata-driven workflow execution, automated data retrieval, and standardized documentation that supports reproducible computational research and machine learning applications requiring well-annotated training datasets.	True	False	http://pep.databio.org/	https://github.com/pepkit/pepspec										doi:10.1093/gigascience/giab077	[B2AI_STANDARD:761]					
B2AI_STANDARD:261	B2AI_STANDARD:BiomedicalStandard	PFB	Portable Format for Bioinformatics	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:1]	A self-describing serialized format for bulk biomedical data called the Portable Format for Biomedical (PFB) data. The Portable Format for Biomedical data is based upon Avro and encapsulates a data model, a data dictionary, the data itself, and pointers to third party controlled vocabularies. In general, each data element in the data dictionary is associated with a third party controlled vocabulary to make it easier for applications to harmonize two or more PFB files.	True	False	https://anvilproject.org/ncpi/technologies#portable-format-for-bioinformatics-pfb	https://github.com/uc-cdis/pypfb										doi:10.1101/2022.07.19.500678						
B2AI_STANDARD:262	B2AI_STANDARD:BiomedicalStandard	PRISMA	Preferred Reporting Items for Systematic Reviews and Meta-Analyses	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:5]	An evidence-based minimum set of items for reporting in systematic reviews and meta-analyses.The aim of the PRISMA Statement is to help authors improve the reporting of systematic reviews and meta-analyses. We have focused on randomized trials, but PRISMA can also be used as a basis for reporting systematic reviews of other types of research, particularly evaluations of interventions. PRISMA may also be useful for critical appraisal of published systematic reviews, although it is not a quality assessment instrument to gauge the quality of a systematic review.	True	False	https://www.prisma-statement.org/											doi:10.1136/bmj.n71						
B2AI_STANDARD:263	B2AI_STANDARD:BiomedicalStandard	PDB	Protein Data Bank Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:27]	An exchange format for reporting experimentally determined three-dimensional structures of biological macromolecules that serves a global community of researchers, educators, and students. The data contained in the archive include atomic coordinates, bibliographic citations, primary and secondary structure, information, and crystallographic structure factors and NMR experimental data	True	False	https://www.cgl.ucsf.edu/chimera/docs/UsersGuide/tutorials/pdbintro.html															[B2AI_ORG:82]		
B2AI_STANDARD:264	B2AI_STANDARD:BiomedicalStandard	PSI-PAR	Proteomics Standards Initiative Protein Affinity Reagent format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:26]	The work on PSI-PAR was initiated as part of the ProteomeBinders project and carried out by EMBL-EBI and the PSI-MI work group. The Proteomics Standards Initiative (PSI) aims to define community standards for data representation in proteomics to facilitate data comparison, exchange and verification. For detailed information on all PSI activities, please see PSI Home Page. The PSI-PAR format is a standardized means of representing protein affinity reagent data and is designed to facilitate the exchange of information between different databases and/or LIMS systems. PSI-PAR is not a proposed database structure. The PSI-PAR format consists of the PSI-MI XML2.5 schema (originally designed for molecular interactions) and the PSI-PAR controlled vocabulary. In addition, PSI-PAR documentation and examples are available on this web page. The scope of PSI-PAR is PAR and target protein production and characterization.	True	False	https://www.psidev.info/psi-par			[B2AI_ORG:41]								doi:10.1074/mcp.M900185-MCP200						
B2AI_STANDARD:265	B2AI_STANDARD:BiomedicalStandard	PSI GelML	PSI GelML	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:26]	GelML is a data exchange format for describing the results of gel electrophoresis experiments. GelML is developed as a HUPO-PSI working group.	True	False	https://www.psidev.info/gelml/1.0			[B2AI_ORG:41]														
B2AI_STANDARD:266	B2AI_STANDARD:BiomedicalStandard	PSI-MI XML	PSI-MI XML	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:28]	The PSI-MI XML 2.5 is a community standard for molecular interactions which has been jointly developed by major data providers (BIND, CellZome, DIP, GSK, HPRD, Hybrigenics, IntAct, MINT, MIPS, Serono, U. Bielefeld, U. Bordeaux, U. Cambridge, and others).This format is stable and used for several years now - published in October 2007 (Broadening the Horizon  Level 2.5 of the HUPO-PSI Format for Molecular Interactions; Samuel Kerrien et al. BioMed Central. 2007.), it has been adapted for many different usages. It can be used for storing any kind of molecular interaction data - complexes and binary interactions not only protein-protein interactions, can describe nucleic acids interactions and others hierarchical complexes modelling by using interactionRef in participants instead of an interactor Data representation in PSI-MI 2.5 XML relies heavily on the use of controlled vocabularies. They can be accessed easily via the Ontology Lookup Service, PSI-MI, PSI-MOD.	True	False	https://www.psidev.info/mif			[B2AI_ORG:41]														
B2AI_STANDARD:267	B2AI_STANDARD:BiomedicalStandard	qcML	qcML format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:28]	An XML format for quality-related data of mass spectrometry and other high-throughput experiments. Quality control is increasingly recognized as a crucial aspect of mass spectrometry based proteomics. Several recent papers discuss relevant parameters for quality control and present applications to extract these from the instrumental raw data. What has been missing, however, is a standard data exchange format for reporting these performance metrics. We therefore developed the qcML format, an XML-based standard that follows the design principles of the related mzML, mzIdentML, mzQuantML, and TraML standards from the HUPO-PSI (Proteomics Standards Initiative). In addition to the XML format, we also provide tools for the calculation of a wide range of quality metrics as well as a database format and interconversion tools, so that existing LIMS systems can easily add relational storage of the quality control data to their existing schema. We here describe the qcML specification, along with possible use cases and an illustrative example of the subsequent analysis possibilities. All information about qcML is available at http://code.google.com/p/qcml	True	False	https://doi.org/10.1074/mcp.M113.035907											doi:10.1074/mcp.M113.035907						
B2AI_STANDARD:268	B2AI_STANDARD:BiomedicalStandard	RDML	Real-time PCR Data Markup Language	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:33]	The RDML file format is developed by the RDML consortium (http://www.rdml.org) and can be used free of charge. The RDML file format was created to encourage the exchange, publication, revision and re-analysis of raw qPCR data. The core of an RDML file is an experiment, not a PCR run. Therefore all the information is collected which is required to understand an experiment. The structure of the file format was inspired by a database structure. In the file are several master elements, which are then referred to in other parts of the file. This structure allows to reduce the amount of redundant information and encourages the user to provide useful information. The Real-time PCR Data Markup Language (RDML) is a structured and universal data standard for exchanging quantitative PCR (qPCR) data. The data standard should contain sufficient information to understand the experimental setup, re-analyse the data and interpret the results. The data standard is a compressed text file in Extensible Markup Language (XML) and enables transparent exchange of annotated qPCR data between instrument software and third-party data analysis packages, between colleagues and collaborators, and between authors, peer reviewers, journals and readers. To support the public acceptance of this standard, both an on-line RDML file generator is available for end users, as well as RDML software libraries to be used by software developers, enabling import and export of RDML data files.	True	False	http://www.rdml.org																	
B2AI_STANDARD:269	B2AI_STANDARD:BiomedicalStandard	REFLECT	Reporting guidelines for randomized controlled trials for livestock and food safety	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:5]	REFLECT (Reporting guidElines For randomized controLled trials for livEstoCk and food safeTy) is a specialized, evidence-based minimum set of 22 reporting items specifically designed to improve the transparency, reproducibility, and quality of randomized controlled trials in livestock production, animal health, and food safety research. This comprehensive reporting guideline addresses the unique challenges and complexities inherent in agricultural and veterinary research, including both field trials conducted in real-world farm settings and controlled challenge studies in laboratory environments. REFLECT covers essential aspects of trial design, implementation, and reporting including study population characteristics, randomization procedures, intervention details, outcome measurements, statistical analyses, and results presentation. The guideline is specifically tailored for trials evaluating therapeutic or preventive interventions that impact production outcomes, animal health parameters, and food safety measures. Available in both MS Word and PDF formats, REFLECT serves as a dynamic, evolving document that is periodically updated as new evidence emerges, helping researchers, editors, reviewers, and regulatory agencies ensure that livestock and food safety trials are reported with sufficient detail for proper scientific evaluation and evidence-based decision making.	True	False	https://meridian.cvm.iastate.edu/reflect/											doi:10.4315/0362-028x-73.3.579						
B2AI_STANDARD:270	B2AI_STANDARD:BiomedicalStandard	REMARK	Reporting recommendations for tumour Marker prognostic studies	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:5]	Despite years of research and hundreds of reports on tumor markers in oncology, the number of markers that have emerged as clinically useful is pitifully small. Often initially reported studies of a marker show great promise, but subsequent studies on the same or related markers yield inconsistent conclusions or stand in direct contradiction to the promising results. It is imperative that we attempt to understand the reasons why multiple studies of the same marker lead to differing conclusions. A variety of methodological problems have been cited to explain these discrepancies. Unfortunately, many tumor marker studies have not been reported in a rigorous fashion, and published articles often lack sufficient information to allow adequate assessment of the quality of the study or the generalizability of study results. The development of guidelines for the reporting of tumor marker studies was a major recommendation of the National Cancer Institute-European Organisation for Research and Treatment of Cancer (NCI-EORTC) First International Meeting on Cancer Diagnostics in 2000. As for the successful CONSORT initiative for randomized trials and for the STARD statement for diagnostic studies, we suggest guidelines to provide relevant information about the study design, preplanned hypotheses, patient and specimen characteristics, assay methods, and statistical analysis methods. In addition, the guidelines provide helpful suggestions on how to present data and important elements to include in discussions. The goal of these guidelines is to encourage transparent and complete reporting so that the relevant information will be available to others to help them to judge the usefulness of the data and understand the context in which the conclusions apply.	True	False	https://www.equator-network.org/reporting-guidelines/reporting-recommendations-for-tumour-marker-prognostic-studies-remark/	https://www.equator-network.org/wp-content/uploads/2016/10/REMARK-checklist-for-EQUATOR-website-002.docx										doi:10.1038/sj.bjc.6602678						
B2AI_STANDARD:271	B2AI_STANDARD:BiomedicalStandard	ReproSchema	ReproSchema	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:31]	A common schema that encodes how the different elements of assessment data and / or the metadata relate to one another.	True	False	https://www.repronim.org/reproschema/	https://github.com/ReproNim/reproschema																
B2AI_STANDARD:272	B2AI_STANDARD:BiomedicalStandard	SCDM	Sentinel Common Data Model	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datamodel]	[B2AI_TOPIC:9]	A standard data structure that allows Sentinel Data Partners to quickly execute distributed programs against local data.	True	False	https://www.sentinelinitiative.org/methods-data-tools/sentinel-common-data-model	https://dev.sentinelsystem.org/projects/SCDM/repos/sentinel_common_data_model/browse		[B2AI_ORG:89]	[{\"id\": \"B2AI_APP:47\", \"category\": \"B2AI:Application\", \"name\": \"FDA Sentinel System and Distributed AI for Drug Safety\", \"description\": \"Sentinel Common Data Model is used in AI applications for active surveillance of medical product safety across distributed healthcare databases, enabling privacy-preserving machine learning for adverse event detection and risk assessment. AI systems leverage SCDM's standardization of claims data, electronic health records, and registries to develop models that identify safety signals, predict adverse events, and characterize treatment patterns across the FDA Sentinel System network. The common data model enables federated learning where AI algorithms execute locally at each data partner without sharing patient-level data, supporting rapid assessment of emerging safety concerns. Machine learning applications use SCDM to train models on massive populations for rare adverse event detection and comparative safety analysis.\", \"used_in_bridge2ai\": false}]	[FDA Sentinel System and Distributed AI for Drug Safety]	[B2AI_APP:47]		[Sentinel Common Data Model is used in AI applications for active surveillance of medical product safety across distributed healthcare databases, enabling privacy-preserving machine learning for adverse event detection and risk assessment. AI systems leverage SCDM's standardization of claims data, electronic health records, and registries to develop models that identify safety signals, predict adverse events, and characterize treatment patterns across the FDA Sentinel System network. The common data model enables federated learning where AI algorithms execute locally at each data partner without sharing patient-level data, supporting rapid assessment of emerging safety concerns. Machine learning applications use SCDM to train models on massive populations for rare adverse event detection and comparative safety analysis.]	[B2AI:Application]	[False]							
B2AI_STANDARD:273	B2AI_STANDARD:BiomedicalStandard	SAM	Sequence Alignment/Map Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:5]	The Sequence Alignment/Map (SAM) format is a TAB-delimited text format for storing sequence alignments against reference sequences, developed as part of the samtools project and maintained by the GA4GH Large Scale Genomics work stream. SAM consists of an optional header section (beginning with @ symbols) containing metadata about reference sequences, read groups, programs, and comments, followed by an alignment section with one line per aligned read. Each alignment line contains 11 mandatory fields including query name (QNAME), bitwise FLAG encoding mapping properties, reference sequence name (RNAME), 1-based leftmost mapping position (POS), mapping quality (MAPQ), CIGAR string describing alignment operations, mate pair information (RNEXT, PNEXT, TLEN), sequence (SEQ), and ASCII-encoded base quality scores (QUAL). The format supports optional fields as TAG:TYPE:VALUE triplets for storing additional information such as edit distance, alternative alignments, and aligner-specific metadata. SAM serves as the text-based companion to the binary BAM and compressed CRAM formats, with bidirectional conversion tools (samtools view) enabling interoperability. The format accommodates various alignment types including mapped, unmapped, secondary, supplementary, and chimeric alignments with proper-pair relationships. SAM files are human-readable for debugging and inspection, widely supported by aligners (BWA, Bowtie2, STAR), variant callers, and genomics toolkits, forming the foundation for standardized sequencing data exchange in next-generation sequencing workflows.	True	False	http://samtools.sourceforge.net/			[B2AI_ORG:34]								doi:10.1093/bioinformatics/btp352						
B2AI_STANDARD:274	B2AI_STANDARD:BiomedicalStandard	SRA-XML	Sequence Read Archive Metadata XML	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datamodel]	[B2AI_TOPIC:12|B2AI_TOPIC:13|B2AI_TOPIC:33]	SRA-XML is an XML-based metadata format used by the Sequence Read Archive (SRA) and the European Nucleotide Archive (ENA) to describe and exchange metadata for raw sequencing data submissions from next-generation sequencing platforms. The format captures detailed information about sequencing experiments, sample attributes, library preparation, instrument models, and data files, enabling standardized data submission, validation, and integration across international repositories. SRA-XML supports the reproducibility and discoverability of sequencing datasets by providing a structured, machine-readable representation of experimental context and provenance, facilitating large-scale genomics research and data sharing.	True	False	https://www.ncbi.nlm.nih.gov/sra/docs/submitmeta/				[{\"id\": \"B2AI_APP:48\", \"category\": \"B2AI:Application\", \"name\": \"Sequence Data Metadata Mining and Dataset Discovery\", \"description\": \"SRA-XML (Sequence Read Archive metadata format) is used in AI applications for automated mining of experimental metadata from genomic studies, enabling dataset discovery, quality assessment, and training of models that learn from experimental design information. Machine learning systems parse SRA metadata to identify relevant datasets for specific research questions, predict data quality issues before download, and extract experimental conditions that inform downstream analysis. AI applications leverage structured metadata to train models for automated experiment type classification, sample relationship inference, and detection of metadata quality issues. The format enables large-scale meta-analyses where AI systems integrate findings across thousands of sequencing experiments by understanding their experimental context.\", \"used_in_bridge2ai\": false}]	[Sequence Data Metadata Mining and Dataset Discovery]	[B2AI_APP:48]		[SRA-XML (Sequence Read Archive metadata format) is used in AI applications for automated mining of experimental metadata from genomic studies, enabling dataset discovery, quality assessment, and training of models that learn from experimental design information. Machine learning systems parse SRA metadata to identify relevant datasets for specific research questions, predict data quality issues before download, and extract experimental conditions that inform downstream analysis. AI applications leverage structured metadata to train models for automated experiment type classification, sample relationship inference, and detection of metadata quality issues. The format enables large-scale meta-analyses where AI systems integrate findings across thousands of sequencing experiments by understanding their experimental context.]	[B2AI:Application]	[False]	doi:10.1093/nar/gkq1019				[B2AI_ORG:74]		
B2AI_STANDARD:275	B2AI_STANDARD:BiomedicalStandard	SOFT	Simple Omnibus Format in Text	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:5]	Simple Omnibus Format in Text (SOFT) is a line-based, plain text format originally developed by NCBI Gene Expression Omnibus (GEO) for batch submission and download of genomic data. Though GEO discontinued accepting SOFT submissions in early 2024, all records remain available for download in SOFT format. The format uses four line types distinguished by first-character markers - caret lines (^) indicate entity types (PLATFORM, SAMPLE, SERIES), bang lines (!) specify entity attributes as label-value pairs, hash lines (#) describe data table headers, and data lines contain tab-delimited table rows. A single SOFT file can concatenate multiple Platform (GPL), Sample (GSM), and Series (GSE) records with their data tables and descriptive metadata. Platform tables require unique row identifiers and trackable sequence identifiers (GenBank/RefSeq accessions, clone IDs, oligo sequences) with standard headers for sequences, organism source, and various accession types. Sample tables must include ID_REF columns matching Platform identifiers and VALUE columns containing normalized, comparable measurements (scaled signals for single-channel or log ratios for dual-channel data). The format is compatible with common spreadsheet and database applications and supports MIAME standards for comprehensive data interpretation.	True	False	https://www.ncbi.nlm.nih.gov/geo/info/soft-seq.html																	
B2AI_STANDARD:276	B2AI_STANDARD:BiomedicalStandard	SMILES	Simplified Molecular Input Line Entry Specification Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:3]	A typographical line notation for specifying chemical structure.	True	False	http://opensmiles.org/	http://opensmiles.org/opensmiles.html																
B2AI_STANDARD:277	B2AI_STANDARD:BiomedicalStandard	SED-ML	Simulation Experiment Description Markup Language	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:5]	SED-ML is an XML-based format for encoding simulation setups, to ensure exchangeability and reproducibility of simulation experiments. It follows the requirements defined in the MIASE guidelines.	True	False	https://sed-ml.org/																	
B2AI_STANDARD:278	B2AI_STANDARD:BiomedicalStandard	SPDI	SPDI data model	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datamodel]	[B2AI_TOPIC:12|B2AI_TOPIC:13|B2AI_TOPIC:35]	Sequence variant data model. Represents all variants as a sequence of four operations. Start at the boundary before the first nucleotide in the sequence S, advance P nucleotides, delete D nucleotides, then Insert the nucleotides in the string.	True	False	https://doi.org/10.1093/bioinformatics/btz856				[{\"id\": \"B2AI_APP:49\", \"category\": \"B2AI:Application\", \"name\": \"Unambiguous Variant Representation for Clinical AI\", \"description\": \"SPDI (Sequence-Position-Deletion-Insertion) notation is used in AI applications for creating precise, unambiguous representations of genetic variants that eliminate nomenclature inconsistencies affecting model training and clinical interpretation. AI systems leverage SPDI's standardized four-element format to normalize variant representations from diverse sources (clinical labs, research databases, literature), enabling consistent feature engineering for machine learning models predicting variant pathogenicity. The notation resolves ambiguities in variant left-normalization and reference sequence specification that can cause the same biological variant to appear as different features to AI models, improving model accuracy and reproducibility. SPDI is particularly valuable for clinical AI systems that must reconcile variants reported in different formats across laboratories.\", \"used_in_bridge2ai\": false, \"references\": [\"https://www.ncbi.nlm.nih.gov/variation/notation/\"]}]	[Unambiguous Variant Representation for Clinical AI]	[B2AI_APP:49]	[['https://www.ncbi.nlm.nih.gov/variation/notation/']]	[SPDI (Sequence-Position-Deletion-Insertion) notation is used in AI applications for creating precise, unambiguous representations of genetic variants that eliminate nomenclature inconsistencies affecting model training and clinical interpretation. AI systems leverage SPDI's standardized four-element format to normalize variant representations from diverse sources (clinical labs, research databases, literature), enabling consistent feature engineering for machine learning models predicting variant pathogenicity. The notation resolves ambiguities in variant left-normalization and reference sequence specification that can cause the same biological variant to appear as different features to AI models, improving model accuracy and reproducibility. SPDI is particularly valuable for clinical AI systems that must reconcile variants reported in different formats across laboratories.]	[B2AI:Application]	[False]	doi:10.1093/bioinformatics/btz856				[B2AI_ORG:74]		
B2AI_STANDARD:279	B2AI_STANDARD:BiomedicalStandard	SEDS	Standard EEG Data Structure	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:37]	A new and more flexible data structure, named the Standard EEG Data Structure (SEDS), was proposed to meet the needs of both small-scale EEG data batch processing in single-site studies and large-scale EEG data sharing and analysis in single-/multisite studies (especially on cloud platforms).	False	False	https://doi.org/10.1016/j.softx.2021.100933											doi:10.1016/j.softx.2021.100933						
B2AI_STANDARD:280	B2AI_STANDARD:BiomedicalStandard	SPREC	Standard PREanalytical Code	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[codesystem]	[B2AI_TOPIC:9]	The Standard PREanalytical Code (SPREC) is a comprehensive coding system developed by the ISBER Biospecimen Science Working Group to standardize documentation of pre-analytical variables that affect biospecimen quality. SPREC uses a seven-element alphanumeric code to capture critical factors including specimen type, primary container, time to processing (warm and cold ischemia), centrifugation parameters (speed and temperature), and long-term storage conditions. This systematic encoding enables biobanks and biorepositories to consistently track and communicate how specimens were collected, processed, and preserved, which directly impacts molecular analyte stability and experimental reproducibility. By providing a standardized language for pre-analytical variation, SPREC facilitates data harmonization across biobanks, enables quality comparisons between specimen collections, supports regulatory compliance, and enhances the value of biological samples for translational research. The code is particularly valuable for multi-center studies where specimen provenance must be tracked and controlled. SPREC version 3.0 covers various specimen types including blood, tissue, urine, and other biological fluids, with specific codes for derivatives like DNA, RNA, and proteins.	True	False	https://www.isber.org/page/SPREC			[B2AI_ORG:48]								doi:10.1089/bio.2017.0109						
B2AI_STANDARD:281	B2AI_STANDARD:BiomedicalStandard	STARD	Standards for Reporting of Diagnostic Accuracy Studies	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:5]	The objective of the STARD initiative is to improve the accuracy and completeness of reporting of studies of diagnostic accuracy, to allow readers to assess the potential for bias in the study (internal validity) and to evaluate its generalisability (external validity).The STARD statement consist of a checklist of 25 items and recommends the use of a flow diagram which describe the design of the study and the flow of patients.	True	False	https://www.equator-network.org/reporting-guidelines/stard/											doi:10.1136/bmjopen-2016-012799						
B2AI_STANDARD:282	B2AI_STANDARD:BiomedicalStandard	Stockholm	Stockholm Multiple Alignment Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12|B2AI_TOPIC:26]	The Stockholm format is a comprehensive system for marking up and annotating features in multiple sequence alignments, widely used by HMMER, Pfam, and Belvu bioinformatics tools. The format supports detailed annotation of secondary structure (SS), surface accessibility (SA), transmembrane regions (TM), posterior probability (PP), ligand binding sites (LI), active sites (AS), and intron positions (IN). The format includes structured headers with STOCKHOLM 1.0 identifier, sequence alignment blocks with name/start-end notation, and comprehensive markup capabilities supporting database references, organism classification, phylogenetic trees in New Hampshire format, and various structural and functional annotations. It provides flexible annotation of aligned sequences with exact one-character-per-column markup for positional features and free-text annotations for sequence and file-level metadata.	True	False	https://sonnhammer.sbc.su.se/Stockholm.html																	
B2AI_STANDARD:283	B2AI_STANDARD:BiomedicalStandard	STREGA	Strengthening the Reporting of Genetic Association studies	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:5]	Making sense of rapidly evolving evidence on genetic associations is crucial to making genuine advances in human genomics and the eventual integration of this information in the practice of medicine and public health. Assessment of the strengths and weaknesses of this evidence, and hence the ability to synthesize it, has been limited by inadequate reporting of results. The STrengthening the REporting of Genetic Association studies (STREGA) initiative builds on the Strengthening the Reporting of Observational Studies in Epidemiology (STROBE) Statement and provides additions to 12 of the 22 items on the STROBE checklist. The additions concern population stratification, genotyping errors, modelling haplotype variation, Hardy-Weinberg equilibrium, replication, selection of participants, rationale for choice of genes and variants, treatment effects in studying quantitative traits, statistical methods, relatedness, reporting of descriptive and outcome data, and the volume of data issues that are important to consider in genetic association studies. The STREGA recommendations do not prescribe or dictate how a genetic association study should be designed but seek to enhance the transparency of its reporting, regardless of choices made during design, conduct, or analysis.	True	False	https://www.equator-network.org/reporting-guidelines/strobe-strega/											doi:10.1002/gepi.20410						
B2AI_STANDARD:284	B2AI_STANDARD:BiomedicalStandard	STROBE	Strengthening the Reporting of Observational studies in Epidemiology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:5]	STROBE stands for an international, collaborative initiative of epidemiologists, methodologists, statisticians, researchers and journal editors involved in the conduct and dissemination of observational studies, with the common aim of STrengthening the Reporting of OBservational studies in Epidemiology. The STROBE Statement is being endorsed by a growing number of biomedical journals. Incomplete and inadequate reporting of research hampers the assessment of the strengths and weaknesses of the studies reported in the medical literature. Readers need to know what was planned (and what was not), what was done, what was found, and what the results mean. Recommendations on the reporting of studies that are endorsed by leading medical journals can improve the quality of reporting.Observational research comprises several study designs and many topic areas. We aimed to establish a checklist of items that should be included in articles reporting such research - the STROBE Statement. We considered it reasonable to initially restrict the recommendations to the three main analytical designs that are used in observational research - cohort, case-control, and cross-sectional studies. We want to provide guidance on how to report observational research well. Our recommendations are not prescriptions for designing or conducting studies. Also, the checklist is not an instrument to evaluate the quality of observational research.	True	False	https://www.strobe-statement.org/			[B2AI_ORG:91]								doi:10.1016/j.jclinepi.2007.11.008						
B2AI_STANDARD:285	B2AI_STANDARD:BiomedicalStandard	SDF	Structure Data Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:27]	SDF is one of a family of chemical-data file formats developed by MDL; it is intended especially for structural information. SDF stands for structure-data file, and SDF files actually wrap the molfile (MDL Molfile) format.	True	False	https://en.wikipedia.org/wiki/Chemical_table_file#SDF																	
B2AI_STANDARD:286	B2AI_STANDARD:BiomedicalStandard	SummarizedExperiment	SummarizedExperiment container	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:13]	The SummarizedExperiment container contains one or more assays, each represented by a matrix-like object of numeric or other mode. The rows typically represent genomic ranges of interest and the columns represent samples.	True	False	https://bioconductor.org/packages/release/bioc/html/SummarizedExperiment.html	https://github.com/Bioconductor/SummarizedExperiment													[B2AI_SUBSTRATE:40]			
B2AI_STANDARD:287	B2AI_STANDARD:BiomedicalStandard	SBOL	Synthetic Biology Open Language	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:1]	Standard for in silico representation of genetic designs. SBOL is designed to allow synthetic biologists and genetic engineers to electronically exchange designs . Send and receive genetic designs to and from biofabrication centers. Facilitate storage of genetic designs in repositories Embed genetic designs in publications.	True	False	https://sbolstandard.org/	https://sbolstandard.org/#specifications																
B2AI_STANDARD:288	B2AI_STANDARD:BiomedicalStandard	SBGN	Systems Biology Graphical Notation	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:1]	Systems Biology Graphical Notation (SBGN) is a standardized visual language for representing biological processes and relationships in maps. SBGN defines the precise meaning of all graphical symbols through three complementary languages - Process Description (PD) for biochemical reactions and molecular interactions, Activity Flow (AF) for the flow of information between biochemical entities, and Entity Relationship (ER) for relationships between biological entities independent of temporal aspects. Each language has specific symbols, syntax rules, and semantic definitions to ensure unambiguous interpretation across different tools and users. SBGN-ML, an XML-based exchange format, enables storage and transfer of SBGN diagrams between software applications, supported by the standard LibSBGN library. The notation is widely adopted in major biological databases including Reactome, PANTHER Pathway, BioModels, and Pathway Commons. Multiple software tools support SBGN diagram creation and editing (CellDesigner, Newt Editor, VANTED/SBGN-ED, PathVisio, yEd), format conversion from KEGG, BioPAX, and SBML, and visualization of pathway models, facilitating standardized communication of complex biological knowledge across the systems biology community.	True	False	https://sbgn.github.io/	https://sbgn.github.io/downloads/specifications/pd_level1_version2.pdf										doi:10.1038/nbt.1558						
B2AI_STANDARD:289	B2AI_STANDARD:BiomedicalStandard	SBML	Systems Biology Markup Language	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:1]	The Systems Biology Markup Language (SBML) is a comprehensive XML-based exchange format specifically designed for representing computational models of biological processes in a machine-readable, standardized form. Developed and maintained by the international SBML community, SBML enables interoperability between diverse systems biology software tools by providing a common model representation language that eliminates translation errors and ensures consistent model interpretation across different platforms. While SBML excels at representing biochemical reaction networks, metabolic pathways, gene regulatory networks, and signal transduction cascades, its flexible architecture supports modeling of various biological phenomena from molecular to cellular scales. The format includes sophisticated features for describing reaction kinetics, species concentrations, compartmentalization, parameter sensitivity, and dynamic behaviors through mathematical expressions and differential equations. SBML's modular design supports extensions for specialized modeling requirements including spatial modeling, flux balance analysis, and multi-scale simulations. Supported by over 270 software tools, SBML serves as the de facto standard for systems biology model exchange, enabling reproducible research, model sharing, database integration, and collaborative development in computational systems biology.	True	False	http://sbml.org/											doi:10.1515/jib-2017-0081	[B2AI_STANDARD:829]			[B2AI_ORG:19]		
B2AI_STANDARD:290	B2AI_STANDARD:BiomedicalStandard	SBRML	Systems Biology Results Markup Language	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:1]	Systems Biology has benefited tremendously from the development and use of SBML, which is a markup language to specify models composed of molecular species, and their interactions (including reactions). SBML is a common format that many systems biology software understand and thus it has become the way in which models are shared and communicated. Despite the popularity of SBML, that has resulted in many models being available in electronic format, there is currently no standard way of communicating the results of the operations carried out with such models (e.g. simulations). Here we propose a new markup language which is complementary to SBML and which is intended to specify results from operations carried out on models SBRML. In fact, this markup language is useful also to communicate experimental data as long as it is possible to express the data in terms of a reference SBML model. Thus SBRML is a means of specifying quantitative results in the context of a systems biology model.	False	False	https://sbrml.sourceforge.net/SBRML/Welcome.html																	
B2AI_STANDARD:291	B2AI_STANDARD:BiomedicalStandard	Tabix	Tabix index file format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:13]	A tab-delimited genome position index file format. The format can handle individual chromosomes up to 512 Mbp (2^29 bases) in length.	True	False	http://www.htslib.org/doc/tabix.html	https://samtools.github.io/hts-specs/tabix.pdf										doi:10.1093/bioinformatics/btq671						
B2AI_STANDARD:292	B2AI_STANDARD:BiomedicalStandard	TCS	Taxonomic Concept Transfer Schema	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datamodel]	[B2AI_TOPIC:1]	The development of an abstract model for a taxonomic concept, which can capture the various models represented and understood by the various data providers, is central to this project. This model is presented as an XML schema document that is proposed as a standard to allow exchange of data between different data models. It aims to capture data as understood by the data owners without distortion, and facilitate the query of different data resources according to the common schema model. The TCS schema was conceived to allow the representation of taxonomic concepts as defined in published taxonomic classifications, revisions and databases. As such, it specifies the structure for XML documents to be used for the transfer of defined concepts. Valid transfer documents may either explicitly detail the defining components of taxon concepts, transfer GUIDs referring to defined taxon concepts (if and when these are available) or a mixture of the two.	True	False	http://www.tdwg.org/standards/117	https://github.com/tdwg/tcs		[B2AI_ORG:93]	[{\"id\": \"B2AI_APP:50\", \"category\": \"B2AI:Application\", \"name\": \"Taxonomic Data Integration and Species Classification AI\", \"description\": \"TCS (Taxonomic Concept Schema) is used in AI applications for managing taxonomic concepts and classifications, enabling machine learning models to understand species relationships, resolve taxonomic synonyms, and integrate biodiversity data across different classification systems. AI systems leverage TCS to train models that automatically map species names across taxonomies, predict taxonomic placement of newly discovered organisms, and reconcile conflicting classifications. The schema supports natural language processing applications that extract taxonomic information from scientific literature, automated quality control of species occurrence records, and machine learning approaches to phylogenetic inference. TCS enables AI systems to reason about taxonomic hierarchies and evolutionary relationships when analyzing biological data.\", \"used_in_bridge2ai\": false}]	[Taxonomic Data Integration and Species Classification AI]	[B2AI_APP:50]		[TCS (Taxonomic Concept Schema) is used in AI applications for managing taxonomic concepts and classifications, enabling machine learning models to understand species relationships, resolve taxonomic synonyms, and integrate biodiversity data across different classification systems. AI systems leverage TCS to train models that automatically map species names across taxonomies, predict taxonomic placement of newly discovered organisms, and reconcile conflicting classifications. The schema supports natural language processing applications that extract taxonomic information from scientific literature, automated quality control of species occurrence records, and machine learning approaches to phylogenetic inference. TCS enables AI systems to reason about taxonomic hierarchies and evolutionary relationships when analyzing biological data.]	[B2AI:Application]	[False]							
B2AI_STANDARD:293	B2AI_STANDARD:BiomedicalStandard	TDWG SDS	Taxonomic Databases Working Group Standards Documentation Standard	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	This document defines how TDWG standards should be presented. Each standard is a logical directory or folder containing two or more files - a cover page outlining basic meta data for the standard and one or more normative files specifying the standard itself. Rules are specified for the naming of standards and files. Human readable files should be in English, follow basic layout principles and be marked up in XHTML. The legal statements that all documents must contain are defined.	True	False	http://www.tdwg.org/standards/147			[B2AI_ORG:93]														
B2AI_STANDARD:294	B2AI_STANDARD:BiomedicalStandard	ToxML	ToxML standard	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:3]	ToxML is an open data exchange standard that allows the representation and communication of toxicological and related data in a well-structured electronic format.	True	False	https://doi.org/10.1080/1062936X.2013.783506											doi:10.1080/1062936X.2013.783506						
B2AI_STANDARD:295	B2AI_STANDARD:BiomedicalStandard	TREND	Transparent Reporting of Evaluations with Nonrandomized Designs	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:5]	Evidence-based public health decisions are based on evaluations of intervention studies with randomized and nonrandomized designs. Transparent reporting is crucial for assessing the validity and efficacy of these intervention studies, and, it facilitates synthesis of the findings for evidence-based recommendations. Therefore, the mission of the Transparent Reporting of Evaluations with Nonrandomized Designs (TREND) group is to improve the reporting standards of nonrandomized evaluations of behavioral and public health intervention	True	False	https://www.cdc.gov/trendstatement/index.html	https://www.cdc.gov/trendstatement/pdf/trendstatement_TREND_Checklist.pdf		[B2AI_ORG:12]								doi:10.2105/ajph.94.3.361						
B2AI_STANDARD:296	B2AI_STANDARD:BiomedicalStandard	TIM+	Trusted Instant Messaging Plus Applicability Statement	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Trusted Instant Messaging (TIM+) defines a protocol that facilitates real-time communication and incorporates secure messaging concepts to ensure information is transmitted securely between known, trusted entities both within and across enterprises. TIM+ will determine the availability or presence of trusted endpoints and support text-based communication and file transfers.	True	False	https://directtrust.app.box.com/s/p3vp3g4bv52cyi4nbpyfpdal6t7z2qdz	https://directtrust.app.box.com/s/p3vp3g4bv52cyi4nbpyfpdal6t7z2qdz														[B2AI_ORG:26]		
B2AI_STANDARD:297	B2AI_STANDARD:BiomedicalStandard	TumorML	TumorML standard	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:4]	Originally developed as part of the FP7 Transatlantic Tumor Model Repositories project, TumorML has been developed as an XML-based domain-specific vocabulary that includes elements from existing vocabularies, to deal with storing and transmitting existing cancer models among research communities.	True	False	https://doi.org/10.1145/2544063.2544064											doi:10.1145/2544063.2544064						
B2AI_STANDARD:298	B2AI_STANDARD:BiomedicalStandard	UNII	Unique Ingredient Identifiers	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[codesystem]	[B2AI_TOPIC:3]	Unique Ingredient Identifiers (UNIIs) are alphanumeric codes generated by the FDA's Global Substance Registration System (GSRS) to uniquely identify substances based on their scientific identity characteristics using ISO 11238 data elements. Each UNII is derived from a substance's molecular structure and/or descriptive information, ensuring that the same substance always receives the same identifier regardless of when or where it is registered in the regulatory lifecycle. UNIIs enable efficient and accurate exchange of substance information across regulatory submissions, product labeling, adverse event reporting, and drug databases without ambiguity from varying nomenclature or trade names. The system covers diverse substance types including chemical compounds, proteins, nucleic acids, polymers, mixtures, structurally diverse materials, and specified substances (salts, stereoisomers, defined mixtures). UNIIs are generated at any time in the regulatory process and do not imply FDA review or approval. The GSRS database provides searchable access to UNIIs along with associated substance names, codes (CAS, INN, EC), structural representations, and attribute data. UNIIs are extensively used in SPL (Structured Product Labeling) documents, NDC (National Drug Code) directory, FAERS (FDA Adverse Event Reporting System), and international regulatory systems, facilitating interoperability between FDA databases and harmonization with global substance identification standards.	True	False	https://precision.fda.gov/uniisearch			[B2AI_ORG:31]														
B2AI_STANDARD:299	B2AI_STANDARD:BiomedicalStandard	VCF	Variant Call Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat|markuplanguage|standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:13|B2AI_TOPIC:35]	Variant Call Format (VCF) is a text file format (most likely stored in a compressed manner). It contains meta-information lines, a header line, and then data lines each containing information about a position in the genome.	True	False	https://en.wikipedia.org/wiki/Variant_Call_Format	https://github.com/samtools/hts-specs	True	[B2AI_ORG:34]	[{\"id\": \"B2AI_APP:51\", \"category\": \"B2AI:Application\", \"name\": \"Variant Pathogenicity Prediction and Genomic AI\", \"description\": \"VCF format is essential for AI applications in genomics, particularly for training machine learning models to predict variant pathogenicity, disease associations, and functional impacts. Deep learning models parse VCF files to extract genetic variants and their annotations for tasks such as rare disease diagnosis, cancer genome interpretation, and pharmacogenomic prediction. AI systems leverage VCF's structured representation of genomic variants, quality scores, and population frequencies to build models that integrate variant-level data with clinical phenotypes, enabling precision medicine applications and automated variant classification according to ACMG guidelines.\", \"used_in_bridge2ai\": false}]	[Variant Pathogenicity Prediction and Genomic AI]	[B2AI_APP:51]		[VCF format is essential for AI applications in genomics, particularly for training machine learning models to predict variant pathogenicity, disease associations, and functional impacts. Deep learning models parse VCF files to extract genetic variants and their annotations for tasks such as rare disease diagnosis, cancer genome interpretation, and pharmacogenomic prediction. AI systems leverage VCF's structured representation of genomic variants, quality scores, and population frequencies to build models that integrate variant-level data with clinical phenotypes, enabling precision medicine applications and automated variant classification according to ACMG guidelines.]	[B2AI:Application]	[False]					[B2AI_ORG:117]		
B2AI_STANDARD:300	B2AI_STANDARD:BiomedicalStandard	VEP	Variant Effect Predictor format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:5]	A text format devised by Ensembl for the eponymous Variant Effect Predictor tool.	True	False	https://useast.ensembl.org/info/docs/tools/vep/vep_formats.html			[B2AI_ORG:124]												[B2AI_ORG:29]		
B2AI_STANDARD:301	B2AI_STANDARD:BiomedicalStandard	VRS	Variation Representation Specification	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:5]	The Variation Representation Specification (VRS, pronounced verse) is a standard developed by the Global Alliance for Genomics and Health (GA4GH) to facilitate and improve sharing of genetic information. The Specification consists of a JSON Schema for representing many classes of genetic variation, conventions to maximize the utility of the schema, and a Python implementation that promotes adoption of the standard.	True	False	https://vrs.ga4gh.org/en/latest/index.html		True	[B2AI_ORG:34]	[{\"id\": \"B2AI_APP:52\", \"category\": \"B2AI:Application\", \"name\": \"Standardized Variant Representation for ML Interoperability\", \"description\": \"VRS (Variation Representation Specification) is used in AI applications to create unambiguous, computationally accessible representations of genetic variants that enable machine learning model interoperability across different genomic databases and variant calling pipelines. AI systems leverage VRS to normalize variant representations, resolve ambiguities in variant nomenclature, and create consistent feature representations for training models that predict variant effects, interpret clinical significance, or perform variant prioritization. VRS's precise coordinate system and allele representation enable AI models to correctly match variants across different genome builds, integrate data from multiple sources, and generate reproducible predictions that can be shared across institutions and validated against standardized variant benchmarks.\", \"used_in_bridge2ai\": false}]	[Standardized Variant Representation for ML Interoperability]	[B2AI_APP:52]		[VRS (Variation Representation Specification) is used in AI applications to create unambiguous, computationally accessible representations of genetic variants that enable machine learning model interoperability across different genomic databases and variant calling pipelines. AI systems leverage VRS to normalize variant representations, resolve ambiguities in variant nomenclature, and create consistent feature representations for training models that predict variant effects, interpret clinical significance, or perform variant prioritization. VRS's precise coordinate system and allele representation enable AI models to correctly match variants across different genome builds, integrate data from multiple sources, and generate reproducible predictions that can be shared across institutions and validated against standardized variant benchmarks.]	[B2AI:Application]	[False]	doi:10.1016/j.xgen.2021.100027				[B2AI_ORG:117]		
B2AI_STANDARD:302	B2AI_STANDARD:BiomedicalStandard	VarioML	VarioML format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12|B2AI_TOPIC:35]	A Locus-specific Database (LSDB) describes the variants discovered on a single gene or members of a gene family and other related functional elements. LSDBs are curated by experts on their respective loci, and as such are typically the best resources of such information available. But LSDBs vary widely in format and completeness, making data integration and exchange among them difficult and time-consuming. To address these difficulties, the VarioML format has been developed for the full range of variation data use-cases, providing semantically well-defined components which can be easily composed to fit specific needs. Using VarioML, data owners can now efficiently enable the integration, federation, and exchange of their variant data. The discoverabiliaty, extensibility, and quality of variation data is immediately enhanced. Critical new avenues of research and knowledge discovery are opened, as data using the VarioML standard can be integrated with the global library of purely genetic data. VarioML is a central prerequisite for effective modelling of phenotype data and genotype-to-phenotype relationships. It removes the long-standing technical obstacles to the effective passing of variant data from discovery laboratories into the biomedical database world. Now all that is needed is the broad participation of the genotype-to-phenotype research community.	True	False	https://github.com/VarioML/VarioML	https://github.com/VarioML/VarioML																
B2AI_STANDARD:303	B2AI_STANDARD:BiomedicalStandard	VMS	Vocabulary Maintenance Standard	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	The Vocabulary Maintenance Standard (VMS) is a TDWG (Biodiversity Information Standards) specification that defines formal procedures for maintaining and evolving controlled vocabularies used in biodiversity informatics. Unlike traditional standards that remain relatively static, VMS recognizes that vocabulary standards must adapt incrementally to meet evolving community needs without requiring the full standards ratification process for every change. The specification categorizes types of vocabulary modifications (additions, modifications, deprecations) and establishes governance mechanisms for implementing these changes through designated vocabulary maintenance groups. VMS defines roles and responsibilities for stakeholders including vocabulary maintainers, Interest Groups, and the broader TDWG community, specifying how change proposals are submitted, reviewed, and approved. The standard ensures that vocabulary evolution remains transparent, traceable, and backward-compatible where possible, while maintaining the stability needed for production systems. VMS applies to major TDWG vocabularies including Darwin Core terms and is essential for maintaining semantic interoperability across biodiversity data systems as scientific understanding and data practices evolve.	True	False	https://www.tdwg.org/standards/vms/															[B2AI_ORG:93]		
B2AI_STANDARD:304	B2AI_STANDARD:BiomedicalStandard	VHI	Voice Handicap Index	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[diagnosticinstrument]	[B2AI_TOPIC:9]	A statistically robust Voice Handicap Index (VHI). An 85-item version of this instrument was administered to 65 consecutive patients seen in the Voice Clinic at Henry Ford Hospital. The data were subjected to measures of internal consistency reliability and the initial 85-item version was reduced to a 30-item final version. This final version was administered to 63 consecutive patients on two occasions in an attempt to assess test-retest stability, which proved to be strong. The findings of the latter analysis demonstrated that a change between two administrations of 18 points represents a significant shift in psychosocial function.	False	False	https://doi.org/10.1044/1058-0360.0603.66											doi:10.1044/1058-0360.0603.66						
B2AI_STANDARD:305	B2AI_STANDARD:BiomedicalStandard	HCLS	W3C HCLS Dataset Description	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Access to consistent, high-quality metadata is critical to finding, understanding, and reusing scientific data. This document describes a consensus among participating stakeholders in the Health Care and the Life Sciences domain on the description of datasets using the Resource Description Framework (RDF). This specification meets key functional requirements, reuses existing vocabularies to the extent that it is possible, and addresses elements of data description, versioning, provenance, discovery, exchange, query, and retrieval.	True	False	https://www.w3.org/TR/hcls-dataset/			[B2AI_ORG:99]														
B2AI_STANDARD:306	B2AI_STANDARD:BiomedicalStandard	WIG	Wiggle Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:12|B2AI_TOPIC:13]	Wiggle files and its bedgraph variant allow you to plot quantitative data as either shades of color (dense mode) or bars of varying height (full and pack mode) on the genome.	True	False	https://genome.ucsc.edu/goldenPath/help/wiggle.html			[B2AI_ORG:119]														
B2AI_STANDARD:307	B2AI_STANDARD:BiomedicalStandard	LSM	Zeiss LSM series confocal microscope image	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[audiovisual|fileformat]	[B2AI_TOPIC:19]	A proprietary image format based on TIFF.	False	False	https://openwetware.org/wiki/Dissecting_LSM_files			[B2AI_ORG:101]											[B2AI_SUBSTRATE:19]			
B2AI_STANDARD:308	B2AI_STANDARD:DataStandard	AMR	Adaptive Multi-Rate Speech Codec	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[audiovisual]	[B2AI_TOPIC:37]	Audio data compression scheme optimized for speech coding, adopted in October 1998 as the standard speech codec by 3GPP (3d Generation Partnership Project) and now widely used in GSM (Global System for Mobile Communications).	True	False	https://www.loc.gov/preservation/digital/formats/fdd/fdd000254.shtml														[B2AI_SUBSTRATE:49]			
B2AI_STANDARD:309	B2AI_STANDARD:DataStandard	WIFF	Analyst native acquisition file format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:28]	Mass spectra output format used by AB SCIEX intstruments.	False	False	https://doi.org/10.1074/mcp.R112.019695											doi:10.1074/mcp.R112.019695						
B2AI_STANDARD:310	B2AI_STANDARD:DataStandard	HDR/IMG	Analyze file format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:15]	Analyze 7.5 can store voxel-based volumes and consists of two files - One file with the actual data in a binary format with the filename extension .img and another file (header with filename extension .hdr) with information about the data such as voxel size and number of voxel in each dimension.	True	False	https://analyzedirect.com/documents/AD_AnalyzeImage75_File_Format.pdf	https://analyzedirect.com/documents/AD_AnalyzeImage75_File_Format.pdf																
B2AI_STANDARD:311	B2AI_STANDARD:DataStandard	Arrow	Apache Arrow	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware like CPUs and GPUs. The Arrow memory format also supports zero-copy reads for lightning-fast data access without serialization overhead.	True	False	https://arrow.apache.org/	https://github.com/apache/arrow														[B2AI_ORG:5]		
B2AI_STANDARD:312	B2AI_STANDARD:DataStandard	Avro	Apache Avro	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:5]	Avro is a row-oriented remote procedure call and data serialization framework developed within Apache's Hadoop project. It uses JSON for defining data types and protocols, and serializes data in a compact binary format.	True	False	https://avro.apache.org/	https://github.com/apache/avro														[B2AI_ORG:5]		
B2AI_STANDARD:313	B2AI_STANDARD:DataStandard	ADMS	Asset Description Metadata Schema	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	ADMS is a profile of DCAT, used to describe semantic assets (or just 'Assets'), defined as highly reusable metadata (e.g. xml schemata, generic data models) and reference data (e.g. code lists, taxonomies, dictionaries, vocabularies) that are used for eGovernment system development.	True	False	https://www.w3.org/TR/vocab-adms/															[B2AI_ORG:99]		
B2AI_STANDARD:314	B2AI_STANDARD:DataStandard	AIFF	Audio Interchange File Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[audiovisual|fileformat]	[B2AI_TOPIC:37]	File format for sound that wraps various sound bitstreams, ranging from uncompressed waveform to MIDI.	True	False	https://www.loc.gov/preservation/digital/formats/fdd/fdd000005.shtml														[B2AI_SUBSTRATE:49]			
B2AI_STANDARD:315	B2AI_STANDARD:DataStandard	AVI	Audio Video Interleave digital multimedia container format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[audiovisual|fileformat]	[B2AI_TOPIC:15|B2AI_TOPIC:37]	AVI files can contain both audio and video data in a file container that allows synchronous audio-with-video playback.	True	False	https://en.wikipedia.org/wiki/Audio_Video_Interleave														[B2AI_SUBSTRATE:49]			
B2AI_STANDARD:316	B2AI_STANDARD:DataStandard	BMP	Bitmap format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[audiovisual|fileformat]	[B2AI_TOPIC:15]	BMP (Bitmap) is a raster graphics file format developed by Microsoft for storing device-independent bitmaps (DIBs) in Windows and OS/2 systems. The format consists of a file header, DIB header (with multiple versions: BITMAPCOREHEADER, BITMAPINFOHEADER, BITMAPV4HEADER, BITMAPV5HEADER), optional color palette, and pixel array storing uncompressed or RLE-compressed bitmap data. BMP supports 1, 4, 8, 16, 24, and 32 bits per pixel, accommodating indexed color (palettes), RGB color, and alpha channels for transparency. The format uses little-endian byte ordering and stores pixel rows bottom-to-top by default, with each row padded to 4-byte boundaries. BMP files can contain color profiles (ICC) for color management and support various compression methods including BI_RGB (uncompressed), BI_RLE4/BI_RLE8 (run-length encoding), and BI_BITFIELDS (custom RGB bit masks). The simplicity, widespread familiarity, and open format specification make BMP common in Windows applications, image processing software, and scientific imaging. While BMP files are typically large due to minimal compression, they compress efficiently with lossless algorithms (ZIP, RAR). BMP is used in GDI (Graphics Device Interface) subsystems, icons (ICO), cursors (CUR), and as an intermediate format for image processing. Software support is extensive: Adobe Photoshop, GIMP, Microsoft Office, browsers (Chrome, Edge), and programming libraries across platforms. In AI/ML imaging applications, BMP serves as a raw, lossless format for medical imaging (preserving pixel accuracy), training data preparation (avoiding compression artifacts), image annotation workflows, and intermediate processing steps where format simplicity facilitates pixel-level manipulation for computer vision tasks, though typically converted to compressed formats for efficient storage and transmission.	True	False	https://en.wikipedia.org/wiki/BMP_file_format														[B2AI_SUBSTRATE:19]			
B2AI_STANDARD:317	B2AI_STANDARD:DataStandard	Cap'n'Proto	Cap'n'Proto	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Capn Proto is an insanely fast data interchange format and capability-based RPC system. Think JSON, except binary. Or think Protocol Buffers, except faster.	True	False	https://capnproto.org/	https://github.com/capnproto/capnproto																
B2AI_STANDARD:318	B2AI_STANDARD:DataStandard	CellML	CellML language	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:5]	CellML language is an open standard based on the XML markup language. CellML is being developed by the Auckland Bioengineering Institute at the University of Auckland and affiliated research groups. The purpose of CellML is to store and exchange computer-based mathematical models. CellML allows scientists to share models even if they are using different model-building software. It also enables them to reuse components from one model in another, thus accelerating model building.	True	False	https://www.cellml.org/											doi:10.1186/1471-2105-11-178						
B2AI_STANDARD:319	B2AI_STANDARD:DataStandard	CFF	citation-file-format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A file format for providing citation metadata for software or datasets in plaintext files that are easy to read by both humans and machines.	True	False	https://citation-file-format.github.io/	https://github.com/citation-file-format/citation-file-format																
B2AI_STANDARD:320	B2AI_STANDARD:DataStandard	CWL	Common Workflow Language	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[workflowlanguage]	[B2AI_TOPIC:5]	specification for describing data analysis workflows and tools	True	False	https://www.commonwl.org/	https://github.com/common-workflow-language/common-workflow-language										doi:10.48550/arXiv.2105.07028						
B2AI_STANDARD:321	B2AI_STANDARD:DataStandard	CURATE(D)	CURATE(D) Checklists	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	The CURATE(D) steps are a teaching and representation tool. This model is useful for onboarding data curators and orienting researchers preparing to share their data. It serves as a demonstration for the type of work involved in robust data curation, and was created to fit within institution-specific data repository workflows.	True	False	https://datacurationnetwork.org/outputs/workflows/	https://docs.google.com/document/d/1RWt2obXOOeJRRFmVo9VAkl4h41cL33Zm5YYny3hbPZ8/edit																
B2AI_STANDARD:322	B2AI_STANDARD:DataStandard	DAMA-DMBOK	DAMA Data Management Body of Knowledge	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Reference guide for processes, best practices, and principles in data management.	False	True	https://www.dama.org/cpages/body-of-knowledge															[B2AI_ORG:22]		
B2AI_STANDARD:323	B2AI_STANDARD:DataStandard	DCAT	Data Catalog Vocabulary	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	An RDF vocabulary designed to facilitate interoperability between data catalogs published on the Web.	True	False	https://www.w3.org/TR/vocab-dcat-1/															[B2AI_ORG:99]		
B2AI_STANDARD:324	B2AI_STANDARD:DataStandard	DataCite	DataCite Metadata Schema	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	The DataCite Metadata Schema is a list of core metadata properties chosen for the accurate and consistent identification of a resource for citation and retrieval purposes, along with recommended use instructions. The resource that is being identified can be of any kind, but it is typically a dataset. We use the term dataset in its broadest sense. We mean it to include not only numerical data, but any other research data outputs.	True	False	https://schema.datacite.org/	https://schema.datacite.org/meta/kernel-4.3/metadata.xsd																
B2AI_STANDARD:325	B2AI_STANDARD:DataStandard	Dataset Cards	Dataset Cards	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Dataset Cards are structured documentation templates for describing machine learning datasets, introduced by Google and adopted widely in the ML community, particularly through Hugging Face's Datasets library. Dataset Cards provide standardized metadata capturing dataset purpose, structure, creation methodology, intended uses, limitations, and ethical considerations. The documentation includes: dataset summary and description; language coverage and data sources; supported ML tasks and features; dataset structure (splits, configurations, data fields); creation process (annotations, quality control, collection methodology); considerations for using the data (social impact, biases, privacy concerns); additional resources (papers, repositories, licenses). Dataset Cards promote responsible AI by making dataset characteristics transparent, helping practitioners assess fitness for purpose, understand potential biases, and evaluate dataset limitations before use. The structured format enables dataset discovery on ML platforms, facilitates reproducibility by documenting provenance, and encourages accountability in dataset creation and curation. Hugging Face hosts 100,000+ dataset cards in their Hub, standardizing documentation for NLP, computer vision, audio, and multimodal datasets. Dataset Cards complement Model Cards by extending documentation principles to training data, addressing data quality, collection practices, annotation procedures, and data ethics. In AI/ML pipelines, Dataset Cards support informed dataset selection, bias mitigation through transparency, reproducible research by documenting data versions, and regulatory compliance (AI Act, GDPR) by clarifying data provenance, consent, and usage restrictions.	True	False	https://huggingface.co/docs/datasets/dataset_card																	
B2AI_STANDARD:326	B2AI_STANDARD:DataStandard	Datasheets	Datasheets for Datasets	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datasheets]	[B2AI_TOPIC:5]	...we propose that every dataset be accompanied with a datasheet that documents its motivation, composition, collection process, recommended uses, and so on.	True	False	https://arxiv.org/abs/1803.09010											https://arxiv.org/abs/1803.09010						
B2AI_STANDARD:327	B2AI_STANDARD:DataStandard	XGMML	eXtensible Graph Markup and Modeling Language	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:21]	eXtensible Graph Markup and Modeling Language is an XML application based on GML which is used for graph description. XGMML uses tags to describe nodes and edges of a graph. The purpose of XGMML is to make possible the exchange of graphs between differents authoring and browsing tools for graphs.	True	False	http://xml.coverpages.org/xgmml.html																	
B2AI_STANDARD:328	B2AI_STANDARD:DataStandard	FAIR4RS	FAIR Principles for Research Software	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:5]	Simple and research software appropriate goalposts to inform those who publish and/or preserve research software.	True	False	https://www.rd-alliance.org/group/fair-research-software-fair4rs-wg/outcomes/fair-principles-research-software-fair4rs											doi:10.1038/s41597-022-01710-x				[B2AI_ORG:83]		
B2AI_STANDARD:329	B2AI_STANDARD:DataStandard	FIPS	Federal Information Processing System Codes for States and Counties	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[codesystem|deprecated]	[B2AI_TOPIC:6]	FIPS codes are numbers which uniquely identify geographic areas. As of database version 55, FIPS has been merged with the Geographic Names Information System (GNIS).	True	False	https://transition.fcc.gov/oet/info/maps/census/fips/fips.txt	https://transition.fcc.gov/oet/info/maps/census/fips/fips.txt																
B2AI_STANDARD:330	B2AI_STANDARD:DataStandard	FieldML	FieldML	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:5]	FieldML is a declarative language for building hierarchical models represented by generalized mathematical fields. FieldML is developed as a data model and accompanying API. Find out more about the FieldML API, where to get the latest release and how to contribute to its development. FieldML is a declarative language for representing hierarchical models using generalized mathematical fields. FieldML can be used to represent the dynamic 3D geometry and solution fields from computational models of cells, tissues and organs. It enables model interchange for the bioengineering and general engineering analysis communities. Example uses are models of tissue structure, the distribution of proteins and other biochemical compounds, anatomical annotation, and other biological annotation.	True	False	https://doi.org/10.1007/s11517-013-1097-7											doi:10.1007/s11517-013-1097-7						
B2AI_STANDARD:331	B2AI_STANDARD:DataStandard	FLAC	Free Lossless Audio Codec	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[audiovisual|fileformat]	[B2AI_TOPIC:37]	FLAC (Free Lossless Audio Codec) is an audio coding format for lossless compression of digital audio.	True	False	https://en.wikipedia.org/wiki/FLAC	https://xiph.org/flac/format.html													[B2AI_SUBSTRATE:49]			
B2AI_STANDARD:332	B2AI_STANDARD:DataStandard	Frictionless	Frictionless Data Package	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Frictionless Data Package is a specification for packaging, describing, and sharing datasets in a lightweight, standardized container format developed by Open Knowledge Foundation. A Data Package consists of a descriptor (datapackage.json) containing metadata about the package and its resources, along with the data files themselves (local or remote). The specification defines comprehensive metadata including: package title, description, licenses, version, sources, contributors, and keywords; resource information for each data file (path/URL, format, schema, encoding); Table Schema for tabular data defining field names, types, constraints, and relationships; and optional goodtables validation rules. Frictionless supports FAIR data principles by ensuring datasets are Findable (rich metadata), Accessible (standard formats, clear paths), Interoperable (JSON-based descriptor, Table Schema), and Reusable (license, provenance, structure documentation). The ecosystem includes libraries for Python, JavaScript, R, Ruby, PHP, and command-line tools for validation, conversion, and publishing. Frictionless integrates with data portals (CKAN, Dataverse), notebooks (Jupyter), and analysis tools, enabling seamless dataset exchange and processing. Extensions support specialized data types (geospatial, time series, budget data) and validation frameworks. In AI/ML workflows, Frictionless Data Packages standardize dataset distribution for reproducible research, provide machine-readable schemas for automated data validation and ingestion, enable data versioning and provenance tracking for ML pipelines, and facilitate dataset documentation complementing model training by ensuring data quality, consistency, and interpretability across collaborative ML projects.	True	False	https://specs.frictionlessdata.io/data-package/	https://github.com/frictionlessdata/specs																
B2AI_STANDARD:333	B2AI_STANDARD:DataStandard	serviceinfo	GA4GH serviceinfo	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Provides a way for an API to expose a set of metadata to help discovery and aggregation of services via computational methods.	True	False	https://github.com/ga4gh-discovery/ga4gh-service-info															[B2AI_ORG:34]		
B2AI_STANDARD:334	B2AI_STANDARD:DataStandard	gxformat2	Galaxy workflow Format 2	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[workflowlanguage]	[B2AI_TOPIC:20]	A schema moving Galaxy's workflow description language toward standards such as the Common Workflow Language.	True	False	https://github.com/galaxyproject/gxformat2	https://github.com/galaxyproject/gxformat2											[B2AI_STANDARD:766]					
B2AI_STANDARD:335	B2AI_STANDARD:DataStandard	GNIS ID	Geographic Names Information System Feature IDs	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:14]	The Geographic Names Information System (GNIS) is the Federal and national standard for geographic nomenclature. The U.S. Geological Survey's National Geospatial Program developed the GNIS in support of the U.S. Board on Geographic Names as the official repository of domestic geographic names data, the official vehicle for geographic names use by all departments of the Federal Government, and the source for applying geographic names to Federal electronic and printed products.	True	False	https://www.usgs.gov/us-board-on-geographic-names/domestic-names	https://www.usgs.gov/u.s.-board-on-geographic-names/download-gnis-data														[B2AI_ORG:98]		
B2AI_STANDARD:336	B2AI_STANDARD:DataStandard	GENC	Geopolitical Entities, Names, and Codes	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[codesystem]	[B2AI_TOPIC:6]	The GENC Standard specifies a profile of ISO 3166 codes for the representation of names of countries and their subdivisions.	True	False	https://www.dni.gov/index.php/who-we-are/organizations/ic-cio/ic-cio-related-menus/ic-cio-related-links/ic-technical-specifications/geopolitical-entities-names-and-codes	https://evs.nci.nih.gov/ftp1/GENC/																
B2AI_STANDARD:337	B2AI_STANDARD:DataStandard	GraphML	Graph Markup Language	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat|markuplanguage]	[B2AI_TOPIC:21]	An XML file format and language for describing graphs.	True	False	http://graphml.graphdrawing.org/specification.html												[B2AI_STANDARD:829]					
B2AI_STANDARD:338	B2AI_STANDARD:DataStandard	GIF	Graphics Interchange Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:15]	Graphics Interchange Format (GIF) is a bitmap image format developed by CompuServe in 1987 that supports lossless LZW compression, up to 256 colors from a 24-bit RGB palette, transparency, and multi-frame animation with per-frame delay controls. GIF became widely adopted on the early web for its efficient compression and cross-platform compatibility, and remains ubiquitous for short animated loops and simple graphics with well-defined edges. The format supports two versions (GIF87a and GIF89a), with the latter adding animation delays, transparent backgrounds, and application-specific metadata. While the LZW patent controversy drove development of PNG as an alternative, GIF's animation capabilities and \"hot\" data accessibility (frames stored sequentially without random access penalties) maintain its relevance for social media reactions, educational demonstrations, and web UI elements. Modern applications leverage GIF's deterministic looping (via Netscape Application Block extension) and universal browser support, though video formats like WebP and MP4 increasingly replace GIF for better compression ratios. The format's 256-color limitation makes it suitable for logos, diagrams, and pixel art but less appropriate for photographs or gradients, where dithering techniques are often applied. GIF files consist of a logical screen descriptor, optional global color table, image descriptors with optional local color tables, and LZW-encoded pixel data stored in sub-blocks. For AI/ML workflows, GIF serves as a compact format for visualizing model predictions over time series, displaying attention mechanisms frame-by-frame, and sharing animated training/validation metrics without requiring video codec dependencies.	True	False	https://en.wikipedia.org/wiki/GIF																	
B2AI_STANDARD:339	B2AI_STANDARD:DataStandard	HDF5	HDF5 format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat|standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:5]	HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data. HDF5 is portable and is extensible, allowing applications to evolve in their use of HDF5. The HDF5 Technology suite includes tools and applications for managing, manipulating, viewing, and analyzing data in the HDF5 format.	True	False	https://www.hdfgroup.org/solutions/hdf5/		True												[B2AI_SUBSTRATE:16]	[B2AI_ORG:116]		
B2AI_STANDARD:340	B2AI_STANDARD:DataStandard	HDMF	Hierarchical Data Modeling Framework for Modern Science Data Standards	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:5|B2AI_TOPIC:48]	HDMF is a hierarchical data modeling framework for modern science data standards. It separates data standardization into three main components: (1) data modeling and specification, (2) data I/O and storage, and (3) data interaction and data APIs. HDMF provides object mapping infrastructure to insulate and integrate these components, supporting flexible development of data standards and extensions, optimized storage backends, and data APIs. It offers advanced data I/O functionality for iterative data write, lazy data load, parallel I/O, and modular data storage. HDMF is particularly used to design NWB 2.0, a data standard for neurophysiology data.	True	False	https://hdmf-common-schema.readthedocs.io/en/latest/format.html	https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8500680/										doi:10.1109/bigdata47090.2019.9005648	[B2AI_STANDARD:339|B2AI_STANDARD:379]		[B2AI_SUBSTRATE:16]			2025-05-29
B2AI_STANDARD:341	B2AI_STANDARD:DataStandard	ISA-Tab	ISA-Tab format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:5]	General-purpose ISA-Tab file format - an extensible, hierarchical structure that focuses on the description of the experimental metadata (i.e. sample characteristics, technology and measurement types, sample-to-data relationships).	True	False	https://isa-specs.readthedocs.io/en/latest/isatab.html	https://github.com/ISA-tools/ISAdatasets														[B2AI_ORG:47]		
B2AI_STANDARD:342	B2AI_STANDARD:DataStandard	ISO 3166	ISO 3166 Country Codes	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[codesystem]	[B2AI_TOPIC:6]	The purpose of ISO 3166 is to define internationally recognized codes of letters and/or numbers that we can use when we refer to countries and their subdivisions. However, it does not define the names of countries  this information comes from United Nations sources (Terminology Bulletin Country Names and the Country and Region Codes for Statistical Use maintained by the United Nations Statistics Divisions).	True	False	https://www.iso.org/iso-3166-country-codes.html	https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes														[B2AI_ORG:49]		
B2AI_STANDARD:343	B2AI_STANDARD:DataStandard	ISO 8601	ISO 8601 Date and time format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A way of presenting dates and times that is clearly defined and understandable to both people and machines.	True	False	https://www.iso.org/iso-8601-date-and-time-format.html															[B2AI_ORG:49]		
B2AI_STANDARD:344	B2AI_STANDARD:DataStandard	JPEG	Joint Photographic Experts Group Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[audiovisual|fileformat|standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:15]	A method of lossy compression for digital images.	True	False	https://en.wikipedia.org/wiki/JPEG		True		[{\"id\": \"B2AI_APP:53\", \"category\": \"B2AI:Application\", \"name\": \"Medical Image Compression and Transfer Learning\", \"description\": \"JPEG format is used in AI applications for efficient storage and transmission of medical images, particularly in telepathology, dermatology AI, and mobile diagnostic applications where bandwidth and storage constraints are critical. Deep learning models are trained on JPEG-compressed whole slide images, dermoscopic images, and fundus photographs to perform tasks such as cancer detection, skin lesion classification, and diabetic retinopathy screening. While DICOM remains the standard for radiology, JPEG enables AI deployment in resource-constrained settings and supports transfer learning from natural image datasets (ImageNet) to medical imaging domains. AI researchers must account for JPEG compression artifacts when training models, and recent work explores AI-optimized compression techniques that preserve diagnostically relevant features.\", \"used_in_bridge2ai\": false}]	[Medical Image Compression and Transfer Learning]	[B2AI_APP:53]		[JPEG format is used in AI applications for efficient storage and transmission of medical images, particularly in telepathology, dermatology AI, and mobile diagnostic applications where bandwidth and storage constraints are critical. Deep learning models are trained on JPEG-compressed whole slide images, dermoscopic images, and fundus photographs to perform tasks such as cancer detection, skin lesion classification, and diabetic retinopathy screening. While DICOM remains the standard for radiology, JPEG enables AI deployment in resource-constrained settings and supports transfer learning from natural image datasets (ImageNet) to medical imaging domains. AI researchers must account for JPEG compression artifacts when training models, and recent work explores AI-optimized compression techniques that preserve diagnostically relevant features.]	[B2AI:Application]	[False]				[B2AI_SUBSTRATE:19]	[B2AI_ORG:116]		
B2AI_STANDARD:345	B2AI_STANDARD:DataStandard	JSON-schema	JSON-schema	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	JSON Schema is a vocabulary for annotating and validating JSON documents, providing a declarative format for describing the structure, constraints, and semantics of JSON data. As an IETF standard (draft specifications progressing toward RFC status), JSON Schema defines a JSON-based format for specifying the expected shape of JSON data, including data types, required properties, value ranges, string patterns, array constraints, and object structures. The schema itself is expressed in JSON, enabling meta-schema validation and recursive definitions. JSON Schema supports multiple vocabularies including Core, Validation, Hyper-Schema for hypermedia, and Format for semantic validation. Widely adopted across industries with 60+ million weekly downloads, it powers API documentation (OpenAPI/Swagger), configuration validation, code generation, form generation in UIs, and data interchange contracts. Major implementations exist in JavaScript, Python, Java, Go, and 40+ other languages, with extensive tooling ecosystem including validators, generators, linters, and editors. JSON Schema enables confident JSON data handling through streamlined testing, seamless data exchange via shared understanding, and clear documentation for developer collaboration. In AI/ML contexts, JSON Schema validates training data structures, API request/response payloads, configuration files for ML pipelines, and ensures data quality for machine learning workflows by catching inconsistencies and schema violations at ingestion time.	True	False	https://json-schema.org/	https://github.com/json-schema-org/json-schema-spec											[B2AI_STANDARD:761]					
B2AI_STANDARD:346	B2AI_STANDARD:DataStandard	KGX	Knowledge Graph Exchange	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:21]	Knowledge Graph Exchange (KGX) is a standardized graph-oriented data format for exchanging knowledge graphs, developed by the Biolink community to facilitate interoperability between biomedical knowledge bases. KGX provides both a formal specification and a Python toolkit (kgx library) for transforming, validating, and exchanging knowledge graphs conforming to the Biolink Model. The format represents graphs as nodes (entities with identifiers, categories, and properties) and edges (relationships with predicates, subject/object references, and provenance), serialized in JSON, TSV, or RDF formats. KGX ensures semantic alignment by enforcing Biolink Model categories (e.g., Gene, Disease, Pathway) and predicates (e.g., causes, treats, interacts_with), enabling consistent cross-database integration. The toolkit supports graph transformations (merging, filtering, mapping), format conversions (Neo4j, RDF, GraphML), and validation against Biolink Model constraints. KGX is foundational for Translator knowledge graphs, integrating data from 150+ biomedical sources including Monarch Initiative, NCATS Biomedical Data Translator, and Clinical Data Commons. In AI/ML applications, KGX-formatted knowledge graphs power link prediction for drug repurposing, knowledge graph embeddings for biomedical entity representation learning, reasoning algorithms for hypothesis generation, and multi-modal knowledge integration combining genomics, phenotypes, pathways, and clinical data to support precision medicine and systems biology research.	True	False	https://github.com/biolink/kgx/blob/master/specification/kgx-format.md												[B2AI_STANDARD:783]		[B2AI_SUBSTRATE:21]			
B2AI_STANDARD:347	B2AI_STANDARD:DataStandard	LinkML	LinkML	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage|standards_process_maturity_development|implementation_maturity_pilot]	[B2AI_TOPIC:5]	LinkML is a flexible modeling language that allows you to author schemas in YAML that describe the structure of your data. Additionally, it is a framework for working with and validating data in a variety of formats (JSON, RDF, TSV), with generators for compiling LinkML schemas to other frameworks.	True	False	https://linkml.io/linkml	https://github.com/linkml/linkml	True		[{\"id\": \"B2AI_APP:54\", \"category\": \"B2AI:Application\", \"name\": \"AI-Ready Data Schema Design and Validation\", \"description\": \"LinkML is used in AI applications to define machine-readable data schemas that enable automated data validation, transformation, and integration for machine learning pipelines. AI systems leverage LinkML schemas to ensure data quality and consistency across heterogeneous biomedical datasets, automatically generate data loaders and validators for ML frameworks, and create semantic mappings that allow AI models to understand relationships between data elements. LinkML's ability to compile to multiple formats (JSON Schema, SHACL, SQL DDL) makes it particularly valuable for building reproducible AI/ML workflows where data provenance and validation are critical.\", \"used_in_bridge2ai\": false}]	[AI-Ready Data Schema Design and Validation]	[B2AI_APP:54]		[LinkML is used in AI applications to define machine-readable data schemas that enable automated data validation, transformation, and integration for machine learning pipelines. AI systems leverage LinkML schemas to ensure data quality and consistency across heterogeneous biomedical datasets, automatically generate data loaders and validators for ML frameworks, and create semantic mappings that allow AI models to understand relationships between data elements. LinkML's ability to compile to multiple formats (JSON Schema, SHACL, SQL DDL) makes it particularly valuable for building reproducible AI/ML workflows where data provenance and validation are critical.]	[B2AI:Application]	[False]				[B2AI_SUBSTRATE:41|B2AI_SUBSTRATE:6]	[B2AI_ORG:116]		
B2AI_STANDARD:348	B2AI_STANDARD:DataStandard	MathML	MathML	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A product of the W3C Math Working Group, MathML is a low-level specification for describing mathematics as a basis for machine to machine communication which provides a much needed foundation for the inclusion of mathematical expressions in Web pages. It is also important in publishing workflows for science and technology and wherever mathematics has to be handled by software.	True	False	https://www.w3.org/Math/whatIsMathML.html															[B2AI_ORG:99]		
B2AI_STANDARD:349	B2AI_STANDARD:DataStandard	xlsx	Microsoft Excel spreadsheet container file	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:5]	Format used by Microsoft Excel spreadsheet software.	False	False	https://www.iso.org/standard/71691.html	https://www.iso.org/standard/71691.html														[B2AI_ORG:56]		
B2AI_STANDARD:350	B2AI_STANDARD:DataStandard	Model Cards	Model Cards	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[modelcards]	[B2AI_TOPIC:5]	Structured documentation detailing performance characteristics of machine learning models.	True	False	https://modelcards.withgoogle.com/about	https://github.com/tensorflow/model-card-toolkit/blob/master/model_card_toolkit/schema/v0.0.2/model_card.schema.json										https://arxiv.org/abs/1810.03993				[B2AI_ORG:37]		
B2AI_STANDARD:351	B2AI_STANDARD:DataStandard	MP3	MPEG-1 Audio Layer 3 \\| MPEG-2 Audio Layer 3	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[audiovisual|fileformat]	[B2AI_TOPIC:37]	MP3 (formally MPEG-1 Audio Layer III or MPEG-2 Audio Layer III) is a coding format for digital audio.	True	False	https://en.wikipedia.org/wiki/MP3	https://www.iso.org/standard/26797.html													[B2AI_SUBSTRATE:49]			
B2AI_STANDARD:352	B2AI_STANDARD:DataStandard	MPEG-4	MPEG-4 Part 14 digital multimedia container format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[audiovisual|fileformat]	[B2AI_TOPIC:15|B2AI_TOPIC:37]	A digital multimedia container format most commonly used to store video and audio, but it can also be used to store other data such as subtitles and still images. Like most modern container formats, it allows streaming over the Internet.	True	False	https://en.wikipedia.org/wiki/MP4_file_format	https://www.loc.gov/preservation/digital/formats/fdd/fdd000155.shtml													[B2AI_SUBSTRATE:49]	[B2AI_ORG:49]		
B2AI_STANDARD:353	B2AI_STANDARD:DataStandard	netCDF	Network Common Data Form	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:3|B2AI_TOPIC:5]	A standardized format for chromatographic data representation.	True	False	https://www.unidata.ucar.edu/software/netcdf/	https://www.astm.org/e1947-98r14.html														[B2AI_ORG:8]		
B2AI_STANDARD:354	B2AI_STANDARD:DataStandard	NNEF	Neural Network Exchange Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:5]	An exchange format for neural network models produced using Torch, Caffe, TensorFlow, Theano, Chainer, Caffe2, PyTorch, or MXNet.	True	False	https://www.khronos.org/nnef	https://github.com/KhronosGroup/NNEF-Tools			[{\"id\": \"B2AI_APP:55\", \"category\": \"B2AI:Application\", \"name\": \"Neural Network Exchange for Medical Device AI\", \"description\": \"NNEF (Neural Network Exchange Format) is used in biomedical AI for deploying models on medical devices and embedded systems where hardware-specific optimization and standardized model representation are critical. Medical device manufacturers leverage NNEF to ensure neural network models can be optimized for diverse hardware accelerators (DSPs, NPUs, custom ASICs) commonly used in portable medical equipment, bedside monitors, and point-of-care devices. The format enables vendor-independent model deployment, facilitates regulatory approval by providing clear model specifications, and supports hardware efficiency optimizations necessary for real-time inference in resource-constrained medical devices. NNEF is particularly valuable for AI-enabled medical devices where power consumption, latency, and deterministic behavior are critical requirements.\", \"used_in_bridge2ai\": false}]	[Neural Network Exchange for Medical Device AI]	[B2AI_APP:55]		[NNEF (Neural Network Exchange Format) is used in biomedical AI for deploying models on medical devices and embedded systems where hardware-specific optimization and standardized model representation are critical. Medical device manufacturers leverage NNEF to ensure neural network models can be optimized for diverse hardware accelerators (DSPs, NPUs, custom ASICs) commonly used in portable medical equipment, bedside monitors, and point-of-care devices. The format enables vendor-independent model deployment, facilitates regulatory approval by providing clear model specifications, and supports hardware efficiency optimizations necessary for real-time inference in resource-constrained medical devices. NNEF is particularly valuable for AI-enabled medical devices where power consumption, latency, and deterministic behavior are critical requirements.]	[B2AI:Application]	[False]		[B2AI_STANDARD:816|B2AI_STANDARD:831|B2AI_STANDARD:834]		[B2AI_SUBSTRATE:27|B2AI_SUBSTRATE:33|B2AI_SUBSTRATE:42]			
B2AI_STANDARD:355	B2AI_STANDARD:DataStandard	OMXML	OGC and ISO Observations and Measurements standard in XML	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:5]	This standard specifies an XML implementation for the OGC and ISO Observations and Measurements (O&M) conceptual model (OGC Observations and Measurements v2.0 also published as ISO/DIS 19156), including a schema for Sampling Features. This encoding is an essential dependency for the OGC Sensor Observation Service (SOS) Interface Standard. More specifically, this standard defines XML schemas for observations, and for features involved in sampling when making observations. These provide document models for the exchange of information describing observation acts and their results, both within and between different scientific and technical communities.	True	False	https://www.ogc.org/standards/om															[B2AI_ORG:49]		
B2AI_STANDARD:356	B2AI_STANDARD:DataStandard	OGG Speex	Ogg Speex Audio Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[audiovisual|fileformat]	[B2AI_TOPIC:37]	File format and bitstream encoding for for spoken content, targeted at a wide range of devices other than mobile phones.	True	False	https://speex.org/docs/manual/speex-manual/node8.html														[B2AI_SUBSTRATE:49]			
B2AI_STANDARD:357	B2AI_STANDARD:DataStandard	ONNX	Open Neural Network Exchange	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:5]	ONNX is an open format built to represent machine learning models.	True	False	https://onnx.ai/	https://github.com/onnx/onnx/blob/main/docs/IR.md			[{\"id\": \"B2AI_APP:56\", \"category\": \"B2AI:Application\", \"name\": \"Cross-Platform Medical AI Model Interoperability\", \"description\": \"ONNX (Open Neural Network Exchange) is used in biomedical AI for creating framework-agnostic models that can be trained in PyTorch, TensorFlow, or other frameworks and deployed across diverse clinical platforms and hardware accelerators. Healthcare AI developers leverage ONNX to ensure vendor independence, enabling models to run on different hospital IT systems, edge devices, and specialized medical hardware regardless of training framework. The standard facilitates regulatory submissions by providing a stable model representation, supports hardware optimization through ONNX Runtime's performance tuning for CPUs, GPUs, and custom accelerators, and enables model sharing across research institutions without requiring framework dependencies. ONNX is particularly valuable for clinical AI products that must support multiple deployment environments.\", \"used_in_bridge2ai\": false, \"references\": [\"https://github.com/microsoft/onnxruntime\"]}]	[Cross-Platform Medical AI Model Interoperability]	[B2AI_APP:56]	[['https://github.com/microsoft/onnxruntime']]	[ONNX (Open Neural Network Exchange) is used in biomedical AI for creating framework-agnostic models that can be trained in PyTorch, TensorFlow, or other frameworks and deployed across diverse clinical platforms and hardware accelerators. Healthcare AI developers leverage ONNX to ensure vendor independence, enabling models to run on different hospital IT systems, edge devices, and specialized medical hardware regardless of training framework. The standard facilitates regulatory submissions by providing a stable model representation, supports hardware optimization through ONNX Runtime's performance tuning for CPUs, GPUs, and custom accelerators, and enables model sharing across research institutions without requiring framework dependencies. ONNX is particularly valuable for clinical AI products that must support multiple deployment environments.]	[B2AI:Application]	[False]				[B2AI_SUBSTRATE:28]			
B2AI_STANDARD:358	B2AI_STANDARD:DataStandard	OpenAPI	OpenAPI Specification	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Standard for describing program interfaces.	True	False	https://spec.openapis.org/oas/latest.html	https://github.com/OAI/OpenAPI-Specification/																
B2AI_STANDARD:359	B2AI_STANDARD:DataStandard	parquet	Parquet	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat|implementation_maturity_production]	[B2AI_TOPIC:5]	Apache Parquet is a free and open-source column-oriented data storage format in the Apache Hadoop ecosystem.	True	False	https://parquet.apache.org/		True		[{\"id\": \"B2AI_APP:57\", \"category\": \"B2AI:Application\", \"name\": \"Efficient Large-Scale ML Data Storage and Processing\", \"description\": \"Apache Parquet is widely adopted in AI/ML pipelines for efficient storage and processing of large-scale biomedical datasets, particularly for training deep learning models on tabular clinical data, multi-omics datasets, and imaging metadata. The columnar storage format enables high-performance data loading during model training, reduces storage costs through efficient compression, and supports predicate pushdown for selective feature reading. AI frameworks like TensorFlow, PyTorch, and scikit-learn leverage Parquet's integration with Apache Arrow for zero-copy data transfer, enabling faster iteration during hyperparameter tuning and model development. Parquet is particularly valuable for storing processed features from EHR data, genomic variant annotations, and large-scale biobank datasets where query performance and storage efficiency are critical.\", \"used_in_bridge2ai\": false}]	[Efficient Large-Scale ML Data Storage and Processing]	[B2AI_APP:57]		[Apache Parquet is widely adopted in AI/ML pipelines for efficient storage and processing of large-scale biomedical datasets, particularly for training deep learning models on tabular clinical data, multi-omics datasets, and imaging metadata. The columnar storage format enables high-performance data loading during model training, reduces storage costs through efficient compression, and supports predicate pushdown for selective feature reading. AI frameworks like TensorFlow, PyTorch, and scikit-learn leverage Parquet's integration with Apache Arrow for zero-copy data transfer, enabling faster iteration during hyperparameter tuning and model development. Parquet is particularly valuable for storing processed features from EHR data, genomic variant annotations, and large-scale biobank datasets where query performance and storage efficiency are critical.]	[B2AI:Application]	[False]				[B2AI_SUBSTRATE:30]	[B2AI_ORG:114]		
B2AI_STANDARD:360	B2AI_STANDARD:DataStandard	PURL	Persistent Uniform Resource Locator	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	PURLs are Web addresses or Uniform Resource Locators (URLs) that act as permanent identifiers in the face of a dynamic and changing Web infrastructure. Instead of resolving directly to Web resources (documents, data, services, people, etc.) PURLs provide a level of indirection that allows the underlying Web addresses of resources to change over time without negatively affecting systems that depend on them. This capability provides continuity of references to network resources that may migrate from machine to machine for business, social or technical reasons.	True	False	https://sites.google.com/site/persistenturls/	https://code.google.com/archive/p/persistenturls/														[B2AI_ORG:43]		
B2AI_STANDARD:361	B2AI_STANDARD:DataStandard	PDF	Portable Document Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:15|B2AI_TOPIC:32]	A file format to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.	True	False	https://en.wikipedia.org/wiki/PDF																	
B2AI_STANDARD:362	B2AI_STANDARD:DataStandard	PNG	Portable Network Graphics	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[audiovisual|fileformat]	[B2AI_TOPIC:15]	A raster-graphics file format that supports lossless data compression.	True	False	https://en.wikipedia.org/wiki/Portable_Network_Graphics														[B2AI_SUBSTRATE:19]			
B2AI_STANDARD:363	B2AI_STANDARD:DataStandard	PS	Postscript Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:15|B2AI_TOPIC:32]	PostScript (PS) is a Turing-complete, stack-based page description language and programming language developed by Adobe Systems (John Warnock, Chuck Geschke, Doug Brotz, Ed Taft, Bill Paxton) and released in 1984, revolutionizing desktop publishing by providing device-independent representation of documents combining text, vector graphics, and raster images. PostScript uses reverse Polish notation and an interpreted execution model where documents are programs that, when executed by a PostScript interpreter (Raster Image Processor or RIP), render pages at the target device's resolution. The language describes graphics using vector primitives (straight lines, cubic Bzier curves) enabling arbitrary scaling, rotation, and transformation without quality loss, crucial for professional typography and technical illustrations. PostScript's sophisticated font system uses outline fonts with font hinting to maintain glyph quality at low resolutions, standardized through Type 1, Type 2, and Type 3 font formats that influenced modern font technologies like TrueType and OpenType. Three major versions exist: PostScript Level 1 (1984) introducing basic page description capabilities, PostScript Level 2 (1991) adding improved speed, image decompression (JPEG support), composite fonts, color separation, and form caching, and PostScript 3 (1997) providing enhanced color handling with up to 4096 gray levels, smooth shading operations, DeviceN color space for spot colors, and better filtering. PostScript powered the Apple LaserWriter (1985), triggering the desktop publishing revolution by enabling WYSIWYG document creation on Macintosh with PageMaker software. The language became the de facto standard for electronic prepress systems, high-end typesetters (Linotronic), and professional printing workflows throughout the 1980s-1990s. PostScript's imaging model directly influenced PDF (Portable Document Format), Adobe's 1993 successor that simplified PostScript for document distribution by removing general-purpose programming features while retaining the imaging model, making PDF documents static data structures rather than executable programs. PostScript remains common in high-end printers, professional publishing, and scientific visualization where precise vector graphics control is required. Open-source implementations like Ghostscript enable PostScript rendering on devices lacking native PostScript support. Scientific applications include generation of publication-quality figures from computational analysis software, precise technical diagrams, and device-independent archival documents.	True	False	https://en.wikipedia.org/wiki/PostScript																	
B2AI_STANDARD:364	B2AI_STANDARD:DataStandard	PMML	Predictive Model Markup Language	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[markuplanguage]	[B2AI_TOPIC:5]	PMML (Predictive Model Markup Language) uses XML to represent mining models. The structure of the models is described by an XML Schema. One or more mining models can be contained in a PMML document. A PMML document is an XML document with a root element of type PMML	True	False	https://dmg.org/pmml/v4-4-1/GeneralStructure.html	https://dmg.org/pmml/v4-4-1/GeneralStructure.html																
B2AI_STANDARD:365	B2AI_STANDARD:DataStandard	protobuf	Protocol Buffers	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Protocol Buffers (a.k.a., protobuf) are Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data.	True	False	https://developers.google.com/protocol-buffers/	https://github.com/protocolbuffers/protobuf														[B2AI_ORG:37]		
B2AI_STANDARD:366	B2AI_STANDARD:DataStandard	PROV	Provenance	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	The PROV Family of Documents defines a model, corresponding serializations and other supporting definitions to enable the inter-operable interchange of provenance information in heterogeneous environments such as the Web.	True	False	https://www.w3.org/TR/prov-overview/															[B2AI_ORG:99]		
B2AI_STANDARD:367	B2AI_STANDARD:DataStandard	pickle	Python pickle format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:5]	Serialization format for a Python object structure. Pickling is the process whereby a Python object hierarchy is converted into a byte stream, and unpickling is the inverse operation, whereby a byte stream (from a binary file or bytes-like object) is converted back into an object hierarchy.	True	False	https://docs.python.org/3.8/library/pickle.html	https://github.com/python/cpython/blob/3.8/Lib/pickle.py																
B2AI_STANDARD:368	B2AI_STANDARD:DataStandard	QuDEx	Qualitative Data Exchange Schema	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	The Qualitative Data Exchange Schema (QuDEx) allows users to discover, find, retrieve and cite complex qualitative data collections in context.	True	False	https://www.data-archive.ac.uk/managing-data/standards-and-procedures/metadata-standards/qudex/	https://dam.data-archive.ac.uk/standards/qudex_v03_01.xsd														[B2AI_ORG:97]		
B2AI_STANDARD:369	B2AI_STANDARD:DataStandard	RDA 10 Things	RDA CURE-FAIR 10 Things for Curating Reproducible and FAIR Research	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:5]	A framework for implementing effective curation workflows for achieving greater FAIR-ness and long-term usability of research data and code.	True	False	https://www.rd-alliance.org/group/cure-fair-wg/outcomes/10-things-curating-reproducible-and-fair-research	https://curating4reproducibility.org/10things/														[B2AI_ORG:83]		
B2AI_STANDARD:370	B2AI_STANDARD:DataStandard	RDFS	RDF Schema	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	RDF Schema (RDFS) is the foundational vocabulary description language that extends the basic RDF vocabulary to provide essential data modeling capabilities for the Semantic Web and linked data applications. As a W3C Recommendation and semantic extension of RDF, RDFS enables the description of groups of related resources and relationships between them by defining classes, properties, and their hierarchical structures. Unlike traditional object-oriented programming models that define classes in terms of their instance properties, RDFS takes a property-centric approach where properties are described in terms of the classes they apply to through domain and range mechanisms. This design philosophy promotes the extensibility principle of the Web, allowing anyone to define additional properties for existing resources without requiring modification of original class definitions. RDFS provides core vocabulary elements including rdfs:Class, rdfs:Resource, rdfs:Property, rdfs:subClassOf, rdfs:subPropertyOf, rdfs:domain, rdfs:range, rdfs:label, and rdfs:comment, which form the basis for more sophisticated ontology languages like OWL. The schema supports the development of machine-readable vocabularies that can be processed automatically, enabling applications to discover and reason about resource relationships, making it an essential component of the Semantic Web infrastructure.	True	False	https://www.w3.org/TR/rdf-schema/															[B2AI_ORG:99]		
B2AI_STANDARD:371	B2AI_STANDARD:DataStandard	RIF-CS	Registry Interchange Format - Collections and Services schema	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	The Registry Interchange Format - Collections and Services (RIF-CS) is an XML-based metadata schema developed by the Australian National Data Service (ANDS) for describing and exchanging information about research collections, services, parties (people and organizations), and activities. RIF-CS serves as the foundational data interchange format for Research Data Australia and enables institutions to contribute metadata about their research assets to national and international discovery services. The schema organizes metadata into four core entity types with rich relationship modeling capabilities - collections (datasets, databases, repositories), services (software tools, web services, facilities), parties (researchers, institutions, funders), and activities (projects, programs, events). Each entity supports comprehensive descriptive metadata including identifiers, names, descriptions, locations, dates, subjects, and crucially, relationships to other entities that create a connected graph of research infrastructure. RIF-CS enables automated harvesting and aggregation of research metadata across institutions, supporting research discovery, collaboration, and compliance with research data management policies.	True	False	https://services.ands.org.au/documentation/rifcs/1.2.0/guidelines/rif-cs.html															[B2AI_ORG:6]		
B2AI_STANDARD:372	B2AI_STANDARD:DataStandard	RO-CRATE	Research Object Crate	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[implementation_maturity_production]	[B2AI_TOPIC:5]	A community effort to establish a lightweight approach to packaging research data with their metadata. It is based on schema.org annotations in JSON-LD, and aims to make best-practice in formal metadata description accessible and practical for use in a wider variety of situations, from an individual researcher working with a folder of data, to large data-intensive computational research environments.	True	False	https://www.researchobject.org/ro-crate/	https://www.researchobject.org/ro-crate/specification.html	True		[{\"id\": \"B2AI_APP:58\", \"category\": \"B2AI:Application\", \"name\": \"ML Model Packaging and Research Object Preservation\", \"description\": \"RO-CRATE is used in AI applications for packaging machine learning models, training data, workflows, and associated metadata into portable, FAIR-compliant research objects. AI researchers leverage RO-CRATE to create self-describing archives that bundle trained models with their training datasets, preprocessing scripts, validation results, and computational environment specifications, ensuring reproducibility and reusability of AI research. The standard enables automated model repositories, facilitates model sharing across institutions, and supports provenance tracking for regulatory compliance in clinical AI applications. RO-CRATE's lightweight JSON-LD format makes it ideal for describing complex AI workflows while maintaining human readability.\", \"used_in_bridge2ai\": false}]	[ML Model Packaging and Research Object Preservation]	[B2AI_APP:58]		[RO-CRATE is used in AI applications for packaging machine learning models, training data, workflows, and associated metadata into portable, FAIR-compliant research objects. AI researchers leverage RO-CRATE to create self-describing archives that bundle trained models with their training datasets, preprocessing scripts, validation results, and computational environment specifications, ensuring reproducibility and reusability of AI research. The standard enables automated model repositories, facilitates model sharing across institutions, and supports provenance tracking for regulatory compliance in clinical AI applications. RO-CRATE's lightweight JSON-LD format makes it ideal for describing complex AI workflows while maintaining human readability.]	[B2AI:Application]	[False]	doi:10.3233/DS-210053				[B2AI_ORG:116]		
B2AI_STANDARD:373	B2AI_STANDARD:DataStandard	RDF	Resource Description Framework	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Resource Description Framework (RDF) is a W3C standard framework for representing information about resources on the Web using subject-predicate-object triples. RDF provides a graph-based data model where resources are identified by IRIs (Internationalized Resource Identifiers) and relationships form a directed, labeled graph. The framework enables decentralized knowledge representation, allowing anyone to make statements about any resource and merge distributed data seamlessly. RDF Schema (RDFS) extends RDF with vocabulary for describing classes, properties, domain, range, and hierarchical relationships, enabling semantic reasoning and inference. RDF supports multiple serialization formats including Turtle (human-readable), RDF/XML (verbose XML), JSON-LD (JSON-based), and N-Triples (line-oriented). The semantic web stack builds upon RDF: OWL for rich ontologies, SPARQL for querying RDF graphs, SHACL for shape validation. RDF powers linked open data initiatives (DBpedia, Wikidata), biomedical ontologies (OBO Foundry, Bio2RDF), knowledge graphs (Google Knowledge Graph principles), and enterprise knowledge management. The framework enables data integration across heterogeneous sources by providing common vocabularies (Dublin Core, FOAF, Schema.org) and federated querying. In AI/ML contexts, RDF graphs serve as structured knowledge bases for knowledge graph embeddings (TransE, DistMult, ComplEx), semantic reasoning for inference rules, ontology-guided feature engineering, and multi-relational learning where symbolic knowledge augments statistical learning, enabling explainable AI and knowledge-driven machine learning systems.	True	False	https://www.w3.org/TR/rdf-schema/															[B2AI_ORG:99]		
B2AI_STANDARD:374	B2AI_STANDARD:DataStandard	Safetensors	Safetensors	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:5]	A new simple format for storing tensors safely (as opposed to pickle) and that is still fast (zero-copy).	True	False	https://github.com/huggingface/safetensors	https://github.com/huggingface/safetensors													[B2AI_SUBSTRATE:42]			
B2AI_STANDARD:375	B2AI_STANDARD:DataStandard	SVG	Scalable Vector Graphics Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[audiovisual|fileformat]	[B2AI_TOPIC:15]	Scalable Vector Graphics (SVG) Version 1.1, a modularized language for describing two-dimensional vector and mixed vector/raster graphics in XML.	True	False	https://www.w3.org/Graphics/SVG/About.html														[B2AI_SUBSTRATE:19]	[B2AI_ORG:99]		
B2AI_STANDARD:376	B2AI_STANDARD:DataStandard	SHACL	Shapes Constraint Language	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:5]	A language for validating RDF graphs against a set of conditions. These conditions are provided as shapes and other constructs expressed in the form of an RDF graph. RDF graphs that are used in this manner are called shapes graphs in SHACL and the RDF graphs that are validated against a shapes graph are called data graphs. As SHACL shape graphs are used to validate that data graphs satisfy a set of conditions they can also be viewed as a description of the data graphs that do satisfy these conditions. Such descriptions may be used for a variety of purposes beside validation, including user interface building, code generation and data integration.	True	False	https://www.w3.org/TR/shacl/															[B2AI_ORG:99]		
B2AI_STANDARD:377	B2AI_STANDARD:DataStandard	SGI	Silicon Graphics image format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[audiovisual|fileformat]	[B2AI_TOPIC:15]	Silicon Graphics Image (SGI or RGB) is a raster graphics file format developed by Silicon Graphics, Inc. for storing and displaying digital images on SGI workstations and UNIX systems. The format supports 8-bit, 16-bit, and 24-bit color depths with optional alpha channel for transparency, allowing representation of grayscale, RGB, and RGBA images with various bit depths per channel. SGI images can be stored uncompressed or with run-length encoding (RLE) compression for reduced file sizes while maintaining lossless quality. The format was widely used in professional computer graphics, visual effects, scientific visualization, and medical imaging during the 1980s-1990s when SGI workstations dominated high-end graphics computing. SGI files use .sgi, .rgb, .rgba, .bw, or .int file extensions depending on color configuration and bit depth. The format specifies image dimensions, number of channels (1 for grayscale, 3 for RGB, 4 for RGBA), compression method, and pixel data in a header-based structure readable by graphics software on big-endian systems. While largely superseded by more modern formats like PNG and TIFF for general use, SGI format remains relevant in legacy scientific visualization applications, particularly in medical imaging archives, computational fluid dynamics visualization, and legacy 3D rendering pipelines. ImageMagick, GIMP, and specialized scientific visualization software maintain SGI format support for backward compatibility with historical image datasets. The format's simplicity and direct pixel representation made it suitable for high-performance graphics rendering on SGI's proprietary hardware and OpenGL-based visualization systems.	True	False	https://en.wikipedia.org/wiki/Silicon_Graphics_Image														[B2AI_SUBSTRATE:19]			
B2AI_STANDARD:378	B2AI_STANDARD:DataStandard	SSSOM	Simple Standard for Sharing Ontological Mappings	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:5]	SSSOM is a Simple Standard for Sharing Ontological Mappings, providing a TSV-based representation for ontology term mappings, a comprehensive set of standard metadata elements to describe mappings, and a standard translation between the TSV and the Web Ontology Language (OWL).	True	False	https://mapping-commons.github.io/sssom/	https://github.com/mapping-commons/sssom										doi:10.1093/database/baac035			[B2AI_SUBSTRATE:41|B2AI_SUBSTRATE:6]			
B2AI_STANDARD:379	B2AI_STANDARD:DataStandard	statismo	Statismo format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:5]	Statismo defines a storage format (Statistical Image And Shape Models) based on HDF5, which includes all the information necessary to use the model, as well as meta-data about the model creation, which helps to make model building reproducible.	True	False	https://github.com/statismo/statismo	https://github.com/statismo/statismo											[B2AI_STANDARD:339]		[B2AI_SUBSTRATE:16]			
B2AI_STANDARD:380	B2AI_STANDARD:DataStandard	SDMX	Statistical Data and Metadata eXchange standard	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	SDMX is an initiative to foster standards for the exchange of statistical information.	True	False	https://sdmx.org/															[B2AI_ORG:88]		
B2AI_STANDARD:381	B2AI_STANDARD:DataStandard	STL	STereoLithography File Format Family	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:5]	STL files describe only the surface geometry of a three-dimensional object without any representation of color, texture or other common CAD model attributes. The STL format specifies both ASCII and binary representations.	True	False	https://www.loc.gov/preservation/digital/formats/fdd/fdd000504.shtml																	
B2AI_STANDARD:382	B2AI_STANDARD:DataStandard	SDD	Structured Descriptive Data	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	The goal of the Structured Descriptive Data (SDD) standard is to allow capture, transport, caching and archiving of descriptive data in all the forms shown above, using a platform- and application-independent, international standard. Such a standard is crucial to enabling lossless porting of data between existing and future software platforms including identification, data-mining and analysis tools, and federated databases.	True	False	https://www.tdwg.org/standards/sdd/	https://github.com/tdwg/sdd														[B2AI_ORG:93]		
B2AI_STANDARD:383	B2AI_STANDARD:DataStandard	TIFF	Tagged Image File Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[audiovisual|fileformat|standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:15]	An image file format for storing raster graphics images.	True	False	https://en.wikipedia.org/wiki/TIFF		True												[B2AI_SUBSTRATE:19]	[B2AI_ORG:116]		
B2AI_STANDARD:384	B2AI_STANDARD:DataStandard	TAR	TAR archive file format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:5]	A tar (tape archive) file format is an archive created by tar, a UNIX-based utility used to package files together for backup or distribution purposes. It contains multiple files (also known as a tarball) stored in an uncompressed format along with metadata about the archive. Tar files are not compressed archive files. They are often compressed with file compression utilities such as gzip or bzip2.	True	False	https://www.loc.gov/preservation/digital/formats/fdd/fdd000531.shtml														[B2AI_SUBSTRATE:52]			
B2AI_STANDARD:385	B2AI_STANDARD:DataStandard	TAPIR	TDWG Access Protocol for Information Retrieval	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	The TDWG Access Protocol for Information Retrieval (TAPIR) is a Web Service protocol and XML schema to perform queries across distributed databases of varied physical and logical structure. It was originally designed to be used by federated networks. TAPIR is intended for communication between applications, using HTTP as the transport mechanism. TAPIR's flexibility makes it suitable to both very simple service implementations where the provider only responds to a set of pre-defined queries, or more advanced implementations where the provider software can dynamically parse complex queries referencing output models supplied by the client.	True	False	https://www.tdwg.org/standards/tapir/															[B2AI_ORG:93]		
B2AI_STANDARD:386	B2AI_STANDARD:DataStandard	UCUM	The Unified Code for Units of Measure	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[codesystem|standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:5]	A common syntax for communication of quantities and their units.	True	False	https://unitsofmeasure.org/ucum															[B2AI_ORG:84]		
B2AI_STANDARD:387	B2AI_STANDARD:DataStandard	WAV	Waveform Audio File Format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[audiovisual|fileformat]	[B2AI_TOPIC:37]	Waveform Audio File Format (WAVE or WAV due to its filename extension is an audio file format standard.	True	False	https://en.wikipedia.org/wiki/WAV	https://sites.google.com/site/musicgapi/technical-documents/wav-file-format													[B2AI_SUBSTRATE:48|B2AI_SUBSTRATE:49]			
B2AI_STANDARD:388	B2AI_STANDARD:DataStandard	OWL	Web Ontology Language	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	The Web Ontology Language (OWL) is a family of knowledge representation languages or ontology languages for authoring ontologies or knowledge bases. The languages are characterized by formal semantics and RDF/XML-based serializations for the Semantic Web. OWL is endorsed by the World Wide Web Consortium (W3C) and has attracted academic, medical and commercial interest. The OWL 2 Web Ontology Language, informally OWL 2, is an ontology language for the Semantic Web with formally defined meaning. OWL 2 ontologies provide classes, properties, individuals, and data values and are stored as Semantic Web documents. OWL 2 ontologies can be used along with information written in RDF, and OWL 2 ontologies themselves are primarily exchanged as RDF documents.	True	False	https://www.w3.org/TR/owl-overview/															[B2AI_ORG:99]		
B2AI_STANDARD:389	B2AI_STANDARD:DataStandard	WDL	Workflow Description Language	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[workflowlanguage]	[B2AI_TOPIC:5]	The Workflow Description Language (WDL) is a way to specify data processing workflows with a human-readable and -writeable syntax. WDL makes it straightforward to define analysis tasks, chain them together in workflows, and parallelize their execution. The language makes common patterns simple to express, while also admitting uncommon or complicated behavior; and strives to achieve portability not only across execution platforms, but also different types of users. Whether one is an analyst, a programmer, an operator of a production system, or any other sort of user, WDL should be accessible and understandable.	True	False	https://openwdl.org/	https://github.com/openwdl/wdl																
B2AI_STANDARD:390	B2AI_STANDARD:DataStandard	XBM	X PixMap bitmap image format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[audiovisual|deprecated|fileformat]	[B2AI_TOPIC:15]	X PixMap (XBM) is an image file format used by the X Window System. Replaced by XPM.	True	False	https://en.wikipedia.org/wiki/X_PixMap														[B2AI_SUBSTRATE:19]			
B2AI_STANDARD:391	B2AI_STANDARD:DataStandard	XPM	X PixMap image format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[audiovisual|fileformat]	[B2AI_TOPIC:15]	X PixMap (XPM) is an image file format used by the X Window System.	True	False	https://en.wikipedia.org/wiki/X_PixMap																	
B2AI_STANDARD:392	B2AI_STANDARD:DataStandard	xarray	xarray	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	An open source project and Python package that introduces labels in the form of dimensions, coordinates, and attributes on top of raw NumPy-like arrays, which allows for more intuitive, more concise, and less error-prone user experience.	True	False	https://xarray.dev/	https://github.com/pydata/xarray													[B2AI_SUBSTRATE:1|B2AI_SUBSTRATE:50]			
B2AI_STANDARD:393	B2AI_STANDARD:DataStandard	YAML	YAML Ain't Markup Language	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:5]	A human-readable data-serialization language. It is commonly used for configuration files and in applications where data is being stored or transmitted.	True	False	https://en.wikipedia.org/wiki/YAML																	
B2AI_STANDARD:394	B2AI_STANDARD:DataStandard	Zarr	Zarr	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A format for storage of large N-dimensional typed arrays. Has implementations in multiple programming languages.	True	False	https://zarr.dev/														[B2AI_SUBSTRATE:1|B2AI_SUBSTRATE:24|B2AI_SUBSTRATE:51]			
B2AI_STANDARD:395	B2AI_STANDARD:DataStandard	ZIP	ZIP compressed file format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:5]	An archive file format that supports lossless data compression.	True	False	https://en.wikipedia.org/wiki/ZIP_(file_format)														[B2AI_SUBSTRATE:52]			
B2AI_STANDARD:396	B2AI_STANDARD:ModelRepository	AdapterHub	AdapterHub	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Adapter refers to a set of newly introduced weights, typically within the layers of a transformer model. Adapters provide an alternative to fully fine-tuning the model for each downstream task, while maintaining performance. AdapterHub builds on the HuggingFace transformers framework, requiring as little as two additional lines of code to train adapters for a downstream task.	True	False	https://adapterhub.ml/	https://github.com/adapter-hub/Hub										doi:10.48550/arXiv.2007.07779						
B2AI_STANDARD:397	B2AI_STANDARD:ModelRepository	Bioimage	Bioimage Model Zoo	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:15]	A community-driven, fully open resource where standardized pre-trained models can be shared, explored, tested, and downloaded for further adaptation or direct deployment in multiple end user-facing tools (e.g., ilastik, deepImageJ, QuPath, StarDist, ImJoy, ZeroCostDL4Mic, CSBDeep).	True	False	https://bioimage.io/#/	https://github.com/bioimage-io/bioimage.io										doi:10.1101/2022.06.07.495102						
B2AI_STANDARD:398	B2AI_STANDARD:ModelRepository	HF Models	Hugging Face  Models	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[dataregistry]	[B2AI_TOPIC:5]	machine learning models, with focus on language models and NLP	True	False	https://huggingface.co/models	https://github.com/huggingface/huggingface_hub																
B2AI_STANDARD:399	B2AI_STANDARD:ModelRepository	modelzoo.co	Model Zoo	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Model Zoo (modelzoo.co) is a curated discovery platform that aggregates pre-trained machine learning models from diverse frameworks (TensorFlow, PyTorch, Caffe, MXNet, ONNX), providing a centralized index for finding models across computer vision, natural language processing, speech recognition, and reinforcement learning domains. Originally launched as a community-driven resource, Model Zoo enables practitioners to search for state-of-the-art architectures by task (object detection, semantic segmentation, translation), dataset (ImageNet, COCO, WMT), or framework compatibility. The platform serves as a model marketplace connecting researchers publishing novel architectures with practitioners seeking production-ready implementations, reducing the barrier to adopting cutting-edge techniques. Model Zoo listings typically include architecture descriptions, training configurations, performance benchmarks (accuracy, inference speed), framework-specific code repositories, and pre-trained weight files for transfer learning. Unlike framework-specific repositories (PyTorch Hub, TensorFlow Hub), Model Zoo provides cross-framework search capabilities, enabling users to discover equivalent architectures implemented in multiple ecosystems and compare performance characteristics across frameworks. The platform supports both academic research models (recent conference publications) and industry-proven architectures (ResNet, BERT variants), with community ratings and download metrics indicating model popularity and reliability. Model Zoo facilitates reproducibility by linking to original papers, training datasets, and hyperparameter configurations, while model cards provide metadata on intended use cases, known limitations, and ethical considerations. For AI/ML practitioners, Model Zoo accelerates prototyping by providing a starting point for transfer learning, enabling rapid experimentation with pre-trained models fine-tuned on domain-specific data rather than training from scratch, particularly valuable when compute resources or labeled data are limited.	True	False	https://modelzoo.co/																	
B2AI_STANDARD:400	B2AI_STANDARD:ModelRepository	OpenML	OpenML	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[dataregistry]	[B2AI_TOPIC:5]	OpenML is an open platform for sharing datasets, algorithms, and experiments	True	True	https://www.openml.org/	https://github.com/openml/OpenML																
B2AI_STANDARD:401	B2AI_STANDARD:ModelRepository	PyTorch Hub	PyTorch Hub	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	PyTorch Hub is the official model repository and distribution system for the PyTorch deep learning framework, providing programmatic access to pre-trained models through a standardized torch.hub API that enables single-line model loading with automatic dependency resolution and weight downloading. Launched by Facebook AI Research (now Meta AI) and the PyTorch Foundation, Hub hosts curated models from leading research institutions including FAIR, NVIDIA, Hugging Face, Intel, and academic labs, covering computer vision (YOLOv5, ResNet, EfficientNet), NLP (Transformers, RoBERTa), speech (Silero models, Tacotron2), and video understanding (SlowFast, X3D) domains. Each Hub model is defined by a hubconf.py file in a GitHub repository that specifies entry points, dependencies, and preprocessing pipelines, ensuring reproducible model instantiation across environments. PyTorch Hub supports both feature extraction (frozen backbone models) and fine-tuning workflows (unfrozen weights), with models returning standard torch.nn.Module objects compatible with PyTorch training loops, data loaders, and distributed training frameworks (DDP, FSDP). The platform integrates with PyTorch's TorchScript compilation for deployment optimization, ONNX export for cross-framework compatibility, and TorchServe for production serving. Hub models include metadata specifying input/output tensor shapes, preprocessing requirements (normalization statistics, resize dimensions), and performance benchmarks (latency, throughput) across hardware configurations (CPU, GPU, mobile). The repository supports version pinning via Git commit hashes or tags, enabling deterministic model loading and reproducible research results. For researchers, PyTorch Hub accelerates experimentation by providing battle-tested implementations of recent architectures (often released alongside conference publications) with pre-trained ImageNet, COCO, or Kinetics weights, reducing training time from weeks to hours through transfer learning. In AI/ML pipelines, Hub models serve as feature extractors for downstream tasks (medical imaging, satellite analysis), few-shot learning backbones, and initialization points for domain adaptation, with the torch.hub.load() API supporting custom repositories for internal enterprise model sharing.	True	False	https://pytorch.org/hub/	https://github.com/pytorch/hub											[B2AI_STANDARD:816]		[B2AI_SUBSTRATE:33]			
B2AI_STANDARD:402	B2AI_STANDARD:ModelRepository	TFHub	Tensorflow Hub	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	TensorFlow Hub (tfhub.dev) is Google's official repository for reusable machine learning modules, providing pre-trained model components as SavedModel or TF2 format assets that integrate seamlessly into TensorFlow and Keras workflows via the tensorflow-hub library. Launched in 2018, TF Hub pioneered the concept of \"transfer learning building blocks\" by packaging not just model weights but complete computational graphs including preprocessing layers, embedding extractors, and prediction heads that can be composed into larger architectures. Hub modules span text embeddings (Universal Sentence Encoder, BERT, LaBSE), image feature vectors (MobileNet, EfficientNet, ResNet), object detection (SSD, Faster R-CNN), image generation (BigGAN, StyleGAN), video understanding (I3D, MoViNet), and audio processing (YAMNet, TRILL). Each module provides a consistent interface via hub.KerasLayer or hub.load(), supporting both feature extraction (trainable=False) and fine-tuning (trainable=True) modes with automatic gradient flow through module internals. TF Hub emphasizes model cards with detailed documentation on training data, performance metrics, intended use cases, and ethical considerations (bias, fairness), promoting responsible AI deployment. The platform supports multiple serving formats including TensorFlow Lite for mobile/edge deployment, TensorFlow.js for in-browser inference, and TensorFlow Serving for production APIs, with modules optimized for quantization and pruning. Hub's standardized interface enables model composition where text embeddings feed into classification heads, or image encoders combine with text encoders for multimodal learning. For researchers, TF Hub reduces training time and computational costs by providing pre-trained representations on large-scale datasets (Wikipedia, ImageNet, YouTube-8M) that transfer effectively to specialized domains with limited data. In AI/ML production systems, Hub modules accelerate deployment by providing battle-tested, versioned components with defined input/output signatures, enabling A/B testing of different encoders and rapid iteration on model architectures without retraining entire pipelines from scratch.	True	False	https://tfhub.dev/															[B2AI_ORG:37]		
B2AI_STANDARD:403	B2AI_STANDARD:OntologyOrVocabulary	ASA/ANSI S3.20	Acoustical Society of America / American National Standards Institute S3.20	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4]	Definitions for terms used in human bioacoustics.	False	True	https://webstore.ansi.org/Standards/ASA/ANSIASAS3202015R2020															[B2AI_ORG:7|B2AI_ORG:4]		
B2AI_STANDARD:404	B2AI_STANDARD:OntologyOrVocabulary	ADO	Alzheimer's Disease Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:7]	concepts related to Alzheimer's Disease	True	False	https://github.com/Fraunhofer-SCAI-Applied-Semantics/ADO	https://github.com/Fraunhofer-SCAI-Applied-Semantics/ADO																
B2AI_STANDARD:405	B2AI_STANDARD:OntologyOrVocabulary	ARO	Antibiotic Resistance Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Antibiotic resistance genes and mutations	True	False	https://github.com/arpcard/aro	https://github.com/arpcard/aro										doi:10.1093/nar/gkz935						
B2AI_STANDARD:406	B2AI_STANDARD:OntologyOrVocabulary	APOLLO_SV	Apollo Structured Vocabulary	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Terms and relations for interoperation between epidemic models and public health application software.	True	False	https://github.com/ApolloDev/apollo-sv	https://github.com/ApolloDev/apollo-sv																
B2AI_STANDARD:407	B2AI_STANDARD:OntologyOrVocabulary	AIO	Artificial Intelligence Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	The Artificial Intelligence Ontology (AIO) is a comprehensive formal ontology that provides standardized terminology and semantic relationships for describing artificial intelligence systems, methods, and concepts. Developed to address the need for consistent AI terminology across research and applications, AIO models classes and relationships describing deep learning networks, their component layers and activation functions, machine learning algorithms, data processing techniques, and potential algorithmic biases. The ontology contains 443 classes organized in a hierarchical structure with a maximum depth of 5 levels, covering fundamental AI concepts from basic computational methods to complex neural architectures. AIO serves as a critical resource for AI researchers, practitioners, and systems developers who need standardized vocabularies for annotating AI models, describing experimental procedures, ensuring reproducibility, and enabling semantic interoperability between AI systems and databases. The ontology facilitates automated reasoning about AI systems, supports metadata annotation for AI workflows, and contributes to the broader goal of making artificial intelligence research more findable, accessible, interoperable, and reusable.	True	False	https://bioportal.bioontology.org/ontologies/AIO	https://github.com/berkeleybop/artificial-intelligence-ontology			[{\"id\": \"B2AI_APP:59\", \"category\": \"B2AI:Application\", \"name\": \"AI/ML Ontology for Model Metadata and Pipeline Documentation\", \"description\": \"AIO (Artificial Intelligence Ontology) is used in biomedical AI for standardizing the description of machine learning models, algorithms, and workflows, enabling semantic search for AI methods, automated model selection, and reproducible research documentation. Researchers leverage AIO to annotate AI models with formal descriptions of their architecture, training data requirements, and applicable use cases, facilitating discovery of appropriate models for specific biomedical tasks. The ontology supports automated reasoning about AI pipeline compatibility, enables knowledge graphs that link models to publications and datasets, and provides structured vocabulary for documenting AI experiments in compliance with reproducibility standards. AIO enables large language models to better understand and recommend AI approaches for biomedical problems.\", \"used_in_bridge2ai\": false}]	[AI/ML Ontology for Model Metadata and Pipeline Documentation]	[B2AI_APP:59]		[AIO (Artificial Intelligence Ontology) is used in biomedical AI for standardizing the description of machine learning models, algorithms, and workflows, enabling semantic search for AI methods, automated model selection, and reproducible research documentation. Researchers leverage AIO to annotate AI models with formal descriptions of their architecture, training data requirements, and applicable use cases, facilitating discovery of appropriate models for specific biomedical tasks. The ontology supports automated reasoning about AI pipeline compatibility, enables knowledge graphs that link models to publications and datasets, and provides structured vocabulary for documenting AI experiments in compliance with reproducibility standards. AIO enables large language models to better understand and recommend AI approaches for biomedical problems.]	[B2AI:Application]	[False]							
B2AI_STANDARD:408	B2AI_STANDARD:OntologyOrVocabulary	BFO	Basic Formal Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Basic Formal Ontology (BFO) is a small, upper-level ontology designed for supporting information retrieval, analysis, and integration in scientific and other domains. As a genuine upper ontology, BFO provides foundational categories that are domain-neutral and applicable across all areas of scientific investigation, rather than containing domain-specific terms. BFO distinguishes between continuants (entities that endure through time, such as objects, qualities, and spatial regions) and occurrents (entities that unfold over time, such as processes and temporal regions), providing a rigorous framework for representing the fundamental structure of reality. The ontology employs formal logic (first-order logic, OWL2) to define its 39 core classes and relations, ensuring precise semantics and enabling automated reasoning. BFO is used by over 550 ontology-driven projects worldwide as the top-level framework for domain ontologies in biomedicine (OBO Foundry ontologies like GO, CHEBI, Uberon), healthcare (OGMS for disease), environmental science, manufacturing, and military intelligence. The ontology promotes interoperability by providing consistent upper-level structure, enabling ontology integration and cross-domain data federation. BFO has been developed through extensive international collaboration, with contributions from philosophers, logicians, and domain scientists, and is continuously refined based on practical applications. In AI/ML contexts, BFO provides foundational structure for knowledge graphs, enabling ontology-guided reasoning, semantic data integration across heterogeneous sources, knowledge representation for explainable AI, and principled ontology alignment, supporting knowledge-driven machine learning where symbolic foundations enhance statistical learning with formal semantics.	True	False	http://basic-formal-ontology.org/	https://github.com/BFO-ontology/BFO										doi:10.3233/AO-160164						
B2AI_STANDARD:409	B2AI_STANDARD:OntologyOrVocabulary	BCO	Biological Collections Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Biodiversity data, including data on museum collections and environmental/metagenomic samples.	True	False	https://github.com/BiodiversityOntologies/bco	https://github.com/BiodiversityOntologies/bco																
B2AI_STANDARD:410	B2AI_STANDARD:OntologyOrVocabulary	FBBI	Biological Imaging Methods Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:15]	Sample preparation, visualization and imaging methods used in biomedical research.	True	False	http://cellimagelibrary.org/	https://github.com/CRBS/Biological_Imaging_Methods_Ontology/																
B2AI_STANDARD:411	B2AI_STANDARD:OntologyOrVocabulary	BSPO	Biological Spatial Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Spatial concepts, anatomical axes, gradients, regions, planes, sides, and surfaces.	True	False	https://github.com/obophenotype/biological-spatial-ontology	https://github.com/obophenotype/biological-spatial-ontology										doi:10.1186/2041-1480-5-34						
B2AI_STANDARD:412	B2AI_STANDARD:OntologyOrVocabulary	BTO	BRENDA tissue / enzyme source	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	A structured controlled vocabulary for the source of an enzyme comprising tissues, cell lines, cell types and cell cultures.	True	False	http://www.brenda-enzymes.org/	https://github.com/BRENDA-Enzymes/BTO										doi:10.1093/nar/gkq968						
B2AI_STANDARD:413	B2AI_STANDARD:OntologyOrVocabulary	WBPHENOTYPE	C. elegans phenotype ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	A structured controlled vocabulary of Caenorhabditis elegans phenotypes.	True	False	https://github.com/obophenotype/c-elegans-phenotype-ontology	https://github.com/obophenotype/c-elegans-phenotype-ontology										doi:10.1186/1471-2105-12-32						
B2AI_STANDARD:414	B2AI_STANDARD:OntologyOrVocabulary	CVDO	Cardiovascular Disease Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:7]	Entities related to cardiovascular diseases.	True	False	https://github.com/OpenLHS/CVDO	https://github.com/OpenLHS/CVDO																
B2AI_STANDARD:415	B2AI_STANDARD:OntologyOrVocabulary	CLO	Cell Line Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Standardize and integrate cell line information and to support computer-assisted reasoning.	True	False	https://github.com/CLO-Ontology/CLO	https://github.com/CLO-Ontology/CLO										doi:10.1186/2041-1480-5-3						
B2AI_STANDARD:416	B2AI_STANDARD:OntologyOrVocabulary	CL	Cell Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	The Cell Ontology (CL) is an OBO Foundry ontology covering biological cell types with curation focused on animal cell types and interoperability with specialized cell type ontologies in other biological domains. CL is tightly integrated with the Uberon multi-species anatomy ontology (for recording cell location) and the Gene Ontology (GO, which uses CL as its main cell type reference and provides cell function annotations). Built on FAIR principles, CL enables community-driven curation with active embedded editors from multiple projects responsive on the issue tracker. The ontology is released in multiple standard formats (OWL/RDF/XML, OBO, JSON obographs) with multiple variants: full (all imports merged, reasoner-classified), base (not pre-reasoned, only CL axioms including non-CL class references), and simple (pre-reasoned, CL classes only), all with resolvable version IRIs for persistent access. CL is integrated into standard tools including Ubergraph (for logical queries like finding cells by location), Ontology Access Kit (OAK), and major browsers (OLS, Ontobee). The ontology supports major initiatives including BICCN cell type knowledge explorer, HubMap Human Reference Atlas, ENCODE, FANTOM5, Single Cell Expression Atlas, Human Cell Atlas, and CellKB. CL enables AI/ML applications including OnClass for automatic cell type classification, Brain Data Standards Ontology (BDSO) for cell type navigation and search, and provides standardized cell type annotations essential for single-cell omics machine learning, cross-dataset integration, and cell type discovery algorithms.	True	False	https://cell-ontology.github.io/	https://github.com/obophenotype/cell-ontology										doi:10.1186/s13326-016-0088-7						
B2AI_STANDARD:417	B2AI_STANDARD:OntologyOrVocabulary	CHIRO	CHEBI Integrated Role Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:3]	A distinct role hierarchy for chemicals.	True	False	https://github.com/obophenotype/chiro	https://github.com/obophenotype/chiro										doi:10.26434/chemrxiv.12591221.v1				[B2AI_ORG:29]		
B2AI_STANDARD:418	B2AI_STANDARD:OntologyOrVocabulary	CHEBI	Chemical Entities of Biological Interest	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:3]	ChEBI is an open-access OBO Foundry database and ontology of chemical entities covering constitutionally or isotopically distinct atoms, molecules, ions, ion pairs, radicals, complexes, conformers, groups, chemical substances, and classes of molecular entities. The database contains over 195,000 entries of naturally occurring molecules or synthetic compounds used to intervene in biological processes, with macromolecules directly encoded by genomes (nucleic acids, proteins, peptides) excluded. ChEBI incorporates ontological classification defining relationships between chemical entities and their parent/child classes, enabling queries based on chemical class and role. The database provides comprehensive information including nomenclature following IUPAC and NC-IUBMB standards, molecular formulas, InChI and SMILES identifiers, literature citations, cross-references to other databases, and species data. ChEBI supports text and structure searches and is released in multiple formats (SDF, OBO, OWL, flat file, SQL dumps). As an ELIXIR Core Data Resource and Global Core Biodata Resource, ChEBI enables AI/ML applications in cheminformatics, drug discovery, metabolomics, systems biology, and chemical-phenotype association studies.	True	False	https://www.ebi.ac.uk/chebi/	https://github.com/ebi-chebi/ChEBI										doi:10.1093/nar/gkv1031				[B2AI_ORG:29]		
B2AI_STANDARD:419	B2AI_STANDARD:OntologyOrVocabulary	CHEMINF	Chemical Information Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:3]	Descriptors commonly used in cheminformatics software applications and the algorithms which generate them.	True	False	https://github.com/semanticchemistry/semanticchemistry	https://github.com/semanticchemistry/semanticchemistry										doi:10.1371/journal.pone.0025513						
B2AI_STANDARD:420	B2AI_STANDARD:OntologyOrVocabulary	CHMO	Chemical Methods Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:3]	The Chemical Methods Ontology (CHMO) provides a comprehensive, standardized vocabulary for describing experimental methods, techniques, and instruments used in chemical analysis, material synthesis, and sample preparation, maintained by the Royal Society of Chemistry (RSC) and aligned with OBO Foundry principles. CHMO encompasses three primary domains: analytical methods for data collection (mass spectrometry including ESI-MS, MALDI-TOF, GC-MS, LC-MS; spectroscopic techniques including NMR, IR, UV-Vis, Raman, X-ray spectroscopy; electron microscopy including SEM, TEM, STEM; and diffraction methods), separation and sample preparation techniques (chromatography including HPLC, GC, TLC, size-exclusion, affinity, ion-exchange; electrophoresis including SDS-PAGE, capillary electrophoresis, isoelectric focusing; sample ionization methods including electrospray, MALDI, electron impact; and extraction procedures), and material synthesis methods (chemical vapor deposition, epitaxy, sol-gel processes, crystallization, polymerization, and nanoparticle synthesis). The ontology also describes instruments and equipment (mass spectrometers, chromatography columns, detectors, vacuum systems, heating/cooling apparatus) and their components, operational parameters (temperature, pressure, flow rate, voltage, resolution), and measurement outputs (spectra, chromatograms, diffraction patterns, images). CHMO integrates with OBI (Ontology for Biomedical Investigations) for process and measurement concepts, CHEBI (Chemical Entities of Biological Interest) for chemical substances, and other OBO ontologies for cross-domain applications in metabolomics, proteomics, and materials science. The ontology provides both textual and formal OWL definitions enabling automated reasoning, classification of methods by input material types, conditions of application, and output data formats. CHMO supports reproducibility in chemical research by standardizing method descriptions in electronic laboratory notebooks, method sections of publications, protocols repositories, and analytical chemistry databases. Applications include semantic search for analytical protocols, automated method selection based on sample properties, FAIRification of chemical data workflows, text mining of chemical literature for method extraction, and quality control in analytical laboratories. The ontology is developed using the Ontology Development Kit (ODK) and distributed under CC-BY-4.0 license, with releases available in OBO, OWL, and JSON formats through http://purl.obolibrary.org/obo/chmo.owl.	True	False	https://github.com/rsc-ontologies/rsc-cmo	https://github.com/rsc-ontologies/rsc-cmo																
B2AI_STANDARD:421	B2AI_STANDARD:OntologyOrVocabulary	LABO	clinical LABoratory Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:4]	An ontology of informational entities formalizing clinical laboratory tests prescriptions and reporting documents.	True	False	https://github.com/OpenLHS/LABO	https://github.com/OpenLHS/LABO										doi:10.5281/zenodo.6522019						
B2AI_STANDARD:422	B2AI_STANDARD:OntologyOrVocabulary	CMO	Clinical measurement ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:4]	Morphological and physiological measurement records generated from clinical and model organism research and health programs.	True	False	https://github.com/rat-genome-database/CMO-Clinical-Measurement-Ontology	https://github.com/rat-genome-database/CMO-Clinical-Measurement-Ontology										doi:10.1186/2041-1480-4-26						
B2AI_STANDARD:423	B2AI_STANDARD:OntologyOrVocabulary	CVX	Clinical Vaccines Administered	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	The Clinical Vaccines Administered (CVX) code set is a comprehensive standardized vocabulary developed and maintained by the CDC's National Center for Immunization and Respiratory Diseases (NCIRD) that provides unique numeric identifiers for all vaccines available in the United States healthcare system. This essential code set includes both active vaccines currently available for patient administration and inactive vaccines that may appear in historical immunization records, enabling complete tracking of vaccination history across a patient's lifetime. CVX codes are specifically designed for use in HL7 Version 2.3.1 and 2.5.1 immunization messages and electronic health record systems, facilitating standardized communication between healthcare providers, immunization information systems (IIS), and public health agencies. Each CVX code entry includes detailed information about vaccine status (active, inactive, pending, non-US, or never active), last update timestamp, and clinical notes providing context about usage and availability. When paired with MVX (manufacturer) codes, CVX codes can precisely identify specific trade-named vaccine products, supporting accurate inventory management, adverse event reporting, vaccine safety monitoring, and public health surveillance activities essential for maintaining population immunity and preventing vaccine-preventable diseases.	True	False	https://www2a.cdc.gov/vaccines/iis/iisstandards/vaccines.asp?rpt=cvx															[B2AI_ORG:13]		
B2AI_STANDARD:424	B2AI_STANDARD:OntologyOrVocabulary	CARO	Common Anatomy Reference Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	An upper level ontology to facilitate interoperability between existing anatomy ontologies for different species.	True	False	https://github.com/obophenotype/caro/	https://github.com/obophenotype/caro/																
B2AI_STANDARD:425	B2AI_STANDARD:OntologyOrVocabulary	CTCAE	Common Terminology Criteria for Adverse Events	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4|B2AI_TOPIC:7]	Common Terminology Criteria for Adverse Events (CTCAE) is widely accepted throughout the oncology community as the standard classification and severity grading scale for adverse events in cancer therapy clinical trials and other oncology settings.	True	False	https://bioportal.bioontology.org/ontologies/CTCAE	https://ctep.cancer.gov/protocoldevelopment/electronic_applications/ctc.htm																
B2AI_STANDARD:426	B2AI_STANDARD:OntologyOrVocabulary	CDAO	Comparative Data Analysis Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	A formalization of concepts and relations relevant to evolutionary comparative analysis.	True	False	https://github.com/evoinfo/cdao	https://github.com/evoinfo/cdao										doi:10.4137/EBO.S2320						
B2AI_STANDARD:427	B2AI_STANDARD:OntologyOrVocabulary	CDNO	Compositional Dietary Nutrition Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Nutritional attributes of material entities that contribute to human diet.	True	False	https://cdno.info/	https://github.com/Southern-Cross-Plant-Science/cdno										doi:10.3389/fnut.2022.928837						
B2AI_STANDARD:428	B2AI_STANDARD:OntologyOrVocabulary	CIO	Confidence Information Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Capture confidence information about annotations.	True	False	https://github.com/BgeeDB/confidence-information-ontology	https://github.com/BgeeDB/confidence-information-ontology										doi:10.1093/database/bav043						
B2AI_STANDARD:429	B2AI_STANDARD:OntologyOrVocabulary	CRO	Contributor Role Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:16]	A classification of the diverse roles performed in the work leading to a published research output in the sciences.	True	False	https://github.com/data2health/contributor-role-ontology	https://github.com/data2health/contributor-role-ontology																
B2AI_STANDARD:430	B2AI_STANDARD:OntologyOrVocabulary	COB	Core Ontology for Biology and Biomedicine	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Terms from a wide range of OBO projects to improve interoperability.	True	False	https://github.com/OBOFoundry/COB	https://github.com/OBOFoundry/COB														[B2AI_ORG:75]		
B2AI_STANDARD:431	B2AI_STANDARD:OntologyOrVocabulary	CIDO	Coronavirus Infectious Disease Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:7]	Ontologically represent and standardize various aspects of coronavirus infectious.	True	False	https://github.com/cido-ontology/cido	https://github.com/cido-ontology/cido										doi:10.1038/s41597-020-0523-6						
B2AI_STANDARD:432	B2AI_STANDARD:OntologyOrVocabulary	CTO	CTO Core Ontology of Clinical Trials	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:4]	A structured resource integrating basic terms and concepts in the context of clinical trials.	True	False	https://github.com/ClinicalTrialOntology/CTO/	https://github.com/ClinicalTrialOntology/CTO/														[B2AI_ORG:33]		
B2AI_STANDARD:433	B2AI_STANDARD:OntologyOrVocabulary	DUO	Data Use Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	The Data Use Ontology (DUO) provides standardized vocabulary for describing data use conditions, restrictions, and requirements in biomedical and genomics research. Developed by the Global Alliance for Genomics and Health (GA4GH), DUO enables semantic tagging of datasets with consent-derived restrictions (e.g., health/medical/biomedical research only, disease-specific research, non-commercial use, research ethics approval). The ontology supports automated data access matching, where algorithms determine whether a researcher's purpose is compatible with dataset restrictions, enabling services like DUOS (Data Use Oversight System) and EGA (European Genome-phenome Archive) to streamline data sharing while respecting participant consent. DUO extends NIH dbGaP data use categories with hierarchical structure for logical inference, includes consent codes for international data sharing, and implements ADA-M (Automated Data Access Matrix) for granular permissions. Used by 60+ million weekly downloads, DUO facilitates GDPR-aware data governance, phenotype-driven differential diagnostics, and translational research. The ontology ensures that informed consent language translates into machine-readable terms, accelerating responsible data reuse for AI/ML training datasets, clinical phenotyping, and genomic diagnostics while maintaining participant privacy and ethical oversight.	True	False	https://github.com/EBISPOT/DUO	https://github.com/EBISPOT/DUO														[B2AI_ORG:34]		
B2AI_STANDARD:434	B2AI_STANDARD:OntologyOrVocabulary	DEB	Devices, Experimental scaffolds and Biomaterials Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	An ontology developed to facilitate information curation in the area of medical devices, experimental scaffolds and biomaterials.	True	False	https://bioportal.bioontology.org/ontologies/DEB	https://github.com/ProjectDebbie/Ontology_DEB										doi:10.1002/adfm.201909910						
B2AI_STANDARD:435	B2AI_STANDARD:OntologyOrVocabulary	devops-infra	Devops Infrastructure Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	This ontology network aims at representing the main sets of entities and relationships used in the context of DevOps infrastructure. It is the result of a collaboration between Huawei Research Ireland and the Ontology Engineering Group at Universidad Politcnica de Madrid. It originally started from an analysis of the Configuration Management Databases used by Huawei Research Ireland for the management of a large part of its DevOps infrastructure, and has evolved into an ontology that may be used as a starting point for the standardisation of the representation of CMDB-related data across vendors.	True	False	https://oeg-upm.github.io/devops-infra/index.html	https://github.com/oeg-upm/devops-infra																
B2AI_STANDARD:436	B2AI_STANDARD:OntologyOrVocabulary	DISDRIV	Disease Drivers Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Ontology for drivers and triggers of human diseases, built to classify ExO ontology exposure stressors. An application ontology.	True	False	https://github.com/DiseaseOntology/DiseaseDriversOntology	https://github.com/DiseaseOntology/DiseaseDriversOntology																
B2AI_STANDARD:437	B2AI_STANDARD:OntologyOrVocabulary	DPO	Drosophila Phenotype Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:25]	Commonly encountered and/or high level Drosophila phenotypes.	True	False	https://github.com/FlyBase/flybase-controlled-vocabulary/wiki	https://github.com/FlyBase/drosophila-phenotype-ontology										doi:10.1186/2041-1480-4-30						
B2AI_STANDARD:438	B2AI_STANDARD:OntologyOrVocabulary	DTO	Drug Target Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:8|B2AI_TOPIC:26]	Drug Target Ontology (DTO) is being developed at the University of Miami in the research group of Stephan Schrer. DTO was developed as part of the Illuminating the Druggable Genome (IDG) project (https://commonfund.nih.gov/idg/overview), is supported by grant (IDG Knowledge Management Center, (U54CA189205). DTO is a novel semantic framework to formalize knowledge about drug targets and is developed as a reference for drug targets with the longer-term goal to create a community standard that will facilitate the integration of diverse drug discovery information from numerous heterogeneous resources.	True	False	https://bioportal.bioontology.org/ontologies/DTO	http://drugtargetontology.org/										doi:10.1186/s13326-017-0161-x						
B2AI_STANDARD:439	B2AI_STANDARD:OntologyOrVocabulary	DIDEO	Drug-drug Interaction and Drug-drug Interaction Evidence Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:8]	The Potential Drug-drug Interaction and Potential Drug-drug Interaction Evidence Ontology	True	False	https://github.com/DIDEO/DIDEO	https://github.com/DIDEO/DIDEO																
B2AI_STANDARD:440	B2AI_STANDARD:OntologyOrVocabulary	EDAM	EMBRACE Data And Methods Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Data types, identifiers, and formats	True	False	https://github.com/edamontology/edamontology	https://github.com/edamontology/edamontology										doi:10.1093/bioinformatics/btt113				[B2AI_ORG:29]		
B2AI_STANDARD:441	B2AI_STANDARD:OntologyOrVocabulary	MFOEM	Emotion Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Affective phenomena such as emotions, moods, appraisals and subjective feelings.	True	False	https://github.com/jannahastings/emotion-ontology	https://github.com/jannahastings/emotion-ontology																
B2AI_STANDARD:442	B2AI_STANDARD:OntologyOrVocabulary	ENVO	Environment Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:11]	Environmental systems, components, and processes.	True	False	http://environmentontology.org/	https://github.com/EnvironmentOntology/envo																
B2AI_STANDARD:443	B2AI_STANDARD:OntologyOrVocabulary	ECTO	Environmental conditions, treatments and exposures ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:11]	Exposures to experimental treatments of plants and model organisms.	True	False	https://github.com/EnvironmentOntology/environmental-exposure-ontology	https://github.com/EnvironmentOntology/environmental-exposure-ontology																
B2AI_STANDARD:444	B2AI_STANDARD:OntologyOrVocabulary	EVI	Evidence Graph Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standards_process_maturity_draft|implementation_maturity_pilot]	[B2AI_TOPIC:5]	The Evidence Graph Ontology (EVI) extends core concepts from the W3C Provenance Ontology [PROV-O] to describe evidence for correctness of findings in biomedical publications. The semantic data model in EVI is expressed using OWL2 Web Ontology Language.	True	False	https://evidencegraph.github.io/EVI/index.html	https://github.com/EvidenceGraph/EVI	True		[{\"id\": \"B2AI_APP:60\", \"category\": \"B2AI:Application\", \"name\": \"Evidence-Based Biomedical Inference and Literature Mining\", \"description\": \"EVI (Evidence Graph) ontology is used in AI applications for representing and reasoning over biomedical evidence from scientific literature, enabling automated hypothesis generation, evidence synthesis, and knowledge graph construction. Machine learning models leverage EVI's formal representation of claims, evidence types, and epistemic confidence to train systems that extract evidence-based relationships from publications, assess the quality and reliability of scientific findings, and generate evidence-weighted knowledge graphs. AI applications include automated systematic review, drug repurposing through evidence integration, and training of large language models that can cite and reason about scientific evidence with appropriate confidence levels. EVI enables AI systems to distinguish between different types of evidence and perform meta-analytic reasoning.\", \"used_in_bridge2ai\": false}]	[Evidence-Based Biomedical Inference and Literature Mining]	[B2AI_APP:60]		[EVI (Evidence Graph) ontology is used in AI applications for representing and reasoning over biomedical evidence from scientific literature, enabling automated hypothesis generation, evidence synthesis, and knowledge graph construction. Machine learning models leverage EVI's formal representation of claims, evidence types, and epistemic confidence to train systems that extract evidence-based relationships from publications, assess the quality and reliability of scientific findings, and generate evidence-weighted knowledge graphs. AI applications include automated systematic review, drug repurposing through evidence integration, and training of large language models that can cite and reason about scientific evidence with appropriate confidence levels. EVI enables AI systems to distinguish between different types of evidence and perform meta-analytic reasoning.]	[B2AI:Application]	[False]					[B2AI_ORG:116]		
B2AI_STANDARD:445	B2AI_STANDARD:OntologyOrVocabulary	ECO	Evidence ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	An ontology for experimental and other evidence statements.	True	False	https://www.evidenceontology.org/	https://github.com/evidenceontology/evidenceontology										doi:10.1093/nar/gkab1025						
B2AI_STANDARD:446	B2AI_STANDARD:OntologyOrVocabulary	XCO	Experimental condition ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Conditions under which physiological and morphological measurements are made both in the clinic and in studies involving humans or model organisms.	True	False	https://rgd.mcw.edu/rgdweb/ontology/view.html?acc_id=XCO:0000000	https://github.com/rat-genome-database/XCO-experimental-condition-ontology										doi:10.1186/2041-1480-4-26						
B2AI_STANDARD:447	B2AI_STANDARD:OntologyOrVocabulary	EXO	Exposure ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:11]	Vocabularies for describing exposure data to inform understanding of environmental health.	True	False	https://github.com/CTDbase/exposure-ontology	https://github.com/CTDbase/exposure-ontology																
B2AI_STANDARD:448	B2AI_STANDARD:OntologyOrVocabulary	FYPO	Fission Yeast Phenotype Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:25]	Phenotypes observed in fission yeast.	True	False	https://github.com/pombase/fypo	https://github.com/pombase/fypo										doi:10.1093/bioinformatics/btt266						
B2AI_STANDARD:449	B2AI_STANDARD:OntologyOrVocabulary	FIDEO	Food Interactions with Drugs Evidence Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:8]	Food-Drug interactions automatically extracted from scientific literature.	True	False	https://gitub.u-bordeaux.fr/erias/fideo	https://github.com/getbordea/fideo/																
B2AI_STANDARD:450	B2AI_STANDARD:OntologyOrVocabulary	FOODON	Food Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	A broadly scoped ontology representing entities which bear a food role. It encompasses materials in natural ecosystems and agriculture tha...	True	False	https://foodon.org/	https://github.com/FoodOntology/foodon/										doi:10.1038/s41538-018-0032-6						
B2AI_STANDARD:451	B2AI_STANDARD:OntologyOrVocabulary	FOBI	Food-Biomarker Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Represent food intake data and associate it with metabolomic data	True	False	https://github.com/pcastellanoescuder/FoodBiomarkerOntology	https://github.com/pcastellanoescuder/FoodBiomarkerOntology										doi:10.1093/bioinformatics/btab626						
B2AI_STANDARD:452	B2AI_STANDARD:OntologyOrVocabulary	FOVT	FuTRES Ontology of Vertebrate Traits	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:25]	Application ontology used to convert vertebrate trait data in spreadsheets to triples.	True	False	https://futres.org/	https://github.com/futres/fovt																
B2AI_STANDARD:453	B2AI_STANDARD:OntologyOrVocabulary	GSSO	Gender, Sex, and Sexual Orientation Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:6]	Terms for annotating interdisciplinary information concerning gender, sex, and sexual orientation.	True	False	https://gsso.research.cchmc.org/	https://github.com/Superraptor/GSSO																
B2AI_STANDARD:454	B2AI_STANDARD:OntologyOrVocabulary	GO	Gene Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:12]	Function of genes and gene products.	True	False	http://geneontology.org/	https://github.com/geneontology/go-ontology										doi:10.1093/nar/gkaa1113				[B2AI_ORG:36]		
B2AI_STANDARD:455	B2AI_STANDARD:OntologyOrVocabulary	GENEPIO	Genomic Epidemiology Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Vocabulary necessary to identify, document and research foodborne pathogens.	True	False	http://genepio.org/	https://github.com/GenEpiO/genepio																
B2AI_STANDARD:456	B2AI_STANDARD:OntologyOrVocabulary	GECKO	Genomics Cohorts Knowledge Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:4|B2AI_TOPIC:6]	The Genomics Cohorts Knowledge Ontology (GECKO) provides a standardized vocabulary for describing attributes of genomics cohorts and individual-level data in population-based research studies. Developed by the CINECA (Common Infrastructure for National Cohorts in Europe, Canada, and Africa) project and maintained for the International HundredK+ Cohorts Consortium (IHCC), GECKO enables harmonized representation of cohort metadata across diverse genomics studies, biobanks, and research consortia. The ontology encompasses five major categories: cohort design characteristics (prospective/retrospective, longitudinal/cross-sectional, case-control, family-based), participant demographics and socioeconomic attributes, phenotypic data collection methods (questionnaires, clinical assessments, biospecimen types), genomics data types (whole genome sequencing, exome sequencing, genome-wide association studies, RNA-seq, methylation arrays), and data access policies with consent frameworks. GECKO standardizes terminology for cohort recruitment strategies, inclusion/exclusion criteria, sample sizes, age ranges, ancestry populations, geographic locations, and follow-up durations critical for cohort discovery and meta-analysis planning. The ontology integrates with other OBO Foundry ontologies including BFO (Basic Formal Ontology) as top-level and OBI (Ontology for Biomedical Investigations) as mid-level, ensuring semantic interoperability. GECKO provides two products: the OBO-compliant gecko.owl for formal ontology applications, and ihcc-gecko.owl tailored for IHCC cohort cataloging with specialized browser labels and categorization. Automated tools based on JSON schema mapping files enable generation of harmonized data dictionaries and FAIRified metadata for cohort studies. Applications include cohort discovery portals enabling researchers to identify suitable cohorts for collaborative studies, standardized phenotype harmonization across studies for meta-GWAS, ethical data sharing frameworks through consent ontology terms, and FAIR data principles implementation in population genomics repositories. GECKO facilitates interoperability between cohort catalogs like BBMRI-ERIC, Maelstrom Research, dbGaP, and EGA by providing common terminology for cohort characteristics, enabling federated queries across international biobanks. The ontology is distributed under Creative Commons Attribution 4.0 International License, ensuring open access for academic and commercial research.	True	False	https://github.com/IHCC-cohorts/GECKO	https://github.com/IHCC-cohorts/GECKO																
B2AI_STANDARD:457	B2AI_STANDARD:OntologyOrVocabulary	GENO	Genotype Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	GENO is an OWL2 OBO Foundry ontology representing levels of genetic variation specified in genotypes to support genotype-to-phenotype (G2P) data aggregation and analysis across diverse research communities and sources. The core model is a graph decomposing genotypes into smaller components of variation, from complete genotypes specifying sequence variation across entire genomes down to specific allelic variants and sequence alterations. This partonomy structure enables integrated analysis of G2P data where phenotype annotations are made at different granularity levels. GENO describes genotype attributes including zygosity, genomic position, expression, dominance, and functional dependencies or consequences of variants. Beyond heritable genomic sequence variation, GENO represents transient variation in gene expression from knockdown reagents or overexpression constructs, representing this variation in terms of targeted genes to parallel sequence variation representation. GENO models G2P associations focusing on genotype-phenotype-environment interplay and uses the Scientific Evidence and Provenance Information Ontology (SEPIO) for provenance and experimental evidence. The ontology is orthogonal to but integrates with the Sequence Ontology (SO), Human Phenotype Ontology (HPO), Feature Annotation Location Description Ontology (FALDO), and Variation Ontology (VariO), supporting AI/ML applications in variant effect prediction, phenotype association analysis, and precision medicine.	True	False	https://github.com/monarch-initiative/GENO-ontology	https://github.com/monarch-initiative/GENO-ontology														[B2AI_ORG:58]		
B2AI_STANDARD:458	B2AI_STANDARD:OntologyOrVocabulary	GEO	Geographical Entity Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:14]	An ontology of geographical entities	True	False	https://github.com/ufbmi/geographical-entity-ontology/	https://github.com/ufbmi/geographical-entity-ontology/														[B2AI_ORG:96]		
B2AI_STANDARD:459	B2AI_STANDARD:OntologyOrVocabulary	GMDN	Global Medical Device Nomenclature	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	The Global Medical Device Nomenclature (GMDN) is a comprehensive set of terms, within a structured category hierarchy, which name and group ALL medical device products including implantables, medical equipment, consumables, and diagnostic devices.	False	True	https://www.gmdnagency.org/															[B2AI_ORG:35]		
B2AI_STANDARD:460	B2AI_STANDARD:OntologyOrVocabulary	GNO	Glycan Naming and Subsumption Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	GlyTouCan provides stable accessions for glycans described at varyious degrees of characterization, including compositions (no linkage).	True	False	https://gnome.glyomics.org/	https://github.com/glygen-glycan-data/GNOme										doi:10.5281/zenodo.6678279						
B2AI_STANDARD:461	B2AI_STANDARD:OntologyOrVocabulary	HSO	Health Surveillance Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	The Health Surveillance Ontology (HSO) provides a comprehensive framework for standardizing terminology and concepts related to health surveillance systems, disease monitoring programs, and public health data collection across human and veterinary medicine. Developed by the Swedish Veterinary Agency (SVA) and aligned with OBO Foundry principles, HSO enables interoperability between surveillance databases, epidemiological studies, and public health information systems by providing consistent vocabulary for surveillance activities, case definitions, reporting requirements, and data quality metrics. The ontology encompasses surveillance system architectures (passive surveillance, active surveillance, sentinel surveillance, syndromic surveillance), data collection methodologies (laboratory-based surveillance, clinical reporting, population surveys, environmental monitoring), case classification criteria (confirmed cases, probable cases, suspect cases based on laboratory/clinical/epidemiological evidence), temporal and spatial granularity specifications (reporting periods, geographic resolution, population denominators), and data quality indicators (completeness, timeliness, representativeness, sensitivity, specificity). HSO supports One Health approaches by bridging human, animal, and environmental health surveillance domains, facilitating detection of zoonotic disease emergence, antimicrobial resistance tracking, and foodborne outbreak investigations. Applications include standardization of surveillance system metadata for interoperability between national and international health agencies (WHO, ECDC, OIE), automated validation of surveillance data submissions, harmonization of case definitions across jurisdictions, and machine-readable representation of surveillance protocols for reproducibility. HSO integrates with disease ontologies (DO, DOID), pathogen ontologies (IDO, NCBITaxon), and geographic ontologies (GAZ) to provide comprehensive semantic framework for epidemiological data. The ontology enables FAIR principles implementation in public health surveillance by making surveillance system documentation findable, accessible, interoperable, and reusable.	True	False	https://w3id.org/hso	https://github.com/SVA-SE/HSO														[B2AI_ORG:92]		
B2AI_STANDARD:462	B2AI_STANDARD:OntologyOrVocabulary	HL7 Vocabulary	HL7 Vocabulary	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	An index to the HL7-supported Code Systems.	True	False	https://www.hl7.org/documentcenter/public/standards/vocabulary/vocabulary_tables/infrastructure/vocabulary/vocabulary.html#voc-systems															[B2AI_ORG:40]		
B2AI_STANDARD:463	B2AI_STANDARD:OntologyOrVocabulary	HOM	Homology Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Concepts related to homology, as well as other concepts used to describe similarity and non-homology.	True	False	https://github.com/BgeeDB/homology-ontology	https://github.com/BgeeDB/homology-ontology										doi:10.1016/j.tig.2009.12.012						
B2AI_STANDARD:464	B2AI_STANDARD:OntologyOrVocabulary	HOOM	HPO - ORDO Ontological Module	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:25]	Orphanet provides phenotypic annotations of the rare diseases in the Orphanet nomenclature using the Human Phenotype Ontology (HPO). HOOM is a module that qualifies the annotation between a clinical entity and phenotypic abnormalities according to a frequency and by integrating the notion of diagnostic criterion.	True	False	https://bioportal.bioontology.org/ontologies/HOOM	https://www.orphadata.com/ontologies/														[B2AI_ORG:80]		
B2AI_STANDARD:465	B2AI_STANDARD:OntologyOrVocabulary	HANCESTRO	Human Ancestry Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	A systematic description of the ancestry concepts used in the NHGRI-EBI Catalog	True	False	https://github.com/EBISPOT/ancestro	https://github.com/EBISPOT/ancestro										doi:10.1186/s13059-018-1396-2						
B2AI_STANDARD:466	B2AI_STANDARD:OntologyOrVocabulary	HSAPDV	Human Developmental Stages	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	The Human Developmental Stages ontology (HsapDv) provides standardized terminology for describing human lifecycle stages from conception through senescence, developed by the Bgee group in collaboration with Uberon and EHDAA2 (Human Developmental Anatomy Ontology) developers to enable precise temporal annotation of biological data across developmental biology, clinical research, and population studies. HsapDv encompasses both embryonic and postnatal stages, utilizing Carnegie staging system for prenatal development (Carnegie stages 1-23 covering days 1-56 post-fertilization, approximately embryonic weeks 1-8) which provides morphology-based developmental milestones independent of gestational age variations. Embryonic stages capture critical developmental events including fertilization, cleavage, blastocyst formation, gastrulation, neurulation, organogenesis, and fetal development through birth. Postnatal stages include neonatal period (birth to 28 days), infancy (1 month to 2 years), early childhood (2-6 years), middle childhood (6-12 years), adolescence (12-18 years encompassing puberty), young adulthood (18-40 years), middle adulthood (40-65 years), and late adulthood/senescence (65+ years) with subdivisions for geriatric populations. Each stage is formally defined with temporal boundaries, morphological characteristics, physiological milestones (motor skills, cognitive development, hormonal changes), and relationships to other developmental stages through \"immediately_preceded_by\" and \"part_of\" relations. HsapDv integrates with Uberon for anatomical structure development timing, enabling queries like \"when does the cerebral cortex develop\" or \"which genes are expressed in neural tube during neurulation.\" Applications span developmental biology research (temporal annotation of gene expression atlases, single-cell RNA-seq developmental trajectories, epigenetic modification timelines), clinical medicine (prenatal diagnosis, developmental delay assessment, age-appropriate clinical reference ranges), teratology studies (critical periods for teratogen exposure), pharmacology (age-specific drug metabolism and dosing), and epidemiology (age-stratified disease incidence). HsapDv enables cross-species developmental comparisons through alignment with other species-specific developmental ontologies (mouse MmusDv, zebrafish ZFS), facilitating translational research and comparative embryology. The ontology is distributed in OBO and OWL formats through http://purl.obolibrary.org/obo/hsapdv.owl and browsable via OBO Foundry portals, supporting reproducible temporal annotation in biomedical databases, developmental atlases, and clinical decision support systems.	True	False	https://github.com/obophenotype/developmental-stage-ontologies/wiki/HsapDv	https://github.com/obophenotype/developmental-stage-ontologies																
B2AI_STANDARD:467	B2AI_STANDARD:OntologyOrVocabulary	DOID	Human Disease Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:7]	The Human Disease Ontology (DOID) is a comprehensive, standardized ontology that provides a hierarchical classification system for human diseases organized primarily by etiology (underlying cause). Developed as part of the Open Biomedical Ontologies (OBO) Foundry, DOID serves as a cornerstone reference for disease terminology in biomedical research, clinical informatics, and healthcare applications. The ontology integrates multiple disease classification systems including ICD, SNOMED CT, UMLS, and MeSH, providing extensive cross-references that enable interoperability between different medical coding systems. DOID structures diseases into logical hierarchies based on disease mechanisms, affected anatomical systems, and causal agents, enabling both broad categorical searches and precise disease identification. Each disease concept includes standardized names, definitions, synonyms, and relationships to parent and child terms, creating a rich semantic network that supports computational analysis of disease data. The ontology is extensively used in genomics databases, electronic health records, biomedical literature annotation, drug discovery pipelines, and epidemiological studies where consistent disease terminology is essential for data integration and comparative analysis.	True	False	http://www.disease-ontology.org	https://github.com/DiseaseOntology/HumanDiseaseOntology																
B2AI_STANDARD:468	B2AI_STANDARD:OntologyOrVocabulary	HPO	Human Phenotype Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:25]	The Human Phenotype Ontology (HPO) provides standardized vocabulary for phenotypic abnormalities encountered in human disease, containing over 18,000 terms and 156,000+ annotations to hereditary diseases. Each HPO term describes a specific phenotypic feature (e.g., \"Atrial septal defect\" HP:0001631) organized in a hierarchical structure from general to specific findings. Developed using medical literature, Orphanet, DECIPHER, and OMIM, HPO enables precise phenotype-driven differential diagnostics, genomic variant interpretation, and translational research. The ontology integrates with major biomedical resources and powers phenotype matching algorithms that rank diseases by clinical feature similarity. HPO is foundational for rare disease diagnosis tools (Exomiser, Phenomizer, PhenoTips), electronic health record phenotyping, and clinical decision support systems. As a Monarch Initiative flagship product and GA4GH driver project, HPO enables semantic integration across species, connecting human phenotypes to model organism phenotypes for translational research. The ontology supports deep phenotyping in genomics studies, electronic health record phenotype extraction, natural language processing for clinical notes, and phenotype-driven gene prioritization. HPO annotations link phenotypes to genes, diseases, and publications, facilitating genotype-phenotype correlation studies. In AI/ML applications, HPO powers phenotype-based similarity learning for rare disease diagnosis, automated phenotype extraction from clinical narratives using NLP, ontology-guided feature engineering for predictive models, knowledge graph embeddings for disease gene discovery, and multi-modal patient representation learning combining genomics, phenotypes, and clinical data to support precision medicine and clinical genomics.	True	False	https://hpo.jax.org/	https://github.com/obophenotype/human-phenotype-ontology											[B2AI_STANDARD:784]					
B2AI_STANDARD:469	B2AI_STANDARD:OntologyOrVocabulary	XLMOD	HUPO-PSI cross-linking and derivatization reagents controlled vocabulary	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:28]	A structured controlled vocabulary for cross-linking reagents used with proteomics mass spectrometry.	True	False	https://www.psidev.info/groups/controlled-vocabularies															[B2AI_ORG:41]		
B2AI_STANDARD:470	B2AI_STANDARD:OntologyOrVocabulary	HTN	Hypertension Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:7]	An ontology for representing clinical data about hypertension.	True	False	https://github.com/aellenhicks/htn_owl	https://github.com/aellenhicks/htn_owl																
B2AI_STANDARD:471	B2AI_STANDARD:OntologyOrVocabulary	IDO	Infectious Disease Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:7]	A set of interoperable ontologies that will together provide coverage of the infectious disease domain. IDO core is the upper-level ontology...	True	False	https://github.com/infectious-disease-ontology/infectious-disease-ontology	https://github.com/infectious-disease-ontology/infectious-disease-ontology																
B2AI_STANDARD:472	B2AI_STANDARD:OntologyOrVocabulary	IAO	Information Artifact Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	An ontology of information entities.	True	False	https://github.com/information-artifact-ontology/IAO	https://github.com/information-artifact-ontology/IAO																
B2AI_STANDARD:473	B2AI_STANDARD:OntologyOrVocabulary	ICO	Informed Consent Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:4]	An ontology of clinical informed consents	True	False	https://github.com/ICO-ontology/ICO	https://github.com/ICO-ontology/ICO																
B2AI_STANDARD:474	B2AI_STANDARD:OntologyOrVocabulary	ICEO	Integrative and Conjugative Element Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	An integrated biological ontology for the description of bacterial integrative and conjugative elements (ICEs).	True	False	http://db-mml.sjtu.edu.cn/ICEberg/	https://github.com/ontoice/ICEO										doi:10.1038/s41597-021-01112-5						
B2AI_STANDARD:475	B2AI_STANDARD:OntologyOrVocabulary	ITO	Intelligence Task Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Comprehensive, curated and interlinked data of artificial intelligence tasks, benchmarks, AI performance metrics, benchmark results and research papers.	True	False	https://openbiolink.github.io/ITOExplorer/	https://github.com/OpenBioLink/ITO														[B2AI_ORG:87]		
B2AI_STANDARD:476	B2AI_STANDARD:OntologyOrVocabulary	INO	Interaction Network Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	An ontology of interactions and interaction networks.	True	False	https://github.com/INO-ontology/ino	https://github.com/INO-ontology/ino										doi:10.1186/2041-1480-6-2						
B2AI_STANDARD:477	B2AI_STANDARD:OntologyOrVocabulary	IOBC	Interlinking Ontology for Biological Concepts	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:1]	biological, biomedical, and related concepts	True	False	https://github.com/kushidat/IOBC	https://github.com/kushidat/IOBC										doi:10.1007/s00354-019-00074-y						
B2AI_STANDARD:478	B2AI_STANDARD:OntologyOrVocabulary	KISAO	Kinetic Simulation Algorithm Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Algorithms for simulating biology, their parameters, and their outputs.	True	False	http://co.mbine.org/standards/kisao	https://github.com/SED-ML/KiSAO																
B2AI_STANDARD:479	B2AI_STANDARD:OntologyOrVocabulary	MP	Mammalian Phenotype Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:25]	Standard terms for annotating mammalian phenotypic data.	True	False	https://github.com/mgijax/mammalian-phenotype-ontology	https://github.com/mgijax/mammalian-phenotype-ontology										doi:10.1007/s00335-012-9421-3						
B2AI_STANDARD:480	B2AI_STANDARD:OntologyOrVocabulary	MVX	Manufacturers of Vaccines	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Code set for active and inactive manufacturers of vaccines in the US.	True	False	https://www2a.cdc.gov/vaccines/iis/iisstandards/vaccines.asp?rpt=mvx															[B2AI_ORG:13]		
B2AI_STANDARD:481	B2AI_STANDARD:OntologyOrVocabulary	MS	Mass spectrometry ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:28]	A structured controlled vocabulary for the annotation of experiments concerned with proteomics mass spectrometry.	True	False	http://www.psidev.info/groups/controlled-vocabularies	https://github.com/HUPO-PSI/psi-ms-CV										doi:10.1093/database/bat009				[B2AI_ORG:41]		
B2AI_STANDARD:482	B2AI_STANDARD:OntologyOrVocabulary	MMO	Measurement method ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	A representation of the variety of methods used to make clinical and phenotype measurements.	True	False	https://rgd.mcw.edu/rgdweb/ontology/view.html?acc_id=MMO:0000000	https://github.com/rat-genome-database/MMO-Measurement-Method-Ontology/										doi:10.1186/2041-1480-4-26						
B2AI_STANDARD:483	B2AI_STANDARD:OntologyOrVocabulary	OLATDV	Medaka Developmental Stages	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	The Medaka Developmental Stages ontology (OlatDv) provides standardized terminology for describing developmental stages of medaka (Oryzias latipes, Japanese rice fish), a key teleost model organism in developmental biology, genetics, toxicology, and comparative vertebrate research, based on Iwamatsu staging system and developed from the original Medaka Fish Ontology (MFO) by Thorsten Henrich. Medaka serves as a powerful model organism complementing zebrafish and mouse due to its transparent embryos enabling live imaging, short generation time (2-3 months), small size suitable for laboratory culture, fully sequenced genome with conserved vertebrate gene organization, and established genetic tools including transgenesis, CRISPR/Cas9 editing, and mutant libraries. OlatDv encompasses embryonic and larval stages from fertilization through sexual maturity, currently focusing on pre-adult development. Embryonic stages follow Iwamatsu's morphological staging system (stages 1-40) covering fertilization (stage 1), cleavage (stages 2-7), blastula (stages 8-10), gastrulation (stages 11-17), neurulation (stages 18-20), organogenesis (stages 21-30), and pre-hatching development (stages 31-40 leading to hatching at approximately day 7-10 post-fertilization at 26C). Each stage is defined by specific morphological landmarks including somite numbers, heart development, pigmentation patterns, fin bud appearance, eye development, and gill filament formation. Post-hatching larval stages capture metamorphosis, scale formation, sex differentiation, and juvenile maturation through first reproduction. OlatDv provides temporal annotations crucial for comparative developmental biology studies examining vertebrate evolution, particularly teleost-specific genome duplication events and developmental innovations. Applications include temporal annotation of gene expression databases (Medaka Expression Database), developmental toxicology studies (OECD fish embryo toxicity tests using medaka as alternative to zebrafish), endocrine disruption research (sex determination mechanisms), carcinogenesis studies (medaka exhibits spontaneous tumor formation), and aging research (short lifespan enables longitudinal studies). Integration with Uberon anatomical ontology enables queries linking developmental stage to organ system maturation, essential for understanding tissue-specific gene expression changes during development. OlatDv facilitates cross-species developmental comparisons through alignment with zebrafish ZFS, frog XAO, and mammalian developmental ontologies, supporting evolutionary developmental biology (evo-devo) research and identification of conserved versus lineage-specific developmental programs across vertebrates. The ontology is distributed through OBO Foundry as olatdv.obo and olatdv.owl formats.	True	False	https://github.com/obophenotype/developmental-stage-ontologies/wiki/OlatDv	https://github.com/obophenotype/developmental-stage-ontologies																
B2AI_STANDARD:484	B2AI_STANDARD:OntologyOrVocabulary	MAXO	Medical Action Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:4]	Terms for medical procedures, interventions, therapies, treatments, and recommendations.	True	False	https://github.com/monarch-initiative/MAxO	https://github.com/monarch-initiative/MAxO														[B2AI_ORG:58]		
B2AI_STANDARD:485	B2AI_STANDARD:OntologyOrVocabulary	MED-RT	Medication Reference Terminology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[codesystem]	[B2AI_TOPIC:8]	Formal ontological representations of medication terminology, pharmacologic classifications, and asserted authoritative relationships between them. Replaces NDF-RT. Provided through UMLS.	True	True	https://evs.nci.nih.gov/ftp1/NDF-RT/Introduction%20to%20MED-RT.pdf															[B2AI_ORG:74]		
B2AI_STANDARD:486	B2AI_STANDARD:OntologyOrVocabulary	MFOMD	Mental Disease Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:7]	Mental diseases such as schizophrenia, annotated with DSM-IV and ICD codes where applicable.	True	False	https://github.com/jannahastings/mental-functioning-ontology	https://github.com/jannahastings/mental-functioning-ontology																
B2AI_STANDARD:487	B2AI_STANDARD:OntologyOrVocabulary	MF	Mental Functioning Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	The Mental Functioning Ontology (MF) is an OBO Foundry ontology providing structured terminology for mental processes, cognitive functions, emotional states, and behavioral phenomena to support standardized annotation of neuroscience data, psychiatric research, psychological assessments, and mental health informatics. Developed by Janna Hastings and collaborators, MF extends the Basic Formal Ontology (BFO) upper-level framework and integrates with domain ontologies including the Mental Disease Ontology (MD) for psychiatric disorders, the Cognitive Atlas for cognitive processes, the Emotion Ontology for affective states, and the Gene Ontology (GO) for molecular underpinnings of neural function. The ontology comprehensively represents cognitive domains including perception (visual, auditory, somatosensory, chemosensory), attention (selective, divided, sustained), memory (working, episodic, semantic, procedural), executive functions (planning, inhibition, cognitive flexibility, decision-making), language processing, and reasoning, alongside emotional and motivational constructs such as valence, arousal, mood states, personality traits, and social cognition (theory of mind, empathy, social perception). MF captures temporal dynamics of mental processes (onset, duration, termination), intensity dimensions, and contextual dependencies, enabling nuanced representation of psychological phenomena. Integration with clinical terminologies like DSM-5, ICD-11, and RDoC (Research Domain Criteria) facilitates translational psychiatry linking basic neuroscience to clinical phenotypes. Applications include standardized annotation of neuroimaging studies identifying brain regions associated with specific mental functions, computational psychiatry modeling mental disorders as disruptions in cognitive and emotional processes, natural language processing extracting mental state descriptions from clinical notes, AI-based mental health assessment systems, meta-analysis of psychological experiments through unified terminology, and personalized medicine approaches tailoring psychiatric treatments to individual cognitive profiles. MF follows OBO Foundry principles with open-source development, logical consistency checking, and community-driven refinement, essential for computational neuroscience, digital mental health platforms, cognitive science research, and integrative brain databases.	True	False	https://github.com/jannahastings/mental-functioning-ontology	https://github.com/jannahastings/mental-functioning-ontology																
B2AI_STANDARD:488	B2AI_STANDARD:OntologyOrVocabulary	MOD	Metadata vocabulary for Ontology Description and Publication	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	An OWL ontology and application profile to capture metadata information for ontologies, vocabularies or semantic resources/artefacts in general.	True	False	https://github.com/sifrproject/MOD-Ontology	https://github.com/sifrproject/MOD-Ontology										doi:10.1007/978-3-319-70863-8_17						
B2AI_STANDARD:489	B2AI_STANDARD:OntologyOrVocabulary	MRO	MHC Restriction Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	The Major Histocompatibility Complex (MHC) Restriction Ontology (MRO) is a specialized ontology developed by the Immune Epitope Database (IEDB) to provide standardized terminology for describing MHC restriction phenomena in immunological experiments and data. MHC restriction refers to the biological process by which T-cell recognition of antigens is limited to peptides presented by specific MHC molecules that are compatible with the T-cell's own MHC background. MRO systematically organizes the complex relationships between MHC alleles, T-cell responses, and antigen presentation contexts that are crucial for understanding adaptive immune responses, vaccine development, and transplantation immunology. The ontology enables precise annotation of immunological experiments by providing controlled vocabulary terms for MHC class I and class II molecules, their allelic variants, restriction patterns, and associated experimental conditions. This standardization is essential for comparative immunology studies, epitope mapping projects, and the development of personalized immunotherapies where accurate description of MHC-restricted immune responses is critical for data interpretation and clinical translation.	True	False	https://github.com/IEDB/MRO	https://github.com/IEDB/MRO																
B2AI_STANDARD:490	B2AI_STANDARD:OntologyOrVocabulary	MIAPA	MIAPA Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	An application ontology to formalize annotation of phylogenetic data.	True	False	https://www.evoio.org/wiki/MIAPA	https://github.com/evoinfo/miapa/										doi:10.1089/omi.2006.10.231						
B2AI_STANDARD:491	B2AI_STANDARD:OntologyOrVocabulary	MPIO	Minimum PDDI Information Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Minimum information regarding potential drug-drug interaction information.	True	False	https://github.com/MPIO-Developers/MPIO	https://github.com/MPIO-Developers/MPIO																
B2AI_STANDARD:492	B2AI_STANDARD:OntologyOrVocabulary	MCRO	Model Card Report Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[modelcards]	[B2AI_TOPIC:5]	An OWL2-based artifact that represents and formalizes model card report information. The current release of this ontology utilizes standard concepts and properties from OBO Foundry ontologies.	True	False	https://github.com/UTHealth-Ontology/MCRO	https://github.com/UTHealth-Ontology/MCRO										doi:10.1186/s12859-022-04797-6						
B2AI_STANDARD:493	B2AI_STANDARD:OntologyOrVocabulary	MI	Molecular Interactions Controlled Vocabulary	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:20|B2AI_TOPIC:26]	Vocabulary for the annotation of experiments concerned with protein-protein interactions.	True	False	https://github.com/HUPO-PSI/psi-mi-CV	https://github.com/HUPO-PSI/psi-mi-CV														[B2AI_ORG:41]		
B2AI_STANDARD:494	B2AI_STANDARD:OntologyOrVocabulary	MOP	Molecular Process Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	The Molecular Process Ontology (MOP) provides standardized vocabulary for describing molecular-level processes, transformations, and mechanisms that occur in chemical and biochemical systems, maintained by the Royal Society of Chemistry as part of the RSC ontology ecosystem. MOP encompasses reaction mechanisms (nucleophilic substitution, electrophilic addition, radical reactions, pericyclic reactions), molecular interactions (hydrogen bonding, van der Waals forces, - stacking, hydrophobic interactions, electrostatic interactions), conformational changes (protein folding, ligand-induced conformational shifts, allosteric transitions), energy transfer processes (fluorescence resonance energy transfer FRET, photoinduced electron transfer, vibrational energy relaxation), and transport phenomena (diffusion, membrane permeation, active transport, facilitated diffusion). The ontology provides detailed mechanistic descriptions including activation energies, transition states, reaction intermediates, rate-determining steps, and catalytic cycles essential for understanding chemical reactivity and biological function at the molecular scale. MOP integrates with the Chemical Methods Ontology (CHMO) for experimental techniques, CHEBI for chemical entities, and the Gene Ontology (GO) for biological processes, enabling comprehensive annotation of molecular transformations from pure chemistry through biochemistry to systems biology. Applications include annotation of reaction databases for synthetic chemistry planning, mechanistic modeling of enzymatic catalysis, drug-target interaction mechanisms for rational drug design, metabolic pathway analysis with detailed reaction mechanisms, and computational chemistry workflow documentation. MOP supports reproducibility in mechanistic studies by standardizing descriptions of reaction conditions, stereochemical outcomes, regioselectivity, and stereoselectivity patterns. The ontology enables semantic searches for reactions by mechanism type, facilitating discovery of analogous transformations across different chemical contexts and supporting retrosynthetic analysis in computer-aided synthesis planning tools.	True	False	https://www.ebi.ac.uk/ols/ontologies/mop	https://github.com/rsc-ontologies/rxno														[B2AI_ORG:29]		
B2AI_STANDARD:495	B2AI_STANDARD:OntologyOrVocabulary	MONDO	Mondo Disease Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:7]	MONDO is an OBO Foundry ontology providing a unified disease terminology that harmonizes disease definitions across multiple resources including HPO, OMIM, SNOMED CT, ICD, ORDO, DO, MedGen, GARD, and others. It addresses the proliferation of inconsistent disease mappings by providing logic-based structure with precise 1:1 equivalence axioms connecting to other resources, validated by OWL reasoning. MONDO contains over 25,880 diseases including 22,919 human diseases (4,727 cancers, 1,074 infectious diseases, 11,601 Mendelian diseases, 15,857 rare diseases) and 2,960 non-human diseases, with 129,785 database cross-references and 108,076 synonyms (exact, narrow, broad, and related). The ontology provides hierarchical classification for disease grouping and rolling up and uses precise semantic annotations for each mapping rather than loose cross-references. MONDO is released in three formats: mondo-with-equivalents.owl (with OWL equivalence axioms and inter-ontology axiomatization using CL, Uberon, GO, HP, RO, NCBITaxon), mondo.obo (simplified with xrefs), and mondo-with-equivalents.json. Coordinated with the Human Phenotype Ontology (HPO) which describes phenotypic features, MONDO supports AI/ML applications in disease classification, phenotype-disease association, rare disease diagnosis, and cross-resource knowledge integration.	True	False	https://mondo.monarchinitiative.org/	https://github.com/monarch-initiative/mondo										doi:10.1093/nar/gkw1128				[B2AI_ORG:58]		
B2AI_STANDARD:496	B2AI_STANDARD:OntologyOrVocabulary	EMAPA	Mouse Developmental Anatomy Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Mouse anatomy covering embryonic development and postnatal stages.	True	False	http://www.informatics.jax.org/expression.shtml	https://github.com/obophenotype/mouse-anatomy-ontology																
B2AI_STANDARD:497	B2AI_STANDARD:OntologyOrVocabulary	MPATH	Mouse pathology ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	A structured controlled vocabulary of mutant and transgenic mouse pathology phenotypes.	True	False	http://www.pathbase.net/	https://github.com/PaulNSchofield/mpath																
B2AI_STANDARD:498	B2AI_STANDARD:OntologyOrVocabulary	RXNO	Name Reaction Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:3]	Connects organic name reactions to their roles in an organic synthesis and to processes in MOP	True	False	https://github.com/rsc-ontologies/rxno	https://github.com/rsc-ontologies/rxno																
B2AI_STANDARD:499	B2AI_STANDARD:OntologyOrVocabulary	NDC	National Drug Code	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:8]	Information about finished drug products, unfinished drugs and compounded drug products	True	False	https://www.accessdata.fda.gov/scripts/cder/ndc/index.cfm															[B2AI_ORG:31]		
B2AI_STANDARD:500	B2AI_STANDARD:OntologyOrVocabulary	NCBITAXON	NCBI organismal classification	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	An ontology representation of the NCBI organismal taxonomy.	True	False	http://www.ncbi.nlm.nih.gov/taxonomy	https://github.com/obophenotype/ncbitaxon														[B2AI_ORG:74]		
B2AI_STANDARD:501	B2AI_STANDARD:OntologyOrVocabulary	NCIT	NCI Thesaurus OBO Edition	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	A reference terminology that includes broad coverage of the cancer domain, including cancer related diseases.	True	False	https://github.com/NCI-Thesaurus/thesaurus-obo-edition	https://github.com/NCI-Thesaurus/thesaurus-obo-edition														[B2AI_ORG:74]		
B2AI_STANDARD:502	B2AI_STANDARD:OntologyOrVocabulary	NBO	Neuro Behavior Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Human and animal behaviours and behavioural phenotypes	True	False	https://github.com/obo-behavior/behavior-ontology/	https://github.com/obo-behavior/behavior-ontology/																
B2AI_STANDARD:503	B2AI_STANDARD:OntologyOrVocabulary	NeuroNames	Neuronames Brain Hierarchy	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:22]	A Comprehensive Hierarchical Nomenclature for Structures of the Primate Brain (human and macaque)	True	False	http://braininfo.rprc.washington.edu/aboutBrainInfo.aspx#NeuroNames											doi:10.1007/s12021-011-9128-8						
B2AI_STANDARD:504	B2AI_STANDARD:OntologyOrVocabulary	NOMEN	Nomenclatural ontology for biological names	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	A nomenclatural ontology for biological names (not concepts). It encodes the goverened rules of nomenclature.	True	False	https://github.com/SpeciesFileGroup/nomen	https://github.com/SpeciesFileGroup/nomen																
B2AI_STANDARD:505	B2AI_STANDARD:OntologyOrVocabulary	NCRO	Non-Coding RNA Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:33]	An ontology for non-coding RNA, both of biological origin, and engineered.	True	False	https://github.com/OmniSearch/ncro	https://github.com/OmniSearch/ncro																
B2AI_STANDARD:506	B2AI_STANDARD:OntologyOrVocabulary	NCPT	Nutrition Care Process Terminology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Terms for nutrition assessment, diagnosis, intervention, and monitoring/evaluation.	False	True	https://www.ncpro.org/																	
B2AI_STANDARD:507	B2AI_STANDARD:OntologyOrVocabulary	OMO	OBO Metadata Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Terms that are used to annotate ontology terms for all OBO ontologies.	True	False	https://github.com/information-artifact-ontology/ontology-metadata	https://github.com/information-artifact-ontology/ontology-metadata																
B2AI_STANDARD:508	B2AI_STANDARD:OntologyOrVocabulary	ONTONEO	Obstetric and Neonatal Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	A structured controlled vocabulary to provide a representation of the data from electronic health records involved in the care of pregnancy.	True	False	https://github.com/ontoneo-project/Ontoneo	https://github.com/ontoneo-project/Ontoneo																
B2AI_STANDARD:509	B2AI_STANDARD:OntologyOrVocabulary	ONTOAVIDA	OntoAvida ontology for Avida digital evolution platform	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Vocabulary for the description of the most widely-used computational approach for studying digital evolution.	True	False	https://gitlab.com/fortunalab/ontoavida	https://gitlab.com/fortunalab/ontoavida																
B2AI_STANDARD:510	B2AI_STANDARD:OntologyOrVocabulary	OBIB	Ontology for Biobanking	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Annotation and modeling of biobank repository and biobanking administration.	True	False	https://github.com/biobanking/biobanking	https://github.com/biobanking/biobanking																
B2AI_STANDARD:511	B2AI_STANDARD:OntologyOrVocabulary	OBI	Ontology for Biomedical Investigations	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Description of life-science and clinical investigations.	True	False	http://obi-ontology.org	https://github.com/obi-ontology/obi										doi:10.1371/journal.pone.0154556						
B2AI_STANDARD:512	B2AI_STANDARD:OntologyOrVocabulary	OGMS	Ontology for General Medical Science	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:4|B2AI_TOPIC:7]	Treatment of disease and diagnosis and on carcinomas and other pathological entities.	True	False	https://github.com/OGMS/ogms	https://github.com/OGMS/ogms																
B2AI_STANDARD:513	B2AI_STANDARD:OntologyOrVocabulary	OMIT	Ontology for MIRNA Target	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Data exchange standards and common data elements in the microRNA (miR) domain.	True	False	https://github.com/OmniSearch/omit	https://github.com/OmniSearch/omit																
B2AI_STANDARD:514	B2AI_STANDARD:OntologyOrVocabulary	ONE	Ontology for Nutritional Epidemiology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Research output of nutritional epidemiologic studies.	True	False	https://github.com/cyang0128/Nutritional-epidemiologic-ontologies	https://github.com/cyang0128/Nutritional-epidemiologic-ontologies										doi:10.3390/nu11061300						
B2AI_STANDARD:515	B2AI_STANDARD:OntologyOrVocabulary	ONS	Ontology for Nutritional Studies	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Description of concepts in the nutritional studies domain.	True	False	https://github.com/enpadasi/Ontology-for-Nutritional-Studies	https://github.com/enpadasi/Ontology-for-Nutritional-Studies										doi:10.1186/s12263-018-0601-y						
B2AI_STANDARD:516	B2AI_STANDARD:OntologyOrVocabulary	OAE	Ontology of Adverse Events	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:4|B2AI_TOPIC:7]	Ontology of Adverse Events (OAE) is a community-developed biomedical ontology following OBO Foundry principles that provides standardized representation and classification of adverse events resulting from medical interventions including drug administration, vaccination, medical devices, procedures, and dietary supplements, enabling systematic analysis of safety data across clinical trials, pharmacovigilance systems, and electronic health records. OAE structures adverse events hierarchically under upper-level classes from the Basic Formal Ontology (BFO), integrating with domain ontologies including OGMS (disease processes), VO (vaccine components), DRON (drug products), and UBERON (anatomical structures) to capture mechanistic relationships between interventions, biological processes, and observed adverse outcomes. The ontology distinguishes between adverse events (any untoward medical occurrence temporally associated with intervention use) and adverse drug reactions (events with causal relationship to intervention), incorporating causality assessment frameworks (Naranjo scale, WHO-UMC criteria) as logical axioms that infer ADR status based on evidence patterns. OAE represents clinical manifestations (symptoms, signs, laboratory abnormalities), severity grades (CTCAE scales from mild to life-threatening), temporal patterns (immediate hypersensitivity, delayed reactions, cumulative toxicity), and anatomical localizations, supporting detailed phenotyping of safety profiles. The ontology enables cross-study aggregation of adverse event data by providing standardized terms that harmonize heterogeneous reporting formats from FDA FAERS, EMA EudraVigilance, WHO VigiBase, and clinical trial databases, facilitating meta-analyses of intervention safety and identification of rare adverse events invisible in individual studies. OAE supports pharmacovigilance signal detection by structuring adverse event hierarchies that enable mining of parent-child term relationships, discovering drug-event associations through disproportionality analysis (ROR, IC025), and prioritizing signals for regulatory review. In vaccine safety surveillance, OAE terms annotate Brighton Collaboration case definitions for standardized adverse event reporting post-vaccination (AEFI), supporting global safety monitoring networks coordinated by WHO. For AI/ML applications, OAE provides structured labels for training natural language processing models to extract adverse event mentions from clinical notes, social media posts, and regulatory documents, enables knowledge graph construction linking drugs, vaccines, adverse events, and patient characteristics for predictive safety modeling, and supports explainable AI systems that generate human-interpretable safety signals by reasoning over ontology-encoded mechanistic pathways connecting interventions to adverse outcomes, ultimately enhancing patient safety through earlier detection and characterization of intervention-related harms.	True	False	https://github.com/OAE-ontology/OAE/	https://github.com/OAE-ontology/OAE/																
B2AI_STANDARD:517	B2AI_STANDARD:OntologyOrVocabulary	OBCS	Ontology of Biological and Clinical Statistics	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Biological and clinical statistics.	True	False	https://github.com/obcs/obcs	https://github.com/obcs/obcs																
B2AI_STANDARD:518	B2AI_STANDARD:OntologyOrVocabulary	OBA	Ontology of Biological Attributes	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	A collection of biological attributes (traits) covering all kingdoms of life.	True	False	https://wiki.geneontology.org/index.php/Extensions/x-attribute	https://github.com/obophenotype/bio-attribute-ontology																
B2AI_STANDARD:519	B2AI_STANDARD:OntologyOrVocabulary	OGSF	Ontology of Genetic Susceptibility Factor	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	An application ontology to represent genetic susceptibility to a specific disease, adverse event, or a pathological process.	True	False	https://github.com/linikujp/OGSF	https://github.com/linikujp/OGSF																
B2AI_STANDARD:520	B2AI_STANDARD:OntologyOrVocabulary	OHPI	Ontology of Host Pathogen Interactions	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Host-pathogen interactions and virulence factors.	True	False	https://github.com/OHPI/ohpi	https://github.com/OHPI/ohpi										doi:10.1093/nar/gky999						
B2AI_STANDARD:521	B2AI_STANDARD:OntologyOrVocabulary	OHMI	Ontology of Host-Microbiome Interactions	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Entities and relations related to microbiomes, microbiome host organisms (e.g., human and mouse), and the interactions between the hosts and microbiomes at different conditions.	True	False	https://github.com/ohmi-ontology/ohmi	https://github.com/ohmi-ontology/ohmi																
B2AI_STANDARD:522	B2AI_STANDARD:OntologyOrVocabulary	OMRSE	Ontology of Medically Related Social Entities	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	This ontology covers the domain of social entities that are related to health care, such as demographic information and the roles of various...	True	False	https://github.com/ufbmi/OMRSE/wiki/OMRSE-Overview	https://github.com/ufbmi/OMRSE										doi:10.1186/s13326-016-0087-8						
B2AI_STANDARD:523	B2AI_STANDARD:OntologyOrVocabulary	OOSTT	Ontology of Organizational Structures of Trauma centers and Trauma systems	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Organizational components of trauma centers and trauma systems.	True	False	https://github.com/OOSTT/OOSTT	https://github.com/OOSTT/OOSTT														[B2AI_ORG:96]		
B2AI_STANDARD:524	B2AI_STANDARD:OntologyOrVocabulary	OPMI	Ontology of Precision Medicine and Investigation	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Entities and relations associated with precision medicine and related investigations at different conditions.	True	False	https://github.com/OPMI/opmi	https://github.com/OPMI/opmi																
B2AI_STANDARD:525	B2AI_STANDARD:OntologyOrVocabulary	ORNASEQ	Ontology of RNA Sequencing	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:33]	An application ontology designed to annotate next-generation sequencing experiments performed on RNA.	True	False	https://github.com/safisher/ornaseq	https://github.com/safisher/ornaseq																
B2AI_STANDARD:526	B2AI_STANDARD:OntologyOrVocabulary	OVAE	Ontology of Vaccine Adverse Events	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:4|B2AI_TOPIC:7]	A biomedical ontology in the domain of vaccine adverse events.	True	False	https://github.com/OVAE-Ontology/ovae	https://github.com/OVAE-Ontology/ovae																
B2AI_STANDARD:527	B2AI_STANDARD:OntologyOrVocabulary	OHD	Oral Health and Disease Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:7]	Content of dental practice health records.	True	False	https://github.com/oral-health-and-disease-ontologies/ohd-ontology	https://github.com/oral-health-and-disease-ontologies/ohd-ontology										doi:10.1186/s13326-020-00222-0						
B2AI_STANDARD:528	B2AI_STANDARD:OntologyOrVocabulary	ORDO	Orphanet Rare Disease Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:7]	The Orphanet Rare Disease ontology (ORDO) is jointly developed by Orphanet and the EBI to provide a structured vocabulary for rare diseases capturing relationships between diseases, genes and other relevant features which will form a useful resource for the computational analysis of rare diseases.	True	False	https://bioportal.bioontology.org/ontologies/ORDO	https://www.orphadata.com/ontologies/														[B2AI_ORG:80]		
B2AI_STANDARD:529	B2AI_STANDARD:OntologyOrVocabulary	PHIPO	Pathogen Host Interaction Phenotype Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Species-neutral phenotypes observed in pathogen-host interactions.	True	False	https://github.com/PHI-base/phipo	https://github.com/PHI-base/phipo										doi:10.1093/nar/gkab1037						
B2AI_STANDARD:530	B2AI_STANDARD:OntologyOrVocabulary	TRANS	Pathogen Transmission Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:7]	An ontology representing the disease transmission process during which the pathogen is transmitted directly or indirectly.	True	False	https://github.com/DiseaseOntology/PathogenTransmissionOntology	https://github.com/DiseaseOntology/PathogenTransmissionOntology										doi:10.1093/nar/gkp832						
B2AI_STANDARD:531	B2AI_STANDARD:OntologyOrVocabulary	PW	Pathway ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:21]	A controlled vocabulary for annotating gene products to pathways.	True	False	http://rgd.mcw.edu/rgdweb/ontology/search.html	https://github.com/rat-genome-database/PW-Pathway-Ontology										doi:10.1186/2041-1480-5-7						
B2AI_STANDARD:532	B2AI_STANDARD:OntologyOrVocabulary	PSDO	Performance Summary Display Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:4]	Ontology to reproducibly study visualizations of clinical performance	True	False	https://github.com/Display-Lab/psdo	https://github.com/Display-Lab/psdo																
B2AI_STANDARD:533	B2AI_STANDARD:OntologyOrVocabulary	PHENIO	Phenomics Integrated Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:3|B2AI_TOPIC:7|B2AI_TOPIC:12|B2AI_TOPIC:21|B2AI_TOPIC:25]	An application ontology for accessing and comparing knowledge concerning phenotypes across species and genetic backgrounds.	True	False	https://github.com/monarch-initiative/phenio	https://github.com/monarch-initiative/phenio														[B2AI_ORG:58]		
B2AI_STANDARD:534	B2AI_STANDARD:OntologyOrVocabulary	PATO	Phenotype And Trait Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:25]	Phenotypic qualities (properties, attributes or characteristics).	True	False	https://github.com/pato-ontology/pato	https://github.com/pato-ontology/pato																
B2AI_STANDARD:535	B2AI_STANDARD:OntologyOrVocabulary	PCO	Population and Community Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Groups of interacting organisms such as populations and communities.	True	False	https://github.com/PopulationAndCommunityOntology/pco	https://github.com/PopulationAndCommunityOntology/pco																
B2AI_STANDARD:536	B2AI_STANDARD:OntologyOrVocabulary	PROCO	Process Chemistry Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:3]	Process chemistry, the chemical field concerned with scaling up laboratory syntheses to commercially viable processes.	True	False	https://github.com/proco-ontology/PROCO	https://github.com/proco-ontology/PROCO																
B2AI_STANDARD:537	B2AI_STANDARD:OntologyOrVocabulary	PSI-MOD	Protein modification	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:26]	PSI-MOD is an ontology consisting of terms that describe protein chemical modifications	True	False	https://www.psidev.info/groups/protein-modifications	https://github.com/HUPO-PSI/psi-mod-CV										doi:10.1038/nbt0808-864				[B2AI_ORG:41]		
B2AI_STANDARD:538	B2AI_STANDARD:OntologyOrVocabulary	PR	PRotein Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:26]	The Protein Ontology (PR) is a comprehensive formal ontology that provides standardized terminology and semantic relationships for describing protein-related entities across the complete spectrum of protein science. Developed by the PRotein Ontology consortium, PR serves as a unifying framework that integrates protein sequence, structure, and functional information into a coherent knowledge representation system. The ontology encompasses multiple levels of protein organization including protein families and complexes, individual protein molecules, protein domains and regions, post-translational modifications, and protein isoforms generated through alternative splicing or processing. PR maintains extensive cross-references to major protein databases including UniProt, NCBI, and Ensembl, enabling seamless integration with existing protein annotation resources. The ontology supports advanced protein function annotation by providing precise vocabulary for describing enzymatic activities, binding sites, regulatory mechanisms, and cellular localization patterns. PR is essential for proteomics data standardization, comparative protein analysis, functional genomics research, and systems biology applications where consistent protein terminology facilitates data integration, automated reasoning, and knowledge discovery across diverse experimental platforms.	True	False	https://github.com/PROconsortium/PRoteinOntology/	https://github.com/PROconsortium/PRoteinOntology/																
B2AI_STANDARD:539	B2AI_STANDARD:OntologyOrVocabulary	PCL	Provisional Cell Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:2]	Cell types that are provisionally defined by experimental techniques such as single cell or single nucleus transcriptomics.	True	False	https://github.com/obophenotype/provisional_cell_ontology	https://github.com/obophenotype/provisional_cell_ontology																
B2AI_STANDARD:540	B2AI_STANDARD:OntologyOrVocabulary	RBO	Radiation Biology Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:1|B2AI_TOPIC:11]	Effects of radiation on biota in terrestrial and space environments.	True	False	https://github.com/Radiobiology-Informatics-Consortium/RBO	https://github.com/Radiobiology-Informatics-Consortium/RBO																
B2AI_STANDARD:541	B2AI_STANDARD:OntologyOrVocabulary	RadLex	RadLex radiology lexicon	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4]	A comprehensive set of radiology terms for use in radiology reporting, decision support, data mining, data registries, education and research.	True	True	https://www.rsna.org/practice-tools/data-tools-and-standards/radlex-radiology-lexicon															[B2AI_ORG:85]		
B2AI_STANDARD:542	B2AI_STANDARD:OntologyOrVocabulary	RS	Rat Strain Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Rat Strain Ontology (RS) is a structured vocabulary maintained by the Rat Genome Database (RGD) at the Medical College of Wisconsin, providing standardized nomenclature and hierarchical classification for laboratory rat strains, wild-derived strains, mutant lines, consomic/congenic strains, and transgenic/knockout rat models used in biomedical research, with integration into the OBO Foundry ecosystem supporting cross-species comparative genomics and phenotype studies. RS catalogs 5,000+ rat strains spanning inbred strains with defined genetic backgrounds (Wistar, Sprague-Dawley, Fischer 344, Brown Norway), outbred stocks with genetic heterogeneity, recombinant inbred lines for QTL mapping, consomic strains where complete chromosomes are substituted between strains, and gene-edited models (CRISPR knockouts, transgenic insertions) targeting specific disease mechanisms. The ontology structures strain relationships through \"derived from\" properties linking parent-progeny strains, \"has genetic background\" properties specifying founding strains for congenic lines, and \"model of\" properties connecting strains to disease phenotypes (hypertension, diabetes, cancer susceptibility), enabling navigation of complex breeding schemes and genetic derivations. RS terms include strain-specific metadata on phenotypic characteristics (coat color, obesity, behavioral traits), genetic markers (microsatellites, SNPs), tissue/cell sources, husbandry requirements, and availability from repositories (Rat Resource and Research Center, Charles River), facilitating experimental planning and reproducibility. The ontology integrates with RGD's comprehensive annotations linking strains to quantitative trait loci (QTL), genes, pathways, diseases, and phenotypes (Mammalian Phenotype Ontology terms), supporting genome-wide association studies (GWAS), eQTL mapping, and systems genetics analyses that connect genetic variation to physiological endpoints. RS enables translational research by mapping rat models to human disease orthologs, facilitating target validation for drug discovery where rat pharmacology and toxicology data inform human clinical trial design, particularly for cardiovascular disease, neurodegeneration, and metabolic disorders where rat models recapitulate human pathophysiology better than mouse models. In AI/ML applications for precision medicine, RS provides structured metadata for training genomic prediction models that associate strain genotypes with phenotypic outcomes, supports knowledge graph construction linking rat genetic data to human disease mechanisms for cross-species inference, and enables meta-analyses aggregating phenotypic data across studies by standardizing strain identifiers, ultimately accelerating translational discoveries by leveraging the rat's role as a premier mammalian model for human disease research.	True	False	http://rgd.mcw.edu/rgdweb/search/strains.html	https://github.com/rat-genome-database/RS-Rat-Strain-Ontology										doi:10.1186/2041-1480-4-36						
B2AI_STANDARD:543	B2AI_STANDARD:OntologyOrVocabulary	RO	Relation Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Relationship types shared across multiple ontologies.	True	False	https://oborel.github.io/	https://github.com/oborel/obo-relations																
B2AI_STANDARD:544	B2AI_STANDARD:OntologyOrVocabulary	RxNorm	RxNorm	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[codesystem|standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:8]	Medication terminology. Provided through UMLS.	True	True	https://www.nlm.nih.gov/research/umls/rxnorm/index.html															[B2AI_ORG:74]		
B2AI_STANDARD:545	B2AI_STANDARD:OntologyOrVocabulary	SEPIO	Scientific Evidence and Provenance Information Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	The Scientific Evidence and Provenance Information Ontology (SEPIO) provides a structured framework for representing scientific evidence and provenance information supporting knowledge claims. It supports rich, computable representations of the evidence and provenance behind scientific assertions, particularly for genetic variants and their clinical interpretations.	True	False	https://github.com/monarch-initiative/SEPIO-ontology	https://sepio-framework.github.io/sepio-linkml/		[B2AI_ORG:58]								doi:10.5281/zenodo.5214269	[B2AI_STANDARD:256]					
B2AI_STANDARD:546	B2AI_STANDARD:OntologyOrVocabulary	SSN	Semantic Sensor Network Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	An ontology for describing sensors and their observations, the involved procedures, the studied features of interest, the samples used to do so, and the observed properties, as well as actuators.	True	False	https://www.w3.org/TR/vocab-ssn/	https://github.com/w3c/sdw														[B2AI_ORG:99]		
B2AI_STANDARD:547	B2AI_STANDARD:OntologyOrVocabulary	SO	Sequence types and features ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	A structured controlled vocabulary for sequence annotation, for the exchange of annotation data and for the description of sequence objects.	True	False	http://www.sequenceontology.org/	https://github.com/The-Sequence-Ontology/SO-Ontologies										doi:10.1016/j.jbi.2010.03.002						
B2AI_STANDARD:548	B2AI_STANDARD:OntologyOrVocabulary	SCDO	Sickle Cell Disease Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	The Sickle Cell Disease Ontology (SCDO) provides comprehensive standardized terminology for describing all aspects of sickle cell disease (SCD), a group of inherited hemoglobin disorders affecting millions globally, particularly populations of African, Mediterranean, Middle Eastern, and Indian ancestry. Developed by the H3ABioNet consortium and aligned with OBO Foundry principles, SCDO encompasses genetic variants (HbS, HbC, HbE, beta-thalassemia mutations), disease phenotypes (sickle cell anemia HbSS, HbSC disease, HbS-beta thalassemia, sickle cell trait), clinical manifestations (vaso-occlusive crises, acute chest syndrome, stroke, priapism, splenic sequestration, chronic organ damage), laboratory findings (hemoglobin electrophoresis patterns, reticulocyte counts, bilirubin levels, fetal hemoglobin percentages), complications (pulmonary hypertension, renal dysfunction, avascular necrosis, leg ulcers, retinopathy), treatment modalities (hydroxyurea, blood transfusions, hematopoietic stem cell transplantation, gene therapy, pain management), and patient outcomes (quality of life measures, hospitalization rates, mortality). The ontology integrates genetic, clinical, laboratory, and treatment concepts to support comprehensive SCD patient data management and research. SCDO enables standardized phenotyping for genotype-phenotype correlation studies identifying disease modifiers (fetal hemoglobin levels, alpha-thalassemia co-inheritance, genetic polymorphisms affecting disease severity), facilitates clinical trial recruitment through precise inclusion/exclusion criteria specification, and supports electronic health record integration for automated SCD surveillance and quality improvement initiatives. Applications include natural history studies characterizing disease progression patterns, pharmacogenomics research examining hydroxyurea response variability, health disparities research documenting access to disease-modifying therapies, and global SCD registries enabling cross-population comparisons. SCDO facilitates data harmonization across international SCD cohorts, enabling meta-analyses and collaborative research essential for rare disease studies where no single center has sufficient patient numbers.	True	False	https://scdontology.h3abionet.org/	https://github.com/scdodev/scdo-ontology																
B2AI_STANDARD:549	B2AI_STANDARD:OntologyOrVocabulary	SWO	Software ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Software tools, their types, tasks, versions, provenance and associated data.	True	False	https://github.com/allysonlister/swo	https://github.com/allysonlister/swo										doi:10.1186/2041-1480-5-25						
B2AI_STANDARD:550	B2AI_STANDARD:OntologyOrVocabulary	CFDs	Standard Current Procedural Terminology Consumer Friendly Descriptors	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Translate each code descriptor from the official CPT code set into language that is easily understood by the average patient and/or his or her caregiver. The objective is to simplify the highly technical CPT code descriptors into something more patient-focused and patient-friendly.	False	True	https://commerce.ama-assn.org/catalog/media/Consumer-and-Clinician-Descriptors-in-CPT-Data-Files.pdf			[B2AI_ORG:3]														
B2AI_STANDARD:551	B2AI_STANDARD:OntologyOrVocabulary	STATO	Statistics Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	The Statistics Ontology (STATO) provides comprehensive standardized vocabulary for statistical methods, experimental design concepts, hypothesis testing procedures, and statistical measures used across life sciences, biomedical research, and data-intensive scientific domains, developed by the ISA-tools community to support reproducible research and transparent reporting of statistical analyses. STATO encompasses five major categories: statistical tests (parametric tests including t-tests, ANOVA, ANCOVA, linear regression, mixed models; non-parametric tests including Mann-Whitney U, Kruskal-Wallis, Wilcoxon signed-rank, Spearman correlation; and specialized methods like survival analysis, multivariate analysis, time series analysis), probability distributions (normal, binomial, Poisson, exponential, chi-square, t-distribution, F-distribution) essential for understanding test assumptions, descriptive statistics (measures of central tendency including mean, median, mode; measures of dispersion including standard deviation, variance, interquartile range; and measures of shape including skewness, kurtosis), data types and variables (categorical/nominal, ordinal, continuous, discrete, dependent/independent variables, covariates, confounding variables), and experimental design concepts (randomization, blocking, replication, control groups, factorial designs, crossover designs, longitudinal studies). STATO provides formal OWL definitions enabling automated reasoning about test conditions of application, linking test selection to data characteristics (e.g., use Mann-Whitney U when comparing two independent groups with non-normal distributions), and capturing assumptions (normality, homoscedasticity, independence) that must be verified before test application. Each statistical method includes textual definitions for human understanding, formal logical definitions for machine reasoning, associated R code snippets via 'R-command' annotations enabling direct implementation, and documentation of appropriate use cases and interpretation guidelines. STATO integrates with BFO (Basic Formal Ontology) as upper-level ontology and OBI (Ontology for Biomedical Investigations) for process definitions, ensuring interoperability across biomedical ontologies. Applications include annotation of analysis methods in ISA-Tab metadata files for omics studies, standardized reporting of statistical procedures in publications to meet journal guidelines (CONSORT, STROBE, ARRIVE), automated validation of statistical analysis workflows in computational notebooks, education and training through formal definitions of statistical concepts with R implementation examples, and text mining of scientific literature to extract statistical method usage patterns. STATO supports reproducibility by precisely specifying analysis procedures, capturing multiple testing correction methods (Bonferroni, Benjamini-Hochberg FDR, permutation tests), effect size measures (Cohen's d, odds ratios, hazard ratios), and confidence interval calculations. The ontology facilitates peer review by enabling reviewers to verify appropriate test selection and assists researchers in choosing correct statistical methods based on study design and data characteristics.	True	False	http://stato-ontology.org/	https://github.com/ISA-tools/stato														[B2AI_ORG:47]		
B2AI_STANDARD:552	B2AI_STANDARD:OntologyOrVocabulary	SYMP	Symptom Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Disease symptoms, with symptoms encompasing perceived changes in function, sensations or appearance reported by a patient.	True	False	http://symptomontologywiki.igs.umaryland.edu/mediawiki/index.php/Main_Page	https://github.com/DiseaseOntology/SymptomOntology										doi:10.1093/nar/gkab1063						
B2AI_STANDARD:553	B2AI_STANDARD:OntologyOrVocabulary	SNOMED CT	Systematized Nomenclature of Medicine - Clinical Terms	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[codesystem|standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:9]	Standard for electronic exchange of clinical health information. Provided through UMLS.	True	True	https://www.nlm.nih.gov/healthit/snomedct/index.html												[B2AI_STANDARD:769]			[B2AI_ORG:74]		
B2AI_STANDARD:554	B2AI_STANDARD:OntologyOrVocabulary	SNMI	Systematized Nomenclature of Medicine, International Version	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Systematized Nomenclature of Medicine International (SNMI) is a historical medical terminology and classification system that preceded SNOMED CT. Developed in the 1960s-1970s by the College of American Pathologists, SNMI represented one of the earliest attempts to create a comprehensive, multi-axial medical nomenclature covering topography (anatomy), morphology (structural changes), etiology (causes), and function (physiological processes). SNMI employed a systematic coding structure where concepts were represented by alphanumeric codes and could be combined using post-coordination to express complex clinical findings (e.g., combining topography codes with morphology codes to describe disease locations and characteristics). The terminology was designed primarily for pathology and clinical documentation, providing structured vocabulary for diagnoses, procedures, and laboratory findings. SNMI evolved through several versions including SNOMED (Systematized Nomenclature of Medicine), SNOMED II, and SNOMED III before being integrated into SNOMED CT (Clinical Terms) in 2002 through merger with UK's Clinical Terms Version 3 (Read Codes). While SNMI itself is now deprecated and replaced by SNOMED CT for current clinical use, it is historically significant as a foundational medical terminology that pioneered multi-axial compositional approaches to medical concept representation. Legacy SNMI data may still exist in historical medical records and research databases, requiring mapping to modern terminologies for interoperability. Understanding SNMI's structure provides context for SNOMED CT's architecture and the evolution of standardized medical terminologies used in electronic health records, clinical research, and healthcare data analytics.	True	False	https://bioportal.bioontology.org/ontologies/SNMI															[B2AI_ORG:74]		
B2AI_STANDARD:555	B2AI_STANDARD:OntologyOrVocabulary	SBO	Systems Biology Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:1]	Terms commonly used in Systems Biology and computational modeling.	True	False	https://github.com/EBI-BioModels/SBO	https://github.com/EBI-BioModels/SBO														[B2AI_ORG:29]		
B2AI_STANDARD:556	B2AI_STANDARD:OntologyOrVocabulary	TAXRANK	Taxonomic rank vocabulary	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:1]	A vocabulary of taxonomic ranks (species, family, phylum, etc).	True	False	https://github.com/phenoscape/taxrank	https://github.com/phenoscape/taxrank										doi:10.1186/2041-1480-4-34						
B2AI_STANDARD:557	B2AI_STANDARD:OntologyOrVocabulary	T4FS	terms4FAIRskills	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	A terminology for the skills necessary to make data FAIR and to keep it FAIR.	True	False	https://obofoundry.org/ontology/t4fs.html	https://github.com/terms4fairskills/FAIRterminology										doi:10.5281/zenodo.4772741						
B2AI_STANDARD:558	B2AI_STANDARD:OntologyOrVocabulary	DRON	The Drug Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:8]	An ontology to support comparative effectiveness researchers studying claims data.	True	False	https://github.com/ufbmi/dron	https://github.com/ufbmi/dron										doi:10.1186/s13326-017-0121-5				[B2AI_ORG:96]		
B2AI_STANDARD:559	B2AI_STANDARD:OntologyOrVocabulary	OBOE	The Extensible Observation Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	The Extensible Observation Ontology (OBOE) is a formal ontology for capturing the semantics of scientific observation and measurement.	True	False	https://bioportal.bioontology.org/ontologies/OBOE	https://github.com/NCEAS/oboe/														[B2AI_ORG:62]		
B2AI_STANDARD:560	B2AI_STANDARD:OntologyOrVocabulary	OGG	The Ontology of Genes and Genomes	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:12|B2AI_TOPIC:13]	A formal ontology of genes and genomes of biological organisms.	True	False	https://bitbucket.org/hegroup/ogg/src/master/	https://bitbucket.org/hegroup/ogg/src/master/																
B2AI_STANDARD:561	B2AI_STANDARD:OntologyOrVocabulary	PDRO	The Prescription of Drugs Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:8]	An ontology to describe entities related to prescription of drugs	True	False	https://github.com/OpenLHS/PDRO	https://github.com/OpenLHS/PDRO										doi:10.3390/ijerph182212025						
B2AI_STANDARD:562	B2AI_STANDARD:OntologyOrVocabulary	TXPO	Toxic Process Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Terms involving toxicity courses and processes.	True	False	https://toxpilot.nibiohn.go.jp/	https://github.com/txpo-ontology/TXPO/																
B2AI_STANDARD:563	B2AI_STANDARD:OntologyOrVocabulary	UBERON	Uberon	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	UBERON is an OBO Foundry ontology providing an integrated cross-species anatomy ontology covering anatomical structures in animals, with a focus on multi-species interoperability. The ontology enables semantic annotation of anatomical entities across diverse species, supporting comparative anatomy research, phenotype studies, and integration with specialized anatomies. UBERON is tightly integrated with the Gene Ontology (GO) and the Cell Ontology (CL), using formal ontology design patterns to represent anatomical locations and relationships. It follows FAIR principles and is released in standard formats (OWL, OBO, JSON obographs) with resolvable version IRIs. UBERON is integrated into standard tools including Ubergraph for logical queries (e.g., finding cell types by location), the Ontology Access Kit (OAK), and major browsers (OLS, Ontobee, BioPortal). The ontology supports AI/ML applications by providing standardized anatomical annotations for training data across species and enabling cross-species knowledge transfer in biomedical machine learning models.	True	False	https://obophenotype.github.io/uberon/	https://github.com/obophenotype/uberon										doi:10.1186/gb-2012-13-1-r5						
B2AI_STANDARD:564	B2AI_STANDARD:OntologyOrVocabulary	Metathesaurus	UMLS Metathesaurus	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Biomedical terminology and hierarchical relationships between concepts.	True	True	https://www.nlm.nih.gov/research/umls/knowledge_sources/metathesaurus/index.html		True													[B2AI_ORG:74|B2AI_ORG:115]		
B2AI_STANDARD:565	B2AI_STANDARD:OntologyOrVocabulary	UPHENO	Unified phenotype ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:25]	Integrates multiple phenotype ontologies into a unified cross-species phenotype ontology.	True	False	https://github.com/obophenotype/upheno	https://github.com/obophenotype/upheno																
B2AI_STANDARD:566	B2AI_STANDARD:OntologyOrVocabulary	UO	Units of measure ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	The Units Ontology - a tool for integrating units of measurement in science	True	False	https://obofoundry.org/ontology/uo.html											doi:10.1093/database/bas033						
B2AI_STANDARD:567	B2AI_STANDARD:OntologyOrVocabulary	UO	Units of measurement ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Metrical units for use in conjunction with PATO.	True	False	https://github.com/bio-ontology-research-group/unit-ontology	https://github.com/bio-ontology-research-group/unit-ontology																
B2AI_STANDARD:568	B2AI_STANDARD:OntologyOrVocabulary	UMDNS	Universal Medical Device Nomenclature System	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4]	Universal Medical Device Nomenclature System (UMDNS) is a nomenclature that has been officially adopted by many nations. UMDNS facilitates identifying, processing, filing, storing, retrieving, transferring, and communicating data about medical devices. The nomenclature is used in applications ranging from hospital inventory and work-order controls to national agency medical device regulatory systems.	False	True	https://www.ecri.org/solutions/umdns															[B2AI_ORG:27]		
B2AI_STANDARD:569	B2AI_STANDARD:OntologyOrVocabulary	VO	Vaccine Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	The Vaccine Ontology (VO) is an OBO Foundry community-based biomedical ontology that systematically represents vaccine components, vaccine types, vaccination procedures, vaccine-induced immune responses, and vaccine-preventable infectious diseases in a standardized machine-readable format to support vaccine research, immunization program management, and vaccinomics data integration. Developed through international collaboration coordinated by the University of Michigan Medical School, VO encompasses comprehensive coverage of licensed human and veterinary vaccines including live-attenuated vaccines, inactivated vaccines, subunit vaccines, toxoid vaccines, mRNA vaccines, viral vector vaccines, and conjugate vaccines, representing their antigenic components, adjuvants, preservatives, manufacturing processes, and delivery routes. The ontology integrates with multiple biomedical ontologies including the Infectious Disease Ontology (IDO) for pathogen-host interactions, the Ontology for Biomedical Investigations (OBI) for vaccination protocols, the Protein Ontology (PRO) for vaccine antigens, and SNOMED CT and ICD codes for clinical documentation, enabling semantic interoperability across vaccine databases and immunization information systems. VO formally represents immunization schedules (primary series, booster doses, catch-up schedules), adverse events following immunization (AEFI) including local reactions and systemic effects, contraindications and precautions for specific populations, vaccine efficacy and effectiveness measures, and herd immunity thresholds. Applications include standardized annotation of vaccine clinical trials data, integration of vaccine safety surveillance systems (VAERS, Vaccine Safety Datalink), comparative vaccine effectiveness studies across populations and time periods, semantic queries of vaccine literature through PubMed and clinical databases, machine learning models predicting vaccine immunogenicity or reactogenicity from vaccine composition, and global immunization data harmonization supporting WHO vaccination coverage monitoring and pandemic preparedness. VO follows OBO Foundry principles with open-source development, versioned releases, and persistent URIs, making it essential infrastructure for computational vaccinology and evidence-based immunization policy.	True	False	https://github.com/vaccineontology/VO	https://github.com/vaccineontology/VO										doi:10.1186/2041-1480-3-17						
B2AI_STANDARD:570	B2AI_STANDARD:OntologyOrVocabulary	VBO	Vertebrate Breed Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Vertebrate Breed Ontology (VBO) is a comprehensive ontology providing standardized nomenclature and hierarchical classification for vertebrate animal breeds across 38 species including cattle, sheep, goats, pigs, horses, chickens, dogs, and cats, developed collaboratively by the Monarch Initiative, Online Mendelian Inheritance in Animals (OMIA), and FAO's Domestic Animal Diversity Information System (DAD-IS). VBO structures breed information using OBO Foundry principles, capturing breed names, synonyms (alternate spellings, historical names, local language variants), geographical origins, breed characteristics (size, coat color, production traits), and relationships to parent breeds or breed groups. The ontology integrates with DAD-IS's 15,000+ national breed populations representing 8,800+ breeds maintained by 182 countries' National Coordinators, ensuring global coverage and continuous updates reflecting new breed registrations and naming conventions. VBO supports cross-species breed queries by organizing breeds under species-specific classes (bovine breeds, equine breeds) while maintaining inter-breed relationships such as \"derived from\" for composite breeds and \"related to\" for breeds sharing genetic heritage. The ontology enables precise phenotype-genotype associations in animal genetics research by providing stable breed identifiers (VBO IDs) that link to genomic datasets, genetic variant databases, and phenotype repositories, facilitating genome-wide association studies (GWAS) and quantitative trait locus (QTL) mapping in livestock. VBO integrates with other biomedical ontologies including the Livestock Breed Ontology (LBO), Mammalian Phenotype Ontology (MP), and UBERON anatomy ontology to support complex queries across breed characteristics, anatomical features, and disease susceptibilities. For agricultural genomics, VBO enables tracking of breed-specific genetic diversity, conservation status of rare breeds (endangered, critical, vulnerable), and lineage documentation for breeding programs aimed at preserving genetic resources. In AI/ML applications, VBO provides structured metadata for training computer vision models on breed classification, natural language processing systems for extracting breed information from veterinary records, and knowledge graphs connecting breed genetics to production traits (milk yield, growth rate, disease resistance) and animal welfare indicators, supporting precision livestock farming and genomic selection programs.	True	False	https://github.com/monarch-initiative/vertebrate-breed-ontology	https://github.com/monarch-initiative/vertebrate-breed-ontology														[B2AI_ORG:58]		
B2AI_STANDARD:571	B2AI_STANDARD:OntologyOrVocabulary	VTO	Vertebrate Taxonomy Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Extinct and extant vertebrate taxa.	True	False	https://github.com/phenoscape/vertebrate-taxonomy-ontology	https://github.com/phenoscape/vertebrate-taxonomy-ontology																
B2AI_STANDARD:572	B2AI_STANDARD:OntologyOrVocabulary	VT	Vertebrate trait ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:25]	The Vertebrate Trait Ontology (VT) provides comprehensive standardized terminology for describing measurable or observable phenotypic characteristics across vertebrate species, maintained by the AnimalGenome organization to support agricultural genomics, comparative biology, quantitative genetics, and translational research linking animal models to human health. VT encompasses broad trait categories including morphological traits (body size, organ dimensions, skeletal measurements, integument characteristics), physiological traits (metabolic parameters, cardiovascular function, respiratory capacity, immune response, reproductive performance), behavioral traits (temperament, learning ability, social interactions, feeding behavior), production traits critical for livestock industries (growth rate, feed efficiency, milk yield, egg production, meat quality, wool production), disease resistance traits (pathogen susceptibility, vaccine response, parasite load), and life history traits (longevity, fertility, developmental timing). Each trait is formally defined with measurement methodologies, units, biological context, and relationships to anatomical structures (via Uberon) and biological processes (via Gene Ontology). VT serves as the primary phenotype ontology for agricultural animal genomics databases including AnimalQTLdb (quantitative trait loci database for cattle, pig, chicken, sheep, horse, rainbow trout), supporting annotation of QTL mapping studies that identify genomic regions influencing economically important traits. The ontology enables cross-species phenotype comparisons essential for translational research, allowing researchers to leverage livestock as biomedical models (e.g., pig cardiovascular traits for human heart disease research, sheep bone traits for osteoporosis studies). Applications span livestock breeding programs through genomic selection where VT standardizes trait definitions for estimated breeding values (EBVs), wildlife conservation genetics for monitoring population health indicators, aquaculture for trait improvement in fish and shellfish, and comparative physiology studies examining adaptive evolution of traits across vertebrate lineages. VT facilitates GWAS (genome-wide association studies) meta-analyses by harmonizing trait definitions across studies, enables phenotype-driven queries in genomics databases (\"find all QTL affecting milk fat percentage in dairy cattle\"), and supports machine learning models predicting complex traits from genotype data by providing structured phenotype representations. Integration with other ontologies including Mammalian Phenotype Ontology (MP), Human Phenotype Ontology (HPO), and Clinical Measurement Ontology (CMO) enables bidirectional translation between agricultural animal research and human biomedicine. VT is distributed under CC-BY-NC 4.0 license through http://purl.obolibrary.org/obo/vt.owl with releases synchronized to OBO Foundry and AgroPortal.	True	False	https://github.com/AnimalGenome/vertebrate-trait-ontology	https://github.com/AnimalGenome/vertebrate-trait-ontology																
B2AI_STANDARD:573	B2AI_STANDARD:OntologyOrVocabulary	EUPATH	VEuPathDB ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:5]	Support ontology for the Eukaryotic Pathogen, Host & Vector Genomics Resource (VEuPathDB; https://veupathdb.org).	True	False	https://github.com/VEuPathDB-ontology/VEuPathDB-ontology	https://github.com/VEuPathDB-ontology/VEuPathDB-ontology										doi:10.5281/zenodo.6685957						
B2AI_STANDARD:574	B2AI_STANDARD:OntologyOrVocabulary	XPO	Xenopus Phenotype Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:25]	The Xenopus Phenotype Ontology (XPO) is a formal ontology for representing anatomical, cellular, and gene function phenotypes observed throughout the development of the African frogs Xenopus laevis and Xenopus tropicalis. XPO enables standardized annotation of phenotypes in Xenopus research, supporting integration with genotype, phenotype, and disease data across species. The ontology is used by Xenbase and other resources to facilitate cross-species comparisons, data sharing, and computational analysis of developmental and functional phenotypes in model organism research.	True	False	https://github.com/obophenotype/xenopus-phenotype-ontology	https://github.com/obophenotype/xenopus-phenotype-ontology										doi:10.1186/s12859-022-04636-8						
B2AI_STANDARD:575	B2AI_STANDARD:OntologyOrVocabulary	ZP	Zebrafish Phenotype Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[obofoundry]	[B2AI_TOPIC:25]	All phenotypes of the Zebrafish model organism.	True	False	https://github.com/obophenotype/zebrafish-phenotype-ontology	https://github.com/obophenotype/zebrafish-phenotype-ontology																
B2AI_STANDARD:576	B2AI_STANDARD:DataStandardOrTool	PHVS_Race_HL7_2x	CDC Race and Ethnicity Code Set	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:6]	A code set for use in coding race and ethnicity data.	True	False	https://www.cdc.gov/phin/resources/vocabulary/documents/cdc-race--ethnicity-background-and-purpose.pdf	https://phinvads.cdc.gov/vads/ViewValueSet.action?id=B246B692-6DF8-E111-B875-001A4BE7FA90														[B2AI_ORG:12|B2AI_ORG:40]		
B2AI_STANDARD:577	B2AI_STANDARD:DataStandardOrTool	RFC 5646	IETF Request for Comment 5646 Tags for Identifying Languages	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5|B2AI_TOPIC:6]	Best practices for the structure, content, construction, and semantics of language tags for use in cases where it is desirable to indicate the language used in an information object.	True	False	https://www.rfc-editor.org/rfc/rfc5646															[B2AI_ORG:45]		
B2AI_STANDARD:578	B2AI_STANDARD:DataStandardOrTool	ITUT E.123	International Telecommunication Union E.123 Notation for national and international telephone numbers, e-mail addresses and web addresses	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:31]	Standard notation for printing telephone numbers, E-mail addresses and Web addresses.	True	False	https://www.itu.int/rec/T-REC-E.123-200102-I/en															[B2AI_ORG:50]		
B2AI_STANDARD:579	B2AI_STANDARD:DataStandardOrTool	ITUT E.164	International Telecommunication Union E.164 The international public telecommunication numbering plan	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:31]	Number structure and functionality for the five categories of numbers used for international public telecommunication - geographic areas, global services, Networks, groups of countries (GoC) and resources for trials.	True	False	https://www.itu.int/rec/T-REC-E.164-201011-I/en															[B2AI_ORG:50]		
B2AI_STANDARD:704	B2AI_STANDARD:Registry	bio.tools	bio.tools	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[dataregistry|softwareregistry]	[B2AI_TOPIC:1]	Registry of software tools, databases and services for bioinformatics and the life sciences.	True	False	https://bio.tools/	https://github.com/bio-tools/biotoolsregistry/																
B2AI_STANDARD:705	B2AI_STANDARD:Registry	Bioconductor	Bioconductor	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[softwareregistry]	[B2AI_TOPIC:5]	The mission of the Bioconductor project is to develop, support, and disseminate free open source software that facilitates rigorous and reproducible analysis of data from current and emerging biological assays. We are dedicated to building a diverse, collaborative, and welcoming community of developers and data scientists.	True	False	https://www.bioconductor.org/	https://github.com/Bioconductor/BiocManager													[B2AI_SUBSTRATE:40]			
B2AI_STANDARD:706	B2AI_STANDARD:Registry	BioPortal	BioPortal	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[ontologyregistry]	[B2AI_TOPIC:1]	BioPortal is a comprehensive, open-access ontology repository and collaborative platform developed by the National Center for Biomedical Ontology (NCBO) at Stanford, hosting over 1,000 biomedical ontologies, terminologies, and vocabularies. The platform provides centralized access to diverse biomedical knowledge organization systems including OBO Foundry ontologies (GO, Uberon, CHEBI, HPO), medical terminologies (SNOMED CT, ICD, RxNorm, LOINC), domain-specific vocabularies, and community-developed ontologies. BioPortal offers rich functionality including: ontology browsing with hierarchical visualization and concept lookup; powerful search across all ontologies with autocomplete and recommendations; mapping services for finding correspondences between ontologies; annotator services for identifying ontology concepts in free text; versioning and change tracking for ontology evolution; widgets and web services (REST APIs) for programmatic access; and community features for commenting, discussions, and ontology reviews. The platform serves as infrastructure for ontology developers (submission, hosting, versioning), data curators (annotation, mapping, validation), and application developers (APIs, widgets, SPARQL endpoints). BioPortal enables semantic annotation of biomedical data, cross-resource data integration through ontology mappings, and standardized vocabularies for clinical research, genomics, drug discovery, and translational medicine. In AI/ML applications, BioPortal supports ontology-based feature engineering for predictive models, semantic search for literature and dataset discovery, text mining and NLP by providing concept recognizers, knowledge graph construction by linking data to standardized ontologies, and explainable AI through ontological reasoning and structured domain knowledge integration.	True	False	https://bioportal.bioontology.org/	https://github.com/ncbo																
B2AI_STANDARD:707	B2AI_STANDARD:Registry	Bioregistry	Bioregistry	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standardsregistry]	[B2AI_TOPIC:5]	The Bioregistry is an open-source, community-curated registry and meta-registry that catalogs prefixes, identifier formats, and metadata for biomedical ontologies, databases, and controlled vocabularies. It integrates and harmonizes information from multiple registries (e.g., OBO Foundry, Identifiers.org, OLS), providing a unified resource for resolving compact URIs (CURIEs) and supporting semantic interoperability. The Bioregistry also functions as a resolver, mapping CURIEs to web resources, and is governed by transparent contribution and review processes. It is widely used for data integration, annotation, and knowledge graph construction in the life sciences.	True	False	https://bioregistry.io/	https://github.com/biopragmatics/bioregistry										doi:10.1101/2022.07.08.499378						
B2AI_STANDARD:708	B2AI_STANDARD:Registry	Bridge2AI registry	Bridge to Artificial Intelligence Registry	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standardsregistry]	[B2AI_TOPIC:5]	Standards, tools, reference implementations, and related resources.	True	False	https://github.com/bridge2ai/b2ai-standards-registry	https://github.com/bridge2ai/b2ai-standards-registry																
B2AI_STANDARD:709	B2AI_STANDARD:Registry	CDISC SHARE	CDISC Shared Health And Research Electronic library	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	CDISC launched the CDISC Shared Health And Research Electronic library (SHARE) to provide the standards metadata in machine-readable formats to facilitate the automated management and implementation of the standards.	True	True	https://www.cdisc.org/faq/share/what-cdisc-share											PUBMED:29888049				[B2AI_ORG:15]		
B2AI_STANDARD:710	B2AI_STANDARD:Registry	Database Commons	Database Commons	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A catalog of worldwide biological databases maintained by the China National Center for Bioinformation,	True	False	https://ngdc.cncb.ac.cn/databasecommons/											doi:10.1016/j.gpb.2022.12.004						
B2AI_STANDARD:711	B2AI_STANDARD:Registry	Dockstore	Dockstore	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[softwareregistry]	[B2AI_TOPIC:5]	Dockstore is a comprehensive, open-source platform developed by the Ontario Institute for Cancer Research (OICR) and other collaborators that serves as a central registry for sharing, discovering, and executing containerized bioinformatics tools and computational workflows. Built on modern container technologies including Docker and Singularity, Dockstore enables researchers to package their analytical pipelines with all dependencies and configurations, ensuring reproducibility across different computing environments. The platform supports multiple workflow languages including Common Workflow Language (CWL), Workflow Description Language (WDL), and Nextflow, providing flexibility for diverse computational approaches in genomics, proteomics, and systems biology. Dockstore integrates with popular code repositories like GitHub, GitLab, and Bitbucket, enabling version-controlled development and automated testing of computational tools. The platform facilitates scientific collaboration by allowing researchers to discover validated, ready-to-use analytical workflows, reducing duplication of effort and accelerating research discovery. With built-in execution engines and cloud integration capabilities, Dockstore supports scalable workflow execution on local clusters, cloud platforms, and high-performance computing systems, making advanced bioinformatics accessible to researchers regardless of their computational expertise.	True	False	https://dockstore.org/	https://github.com/dockstore/dockstore														[B2AI_ORG:34]		
B2AI_STANDARD:712	B2AI_STANDARD:Registry	Fairsharing	Fairsharing	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standardsregistry]	[B2AI_TOPIC:5]	FAIRsharing (fairsharing.org) is a comprehensive, curated registry of research data standards, databases, repositories, and policies that promotes Findable, Accessible, Interoperable, and Reusable (FAIR) data practices across life sciences, environmental sciences, social sciences, and humanities domains. Established by the UK BBSRC, ELIXIR, and Oxford e-Research Centre, FAIRsharing catalogs 2,000+ data standards (ontologies, terminologies, formats, models), 1,800+ databases and repositories (discipline-specific archives, institutional repositories, generalist repositories), and 800+ data policies from funders, journals, and institutions, providing a one-stop resource for discovering community-endorsed resources that facilitate data reuse and reproducibility. Each FAIRsharing record includes structured metadata describing the resource's scope, governance (community-driven, institutional, commercial), licenses, interoperability capabilities, and relationships to other standards/databases, with persistent identifiers (FAIRsharing DOIs) enabling stable citations in data management plans and publications. The registry employs a domain-tagging system covering biomedical sciences (genomics, proteomics, metabolomics), environmental sciences (climate, biodiversity, geosciences), social sciences, and interdisciplinary fields, with cross-links to related resources illustrating standard-database-policy connections within research ecosystems. FAIRsharing supports researchers in selecting appropriate standards for data annotation (MIAME for microarrays, STROBE for epidemiology) and repositories for data deposition (GEO for gene expression, PDB for protein structures), facilitating compliance with journal and funder mandates for open data sharing. The platform integrates with research infrastructure projects (ELIXIR, RDA, CODATA) and provides APIs for embedding FAIRsharing recommendations into data management planning tools, electronic lab notebooks, and repository submission interfaces. For data stewards, FAIRsharing enables discovery of domain-specific controlled vocabularies (ontologies, taxonomies) that enhance dataset interoperability, semantic search capabilities, and cross-study integration in meta-analyses. In AI/ML research, FAIRsharing guides selection of training data repositories with rich metadata, standardized formats that minimize preprocessing overhead, and permissive licenses enabling model training, while the registry's coverage of ML-specific resources (model registries, benchmark datasets, evaluation metrics) supports reproducible AI research aligned with FAIR principles for algorithms and models (FAIR4ML).	True	False	https://fairsharing.org/	https://github.com/FAIRsharing													[B2AI_SUBSTRATE:9]			
B2AI_STANDARD:713	B2AI_STANDARD:DataStandardOrTool	LinkML registry	LinkML schema registry	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standardsregistry]	[B2AI_TOPIC:5]	schemas and machine-actionable standards	True	False	https://linkml.io/linkml-registry/registry/	https://github.com/linkml/linkml-registry/																
B2AI_STANDARD:714	B2AI_STANDARD:Registry	NCI caDSR	National Cancer Institute Cancer Data Standards Repository	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standardsregistry]	[B2AI_TOPIC:4|B2AI_TOPIC:7]	Registry and repository for oncology research common data elements and forms.	True	False	https://datascience.cancer.gov/resources/metadata	https://cdebrowser.nci.nih.gov/cdebrowserClient/cdeBrowser.html#/search														[B2AI_ORG:71]		
B2AI_STANDARD:715	B2AI_STANDARD:Registry	NRDR	National Radiology Data Registry	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[dataregistry]	[B2AI_TOPIC:5]	The primary purpose of NRDR is to aid facilities with their quality improvement programs and efforts to improve patient care by comparing facility data to that of their region and the nation. A practice or facility may choose to participate in any or all registries as appropriate for their practice. When a facility joins more than one registry, the warehouse allows information to be shared across registries within the facility.	False	True	https://nrdr.acr.org/Portal/Nrdr/Main/page.aspx											doi:10.1016/j.jacr.2011.05.014						
B2AI_STANDARD:716	B2AI_STANDARD:Registry	OBO Foundry	Open Biological and Biomedical Ontology Foundry	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[ontologyregistry]	[B2AI_TOPIC:5]	The OBO Foundry is a collaborative community initiative for developing interoperable ontologies for the biological and biomedical sciences. It establishes and maintains a set of principles for ontology development to ensure quality, consistency, and interoperability across its library of domain ontologies. The OBO Foundry principles address key requirements including open availability, common syntax and semantics, clearly defined scope and content, use of well-documented collaborative procedures, orthogonality with other ontologies, provision of unique identifiers, and adherence to established naming conventions. The Foundry provides comprehensive community resources including the OBO tutorial, ontology browsers and tools, operations committees and working groups, and communication channels (mailing list, Slack workspace). The OBO Library registry provides standardized access to member ontologies in multiple formats (YAML, JSON-LD, RDF/Turtle) and includes domain-specific ontologies covering anatomy (UBERON), cell types (CL), diseases (MONDO), chemicals (ChEBI), genotypes (GENO), phenotypes (HPO), and many others. The OBO Foundry infrastructure includes consistent use of permanent URLs (PURLs), version control via GitHub, automated quality checks, and standardized release workflows. By ensuring semantic interoperability through shared upper-level ontologies and design patterns, OBO Foundry enables cross-domain data integration essential for AI/ML applications in biomedical knowledge graphs, automated reasoning, data harmonization, and multi-modal machine learning across diverse biological datasets.	True	False	https://obofoundry.org/											doi:10.1093/database/baab069				[B2AI_ORG:75]		
B2AI_STANDARD:717	B2AI_STANDARD:Registry	PwC	Papers With Code	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A free and open resource with Machine Learning papers, code, datasets, methods and evaluation tables.	True	False	https://paperswithcode.com/	https://github.com/paperswithcode																
B2AI_STANDARD:718	B2AI_STANDARD:DataStandardOrTool	r3data	Registry of Research Data Repositories	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[dataregistry]	[B2AI_TOPIC:5]	re3data is a global registry of research data repositories. The registry covers research data repositories from different academic disciplines. re3data presents repositories for the permanent storage and access of data sets to researchers, funding bodies, publishers and scholarly institutions. re3data aims to promote a culture of sharing, increased access and better visibility of research data.	True	False	https://www.re3data.org/											doi:10.5281/zenodo.6697943						
B2AI_STANDARD:719	B2AI_STANDARD:Registry	3DP Registry	RSNA-ACR 3D Printing Registry	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[dataregistry]	[B2AI_TOPIC:5]	The joint RSNA and American College of Radiology (ACR) 3D printing clinical data registry collects 3D printing data at the point of clinical care. With the goal of improving both patient care and characterizing resource utilization, the brand-new registry collects anonymized 3D printing case information, clinical indications and intended uses for printed models, source imaging, model construction techniques and effort, 3D printing techniques and effort, and the clinical impact of the models.	False	True	https://www.rsna.org/practice-tools/RSNA-ACR-3D-printing-registry															[B2AI_ORG:85]		
B2AI_STANDARD:720	B2AI_STANDARD:SoftwareOrTool	clinical-problem-standardization	A corpus-driven standardization framework for encoding clinical problems with HL7 FHIR	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4]	A framework for transforming free-text problem descriptions into standardized Health Level 7 (HL7) Fast Healthcare Interoperability Resources (FHIR) models.	True	False	https://github.com/OHNLP/clinical-problem-standardization	https://github.com/OHNLP/clinical-problem-standardization										doi:10.1016/j.jbi.2020.103541	[B2AI_STANDARD:109]	[B2AI_STANDARD:845|B2AI_STANDARD:846|B2AI_STANDARD:847|B2AI_STANDARD:850]		[B2AI_ORG:40]		
B2AI_STANDARD:721	B2AI_STANDARD:SoftwareOrTool	ARES	A Research Exploration System	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4]	A Research Exploration System designed to improved the transparency of observational data research. ARES is an opinionated framework that delineates three levels of observational data assessment.	True	False	https://github.com/OHDSI/Ares	https://github.com/OHDSI/Ares														[B2AI_ORG:76]		
B2AI_STANDARD:722	B2AI_STANDARD:SoftwareOrTool	Aesara	Aesara	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[machinelearningframework]	[B2AI_TOPIC:5]	Aesara is a Python library that allows one to define, optimize, and efficiently evaluate mathematical expressions involving multi-dimensional arrays.	True	False	https://github.com/aesara-devs/aesara	https://github.com/aesara-devs/aesara			[{\"id\": \"B2AI_APP:63\", \"category\": \"B2AI:Application\", \"name\": \"Probabilistic Modeling and Bayesian Inference for Biomedicine\", \"description\": \"Aesara (successor to Theano) is used in biomedical AI for implementing probabilistic graphical models, Bayesian neural networks, and statistical inference algorithms that quantify uncertainty in clinical predictions. Researchers leverage Aesara's symbolic math capabilities and automatic differentiation to build sophisticated probabilistic models for tasks like uncertainty-aware disease risk prediction, Bayesian optimization of drug dosing regimens, and hierarchical models for multi-center clinical studies. The library enables AI applications that provide calibrated confidence intervals for predictions, perform approximate Bayesian inference through variational methods, and integrate domain knowledge through informative priors. Aesara is particularly valuable when prediction uncertainty quantification is critical for clinical decision-making.\", \"used_in_bridge2ai\": false}]	[Probabilistic Modeling and Bayesian Inference for Biomedicine]	[B2AI_APP:63]		[Aesara (successor to Theano) is used in biomedical AI for implementing probabilistic graphical models, Bayesian neural networks, and statistical inference algorithms that quantify uncertainty in clinical predictions. Researchers leverage Aesara's symbolic math capabilities and automatic differentiation to build sophisticated probabilistic models for tasks like uncertainty-aware disease risk prediction, Bayesian optimization of drug dosing regimens, and hierarchical models for multi-center clinical studies. The library enables AI applications that provide calibrated confidence intervals for predictions, perform approximate Bayesian inference through variational methods, and integrate domain knowledge through informative priors. Aesara is particularly valuable when prediction uncertainty quantification is critical for clinical decision-making.]	[B2AI:Application]	[False]				[B2AI_SUBSTRATE:1]			
B2AI_STANDARD:723	B2AI_STANDARD:SoftwareOrTool	AWS	Amazon Web Services	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[cloudservice]	[B2AI_TOPIC:5]	Amazon Web Services (AWS) is the world's most comprehensive and broadly adopted cloud computing platform, providing 200+ fully managed services spanning compute (EC2, Lambda serverless), storage (S3 object storage, EBS block storage, EFS file systems), databases (RDS, DynamoDB, Redshift), networking (VPC, CloudFront CDN), machine learning (SageMaker, Bedrock, Rekognition), analytics (EMR, Athena, Kinesis), security (IAM, KMS, CloudHSM), and application integration (SQS, SNS, EventBridge) across 38 geographic regions with 120 availability zones globally. Launched in 2006, AWS pioneered the Infrastructure-as-a-Service (IaaS) model, transforming IT economics by offering pay-as-you-go pricing, elastic scalability, and eliminating upfront capital expenditures for hardware, enabling organizations from startups to enterprises to access enterprise-grade infrastructure previously available only to large corporations. AWS's compute offerings range from general-purpose EC2 instances for traditional workloads to specialized instance types optimized for memory-intensive applications (R-series), compute-intensive HPC (C-series, Hpc7a), GPU-accelerated deep learning (P5, G5, Trn1 with AWS Trainium/Inferentia chips), and serverless Lambda functions billed per millisecond execution time. The platform's storage hierarchy includes S3 for durable object storage with 99.999999999% durability, S3 Glacier for archival with retrieval SLAs from minutes to hours, EBS for low-latency block storage attached to EC2 instances, and EFS for scalable shared file systems supporting concurrent access across thousands of compute nodes. AWS's managed database services eliminate operational overhead of patching, backups, and replication, offering relational databases (Amazon RDS for MySQL, PostgreSQL, Oracle, SQL Server; Amazon Aurora with MySQL/PostgreSQL compatibility), NoSQL databases (DynamoDB for key-value, DocumentDB for MongoDB workloads), graph databases (Neptune), time-series databases (Timestream), and data warehouses (Redshift) with columnar storage for OLAP queries on petabyte-scale datasets. For AI/ML workloads, AWS SageMaker provides end-to-end ML lifecycle management with built-in algorithms, Jupyter notebooks, distributed training across GPU clusters, hyperparameter tuning, model deployment to real-time/batch inference endpoints, and MLOps capabilities (model registry, monitoring, CI/CD pipelines). AWS Bedrock offers access to foundation models from AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon Titan via unified API with retrieval-augmented generation (RAG) capabilities connecting models to enterprise data in S3 or knowledge bases. The platform's security model includes IAM for fine-grained access control with role-based policies, KMS for encryption key management, CloudHSM for FIPS 140-2 Level 3 hardware security modules, AWS Shield for DDoS protection, GuardDuty for threat detection, and comprehensive compliance certifications (SOC 1/2/3, PCI-DSS, HIPAA, FedRAMP, ISO 27001) enabling deployment of regulated workloads. AWS supports hybrid cloud architectures through AWS Outposts (on-premises hardware running AWS services), AWS Direct Connect (dedicated network connections bypassing public internet), and Storage Gateway (seamless cloud-extension of on-premises storage), enabling gradual cloud migrations and latency-sensitive edge computing scenarios. For AI/ML infrastructure in Bridge2AI and biomedical research, AWS provides scalable compute for genome sequencing analysis (AWS Batch, EC2 Spot instances), secure storage for HIPAA-compliant patient data (S3 with server-side encryption, VPC endpoints preventing internet exposure), managed databases for clinical trial data (RDS with automated backups, multi-AZ high availability), federated learning frameworks (AWS HealthLake for FHIR-based interoperability), and specialized AI services for medical imaging analysis (AWS HealthImaging, Amazon Rekognition Custom Labels), accelerating research workflows while maintaining data privacy, auditability, and compliance with healthcare regulations.	False	True	https://aws.amazon.com/		True															
B2AI_STANDARD:724	B2AI_STANDARD:SoftwareOrTool	Amundsen	Amundsen	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Amundsen is a data discovery and metadata engine for improving the productivity of data analysts, data scientists and engineers when interacting with data. It does that today by indexing data resources (tables, dashboards, streams, etc.) and powering a page-rank style search based on usage patterns (e.g. highly queried tables show up earlier than less queried tables).	True	False	https://www.amundsen.io/	https://github.com/amundsen-io/amundsen																
B2AI_STANDARD:725	B2AI_STANDARD:SoftwareOrTool	Anduril	Anduril	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[toolkit]	[B2AI_TOPIC:20]	Anduril is a workflow platform for analyzing large data sets. Anduril provides facilities for analyzing high-thoughput data in biomedical research, and the platform is fully extensible by third parties.	True	False	https://anduril.org/site/	https://bitbucket.org/anduril-dev/anduril/src/stable/										doi:10.1093/bioinformatics/btz133						
B2AI_STANDARD:726	B2AI_STANDARD:SoftwareOrTool	Apache Atlas	Apache Atlas	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Apache Atlas provides open metadata management and governance capabilities for organizations to build a catalog of their data assets, classify and govern these assets and provide collaboration capabilities around these data assets for data scientists, analysts and the data governance team.	True	False	https://atlas.apache.org/	https://github.com/apache/atlas														[B2AI_ORG:5]		
B2AI_STANDARD:727	B2AI_STANDARD:SoftwareOrTool	Spark	Apache Spark	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs.	True	False	https://spark.apache.org/docs/latest/index.html	https://github.com/apache/spark														[B2AI_ORG:5]		
B2AI_STANDARD:728	B2AI_STANDARD:SoftwareOrTool	Taverna	Apache Taverna	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[deprecated|workflowlanguage]	[B2AI_TOPIC:5]	Taverna is a domain-independent suite of tools used to design and execute data-driven workflows.	True	False	https://incubator.apache.org/projects/taverna.html																	
B2AI_STANDARD:729	B2AI_STANDARD:SoftwareOrTool	Appyters	Appyters	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datavisualization|notebookplatform]	[B2AI_TOPIC:5]	Appyters extend the Jupyter Notebook language to support external, end-user configurable variables. Appyters can be considered a meta Jupyter Notebook language that is compatible with standard Jupyter Notebook execution.	True	False	https://appyters.maayanlab.cloud/	https://github.com/MaayanLab/appyter-catalog										doi:10.1016/j.patter.2021.100213						
B2AI_STANDARD:730	B2AI_STANDARD:SoftwareOrTool	ATHENA	ATHENA	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A resource of searchable and loadable standardized vocabularies.	True	False	https://athena.ohdsi.org/search-terms/terms	https://github.com/OHDSI/Athena	True											[B2AI_STANDARD:844]		[B2AI_ORG:76]		
B2AI_STANDARD:731	B2AI_STANDARD:SoftwareOrTool	ATLAS	ATLAS	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4]	An open source software tool for researchers to conduct scientific analyses on standardized observational data converted to the OMOP Common Data Model V5. Researchers can create cohorts by defining groups of people based on an exposure to a drug or diagnosis of a particular condition using healthcare claims data.	True	False	https://atlas-demo.ohdsi.org/#/home	https://github.com/OHDSI/Atlas												[B2AI_STANDARD:844]		[B2AI_ORG:76]		
B2AI_STANDARD:732	B2AI_STANDARD:SoftwareOrTool	AtriumDB	AtriumDB	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4|B2AI_TOPIC:37]	A database of continuously-recorded physiological waveform data and other associated clinical and medical device data. Also the platform for storage and retrieval of clinical waveform data.	False	True	https://laussenlabs.ca/atriumdb/		True		[{\"id\": \"B2AI_APP:64\", \"category\": \"B2AI:Application\", \"name\": \"High-Frequency Physiological Waveform Analysis\", \"description\": \"AtriumDB is used in AI applications for managing and analyzing high-frequency physiological waveform data at scale, enabling deep learning models for intensive care monitoring, perioperative risk prediction, and continuous vital sign analysis. The platform's efficient storage and query capabilities support training of neural networks on bedside monitor data including ECG, arterial blood pressure, and other high-resolution physiological signals collected over extended periods. AI systems leverage AtriumDB to access synchronized multi-parameter waveforms for developing early warning systems, detecting subtle physiological deterioration, and predicting adverse events in critically ill patients. The database's time-series optimization enables real-time AI inference on streaming waveform data.\", \"used_in_bridge2ai\": false}]	[High-Frequency Physiological Waveform Analysis]	[B2AI_APP:64]		[AtriumDB is used in AI applications for managing and analyzing high-frequency physiological waveform data at scale, enabling deep learning models for intensive care monitoring, perioperative risk prediction, and continuous vital sign analysis. The platform's efficient storage and query capabilities support training of neural networks on bedside monitor data including ECG, arterial blood pressure, and other high-resolution physiological signals collected over extended periods. AI systems leverage AtriumDB to access synchronized multi-parameter waveforms for developing early warning systems, detecting subtle physiological deterioration, and predicting adverse events in critically ill patients. The database's time-series optimization enables real-time AI inference on streaming waveform data.]	[B2AI:Application]	[False]	doi:10.1088/1361-6579/ab7cb5				[B2AI_ORG:117]		
B2AI_STANDARD:733	B2AI_STANDARD:SoftwareOrTool	ACHILLES	Automated Characterization of Health Information at Large-scale Longitudinal Evidence System	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4]	Provides descriptive statistics on an OMOP CDM database.	True	False	https://ohdsi.github.io/TheBookOfOhdsi/DataQuality.html#data-quality-checks	https://github.com/OHDSI/Achilles											[B2AI_STANDARD:243]	[B2AI_STANDARD:844]		[B2AI_ORG:76]		
B2AI_STANDARD:734	B2AI_STANDARD:SoftwareOrTool	balance	balance package	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A Python package for balancing biased data samples.	True	False	https://import-balance.org/	https://github.com/facebookresearch/balance														[B2AI_ORG:55]		
B2AI_STANDARD:735	B2AI_STANDARD:SoftwareOrTool	BigQuery	BigQuery	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A fully managed, serverless data warehouse that enables scalable analysis over petabytes of data. It is a Platform as a Service (PaaS) that supports querying using ANSI SQL.	False	True	https://cloud.google.com/bigquery														[B2AI_SUBSTRATE:4]	[B2AI_ORG:37]		
B2AI_STANDARD:736	B2AI_STANDARD:SoftwareOrTool	Biopython	Biopython	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[toolkit]	[B2AI_TOPIC:1]	Biopython is a set of freely available tools for biological computation written in Python by an international team of developers. It is a distributed collaborative effort to develop Python libraries and applications which address the needs of current and future work in bioinformatics. The source code is made available under the Biopython License, which is extremely liberal and compatible with almost every license in the world.	True	False	https://biopython.org/	https://github.com/biopython/biopython																
B2AI_STANDARD:737	B2AI_STANDARD:SoftwareOrTool	Biotite	Biotite	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[toolkit]	[B2AI_TOPIC:20]	This package bundles popular tasks in computational molecular biology into a uniform Python library.	True	False	https://www.biotite-python.org/	https://github.com/biotite-dev/biotite																
B2AI_STANDARD:738	B2AI_STANDARD:SoftwareOrTool	BrainStat	BrainStat	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:22]	A toolbox for the statistical analysis and context decoding of neuroimaging data. It implements both univariate and multivariate linear models and interfaces with the BigBrain Atlas, Allen Human Brain Atlas and Nimare databases.	True	False	https://brainstat.readthedocs.io/	https://github.com/MICA-MNI/BrainStat										doi:10.1016/j.neuroimage.2022.119807						
B2AI_STANDARD:739	B2AI_STANDARD:SoftwareOrTool	Cavatica	Cavatica data analysis platform	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4]	A data analysis and sharing platform designed to accelerate discovery in a scalable, cloud-based compute environment where data, results, and workflows are shared among the world's research community. Developed by Seven Bridges and funded in-part by a grant from the National Institutes of Health (NIH) Common Fund, CAVATICA is continuously updated with new tools and datasets.	False	True	https://www.cavatica.org/															[B2AI_ORG:1]		
B2AI_STANDARD:740	B2AI_STANDARD:SoftwareOrTool	Celery	Celery - Distributed Task Queue	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Celery is a simple, flexible, and reliable distributed system to process vast amounts of messages, while providing operations with the tools required to maintain such a system. Its a task queue with focus on real-time processing, while also supporting task scheduling.	True	False	https://docs.celeryq.dev/en/latest/	https://github.com/celery/celery																
B2AI_STANDARD:741	B2AI_STANDARD:SoftwareOrTool	CLiXO	Clique eXtracted Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:12|B2AI_TOPIC:21]	An updated version of the CliXO (Clique eXtracted Ontology) algorithm for inferring gene ontology terms from pairwise gene similarity data.	True	False	https://github.com/fanzheng10/CliXO-1.0	https://github.com/fanzheng10/CliXO-1.0										doi:10.1126/science.abf3067						
B2AI_STANDARD:742	B2AI_STANDARD:SoftwareOrTool	Comet	Comet	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Platform for managing machine learning models.	False	True	https://www.comet.com/	https://github.com/comet-ml																
B2AI_STANDARD:743	B2AI_STANDARD:SoftwareOrTool	CDAPS	Community Detection APplication and Service	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:12|B2AI_TOPIC:21]	Performs multiscale community detection and functional enrichment for network analysis through a service-oriented architecture. These features are provided by integrating popular community detection algorithms and enrichment tools. All the algorithms and tools run remotely on a dedicated server.	True	False	https://cdaps.readthedocs.io/	https://github.com/cytoscape/cy-community-detection										doi:10.1371/journal.pcbi.1008239						
B2AI_STANDARD:744	B2AI_STANDARD:SoftwareOrTool	CD-CLiXO	Community Detection CliXO	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:12|B2AI_TOPIC:21]	Builds a CDAPS compatible community detection Docker image using CliXO.	True	False	https://github.com/idekerlab/cdclixo	https://github.com/idekerlab/cdclixo																
B2AI_STANDARD:745	B2AI_STANDARD:SoftwareOrTool	CORAL	Contextual Ontology-based Repository Analysis Library	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datamodel]	[B2AI_TOPIC:5]	A framework for rigorous self-validated data modeling and integrative, reproducible data analysis	True	False	https://github.com/jmchandonia/CORAL	https://github.com/jmchandonia/CORAL										doi:10.1093/gigascience/giac089						
B2AI_STANDARD:746	B2AI_STANDARD:SoftwareOrTool	CML library	Continuous Machine Learning	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Continuous Machine Learning (CML) is an open-source library for implementing continuous integration & delivery (CI/CD) in machine learning projects. Use it to automate parts of your development workflow, including model training and evaluation, comparing ML experiments across your project history, and monitoring changing datasets.	True	False	https://cml.dev/	https://github.com/iterative/cml																
B2AI_STANDARD:747	B2AI_STANDARD:SoftwareOrTool	Cromwell	Cromwell Workflow Management System	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Cromwell is an open-source Workflow Management System for bioinformatics.	True	False	https://cromwell.readthedocs.io/en/stable/	https://github.com/broadinstitute/cromwell										doi:10.7490/f1000research.1114634.1						
B2AI_STANDARD:748	B2AI_STANDARD:SoftwareOrTool	Cytoscape	Cytoscape software	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:21]	An open source software platform for visualizing complex networks and integrating these with any type of attribute data.	True	False	https://cytoscape.org/	https://github.com/cytoscape/cytoscape																
B2AI_STANDARD:749	B2AI_STANDARD:SoftwareOrTool	Dagster	Dagster	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Open source orchestration platform for the development, production, and observation of data assets.	False	True	https://dagster.io/	https://github.com/dagster-io/dagster																
B2AI_STANDARD:750	B2AI_STANDARD:SoftwareOrTool	DANCE	DANCE platform	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[scrnaseqanalysis]	[B2AI_TOPIC:34]	A Python toolkit to support deep learning models for analyzing single-cell gene expression at scale.	True	False	https://omicsml.ai/	https://github.com/OmicsML/dance																
B2AI_STANDARD:751	B2AI_STANDARD:SoftwareOrTool	DRS	Data Repository Service	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	The Data Repository Service (DRS) is a standard for describing and accessing data objects in a data repository. The DRS provides a RESTful API for accessing data objects, as well as a set of metadata fields for describing the data objects. The DRS is designed to be used in conjunction with the GA4GH Data Repository Service API, which provides a standard way to access data objects in a data repository.	True	False	https://ga4gh.github.io/data-repository-service-schemas/preview/release/drs-1.2.0/docs/															[B2AI_ORG:34]		
B2AI_STANDARD:752	B2AI_STANDARD:SoftwareOrTool	DVC	Data Version Control	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Data Version Control (DVC) is an open-source version control system for data science and machine learning projects. It is designed to help data scientists and machine learning engineers manage their data, models, and experiments in a reproducible and collaborative way.	True	False	https://dvc.org/	https://github.com/iterative/dvc																
B2AI_STANDARD:753	B2AI_STANDARD:SoftwareOrTool	DataHub	DataHub	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	DataHub is a metadata platform. It provides a central place to manage and discover data assets, including datasets, dashboards, and machine learning models. DataHub is designed to be extensible and customizable.	True	False	https://datahubproject.io/	https://github.com/linkedin/datahub																
B2AI_STANDARD:754	B2AI_STANDARD:SoftwareOrTool	Datasette	Datasette	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Datasette is a comprehensive open-source tool ecosystem designed for exploring, analyzing, and publishing data through an intuitive web interface and accompanying JSON API. Created by Simon Willison, Datasette transforms data of any shape into interactive websites that enable users to perform exploratory data analysis, create visualizations, and share findings with colleagues without requiring extensive technical expertise. The platform excels in three primary use cases: exploratory data analysis (automatically detecting patterns in CSV, JSON, and database data), instant data publishing (using the `datasette publish` command to deploy data to cloud hosting providers like Google Cloud Run, Heroku, or Vercel), and rapid prototyping (spinning up JSON APIs in minutes for proof-of-concept development). Datasette is part of a broader ecosystem of 44 related tools and 154 plugins that enhance productivity when working with structured data. The tool supports multiple data import formats, provides built-in security features for data access control, and offers extensive customization options through its plugin architecture. Datasette serves data journalists, museum curators, archivists, local governments, scientists, researchers, and anyone seeking to make their data more accessible and discoverable through web-based interfaces.	True	False	https://datasette.io/	https://github.com/simonw/datasette																
B2AI_STANDARD:755	B2AI_STANDARD:SoftwareOrTool	DENDRO	DENDRO	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[scrnaseqanalysis]	[B2AI_TOPIC:34]	An analysis method for scRNA-seq data that clusters single cells into genetically distinct subclones and reconstructs the phylogenetic tree relating the subclones.	True	False	https://github.com/zhouzilu/DENDRO	https://github.com/zhouzilu/DENDRO										doi:10.1186/s13059-019-1922-x						
B2AI_STANDARD:756	B2AI_STANDARD:SoftwareOrTool	Determined	Determined	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[machinelearningframework]	[B2AI_TOPIC:5]	Determined is an open-source deep learning training platform.	True	False	https://www.determined.ai/	https://github.com/determined-ai/determined			[{\"id\": \"B2AI_APP:66\", \"category\": \"B2AI:Application\", \"name\": \"Scalable ML Training Platform for Healthcare Research\", \"description\": \"Determined is used in biomedical AI for managing large-scale deep learning experiments, hyperparameter tuning, and distributed training across GPU clusters in research and healthcare organizations. AI teams leverage Determined's automated experiment tracking, resource scheduling, and hyperparameter optimization to efficiently develop medical imaging models, genomic prediction algorithms, and clinical NLP systems. The platform provides reproducible experiment management with automatic checkpointing, fault tolerance for long-running medical imaging training jobs, and collaborative features for multi-institutional research teams. Determined's integration with healthcare security requirements and support for diverse model frameworks makes it valuable for organizations scaling from research to production clinical AI.\", \"used_in_bridge2ai\": false, \"references\": [\"https://docs.determined.ai/\"]}]	[Scalable ML Training Platform for Healthcare Research]	[B2AI_APP:66]	[['https://docs.determined.ai/']]	[Determined is used in biomedical AI for managing large-scale deep learning experiments, hyperparameter tuning, and distributed training across GPU clusters in research and healthcare organizations. AI teams leverage Determined's automated experiment tracking, resource scheduling, and hyperparameter optimization to efficiently develop medical imaging models, genomic prediction algorithms, and clinical NLP systems. The platform provides reproducible experiment management with automatic checkpointing, fault tolerance for long-running medical imaging training jobs, and collaborative features for multi-institutional research teams. Determined's integration with healthcare security requirements and support for diverse model frameworks makes it valuable for organizations scaling from research to production clinical AI.]	[B2AI:Application]	[False]							
B2AI_STANDARD:757	B2AI_STANDARD:SoftwareOrTool	Dicoogle	Dicoogle Picture Archiving and Communications System	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:15]	Dicoogle is an open source Picture Archiving and Communications System (PACS) archive. Its modular architecture allows the quick development of new functionalities, due the availability of a Software Development Kit (SDK).	True	False	https://dicoogle.com/	https://github.com/bioinformatics-ua/dicoogle										doi:10.1109/ISCC50000.2020.9219545						
B2AI_STANDARD:758	B2AI_STANDARD:SoftwareOrTool	DigitalOcean	DigitalOcean	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[cloudservice]	[B2AI_TOPIC:5]	DigitalOcean is a comprehensive cloud infrastructure platform that provides scalable computing resources designed to simplify cloud deployment for developers, startups, and enterprises. The platform offers a complete suite of cloud services including Droplets (virtual machines), Kubernetes clusters, App Platform for application deployment, managed databases, object storage (Spaces), load balancers, and specialized AI/ML infrastructure including GPU-powered Droplets with NVIDIA H100, H200, and AMD MI325X accelerators. DigitalOcean distinguishes itself through developer-friendly interfaces, predictable pricing without bandwidth overage charges, and extensive community-contributed tutorials and documentation. The platform serves over 600,000 customers ranging from individual developers to large enterprises, offering both self-service infrastructure management and managed services with premium support options. DigitalOcean's global data center network provides 99.99% uptime SLAs and supports diverse use cases including web hosting, application development, AI inference workloads, container orchestration, and data processing pipelines, making it particularly attractive for cost-conscious organizations seeking reliable cloud infrastructure without the complexity of larger cloud providers.	False	True	https://www.digitalocean.com/																	
B2AI_STANDARD:759	B2AI_STANDARD:SoftwareOrTool	DoubletDecon	DoubletDecon	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[scrnaseqanalysis]	[B2AI_TOPIC:34]	An approach that detects doublet cell capture artifacts in scRNA-seq data with a combination of deconvolution analyses and the identification of unique cell-state gene expression.	True	False	https://github.com/EDePasquale/DoubletDecon	https://github.com/EDePasquale/DoubletDecon										doi:10.1016/j.celrep.2019.09.082						
B2AI_STANDARD:760	B2AI_STANDARD:SoftwareOrTool	Egeria	Egeria	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Open metadata and governance for enterprises - automatically capturing, managing and exchanging metadata between tools and platforms, no matter the vendor.	True	False	https://egeria-project.org/	https://github.com/odpi/egeria																
B2AI_STANDARD:761	B2AI_STANDARD:SoftwareOrTool	Eido	Eido	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Eido is used to 1) validate or 2) convert format of sample metadata. Sample metadata is stored according to the standard PEP specification. For validation, eido is based on JSON Schema and extends it with new features, like required input files.	True	False	http://eido.databio.org/	https://github.com/pepkit/eido										doi:10.1093/gigascience/giab077	[B2AI_STANDARD:260|B2AI_STANDARD:345]					
B2AI_STANDARD:762	B2AI_STANDARD:SoftwareOrTool	ENHANCE	Expression denoising heuristic using aggregation of neighbors and principal component extraction	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[scrnaseqanalysis]	[B2AI_TOPIC:34]	ENHANCE is a computational method for accurate denoising of single-cell RNA-sequencing data that addresses technical noise while preserving biological signal. The algorithm uses k-nearest neighbor aggregation to identify similar cells, followed by principal component analysis (PCA) to remove noise components while retaining significant biological variation. The method automatically determines the optimal number of neighbors (k) based on target transcript counts and identifies significant principal components using variance fold-thresholding against simulated noise datasets. ENHANCE processes UMI-count matrices in tab-separated format, supports gzip compression, and provides configurable parameters for neighbor selection, PC threshold determination, and precision control. The tool enhances downstream analyses such as clustering, trajectory inference, and differential expression by reducing technical artifacts while maintaining cell type distinctions and biological relationships in single-cell transcriptomic data.	True	False	https://github.com/yanailab/enhance	https://github.com/yanailab/enhance										doi:10.1101/655365						
B2AI_STANDARD:763	B2AI_STANDARD:SoftwareOrTool	FDP	FAIR Data Point	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	FAIR Data Point (FDP) is a REST API for creating, storing, and serving FAIR metadata. This FDP implementation also presents a Web-based graphical user interface (GUI). The metadata contents are generated semi-automatically according to the FAIR Data Point software specification document.	True	False	https://github.com/FAIRDataTeam/FAIRDataPoint	https://github.com/FAIRDataTeam/FAIRDataPoint										doi:10.1162/dint_a_00160						
B2AI_STANDARD:764	B2AI_STANDARD:SoftwareOrTool	FAIRSCAPE	FAIRSCAPE digital commons framework	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[implementation_maturity_pilot]	[B2AI_TOPIC:5]	A reusable computational framework, enabling simplified access to modern scalable cloud-based components. FAIRSCAPE fully implements the FAIR data principles and extends them to provide fully FAIR Evidence, including machine-interpretable provenance of datasets, software and computations, as metadata for all computed results.	True	False	https://fairscape.github.io/	https://github.com/fairscape/fairscape	True		[{\"id\": \"B2AI_APP:67\", \"category\": \"B2AI:Application\", \"name\": \"FAIR ML Workflows and Computational Reproducibility\", \"description\": \"FAIRSCAPE is used in AI applications to create Findable, Accessible, Interoperable, and Reusable machine learning workflows with comprehensive provenance tracking and metadata management. AI researchers leverage FAIRSCAPE to package complete ML pipelines including data preprocessing, model training, validation, and deployment with detailed computational provenance, enabling reproducible AI research and regulatory compliance. The framework supports automated metadata capture during model development, versioning of datasets and models, and generation of computational notebooks that document the entire ML lifecycle. FAIRSCAPE is particularly valuable for multi-institutional AI studies where workflow transparency, data lineage, and reproducibility are essential for scientific validity and clinical translation.\", \"used_in_bridge2ai\": false}]	[FAIR ML Workflows and Computational Reproducibility]	[B2AI_APP:67]		[FAIRSCAPE is used in AI applications to create Findable, Accessible, Interoperable, and Reusable machine learning workflows with comprehensive provenance tracking and metadata management. AI researchers leverage FAIRSCAPE to package complete ML pipelines including data preprocessing, model training, validation, and deployment with detailed computational provenance, enabling reproducible AI research and regulatory compliance. The framework supports automated metadata capture during model development, versioning of datasets and models, and generation of computational notebooks that document the entire ML lifecycle. FAIRSCAPE is particularly valuable for multi-institutional AI studies where workflow transparency, data lineage, and reproducibility are essential for scientific validity and clinical translation.]	[B2AI:Application]	[False]	doi:10.1007/s12021-021-09529-4				[B2AI_ORG:116]		
B2AI_STANDARD:765	B2AI_STANDARD:SoftwareOrTool	FastAI	FastAI	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[machinelearningframework]	[B2AI_TOPIC:5]	fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library.	True	False	https://github.com/fastai/fastai				[{\"id\": \"B2AI_APP:68\", \"category\": \"B2AI:Application\", \"name\": \"Rapid Medical AI Prototyping and Transfer Learning\", \"description\": \"FastAI is used in biomedical research for rapid prototyping and training of deep learning models with minimal code, particularly valuable for researchers without extensive ML engineering backgrounds. Healthcare researchers leverage FastAI's high-level APIs, best-practice defaults, and powerful transfer learning capabilities to quickly develop models for medical image classification, clinical text analysis, and tabular clinical data prediction. The library's sophisticated learning rate scheduling, progressive resizing, and mixed precision training enable efficient use of computational resources, while its extensive documentation and educational materials democratize AI development in medicine. FastAI is particularly popular for exploratory research, kaggle-style medical AI competitions, and educational settings teaching clinical AI.\", \"used_in_bridge2ai\": false}]	[Rapid Medical AI Prototyping and Transfer Learning]	[B2AI_APP:68]		[FastAI is used in biomedical research for rapid prototyping and training of deep learning models with minimal code, particularly valuable for researchers without extensive ML engineering backgrounds. Healthcare researchers leverage FastAI's high-level APIs, best-practice defaults, and powerful transfer learning capabilities to quickly develop models for medical image classification, clinical text analysis, and tabular clinical data prediction. The library's sophisticated learning rate scheduling, progressive resizing, and mixed precision training enable efficient use of computational resources, while its extensive documentation and educational materials democratize AI development in medicine. FastAI is particularly popular for exploratory research, kaggle-style medical AI competitions, and educational settings teaching clinical AI.]	[B2AI:Application]	[False]		[B2AI_STANDARD:816]		[B2AI_SUBSTRATE:33]			
B2AI_STANDARD:766	B2AI_STANDARD:SoftwareOrTool	Galaxy	Galaxy	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[toolkit]	[B2AI_TOPIC:20]	Galaxy is an open source, web-based platform for data intensive biomedical research.	True	False	https://usegalaxy.org/	https://github.com/galaxyproject/galaxy										doi:10.1093/nar/gky379	[B2AI_STANDARD:334]					
B2AI_STANDARD:767	B2AI_STANDARD:SoftwareOrTool	GCP	Google Cloud Platform	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[cloudservice]	[B2AI_TOPIC:5]	Google Cloud Platform (GCP) is a comprehensive cloud computing platform offering over 200+ services spanning compute, storage, databases, networking, AI/machine learning, data analytics, and developer tools across Google's global infrastructure. Core compute offerings include Compute Engine VMs, App Engine for platform-as-a-service deployment, Google Kubernetes Engine for container orchestration, and Cloud Functions for serverless computing. For AI and machine learning, GCP provides Vertex AI as a unified platform for building, deploying, and scaling ML models, including access to Google's Gemini large language models, AutoML capabilities, and specialized hardware like Tensor Processing Units (TPUs) for accelerated training. Storage solutions include Cloud Storage for object storage, persistent disks for block storage, and Filestore for managed file storage. Database services encompass Cloud SQL for managed relational databases (MySQL, PostgreSQL, SQL Server), Cloud Spanner for globally distributed relational databases, BigQuery for serverless data warehousing and analytics, Firestore for NoSQL document databases, and Bigtable for wide-column NoSQL. Healthcare and life sciences researchers benefit from specialized services like Healthcare API for FHIR-based health data exchange, Life Sciences API for genomics pipeline execution, and Cloud Healthcare Console for managing sensitive patient data with HIPAA and HITRUST compliance. BigQuery ML enables researchers to create and execute machine learning models using SQL queries directly on large biomedical datasets. GCP is explicitly used in Bridge2AI projects for scalable biomedical data processing, AI model training and deployment, and collaborative research infrastructure. The platform emphasizes security with encryption at rest and in transit, IAM for access control, VPC for network isolation, and compliance certifications including HIPAA, FedRAMP, ISO 27001, and SOC 2. GCP's global network spans 40+ regions and 120+ edge locations, providing low-latency access and data residency options for international research collaborations. Integration with Google Workspace, Looker for business intelligence, and open-source frameworks like TensorFlow and PyTorch facilitates comprehensive workflows from data ingestion through analysis, visualization, and publication.	False	True	https://cloud.google.com/		True		[{\"id\": \"B2AI_APP:69\", \"category\": \"B2AI:Application\", \"name\": \"Cloud-Based Biomedical AI Infrastructure and Model Deployment\", \"description\": \"Google Cloud Platform is extensively used in AI applications for scalable biomedical data processing, large-scale model training, and deployment of clinical AI systems with healthcare-compliant infrastructure. AI researchers leverage GCP's specialized services including Vertex AI for managed ML pipelines, BigQuery for analyzing massive clinical datasets, and Cloud Healthcare API for FHIR-based data integration. The platform enables distributed training of deep learning models on genomic sequences, medical images, and electronic health records at population scale, while providing HIPAA-compliant infrastructure for clinical AI deployment. GCP's TPU infrastructure accelerates training of large biomedical language models and computer vision systems, and its AutoML capabilities democratize AI development for healthcare institutions.\", \"used_in_bridge2ai\": false, \"references\": [\"https://cloud.google.com/healthcare-api\", \"https://cloud.google.com/vertex-ai\"]}]	[Cloud-Based Biomedical AI Infrastructure and Model Deployment]	[B2AI_APP:69]	[['https://cloud.google.com/healthcare-api', 'https://cloud.google.com/vertex-ai']]	[Google Cloud Platform is extensively used in AI applications for scalable biomedical data processing, large-scale model training, and deployment of clinical AI systems with healthcare-compliant infrastructure. AI researchers leverage GCP's specialized services including Vertex AI for managed ML pipelines, BigQuery for analyzing massive clinical datasets, and Cloud Healthcare API for FHIR-based data integration. The platform enables distributed training of deep learning models on genomic sequences, medical images, and electronic health records at population scale, while providing HIPAA-compliant infrastructure for clinical AI deployment. GCP's TPU infrastructure accelerates training of large biomedical language models and computer vision systems, and its AutoML capabilities democratize AI development for healthcare institutions.]	[B2AI:Application]	[False]					[B2AI_ORG:37|B2AI_ORG:115]		
B2AI_STANDARD:768	B2AI_STANDARD:SoftwareOrTool	GrAPE	Graph Representation leArning, Predictions and Evaluation library	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[graphdataplatform|machinelearningframework]	[B2AI_TOPIC:21]	A fast graph processing and embedding library, designed to scale with big graphs and to run on both off-the-shelf laptop and desktop computers and High Performance Computing clusters of workstations.	True	False	https://github.com/AnacletoLAB/grape	https://github.com/AnacletoLAB/grape			[{\"id\": \"B2AI_APP:70\", \"category\": \"B2AI:Application\", \"name\": \"Graph Embedding and Network Medicine AI\", \"description\": \"GrAPE (Graph Automatic Programming Environment) is used in biomedical AI for creating graph embeddings and training graph neural networks on biological networks, enabling applications in drug discovery, protein function prediction, and disease gene prioritization. Researchers leverage GrAPE's efficient graph processing capabilities to learn low-dimensional representations of large-scale biological networks including protein-protein interactions, gene regulatory networks, and knowledge graphs. The tool supports AI applications that predict novel drug-target interactions, identify disease modules, and perform link prediction in biomedical knowledge graphs. GrAPE's optimization for large graphs enables training on genome-scale networks and integration of multi-omics data through graph-based representations.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1093/gigascience/giab044\"]}]	[Graph Embedding and Network Medicine AI]	[B2AI_APP:70]	[['https://doi.org/10.1093/gigascience/giab044']]	[GrAPE (Graph Automatic Programming Environment) is used in biomedical AI for creating graph embeddings and training graph neural networks on biological networks, enabling applications in drug discovery, protein function prediction, and disease gene prioritization. Researchers leverage GrAPE's efficient graph processing capabilities to learn low-dimensional representations of large-scale biological networks including protein-protein interactions, gene regulatory networks, and knowledge graphs. The tool supports AI applications that predict novel drug-target interactions, identify disease modules, and perform link prediction in biomedical knowledge graphs. GrAPE's optimization for large graphs enables training on genome-scale networks and integration of multi-omics data through graph-based representations.]	[B2AI:Application]	[False]	doi:10.48550/arXiv.2110.06196			[B2AI_SUBSTRATE:14|B2AI_SUBSTRATE:15]			
B2AI_STANDARD:769	B2AI_STANDARD:SoftwareOrTool	HESML	Half-Edge Semantic Measures Library	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:32|B2AI_TOPIC:16]	HESML is an efficient, scalable and large Java software library of ontology-based semantic similarity measures and Information Content (IC) models based on WordNet, SNOMED-CT, MeSH or any other OBO-based ontology.	True	False	http://hesml.lsi.uned.es/	https://github.com/jjlastra/HESML										doi:10.1186/S12859-021-04539-0	[B2AI_STANDARD:553]					
B2AI_STANDARD:770	B2AI_STANDARD:SoftwareOrTool	Hangar	Hangar	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Hangar is a version control system specifically designed for managing tensor data and numerical arrays in machine learning and scientific computing workflows, providing Git-like versioning capabilities for datasets that are too large or complex for traditional version control systems. Built with Python and optimized for performance, Hangar enables data scientists and ML engineers to track changes in training datasets, model weights, embeddings, and experimental results with full history, branching, merging, and collaboration features. The system uses content-addressable storage with automatic data deduplication, storing only changed array blocks rather than entire files, making it efficient for large multidimensional datasets common in deep learning (image datasets, genomic sequences, time series, medical imaging volumes). Hangar provides atomic commits ensuring data consistency, supports concurrent read access for distributed training, and enables reproducible machine learning by linking dataset versions to specific model training runs. Key features include automatic detection of array schema changes, efficient storage of sparse arrays, lazy loading for memory-efficient data access, and integration with NumPy, PyTorch, and TensorFlow workflows. Applications span ML experiment tracking where different dataset versions are tested with model architectures, collaborative dataset curation where multiple researchers contribute annotations or corrections, model debugging through inspection of training data that caused specific failures, and regulatory compliance in medical AI where dataset provenance and versioning history must be maintained. Hangar addresses critical gaps in ML operations by providing reproducibility (exact dataset version used for published results), collaboration (merge dataset contributions from multiple sources), and auditability (full history of dataset modifications with timestamps and contributors), essential for rigorous scientific research and production ML systems.	True	False	https://hangar-py.readthedocs.io/en/stable/	https://github.com/tensorwerk/hangar-py													[B2AI_SUBSTRATE:42]			
B2AI_STANDARD:771	B2AI_STANDARD:SoftwareOrTool	HADES	Health Analytics Data-to-Evidence Suite	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4]	HADES (formally known as the OHDSI Methods Library) is a set of open source R packages for large scale analytics, including population characterization, population-level causal effect estimation, and patient-level prediction.	True	False	https://ohdsi.github.io/Hades/	https://github.com/OHDSI/Hades														[B2AI_ORG:76]		
B2AI_STANDARD:772	B2AI_STANDARD:SoftwareOrTool	HAIM	Holistic AI in Medicine framework	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[multimodal]	[B2AI_TOPIC:5]	A unified Holistic AI in Medicine (HAIM) framework to facilitate the generation and testing of AI systems that leverage multimodal inputs.	True	False	https://github.com/lrsoenksen/HAIM	https://github.com/lrsoenksen/HAIM			[{\"id\": \"B2AI_APP:71\", \"category\": \"B2AI:Application\", \"name\": \"Holistic Multi-Modal Healthcare AI Integration\", \"description\": \"HAIM (Holistic AI in Medicine) framework is used for developing and evaluating multi-modal AI systems that integrate diverse healthcare data types including imaging, time-series, tabular clinical data, and text for comprehensive patient assessment. The framework enables training of models that leverage complementary information across modalities to improve diagnostic accuracy, risk prediction, and clinical decision support beyond single-modality approaches. HAIM provides standardized methods for multi-modal data fusion, handles missing modalities gracefully, and enables systematic evaluation of how different data types contribute to model performance. Applications include integrated ICU monitoring systems, comprehensive cancer diagnosis combining radiology and pathology, and patient deterioration prediction using all available clinical data streams.\", \"used_in_bridge2ai\": false, \"references\": [\"https://doi.org/10.1038/s41746-022-00689-4\"]}]	[Holistic Multi-Modal Healthcare AI Integration]	[B2AI_APP:71]	[['https://doi.org/10.1038/s41746-022-00689-4']]	[HAIM (Holistic AI in Medicine) framework is used for developing and evaluating multi-modal AI systems that integrate diverse healthcare data types including imaging, time-series, tabular clinical data, and text for comprehensive patient assessment. The framework enables training of models that leverage complementary information across modalities to improve diagnostic accuracy, risk prediction, and clinical decision support beyond single-modality approaches. HAIM provides standardized methods for multi-modal data fusion, handles missing modalities gracefully, and enables systematic evaluation of how different data types contribute to model performance. Applications include integrated ICU monitoring systems, comprehensive cancer diagnosis combining radiology and pathology, and patient deterioration prediction using all available clinical data streams.]	[B2AI:Application]	[False]	doi:10.1038/s41746-022-00689-4						
B2AI_STANDARD:773	B2AI_STANDARD:SoftwareOrTool	HTSeq	HTSeq	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:13]	A Python library to facilitate the rapid development of high throughput sequencing data analysis scripts. HTSeq offers parsers for many common data formats in HTS projects, as well as classes to represent data, such as genomic coordinates, sequences, sequencing reads, alignments, gene model information and variant calls, and provides data structures that allow for querying via genomic coordinates.	True	False	https://github.com/htseq/htseq	https://github.com/htseq/htseq										doi:10.1093/bioinformatics/btu638						
B2AI_STANDARD:774	B2AI_STANDARD:SoftwareOrTool	ImJoy	ImJoy	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:15]	A plugin powered hybrid computing platform for deploying deep learning applications such as advanced image analysis tools. ImJoy runs on mobile and desktop environment cross different operating systems, plugins can run in the browser, localhost, remote and cloud servers.	True	False	https://imjoy.io/	https://github.com/imjoy-team/ImJoy										doi:10.1038/s41592-019-0627-0						
B2AI_STANDARD:775	B2AI_STANDARD:SoftwareOrTool	i2b2	Informatics for Integrating Biology and the Bedside platform	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:9]	A system for searching and exchanging clinical data.	True	False	https://www.i2b2.org/software/index.html	https://github.com/i2b2										doi:10.1093/jamia/ocv188				[B2AI_ORG:42]		
B2AI_STANDARD:776	B2AI_STANDARD:SoftwareOrTool	ITK	Insight Segmentation and Registration Toolkit	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:15]	The Insight Toolkit (ITK) is an open-source, cross-platform toolkit for N-dimensional scientific image processing, segmentation, and registration.	True	False	https://itk.org/	https://github.com/InsightSoftwareConsortium/ITK																
B2AI_STANDARD:777	B2AI_STANDARD:SoftwareOrTool	IMPatienT	Integrated digital Multimodal PATIENt daTa framework	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:9|B2AI_TOPIC:15]	A free and open-source web application to digitize, process and explore multimodal patient data. IMPatienT has a modular architecture, including four components to (i) create a standard vocabulary for a domain, (ii) digitize and process free-text data by mapping it to a set of standard terms, (iii) annotate images and perform image segmentation, and (iv) generate an automatic visualization dashboard to provide insight on the data and perform automatic diagnosis suggestions.	True	False	https://impatient.lbgi.fr	https://github.com/lambda-science/IMPatienT																
B2AI_STANDARD:778	B2AI_STANDARD:SoftwareOrTool	IMP	Integrative Modeling Platform	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:27]	An open source C++ and Python toolbox for solving complex modeling problems, and a number of applications for tackling some common problems in a user-friendly way. IMP can also be used from the Chimera molecular modeling system, or via one of several web applications.	True	False	https://integrativemodeling.org/	https://github.com/salilab/imp																
B2AI_STANDARD:779	B2AI_STANDARD:SoftwareOrTool	Jupyter	Jupyter Notebook	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[notebookplatform]	[B2AI_TOPIC:5]	A web-based notebook environment for interactive computing.	True	False	https://jupyter.org/	https://github.com/jupyter/notebook	True															
B2AI_STANDARD:780	B2AI_STANDARD:SoftwareOrTool	Kaldi	Kaldi Speech Recognition Toolkit	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:37]	Kaldi is an open-source toolkit for speech recognition research and development, written primarily in C++ with scripting interfaces, providing comprehensive implementations of state-of-the-art automatic speech recognition (ASR) algorithms including deep neural networks (DNNs), hidden Markov models (HMMs), Gaussian mixture models (GMMs), and modern end-to-end architectures. Developed by researchers at Johns Hopkins University and maintained by a global community, Kaldi emphasizes flexibility, efficiency, and reproducibility in speech recognition pipelines. The toolkit provides modular components for acoustic feature extraction (MFCCs, PLPs, filterbank energies, pitch features), acoustic modeling (DNN-HMM hybrid systems, time-delay neural networks TDNNs, chain models with lattice-free MMI training), language modeling (n-gram models, recurrent neural network language models), and decoding (weighted finite-state transducers WFSTs for efficient search). Kaldi supports both speaker-independent and speaker-adaptive recognition, with tools for vocal tract length normalization, feature-space maximum likelihood linear regression (fMLLR), and i-vector/x-vector speaker embeddings for robust recognition across diverse acoustic conditions. The toolkit includes recipes for training ASR systems on standard benchmarks (LibriSpeech, WSJ, Switchboard, Fisher, TED-LIUM, AMI, CHiME) with documented pipelines enabling researchers to replicate published results and adapt models to new datasets and languages. Applications span transcription of clinical conversations for medical documentation, analysis of patient-physician interactions, voice biomarker extraction for neurological disease monitoring (Parkinson's, ALS, dementia), mental health assessment through speech characteristics, and accessibility tools for hearing-impaired patients. Kaldi's extensive documentation, active mailing list, and well-tested recipes make it accessible for both research and production deployment despite its complexity, bridging traditional statistical ASR approaches with modern deep learning methods.	True	False	https://kaldi-asr.org/	https://github.com/kaldi-asr/kaldi																
B2AI_STANDARD:781	B2AI_STANDARD:SoftwareOrTool	Keepsake	Keepsake	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Version control for machine learning. A Python library that uploads files and metadata (like hyperparameters) to Amazon S3 or Google Cloud Storage.	True	False	https://keepsake.ai/	https://github.com/replicate/keepsake																
B2AI_STANDARD:782	B2AI_STANDARD:SoftwareOrTool	khmer	khmer	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	khmer is a free, open-source software library and suite of command-line tools for efficient analysis of high-throughput DNA sequencing data, including genomes, transcriptomes, metagenomes, and single-cell datasets. It implements advanced algorithms for probabilistic k-mer counting, digital normalization, compressible De Bruijn graph representation, and graph partitioning, enabling scalable de novo assembly, error correction, and data reduction. khmer is designed for use in UNIX environments and is supported by extensive documentation, community protocols, and integration with popular bioinformatics workflows. It is widely used for preprocessing and quality control in large-scale sequencing projects.	True	False	https://khmer.readthedocs.io/	https://github.com/dib-lab/khmer/										doi:10.12688/f1000research.6924.1	[B2AI_STANDARD:183]					
B2AI_STANDARD:783	B2AI_STANDARD:SoftwareOrTool	Koza	Koza data transformation framework	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Transform csv, json, yaml, jsonl, and xml and converting them to a target csv, json, or jsonl format based on your dataclass model. Koza also can output data in the KGX format. Write data transforms in semi-declarative Python. Configure source files, expected columns/json properties and path filters, field filters, and metadata in yaml. Create or import mapping files to be used in ingests (eg id mapping, type mappings). Create and use translation tables to map between source and target vocabularies.	True	False	https://github.com/monarch-initiative/koza	https://github.com/monarch-initiative/koza											[B2AI_STANDARD:346]		[B2AI_SUBSTRATE:6]	[B2AI_ORG:58]		
B2AI_STANDARD:784	B2AI_STANDARD:SoftwareOrTool	LIRICAL	LIkelihood Ratio Interpretation of Clinical AbnormaLities	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:7|B2AI_TOPIC:25|B2AI_TOPIC:35]	Performs phenotype-driven prioritization of candidate diseases and genes in the setting of genomic diagnostics (exome or genome) in which the phenotypic abnormalities are described as Human Phenotype Ontology (HPO) terms.	True	False	https://lirical.readthedocs.io/	https://github.com/TheJacksonLaboratory/LIRICAL										doi:10.1016/j.ajhg.2020.06.021	[B2AI_STANDARD:468]			[B2AI_ORG:58]		
B2AI_STANDARD:785	B2AI_STANDARD:SoftwareOrTool	Linode	Linode	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[cloudservice]	[B2AI_TOPIC:5]	Linode (now part of Akamai Connected Cloud) is a developer-focused cloud computing platform emphasizing simplicity, predictable pricing, and high performance for compute, storage, and networking services across a globally distributed infrastructure. Core compute offerings include Essential Compute instances (Shared CPU, Dedicated CPU, High Memory, and Premium CPU), GPU instances for parallel processing workloads including machine learning and scientific computing, and Accelerated Compute for specialized workloads. Container orchestration is provided through Linode Kubernetes Engine (LKE) for managed Kubernetes clusters, and App Platform for rapid cloud-native application deployment with automated infrastructure management. Storage solutions encompass Block Storage for scalable persistent volumes, Object Storage with S3-compatible API for unstructured data, and automated Backups for data protection. Managed Databases support MySQL and PostgreSQL with automated maintenance, backups, and high availability configurations. Networking features include NodeBalancers for load distribution, Cloud Firewall for security, DNS Manager, Private Networking (VLAN), and DDoS protection. Linode differentiates itself through transparent flat pricing bundling CPU, transfer, storage, and RAM without hidden egress fees or complex tier structures, making it particularly attractive for budget-conscious researchers and startups. The platform provides comprehensive API access, CLI tools, Terraform provider, and extensive documentation for programmatic infrastructure management. Following acquisition by Akamai in 2022, Linode benefits from integration with Akamai's global CDN network and security capabilities while maintaining its developer-friendly approach and competitive pricing. Linode's global presence includes data centers across North America, Europe, Asia-Pacific, and emerging markets, providing edge computing capabilities and reduced latency for distributed applications. The platform offers free DDoS protection, 24/7/365 support, and a Cloud Computing Foundations Certification program for developer education. While more streamlined than hyperscale competitors, Linode provides essential cloud services suitable for web applications, data pipelines, containerized workloads, and computational research without vendor lock-in through standard open-source technologies.	False	True	https://www.linode.com/																	
B2AI_STANDARD:786	B2AI_STANDARD:SoftwareOrTool	MAGIC	Markov Affinity-based Graph Imputation of Cells	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[scrnaseqanalysis]	[B2AI_TOPIC:34]	A method for imputing missing values in scRNA-seq data.	True	False	https://www.krishnaswamylab.org/projects/magic	https://github.com/KrishnaswamyLab/MAGIC										doi:10.1016/j.cell.2018.05.061						
B2AI_STANDARD:787	B2AI_STANDARD:SoftwareOrTool	Marquez	Marquez	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Marquez is an open source metadata service for the collection, aggregation, and visualization of a data ecosystem's metadata. It maintains the provenance of how datasets are consumed and produced, provides global visibility into job runtime and frequency of dataset access, centralization of dataset lifecycle management, and much more. Marquez was released and open sourced by WeWork.	True	False	https://lfaidata.foundation/projects/marquez/	https://github.com/MarquezProject/marquez																
B2AI_STANDARD:788	B2AI_STANDARD:SoftwareOrTool	MONAI	Medical Open Network for AI	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[machinelearningframework]	[B2AI_TOPIC:5]	MONAI is a PyTorch-based, open-source framework for deep learning in healthcare imaging, part of PyTorch Ecosystem.	True	False	https://monai.io/	https://github.com/Project-MONAI/MONAI			[{\"id\": \"B2AI_APP:72\", \"category\": \"B2AI:Application\", \"name\": \"Medical Imaging Deep Learning Framework and Workflows\", \"description\": \"MONAI (Medical Open Network for AI) is a specialized PyTorch-based framework extensively used for developing deep learning models on 3D medical imaging data across radiology, pathology, and microscopy. Researchers and clinical AI developers leverage MONAI's domain-specific transforms (intensity normalization, resampling, augmentation), pretrained models for medical imaging tasks, and optimized data loaders for large 3D volumes to accelerate development of segmentation, classification, and detection models. The framework provides standardized workflows for common medical imaging AI tasks, federated learning capabilities for multi-institutional collaboration, and Auto3DSeg for automated model development. MONAI's integration with clinical imaging standards and deployment tools makes it a bridge between research and clinical implementation.\", \"used_in_bridge2ai\": false}]	[Medical Imaging Deep Learning Framework and Workflows]	[B2AI_APP:72]		[MONAI (Medical Open Network for AI) is a specialized PyTorch-based framework extensively used for developing deep learning models on 3D medical imaging data across radiology, pathology, and microscopy. Researchers and clinical AI developers leverage MONAI's domain-specific transforms (intensity normalization, resampling, augmentation), pretrained models for medical imaging tasks, and optimized data loaders for large 3D volumes to accelerate development of segmentation, classification, and detection models. The framework provides standardized workflows for common medical imaging AI tasks, federated learning capabilities for multi-institutional collaboration, and Auto3DSeg for automated model development. MONAI's integration with clinical imaging standards and deployment tools makes it a bridge between research and clinical implementation.]	[B2AI:Application]	[False]		[B2AI_STANDARD:816]		[B2AI_SUBSTRATE:33]			
B2AI_STANDARD:789	B2AI_STANDARD:SoftwareOrTool	Metacat	Metacat	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Metacat is a unified metadata exploration API service. You can explore Hive, RDS, Teradata, Redshift, S3 and Cassandra. Metacat provides you information about what data you have, where it resides and how to process it.	True	False	https://github.com/Netflix/metacat	https://github.com/Netflix/metacat														[B2AI_ORG:65]		
B2AI_STANDARD:790	B2AI_STANDARD:SoftwareOrTool	Azure	Microsoft Azure	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[cloudservice]	[B2AI_TOPIC:5]	Microsoft Azure is a comprehensive enterprise-grade cloud computing platform offering 200+ services spanning compute, storage, databases, networking, AI/machine learning, analytics, IoT, and developer tools with deep integration into Microsoft's enterprise ecosystem. Core compute services include Azure Virtual Machines for Windows and Linux workloads, Azure App Service for web application hosting, Azure Kubernetes Service (AKS) for container orchestration, Azure Container Instances for serverless containers, and Azure Functions for event-driven serverless computing. For AI and machine learning, Azure provides Azure AI Studio (including Microsoft Foundry for AI agent development), Azure Machine Learning for end-to-end ML workflows, Azure Cognitive Services for pre-trained AI models (vision, speech, language), and Azure OpenAI Service for accessing GPT, Codex, and DALL-E models. Data and analytics capabilities include Azure Synapse Analytics for unified data warehousing and big data analytics, Azure Databricks for Apache Spark-based analytics, Azure Data Lake for scalable data storage, and Microsoft Fabric for unified data platform integration. Database services encompass Azure SQL Database for managed relational databases, Azure Cosmos DB for globally distributed NoSQL, Azure Database for PostgreSQL/MySQL/MariaDB, and Azure DocumentDB for MongoDB-compatible document storage. Healthcare and life sciences benefit from Azure Health Data Services for FHIR-based health data management, Azure Genomics for scalable genomics pipeline execution, Azure Healthcare APIs, and compliance with HIPAA, HITRUST, GxP, and FDA 21 CFR Part 11 regulations. Researchers leverage Azure Machine Learning for model training with GPU/FPGA acceleration, automated ML (AutoML) capabilities, MLOps for model lifecycle management, and integration with popular frameworks like TensorFlow, PyTorch, and scikit-learn. Azure's hybrid cloud capabilities through Azure Arc enable consistent management across on-premises, multi-cloud, and edge environments, crucial for institutions with existing infrastructure investments. Enterprise integration with Active Directory, Microsoft 365, Windows Server, SQL Server, and Visual Studio provides seamless authentication, collaboration, and development workflows. Azure's global infrastructure spans 60+ regions worldwide with data residency and compliance options for international regulations. Security features include Azure Security Center, Azure Sentinel for SIEM, Azure Active Directory for identity management, encryption at rest and in transit, and extensive compliance certifications across healthcare, government, and financial sectors.	False	True	https://azure.microsoft.com/															[B2AI_ORG:56]		
B2AI_STANDARD:791	B2AI_STANDARD:SoftwareOrTool	MinIO	MinIO	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[cloudservice]	[B2AI_TOPIC:5]	MinIO is a high-performance object storage system that implements the Amazon S3 API for cloud-native environments. The system is designed as software-defined storage and provides S3-compatible operations with reported latency under 10ms and high throughput capabilities. MinIO is designed to run on Kubernetes and can be deployed across public cloud platforms, private cloud infrastructure, and edge environments. The architecture uses a distributed design that supports scaling beyond traditional storage system limits, with deployment capabilities from single servers to exabyte-scale installations using a single namespace. The platform includes automated data management features such as self-healing, load balancing, and data protection with erasure coding. Security features include encryption at rest and in transit, identity and access management integration, and policy-based access controls. MinIO is used in AI and machine learning workflows, providing storage integration with frameworks like PyTorch and data lakehouse technologies including Apache Iceberg. The system supports data lakehouse analytics engines such as Apache Spark and Trino for structured and unstructured data processing. Client libraries are available for multiple programming languages including Go, Python, Java, .NET, Rust, and JavaScript. Associated tools include mc (command-line client for object operations), DirectPV (a Kubernetes CSI driver), and a Kubernetes operator for cluster deployment and management. The source code is released under the GNU AGPLv3 license. The project reports over 2 billion downloads and maintains an active development community with more than 50,000 stars on GitHub.	True	False	https://min.io/	https://github.com/minio/																
B2AI_STANDARD:792	B2AI_STANDARD:SoftwareOrTool	MLMD	ML Metadata	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	ML Metadata (MLMD) is a library for recording and retrieving metadata associated with ML developer and data scientist workflows.	True	False	https://www.tensorflow.org/tfx/guide/mlmd	https://github.com/google/ml-metadata														[B2AI_ORG:37]		
B2AI_STANDARD:793	B2AI_STANDARD:SoftwareOrTool	MLflow	MLflow platform	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	MLflow is an open source platform to manage the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry	True	False	https://mlflow.org/	https://github.com/mlflow/mlflow/																
B2AI_STANDARD:794	B2AI_STANDARD:SoftwareOrTool	MLPro	MLPro framework	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[machinelearningframework]	[B2AI_TOPIC:5]	MLPro (Machine Learning Professional) is a middleware framework for standardized machine learning development in Python. The framework provides an object-oriented architecture that enables the integration and standardization of multiple machine learning paradigms through reusable process models and templates. MLPro consists of several sub-frameworks organized as modules: MLPro-BF (Basic Functions) provides cross-sectional infrastructure including mathematics, data management, plotting capabilities, and logging; MLPro-SL supports supervised learning workflows; MLPro-RL implements reinforcement learning processes; MLPro-GT covers game theory applications; and MLPro-OA focuses on online adaptive machine learning. The architecture uses standardized interfaces that allow ML models to be embedded into training pipelines and operational workflows in a modular fashion. The framework integrates with external machine learning libraries including River (for online learning), scikit-learn (for supervised learning), and OpenML (for dataset and experiment management), providing wrappers that standardize their usage within MLPro processes. Development follows object-oriented design principles with test-driven development for quality assurance and continuous integration/deployment (CI/CD) practices. The framework includes an extension hub for third-party integrations, comprehensive documentation with API references, and an example pool demonstrating various use cases. MLPro supports hybrid ML applications that combine multiple learning paradigms and real-time adaptive systems. The project is developed and maintained by the Group for Automation Technology and Learning Systems at South Westphalia University of Applied Sciences in Germany. The source code is released under the Apache 2.0 license and is available through PyPI for installation.	True	False	https://mlpro.readthedocs.io/	https://github.com/fhswf/MLPro			[{\"id\": \"B2AI_APP:73\", \"category\": \"B2AI:Application\", \"name\": \"Reinforcement Learning Framework for Adaptive Healthcare Systems\", \"description\": \"MLPro is used in biomedical AI for developing reinforcement learning agents that learn optimal treatment strategies, adaptive monitoring protocols, and personalized intervention policies through interaction with healthcare environments. Researchers leverage MLPro's modular framework to implement and compare different RL algorithms for applications like dynamic treatment regimen optimization, adaptive clinical trial design, and automated ICU management. The platform supports simulation-based learning where agents train on synthetic patients before clinical deployment, multi-agent systems for coordinated care, and safe exploration strategies essential for healthcare applications. MLPro enables systematic evaluation of RL approaches for sequential decision-making problems in medicine where actions have long-term consequences.\", \"used_in_bridge2ai\": false}]	[Reinforcement Learning Framework for Adaptive Healthcare Systems]	[B2AI_APP:73]		[MLPro is used in biomedical AI for developing reinforcement learning agents that learn optimal treatment strategies, adaptive monitoring protocols, and personalized intervention policies through interaction with healthcare environments. Researchers leverage MLPro's modular framework to implement and compare different RL algorithms for applications like dynamic treatment regimen optimization, adaptive clinical trial design, and automated ICU management. The platform supports simulation-based learning where agents train on synthetic patients before clinical deployment, multi-agent systems for coordinated care, and safe exploration strategies essential for healthcare applications. MLPro enables systematic evaluation of RL approaches for sequential decision-making problems in medicine where actions have long-term consequences.]	[B2AI:Application]	[False]	doi:10.1016/j.simpa.2022.100421						
B2AI_STANDARD:795	B2AI_STANDARD:SoftwareOrTool	MoClust	MoClust	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[scrnaseqanalysis]	[B2AI_TOPIC:23]	A novel joint clustering framework that can be applied to several types of single-cell multi-omics data. A selective automatic doublet detection module that can identify and filter out doublets is introduced in the pretraining stage to improve data quality. Omics-specific autoencoders are introduced to characterize the multi-omics data.	True	False	https://zenodo.org/record/7306504											doi:10.1093/bioinformatics/btac736						
B2AI_STANDARD:796	B2AI_STANDARD:SoftwareOrTool	MCT	Model Card Toolkit	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[modelcards]	[B2AI_TOPIC:5]	The Model Card Toolkit (MCT) streamlines and automates generation of Model Cards, machine learning documents that provide context and transparency into a model's development and performance.	True	False	https://github.com/tensorflow/model-card-toolkit	https://github.com/tensorflow/model-card-toolkit										doi:10.48550/arXiv.1810.03993				[B2AI_ORG:37]		
B2AI_STANDARD:797	B2AI_STANDARD:SoftwareOrTool	MongoDB	MongoDB	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A non-relational document database that provides support for JSON-like storage.	True	False	https://www.mongodb.com/	https://github.com/mongodb/mongo													[B2AI_SUBSTRATE:13|B2AI_SUBSTRATE:22|B2AI_SUBSTRATE:9]			
B2AI_STANDARD:798	B2AI_STANDARD:SoftwareOrTool	Monocle2	Monocle 2	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:21]	An algorithm that uses reversed graph embedding to describe multiple fate decisions in a fully unsupervised manner.	True	False	https://github.com/cole-trapnell-lab/monocle-release	https://github.com/cole-trapnell-lab/monocle-release										doi:10.1038/nmeth.4402						
B2AI_STANDARD:799	B2AI_STANDARD:SoftwareOrTool	MIMaL	Multi-Omic Integration by Machine Learning	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[multimodal]	[B2AI_TOPIC:23]	MIMaL is a new method for integrating multiomic data using SHAP model explanations.	True	False	https://mimal.app/	https://github.com/jessegmeyerlab/MIMaL										doi:10.1093/bioinformatics/btac631						
B2AI_STANDARD:800	B2AI_STANDARD:SoftwareOrTool	MuSIC	Multi-Scale Integrated Cell pipeline	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[multimodal]	[B2AI_TOPIC:26|B2AI_TOPIC:28]	MuSIC is a hierarchical map of human cell architecture created from integrating immunofluorescence images in the Human Protein Atlas with affinity purification experiments from the BioPlex resource. Integration involves configuring each approach to produce a general measure of protein distance, then calibrating the two measures using machine learning.	True	False	https://nrnb.org/music/	https://github.com/idekerlab/MuSIC	True		[{\"id\": \"B2AI_APP:74\", \"category\": \"B2AI:Application\", \"name\": \"Multi-Scale Cell Type Annotation and Classification\", \"description\": \"MuSIC (Multi-Scale Integrated Cell classification) is used in AI applications for automated cell type annotation in single-cell RNA-seq data by leveraging reference datasets and hierarchical classification strategies. The tool employs machine learning algorithms that integrate multiple sources of evidence including marker genes, reference atlases, and cross-dataset mapping to assign cell type labels with confidence scores. AI systems build upon MuSIC's probabilistic framework to develop more sophisticated deep learning models for cell state identification, rare cell type discovery, and cross-species cell type mapping. The multi-scale approach enables AI models to make predictions at varying levels of granularity, from broad cell classes to fine-grained subtypes, which is essential for biological interpretation and downstream analysis.\", \"used_in_bridge2ai\": false}]	[Multi-Scale Cell Type Annotation and Classification]	[B2AI_APP:74]		[MuSIC (Multi-Scale Integrated Cell classification) is used in AI applications for automated cell type annotation in single-cell RNA-seq data by leveraging reference datasets and hierarchical classification strategies. The tool employs machine learning algorithms that integrate multiple sources of evidence including marker genes, reference atlases, and cross-dataset mapping to assign cell type labels with confidence scores. AI systems build upon MuSIC's probabilistic framework to develop more sophisticated deep learning models for cell state identification, rare cell type discovery, and cross-species cell type mapping. The multi-scale approach enables AI models to make predictions at varying levels of granularity, from broad cell classes to fine-grained subtypes, which is essential for biological interpretation and downstream analysis.]	[B2AI:Application]	[False]	doi:10.1038/s41586-021-04115-9				[B2AI_ORG:116]		
B2AI_STANDARD:801	B2AI_STANDARD:SoftwareOrTool	MySQL	MySQL	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A relational database management system developed by Oracle that is based on structured query language (SQL).	True	False	https://www.mysql.com/	https://github.com/mysql/mysql-server													[B2AI_SUBSTRATE:23|B2AI_SUBSTRATE:37|B2AI_SUBSTRATE:9]			
B2AI_STANDARD:802	B2AI_STANDARD:SoftwareOrTool	Neo4j	Neo4j Graph Data Platform	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[graphdataplatform]	[B2AI_TOPIC:21]	Neo4j is a native graph database platform implementing property graph model with ACID-compliant transactions, designed for storing and querying highly connected data through nodes, relationships, and properties, enabling efficient traversal of complex relationship patterns at scale. Developed by Neo4j, Inc. and available in both open-source Community Edition and commercial Enterprise Edition, Neo4j uses the declarative Cypher query language allowing pattern-matching queries that naturally express graph relationships (e.g., `MATCH (person:Person)-[:FRIENDS_WITH]->(friend) RETURN person, friend`) without complex join operations required in relational databases. The architecture employs index-free adjacency where each node directly references its adjacent nodes enabling constant-time traversals regardless of graph size, native graph storage optimized for relationship-heavy queries, and clustered deployment supporting high availability, horizontal scaling, and causal consistency across distributed systems. Key features include rich graph algorithms library (Graph Data Science) implementing PageRank, community detection, shortest path, centrality measures, and similarity algorithms; APOC (Awesome Procedures on Cypher) extending functionality with graph refactoring, data integration, and advanced algorithms; support for temporal queries and multiple graph projections; and integration with analytics tools (Apache Spark, Python data science stack) and machine learning frameworks. Biomedical and AI applications span knowledge graphs integrating biomedical ontologies, drug databases, genomic data, and literature for drug repurposing and target discovery; clinical decision support modeling patient-symptom-disease-treatment relationships for diagnosis recommendation; biological network analysis representing protein-protein interactions, metabolic pathways, gene regulatory networks for systems biology research; healthcare interoperability mapping relationships between FHIR resources, HL7 messages, and clinical terminologies; and ML feature engineering extracting graph embeddings, network features, and relationship patterns as input for predictive models. Neo4j enables real-time recommendation systems, fraud detection networks, identity and access management, supply chain optimization, and social network analysis, essential for data scientists working with connected data, bioinformaticians building biological knowledge bases, clinical informaticians developing patient care pathways, and ML engineers requiring graph-structured features.	True	False	https://neo4j.com/	https://github.com/neo4j/neo4j													[B2AI_SUBSTRATE:14|B2AI_SUBSTRATE:15|B2AI_SUBSTRATE:25|B2AI_SUBSTRATE:9]			
B2AI_STANDARD:803	B2AI_STANDARD:SoftwareOrTool	NETME	NETME	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:16]	Starting from a set of fulltext obtained from PubMed, through an easy-to-use web interface, interactively extracts a group of biological elements stored into a selected list of ontological databases and then synthesizes a network with inferred relations among such elements.	True	False	https://netme.click/#/											doi:10.1007/S41109-021-00435-X						
B2AI_STANDARD:804	B2AI_STANDARD:SoftwareOrTool	NetSeekR	NetSeekR network analysis R package	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[scrnaseqanalysis]	[B2AI_TOPIC:34]	A network analysis pipeline for RNA-Seq time series data.	True	False	https://github.com/igbb-popescu-lab/NetSeekR	https://github.com/igbb-popescu-lab/NetSeekR										doi:10.1186/S12859-021-04554-1						
B2AI_STANDARD:805	B2AI_STANDARD:SoftwareOrTool	NDEx	Network Data Exchange	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[graphdataplatform]	[B2AI_TOPIC:21]	The NDEx Project provides an open-source framework where scientists and organizations can store, share, manipulate, and publish biological network knowledge.	True	True	https://www.ndexbio.org/																	
B2AI_STANDARD:806	B2AI_STANDARD:SoftwareOrTool	NeuCA	NeuCA - Neural-network based Cell Annotation tool	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[scrnaseqanalysis]	[B2AI_TOPIC:34]	A R/Bioconductor tool for cell type annotation using single-cell RNA-seq data. It is a supervised cell label assignment method that uses existing scRNA-seq data with known labels to train a neural network-based classifier, and then predict cell labels in single-cell RNA-seq data of interest.	True	False	https://github.com/haoharryfeng/NeuCA	https://github.com/haoharryfeng/NeuCA			[{\"id\": \"B2AI_APP:75\", \"category\": \"B2AI:Application\", \"name\": \"Cell Type Annotation in Single-Cell Genomics\", \"description\": \"NeuCA (Neural network-based Cell Annotation) is used in AI applications for automated cell type identification in single-cell RNA sequencing data, leveraging deep learning to achieve accurate, scalable annotation across diverse tissues and species. The tool employs neural network architectures optimized for single-cell expression profiles to classify cells based on marker gene expression patterns, enabling rapid analysis of large-scale atlas projects and clinical samples. NeuCA's transfer learning capabilities allow models trained on reference atlases to annotate new datasets with limited manual curation, supporting applications in cancer cell identification, immune profiling, and developmental biology. The method provides confidence scores for cell type assignments and can identify novel or transitional cell states.\", \"used_in_bridge2ai\": false}]	[Cell Type Annotation in Single-Cell Genomics]	[B2AI_APP:75]		[NeuCA (Neural network-based Cell Annotation) is used in AI applications for automated cell type identification in single-cell RNA sequencing data, leveraging deep learning to achieve accurate, scalable annotation across diverse tissues and species. The tool employs neural network architectures optimized for single-cell expression profiles to classify cells based on marker gene expression patterns, enabling rapid analysis of large-scale atlas projects and clinical samples. NeuCA's transfer learning capabilities allow models trained on reference atlases to annotate new datasets with limited manual curation, supporting applications in cancer cell identification, immune profiling, and developmental biology. The method provides confidence scores for cell type assignments and can identify novel or transitional cell states.]	[B2AI:Application]	[False]	doi:10.1038/s41598-021-04473-4						
B2AI_STANDARD:807	B2AI_STANDARD:SoftwareOrTool	Nextflow	Nextflow	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:20]	Enables scalable and reproducible scientific workflows using software containers. It allows the adaptation of pipelines written in the most common scripting languages.	True	False	https://www.nextflow.io/	https://github.com/nextflow-io/nextflow														[B2AI_ORG:90]		
B2AI_STANDARD:808	B2AI_STANDARD:SoftwareOrTool	AnVIL	NHGRI Analysis Visualization and Informatics Lab-space	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[cloudplatform]	[B2AI_TOPIC:13]	AnVIL is NHGRI's Genomic Data Science Analysis, Visualization, and Informatics Lab-Space.	True	True	https://anvilproject.org/	https://github.com/anvilproject														[B2AI_ORG:73]		
B2AI_STANDARD:809	B2AI_STANDARD:SoftwareOrTool	OmicsEV	OmicsEV	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:23]	An R package for quality evaluation of omics data tables. For each data table, OmicsEV uses a series of methods to evaluate data depth, data normalization, batch effect, biological signal, platform reproducibility, and multi-omics concordance, producing comprehensive visual and quantitative evaluation results that help assess data quality of individual data tables and facilitate the identification of the optimal data processing method and parameters for the omics study under investigation.	True	False	https://github.com/bzhanglab/OmicsEV	https://github.com/bzhanglab/OmicsEV										doi:10.1093/bioinformatics/btac698						
B2AI_STANDARD:810	B2AI_STANDARD:SoftwareOrTool	OAK	Ontology Access Kit	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	OAK provides a collection of interfaces for various ontology operations.	True	False	https://incatools.github.io/ontology-access-kit/	https://github.com/INCATools/ontology-access-kit														[B2AI_ORG:58]		
B2AI_STANDARD:811	B2AI_STANDARD:SoftwareOrTool	ODK	Ontology Development Kit	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A toolkit and workflow system for managing the ontology life-cycle.	True	False	https://github.com/INCATools/ontology-development-kit	https://github.com/INCATools/ontology-development-kit										doi:10.1093/database/baac087				[B2AI_ORG:58]		
B2AI_STANDARD:812	B2AI_STANDARD:SoftwareOrTool	OpenHealth	OpenHealth	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:18]	An open-source platform for wearable health monitoring. It aims to design a standard set of hardware/software and wearable devices that can enable autonomous collection of clinically relevant data.	True	False	https://sites.google.com/view/openhealth-wearable-health/home											doi:10.1109/MDAT.2019.2906110						
B2AI_STANDARD:813	B2AI_STANDARD:SoftwareOrTool	pandas	pandas	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	An open source data analysis and manipulation tool built on top of the Python programming language.	True	False	https://pandas.pydata.org/														[B2AI_SUBSTRATE:29|B2AI_SUBSTRATE:8]			
B2AI_STANDARD:814	B2AI_STANDARD:SoftwareOrTool	Panel	Panel	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datavisualization]	[B2AI_TOPIC:5]	An open-source Python library that lets you create custom interactive web apps and dashboards by connecting user-defined widgets to plots, images, tables, or text.	True	False	https://panel.holoviz.org/	https://github.com/holoviz/panel																
B2AI_STANDARD:815	B2AI_STANDARD:SoftwareOrTool	PostgreSQL	PostgreSQL	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	An open source object-relational database system.	True	False	https://www.postgresql.org/	https://github.com/postgres/postgres													[B2AI_SUBSTRATE:31|B2AI_SUBSTRATE:37|B2AI_SUBSTRATE:9]			
B2AI_STANDARD:816	B2AI_STANDARD:SoftwareOrTool	PyTorch	PyTorch	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[machinelearningframework]	[B2AI_TOPIC:5]	A popular machine learning platform.	True	False	https://pytorch.org/	https://github.com/pytorch/pytorch			[{\"id\": \"B2AI_APP:76\", \"category\": \"B2AI:Application\", \"name\": \"Deep Learning for Biomedical Research and Clinical AI\", \"description\": \"PyTorch is the dominant framework for biomedical AI research, used extensively for developing deep learning models across medical imaging, genomics, drug discovery, and clinical prediction. Researchers leverage PyTorch's dynamic computational graphs, extensive ecosystem (torchvision, torchaudio, TorchIO), and pretrained models to build custom neural architectures for tasks like cancer detection in pathology images, protein structure prediction, medical report generation, and patient outcome forecasting. PyTorch's flexibility enables rapid prototyping of novel architectures, its strong academic community supports reproducible research through shared model implementations, and its production deployment tools (TorchServe, TorchScript) facilitate clinical translation of research models.\", \"used_in_bridge2ai\": false}]	[Deep Learning for Biomedical Research and Clinical AI]	[B2AI_APP:76]		[PyTorch is the dominant framework for biomedical AI research, used extensively for developing deep learning models across medical imaging, genomics, drug discovery, and clinical prediction. Researchers leverage PyTorch's dynamic computational graphs, extensive ecosystem (torchvision, torchaudio, TorchIO), and pretrained models to build custom neural architectures for tasks like cancer detection in pathology images, protein structure prediction, medical report generation, and patient outcome forecasting. PyTorch's flexibility enables rapid prototyping of novel architectures, its strong academic community supports reproducible research through shared model implementations, and its production deployment tools (TorchServe, TorchScript) facilitate clinical translation of research models.]	[B2AI:Application]	[False]		[B2AI_STANDARD:354]		[B2AI_SUBSTRATE:33]			
B2AI_STANDARD:817	B2AI_STANDARD:SoftwareOrTool	Quarto	Quarto publishing system	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[notebookplatform]	[B2AI_TOPIC:5]	An open-source scientific and technical publishing system built on Pandoc.	True	False	https://quarto.org/	https://github.com/quarto-dev/quarto-cli																
B2AI_STANDARD:818	B2AI_STANDARD:SoftwareOrTool	RDCA-DAP	Rare Disease Cures Accelerator-Data and Analytics Platform	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[cloudplatform]	[B2AI_TOPIC:4]	The Rare Disease Cures Accelerator-Data and Analytics Platform (RDCA-DAP) is an FDA-funded initiative that provides a centralized and standardized infrastructure to support and accelerate rare disease characterization, with the goal of accelerating therapy development across rare diseases.	False	True	https://c-path.org/programs/rdca-dap/															[B2AI_ORG:31]		
B2AI_STANDARD:819	B2AI_STANDARD:SoftwareOrTool	refget	refget API	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:13]	Enables access to reference genomic sequences without ambiguity from different databases and servers using a checksum identifier based on the sequence content itself.	True	False	https://samtools.github.io/hts-specs/refget.html	https://samtools.github.io/hts-specs/refget.html														[B2AI_ORG:34]		
B2AI_STANDARD:820	B2AI_STANDARD:SoftwareOrTool	Relexi	Relexi	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[machinelearningframework]	[B2AI_TOPIC:5]	Relexi is an open source reinforcement learning (RL) framework written in Python and based on TensorFlow's RL library TF-Agents. Relexi allows to employ RL for environments that require computationally intensive simulations like applications in computational fluid dynamics. For this, Relexi couples legacy simulation codes with the RL library TF-Agents at scale on modern high-performance computing (HPC) hardware using the SmartSim library. Relexi thus provides an easy way to explore the potential of RL for HPC applications.	True	False	https://github.com/flexi-framework/relexi	https://github.com/flexi-framework/relexi			[{\"id\": \"B2AI_APP:77\", \"category\": \"B2AI:Application\", \"name\": \"Explainable Reinforcement Learning for Clinical Decision Support\", \"description\": \"Relexi is used in biomedical AI for developing interpretable reinforcement learning systems that can explain their decision-making process, crucial for clinical applications where treatment recommendations must be understandable to physicians. The framework enables training of RL agents for sequential clinical decisions (medication dosing, ventilator management, treatment timing) while maintaining explainability through attention mechanisms and policy distillation. Relexi supports safe exploration in healthcare settings by incorporating domain constraints and enabling clinicians to understand why specific actions are recommended. Applications include explainable sepsis treatment protocols, interpretable insulin dosing algorithms, and transparent clinical trial enrollment strategies where understanding the agent's reasoning is essential for clinical trust and regulatory approval.\", \"used_in_bridge2ai\": false}]	[Explainable Reinforcement Learning for Clinical Decision Support]	[B2AI_APP:77]		[Relexi is used in biomedical AI for developing interpretable reinforcement learning systems that can explain their decision-making process, crucial for clinical applications where treatment recommendations must be understandable to physicians. The framework enables training of RL agents for sequential clinical decisions (medication dosing, ventilator management, treatment timing) while maintaining explainability through attention mechanisms and policy distillation. Relexi supports safe exploration in healthcare settings by incorporating domain constraints and enabling clinicians to understand why specific actions are recommended. Applications include explainable sepsis treatment protocols, interpretable insulin dosing algorithms, and transparent clinical trial enrollment strategies where understanding the agent's reasoning is essential for clinical trust and regulatory approval.]	[B2AI:Application]	[False]	doi:10.1016/j.simpa.2022.100422						
B2AI_STANDARD:821	B2AI_STANDARD:SoftwareOrTool	REDCap	Research Electronic Data Capture	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:31]	Electronic data capture software and workflow methodology for designing clinical and translational research databases.	True	True	https://www.project-redcap.org/		True		[{\"id\": \"B2AI_APP:78\", \"category\": \"B2AI:Application\", \"name\": \"Clinical Research Data Capture for ML Model Development\", \"description\": \"REDCap is widely used in AI applications as a platform for collecting high-quality, structured clinical research data that feeds machine learning model development and validation studies. AI researchers leverage REDCap's data dictionaries, validation rules, and standardized data collection instruments to create clean training datasets for predictive models in clinical trials, cohort studies, and patient registries. REDCap's API enables automated data extraction for ML pipelines, and its audit trails and data quality features ensure reproducibility in AI research. The platform is particularly valuable for multi-site AI studies where standardized data collection across institutions is essential for model generalizability.\", \"used_in_bridge2ai\": false}]	[Clinical Research Data Capture for ML Model Development]	[B2AI_APP:78]		[REDCap is widely used in AI applications as a platform for collecting high-quality, structured clinical research data that feeds machine learning model development and validation studies. AI researchers leverage REDCap's data dictionaries, validation rules, and standardized data collection instruments to create clean training datasets for predictive models in clinical trials, cohort studies, and patient registries. REDCap's API enables automated data extraction for ML pipelines, and its audit trails and data quality features ensure reproducibility in AI research. The platform is particularly valuable for multi-site AI studies where standardized data collection across institutions is essential for model generalizability.]	[B2AI:Application]	[False]	doi:10.1016/j.jbi.2008.08.010				[B2AI_ORG:114|B2AI_ORG:117]		
B2AI_STANDARD:822	B2AI_STANDARD:SoftwareOrTool	Synapse	Sage Synapse	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Synapse is a collaborative research platform developed by Sage Bionetworks that provides web services and tools for aggregating, organizing, analyzing, and sharing scientific data, code, and insights across biomedical research communities. The platform implements a comprehensive data management infrastructure through its Repository Services, offering RESTful JSON APIs for entity management, file storage, versioning, and access control. Core functionality includes hierarchical project organization with entities such as Projects, Folders, Files, Tables, and Views that support structured data organization and metadata annotation. Synapse provides advanced data access control through Access Requirements and Access Approvals, enabling researchers to implement controlled access policies including ACTAccessRequirement for managed access to sensitive data. The platform supports collaborative research through Teams, subscription services for change notifications, messaging capabilities, and wiki documentation integrated with research artifacts. Data governance features include a Qualified Research Program that balances broad researcher access with participant protections, research project management for data access requests, and submission workflows for access review by the Access and Compliance Team (ACT). Technical capabilities include multi-part file uploads, version tracking with provenance through Activity records, search indexing using OpenSearch for entity discovery, DOI minting for permanent identifiers, and integration with OAuth2 authentication and OpenID Connect. Synapse implements the GA4GH DRS (Data Repository Service) API specification for standardized data access, supports tabular data through Table entities with SQL-like querying, and provides form-based data collection with review workflows. The platform serves as infrastructure for open science initiatives, enabling data sharing while encoding contractual protections for research participants, and has been deployed in numerous biomedical studies including mHealth research such as the mPower Parkinson disease observational study.	True	True	https://www.synapse.org/	https://github.com/Sage-Bionetworks/Synapse-Repository-Services	True									doi:10.2139/ssrn.3502410				[B2AI_ORG:86]		
B2AI_STANDARD:823	B2AI_STANDARD:SoftwareOrTool	SemEHR	SemEHR	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:9]	An open source semantic search and analytics tool for EHRs.	True	False	https://github.com/CogStack/CogStack-SemEHR	https://github.com/CogStack/CogStack-SemEHR										doi:10.1093/jamia/ocx160						
B2AI_STANDARD:824	B2AI_STANDARD:SoftwareOrTool	SAVER	Single-cell Analysis Via Expression Recovery	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[scrnaseqanalysis]	[B2AI_TOPIC:34]	A regularized regression prediction and empirical Bayes method to recover the true gene expression profile in noisy and sparse scRNA-seq data.	True	False	https://mohuangx.github.io/SAVER/	https://github.com/mohuangx/SAVER										doi:10.1038/s41592-018-0033-z						
B2AI_STANDARD:825	B2AI_STANDARD:SoftwareOrTool	Snorkel	Snorkel	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[deprecated]	[B2AI_TOPIC:5]	Snorkel is a platform for automated data labeling. It has since been extended into a full machine learning platform (see https://snorkel.ai/).	False	True	https://www.snorkel.org/	https://github.com/snorkel-team/snorkel																
B2AI_STANDARD:826	B2AI_STANDARD:SoftwareOrTool	SnpEff	SnpEff	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:25|B2AI_TOPIC:35]	SnpEff is a variant annotation and effect prediction tool. It annotates and predicts the effects of genetic variants (such as amino acid changes).	True	False	https://pcingola.github.io/SnpEff/se_introduction/	https://github.com/pcingola/SnpEff										doi:10.4161/fly.19695						
B2AI_STANDARD:827	B2AI_STANDARD:SoftwareOrTool	Souporcell	Souporcell	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[scrnaseqanalysis]	[B2AI_TOPIC:34]	A method to cluster cells using the genetic variants detected within the scRNA-seq reads.	True	False	https://github.com/wheaton5/souporcell	https://github.com/wheaton5/souporcell										doi:10.1038/s41592-020-0820-1						
B2AI_STANDARD:828	B2AI_STANDARD:SoftwareOrTool	STAR	Spliced Transcripts Alignment to a Reference	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:33]	Software based on an RNA-seq alignment algorithm that uses sequential maximum mappable seed search in uncompressed suffix arrays followed by seed clustering and stitching procedure.	True	False	https://github.com/alexdobin/STAR	https://github.com/alexdobin/STAR										doi:10.1093/bioinformatics/bts635						
B2AI_STANDARD:829	B2AI_STANDARD:SoftwareOrTool	SyBLaRS	Systems Biology Layout and Rendering Service	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A web service for automatic layout of biological data in various standard formats as well as construction of customized images in both raster image and scalable vector formats of these maps. Some of the supported standards are more generic such as GraphML and JSON, whereas others are specialized to biology such as SBGNML (The Systems Biology Graphical Notation Markup Language) and SBML (The Systems Biology Markup Language).	True	False	http://syblars.cs.bilkent.edu.tr/	https://github.com/iVis-at-Bilkent/syblars										doi:10.1371/journal.pcbi.1010635	[B2AI_STANDARD:337|B2AI_STANDARD:289]		[B2AI_SUBSTRATE:36]			
B2AI_STANDARD:830	B2AI_STANDARD:SoftwareOrTool	TES	Task Execution Service	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	The Task Execution Service (TES) API is a proposed standard for describing and executing tasks in a platform-agnostic way. Includes a TES API validator and a Task Execution API specification.	True	False	https://ga4gh.github.io/task-execution-schemas/docs/															[B2AI_ORG:34]		
B2AI_STANDARD:831	B2AI_STANDARD:SoftwareOrTool	TF	Tensorflow	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[machinelearningframework]	[B2AI_TOPIC:5]	TensorFlow is an end-to-end open source platform for machine learning. It has an ecosystem of tools, libraries and community resources that lets researchers and developers easily build and deploy ML powered applications.	True	False	https://www.tensorflow.org/	https://github.com/tensorflow/tensorflow			[{\"id\": \"B2AI_APP:79\", \"category\": \"B2AI:Application\", \"name\": \"Production-Scale Healthcare AI Systems and Deployment\", \"description\": \"TensorFlow is widely used for deploying production-grade AI systems in healthcare, particularly for applications requiring high-throughput inference, mobile deployment, and integration with enterprise IT infrastructure. Healthcare organizations leverage TensorFlow's mature ecosystem (TF Serving, TF Lite, TF.js) to deploy models for real-time clinical decision support, mobile diagnostic apps, and edge computing in medical devices. The framework's strong support for model optimization, quantization, and hardware acceleration (TPUs, GPUs) enables efficient deployment of complex models like retinal disease screening systems, ECG interpretation algorithms, and clinical NLP pipelines. TensorFlow Extended (TFX) provides production ML pipelines for managing data validation, model training, and continuous monitoring in regulated healthcare environments.\", \"used_in_bridge2ai\": false, \"references\": [\"https://www.tensorflow.org/tfx\"]}]	[Production-Scale Healthcare AI Systems and Deployment]	[B2AI_APP:79]	[['https://www.tensorflow.org/tfx']]	[TensorFlow is widely used for deploying production-grade AI systems in healthcare, particularly for applications requiring high-throughput inference, mobile deployment, and integration with enterprise IT infrastructure. Healthcare organizations leverage TensorFlow's mature ecosystem (TF Serving, TF Lite, TF.js) to deploy models for real-time clinical decision support, mobile diagnostic apps, and edge computing in medical devices. The framework's strong support for model optimization, quantization, and hardware acceleration (TPUs, GPUs) enables efficient deployment of complex models like retinal disease screening systems, ECG interpretation algorithms, and clinical NLP pipelines. TensorFlow Extended (TFX) provides production ML pipelines for managing data validation, model training, and continuous monitoring in regulated healthcare environments.]	[B2AI:Application]	[False]		[B2AI_STANDARD:354]		[B2AI_SUBSTRATE:42]	[B2AI_ORG:37]		
B2AI_STANDARD:832	B2AI_STANDARD:SoftwareOrTool	Terra	Terra Community Workbench	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[cloudplatform]	[B2AI_TOPIC:20]	Terra is a cloud-native platform for biomedical researchers to access data, run analysis tools, and collaborate.	True	True	https://app.terra.bio/	https://github.com/DataBiosphere/terra-ui														[B2AI_ORG:71]		
B2AI_STANDARD:833	B2AI_STANDARD:SoftwareOrTool	R	The R Project for Statistical Computing	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A free software environment for statistical computing and graphics.	True	False	https://www.r-project.org/		True												[B2AI_SUBSTRATE:34|B2AI_SUBSTRATE:35|B2AI_SUBSTRATE:40]			
B2AI_STANDARD:834	B2AI_STANDARD:SoftwareOrTool	Theano	Theano	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[deprecated|machinelearningframework]	[B2AI_TOPIC:5]	A Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. It is being continued as aesara.	True	False	https://github.com/Theano/Theano	https://github.com/Theano/Theano			[{\"id\": \"B2AI_APP:80\", \"category\": \"B2AI:Application\", \"name\": \"Legacy Deep Learning Models and Reproducible Research\", \"description\": \"Theano was historically important in early biomedical deep learning research and continues to be relevant for reproducing published models and maintaining legacy clinical AI systems. Many influential early papers in medical AI used Theano, and researchers still need to run these models for comparison benchmarks, reproduce published results, and maintain systems deployed before modern frameworks emerged. The library's symbolic computation approach and automatic differentiation influenced the design of current frameworks, and understanding Theano remains valuable for historical context in AI research. While active development has ceased (succeeded by Aesara), Theano-based code remains in production in some clinical settings and research archives.\", \"used_in_bridge2ai\": false, \"references\": [\"https://theano-pymc.readthedocs.io/\"]}]	[Legacy Deep Learning Models and Reproducible Research]	[B2AI_APP:80]	[['https://theano-pymc.readthedocs.io/']]	[Theano was historically important in early biomedical deep learning research and continues to be relevant for reproducing published models and maintaining legacy clinical AI systems. Many influential early papers in medical AI used Theano, and researchers still need to run these models for comparison benchmarks, reproduce published results, and maintain systems deployed before modern frameworks emerged. The library's symbolic computation approach and automatic differentiation influenced the design of current frameworks, and understanding Theano remains valuable for historical context in AI research. While active development has ceased (succeeded by Aesara), Theano-based code remains in production in some clinical settings and research archives.]	[B2AI:Application]	[False]		[B2AI_STANDARD:354]		[B2AI_SUBSTRATE:1]			
B2AI_STANDARD:835	B2AI_STANDARD:SoftwareOrTool	TRS	Tool Registry Service	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A proposed standard for sharing and discovering tools and workflows in a platform-agnostic way. Includes a Tool Registry Search validator and a Tool Discovery API specification.	True	False	https://ga4gh.github.io/tool-registry-service-schemas/															[B2AI_ORG:34]		
B2AI_STANDARD:836	B2AI_STANDARD:SoftwareOrTool	U-BRITE	UAB Biomedical Research Information Technology Enhancement Commons Program	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4|B2AI_TOPIC:13]	U-BRITE (UAB Biomedical Research Information Technology Enhancement) assembles new and existing HIPAA-compliant, high-performance informatics tools to provide researchers with a means to better manage and analyze clinical and genomic data sets and implements a translational research commons to facilitate and enable interdisciplinary team science across geographical locations.	False	True	https://ubrite.org/															[B2AI_ORG:94]		
B2AI_STANDARD:837	B2AI_STANDARD:SoftwareOrTool	Usagi	Usagi	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	An application to help create mappings between coding systems and the Vocabulary standard concepts.	True	False	https://github.com/OHDSI/Usagi	https://github.com/OHDSI/Usagi												[B2AI_STANDARD:844]		[B2AI_ORG:76]		
B2AI_STANDARD:838	B2AI_STANDARD:SoftwareOrTool	Vireo	Vireo	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:34]	A computationally efficient Bayesian model to demultiplex single-cell data from pooled experimental designs.	True	False	https://github.com/single-cell-genetics/vireo	https://github.com/single-cell-genetics/vireo										doi:10.1186/s13059-019-1865-2						
B2AI_STANDARD:839	B2AI_STANDARD:SoftwareOrTool	Wasabi	Wasabi Cloud Storage	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[cloudservice]	[B2AI_TOPIC:5]	Wasabi Cloud Storage is a high-performance, S3-compatible object storage service providing hot cloud storage with predictable pricing, no egress fees, and 80% lower total cost of ownership compared to hyperscaler alternatives (AWS S3, Azure Blob, Google Cloud Storage), designed for data-intensive workloads requiring frequent access and large-scale data archival. Founded in 2017 by Carbonite co-founders, Wasabi operates 16 globally distributed storage regions across North America, Europe, Asia-Pacific, with SOC-2, ISO 27001, and PCI-DSS certified data centers providing enterprise-grade security, immutability features (WORM, Object Lock), and compliance certifications (HIPAA, GDPR, FERPA) suitable for regulated industries. Wasabi's \"always hot\" architecture eliminates tiered storage complexity by storing all data on high-performance disk arrays with consistent millisecond latency for reads/writes, avoiding cold storage retrieval delays that plague glacier-tier alternatives while maintaining cost parity with archival storage services. The platform's S3 API compatibility ensures drop-in replacement for existing AWS workflows, supporting standard S3 operations (PUT, GET, LIST, multipart uploads), bucket policies, IAM-style access controls, and seamless integration with S3-compatible tools (AWS CLI, SDKs, third-party backup software, media asset managers). Wasabi's zero-fee model for egress, API requests, and reads eliminates the unpredictable costs that typically double hyperscaler storage bills, enabling cost-effective access for AI/ML training pipelines, video surveillance archives, genomic datasets, and medical imaging repositories where frequent data retrieval is essential. The service provides native integrations with major backup platforms (Veeam, Commvault, Rubrik), media workflows (Adobe Premiere, Frame.io), surveillance systems (Milestone, Hanwha), and object storage gateways, supporting hybrid cloud architectures where on-premises applications seamlessly tier to Wasabi for capacity expansion and disaster recovery. For AI/ML workloads, Wasabi's high-throughput object storage (up to 10 Gbps per connection) supports rapid dataset ingestion for model training, low-latency access to training data during distributed training across GPU clusters, and cost-effective storage for model checkpoints, experiment artifacts, and inference result archives without egress penalties for iterative model development and hyperparameter tuning requiring repeated dataset access.	False	True	https://wasabi.com/																	
B2AI_STANDARD:840	B2AI_STANDARD:SoftwareOrTool	W&B	Weights and Balances platform	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Platform for tracking, comparing, and visualizing machine learning experiments.	True	True	https://wandb.ai/																	
B2AI_STANDARD:841	B2AI_STANDARD:SoftwareOrTool	WES	Workflow Execution Service	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	The Workflow Execution Service (WES) is a GA4GH standard API specification that provides a platform-agnostic approach for submitting and monitoring workflow execution across different computational environments. WES enables researchers to run standardized workflows, currently supporting Common Workflow Language (CWL) and Workflow Description Language (WDL) formats, on multiple platforms, clouds, and execution systems using a consistent interface. The API specification is written in OpenAPI and embodies RESTful service principles, using JSON for requests and responses with standard HTTP/HTTPS transport. Core functionality includes workflow submission with parameter passing, run status monitoring through detailed logs capturing stdout and stderr output, task-level execution tracking with timing and exit codes, and workflow cancellation capabilities. WES addresses critical use cases such as \"bring your code to the data\" scenarios where researchers submit custom analyses to run on externally-owned datasets without data transfer, and best-practices pipeline execution where researchers discover workflows from shared repositories like Dockstore and execute them over controlled data environments. The service implements OAuth2 bearer token authentication and authorization, with implementations responsible for verifying user credentials and enforcing submission policies. WES provides comprehensive service introspection through its service-info endpoint, reporting supported workflow types, versions, filesystem protocols, workflow engines, and system state information. The API supports paginated listing of workflow runs, detailed run logs with output file locations, and granular task-level monitoring. Run states include QUEUED, INITIALIZING, RUNNING, COMPLETE, EXECUTOR_ERROR, SYSTEM_ERROR, CANCELED, and PREEMPTED, enabling detailed tracking of workflow execution lifecycle. As a GA4GH standard, WES promotes interoperability across bioinformatics workflow execution platforms and supports integration with related GA4GH standards including the Data Object Service for credential management and the Task Execution Service for extended task definitions.	True	False	https://ga4gh.github.io/workflow-execution-service-schemas/docs/															[B2AI_ORG:34]		
B2AI_STANDARD:842	B2AI_STANDARD:SoftwareOrTool	Xethub	Xethub	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Git-based collaboration to large scale repositories of data, code, or any combination of files.	True	True	https://xethub.com/assets/docs/																	
B2AI_STANDARD:843	B2AI_STANDARD:SoftwareOrTool	ZenML	ZenML	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[machinelearningframework]	[B2AI_TOPIC:5]	ZenML is an extensible, open-source MLOps framework for creating portable, production-ready MLOps pipelines.	True	False	https://zenml.io/	https://github.com/zenml-io/zenml			[{\"id\": \"B2AI_APP:81\", \"category\": \"B2AI:Application\", \"name\": \"MLOps Orchestration for Biomedical AI Pipelines\", \"description\": \"ZenML is used in biomedical AI for building reproducible, production-grade machine learning pipelines with comprehensive experiment tracking, model versioning, and deployment orchestration. Healthcare AI teams leverage ZenML to standardize workflows from data ingestion through model deployment, ensuring compliance with regulatory requirements for traceability and reproducibility. The platform integrates with diverse healthcare data sources, model registries, and deployment targets while maintaining complete lineage tracking essential for clinical AI validation. ZenML enables teams to implement best practices for ML operations including automated testing, continuous training, and model monitoring in healthcare settings where reliability and auditability are critical.\", \"used_in_bridge2ai\": false}]	[MLOps Orchestration for Biomedical AI Pipelines]	[B2AI_APP:81]		[ZenML is used in biomedical AI for building reproducible, production-grade machine learning pipelines with comprehensive experiment tracking, model versioning, and deployment orchestration. Healthcare AI teams leverage ZenML to standardize workflows from data ingestion through model deployment, ensuring compliance with regulatory requirements for traceability and reproducibility. The platform integrates with diverse healthcare data sources, model registries, and deployment targets while maintaining complete lineage tracking essential for clinical AI validation. ZenML enables teams to implement best practices for ML operations including automated testing, continuous training, and model monitoring in healthcare settings where reliability and auditability are critical.]	[B2AI:Application]	[False]							
B2AI_STANDARD:844	B2AI_STANDARD:TrainingProgram	OHDSI Tutorials	2019 OHDSI Tutorials - OMOP Common Data Model and Standardized Vocabularies	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4|B2AI_TOPIC:52]	This workshop is for data holders who want to apply OHDSI's data standards to their own observational datasets and researchers who want to be aware of OHDSI's data standards, so they can leverage data in OMOP CDM format for their own research purposes.	True	False	https://www.ohdsi.org/2019-tutorials-omop-common-data-model-and-standardized-vocabularies/	https://github.com/OHDSI/Tutorial-CDM														[B2AI_ORG:76]		
B2AI_STANDARD:845	B2AI_STANDARD:TrainingProgram	CDC Introduction to FHIR	CDC Introduction to FHIR - Training Recordings	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:52]	A series of HL7 FAIR training lecture recordings made available through YouTube.	True	False	https://www.cdc.gov/nchs/data/nvss/modernization/Introductory-Training-FHIR.pdf															[B2AI_ORG:40]		
B2AI_STANDARD:846	B2AI_STANDARD:TrainingProgram	FHIR Drills	FHIR Drills	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:52]	This set of pages contains a series of FHIR tutorials for those just beginning to learn the new specification. The tutorials require no prior knowledge of FHIR or REST. At present these tutorials are in their beta stage of development and we would appreciate any feedback you may have as we plan to build upon these in time to create a full set of tutorials from the very basic to the more complex.	True	False	https://fhir-drills.github.io/															[B2AI_ORG:40]		
B2AI_STANDARD:847	B2AI_STANDARD:TrainingProgram	FHIR Fundamentals	HL7 FHIR Fundamentals Course	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:52]	This is an asynchronous, instructor-led online course that allows you to work at your own pace. Learning takes place through discussions with the instructor, tutors and peers. Assessments are in the form of weekly assignments, quizzes, exams and projects. Plan on spending 5 to 7 hours per week. There are no live lectures to attend.	False	True	https://www.hl7.org/training/fhir-fundamentals.cfm	https://courses.hl7fundamentals.org/campus/														[B2AI_ORG:40]		
B2AI_STANDARD:848	B2AI_STANDARD:TrainingProgram	Learn LOINC	Learn LOINC	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:52]	Welcome to the LOINC Library. This is our A to Z collection of resources that we've collected to help you learn about LOINC and get connected to the community.	True	False	https://loinc.org/learn/															[B2AI_ORG:53]		
B2AI_STANDARD:849	B2AI_STANDARD:TrainingProgram	Microsoft Medical Imaging	Microsoft Learn - Work with medical imaging data and DICOM	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:15|B2AI_TOPIC:52]	Learn why DICOM standards are important. Explore the DICOM standards and DICOM service. Review the use case for radiology data in cancer treatment with examples.	True	False	https://learn.microsoft.com/en-us/training/modules/medical-imaging-data/														[B2AI_SUBSTRATE:11]	[B2AI_ORG:25|B2AI_ORG:56]		
B2AI_STANDARD:850	B2AI_STANDARD:TrainingProgram	Udemy FHIR	Udemy - Introduction to FHIR	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:52]	This course will help you understand the basics of FHIR. It is a FREE sample of a comprehensive hands-on introductory course (details inside). The full course includes direct access to the course creator via a private members-only Slack room.	True	True	https://www.udemy.com/course/introduction-to-fhir/															[B2AI_ORG:40]		
B2AI_STANDARD:851	B2AI_STANDARD:BiomedicalStandard	FHIR US Core	Fast Healthcare Interoperability Resources - US Core	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:5]	This is subset of all FHIR profiles for the US Realm, i.e., those supporting the minimum requirements for clinical data exchange in the United States.	True	False	https://build.fhir.org/ig/HL7/US-Core/index.html		True	[B2AI_ORG:40]	[{\"id\": \"B2AI_APP:82\", \"category\": \"B2AI:Application\", \"name\": \"US-Specific Clinical AI and Regulatory Compliance\", \"description\": \"FHIR US Core Implementation Guide is used in AI applications that require compliance with US healthcare regulations and interoperability requirements, enabling standardized data extraction from US-based EHR systems for training clinical prediction models. AI systems leverage US Core's profiles for patient demographics, vital signs, laboratory results, medications, and conditions to build models that meet ONC certification requirements and support CMS quality measures. The implementation guide ensures AI applications can reliably access structured clinical data across diverse US healthcare systems, supporting use cases in risk adjustment, quality metric prediction, social determinants of health analysis, and value-based care optimization. US Core's mandatory data elements provide a consistent feature set for federated learning across US hospitals.\", \"used_in_bridge2ai\": false, \"references\": [\"https://www.healthit.gov/isa/united-states-core-data-interoperability-uscdi\"]}]	[US-Specific Clinical AI and Regulatory Compliance]	[B2AI_APP:82]	[['https://www.healthit.gov/isa/united-states-core-data-interoperability-uscdi']]	[FHIR US Core Implementation Guide is used in AI applications that require compliance with US healthcare regulations and interoperability requirements, enabling standardized data extraction from US-based EHR systems for training clinical prediction models. AI systems leverage US Core's profiles for patient demographics, vital signs, laboratory results, medications, and conditions to build models that meet ONC certification requirements and support CMS quality measures. The implementation guide ensures AI applications can reliably access structured clinical data across diverse US healthcare systems, supporting use cases in risk adjustment, quality metric prediction, social determinants of health analysis, and value-based care optimization. US Core's mandatory data elements provide a consistent feature set for federated learning across US hospitals.]	[B2AI:Application]	[False]			[B2AI_STANDARD:845|B2AI_STANDARD:846|B2AI_STANDARD:847|B2AI_STANDARD:850]		[B2AI_ORG:103|B2AI_ORG:117]	[B2AI_STANDARD:109]	
B2AI_STANDARD:852	B2AI_STANDARD:BiomedicalStandard	FHIR USCDI	Fast Healthcare Interoperability Resources - US Core Data for Interoperability	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:5]	USCDI is the set of basic healthcare data types expected to supported by other systems. This is the FHIR US Core profile with all elements required by USCDI.	True	False	https://build.fhir.org/ig/HL7/US-Core/uscdi.html		True	[B2AI_ORG:40]	[{\"id\": \"B2AI_APP:83\", \"category\": \"B2AI:Application\", \"name\": \"Standardized Data Element Mapping for Healthcare AI\", \"description\": \"FHIR USCDI (US Core Data for Interoperability) implementation is used in AI applications to ensure consistent access to core clinical data elements required by federal regulations, enabling interoperable AI systems across US healthcare. Machine learning models leverage USCDI-compliant data elements including allergies, procedures, immunizations, lab results, and clinical notes to train on standardized features that are guaranteed to be available across certified EHR systems. This standardization is critical for AI applications that need to be deployed broadly across healthcare organizations, ensuring model portability and consistent performance. USCDI compliance enables AI researchers to develop models that align with national interoperability goals and can participate in health information exchange networks.\", \"used_in_bridge2ai\": false, \"references\": [\"https://www.healthit.gov/isa/united-states-core-data-interoperability-uscdi\"]}]	[Standardized Data Element Mapping for Healthcare AI]	[B2AI_APP:83]	[['https://www.healthit.gov/isa/united-states-core-data-interoperability-uscdi']]	[FHIR USCDI (US Core Data for Interoperability) implementation is used in AI applications to ensure consistent access to core clinical data elements required by federal regulations, enabling interoperable AI systems across US healthcare. Machine learning models leverage USCDI-compliant data elements including allergies, procedures, immunizations, lab results, and clinical notes to train on standardized features that are guaranteed to be available across certified EHR systems. This standardization is critical for AI applications that need to be deployed broadly across healthcare organizations, ensuring model portability and consistent performance. USCDI compliance enables AI researchers to develop models that align with national interoperability goals and can participate in health information exchange networks.]	[B2AI:Application]	[False]			[B2AI_STANDARD:845|B2AI_STANDARD:846|B2AI_STANDARD:847|B2AI_STANDARD:850]		[B2AI_ORG:103]	[B2AI_STANDARD:851]	
B2AI_STANDARD:853	B2AI_STANDARD:BiomedicalStandard	FHIR USCDI v1	Fast Healthcare Interoperability Resources - US Core Data for Interoperability, version 1	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:5]	USCDI is the set of basic healthcare data types expected to supported by other systems. This is the FHIR US Core profile with all elements required by USCDI v1.	True	False	https://build.fhir.org/ig/HL7/US-Core/uscdi.html		True	[B2AI_ORG:40]	[{\"id\": \"B2AI_APP:84\", \"category\": \"B2AI:Application\", \"name\": \"Foundation Models on Core Clinical Data Elements\", \"description\": \"FHIR USCDI v1 provides the baseline data elements for AI applications requiring backward compatibility and stable data definitions for longitudinal model training. AI systems developed against USCDI v1 can reliably access essential clinical information including patient demographics, problems, medications, allergies, lab results, vital signs, and procedures across time, enabling training of models on historical data and ensuring consistency in production deployments. This version stability is crucial for validating AI models in clinical trials, meeting regulatory requirements for locked algorithms, and maintaining model performance monitoring over multi-year periods. USCDI v1 compliance ensures AI applications can function across healthcare systems at different stages of EHR modernization.\", \"used_in_bridge2ai\": false, \"references\": [\"https://www.healthit.gov/isa/united-states-core-data-interoperability-uscdi-v1\"]}]	[Foundation Models on Core Clinical Data Elements]	[B2AI_APP:84]	[['https://www.healthit.gov/isa/united-states-core-data-interoperability-uscdi-v1']]	[FHIR USCDI v1 provides the baseline data elements for AI applications requiring backward compatibility and stable data definitions for longitudinal model training. AI systems developed against USCDI v1 can reliably access essential clinical information including patient demographics, problems, medications, allergies, lab results, vital signs, and procedures across time, enabling training of models on historical data and ensuring consistency in production deployments. This version stability is crucial for validating AI models in clinical trials, meeting regulatory requirements for locked algorithms, and maintaining model performance monitoring over multi-year periods. USCDI v1 compliance ensures AI applications can function across healthcare systems at different stages of EHR modernization.]	[B2AI:Application]	[False]			[B2AI_STANDARD:845|B2AI_STANDARD:846|B2AI_STANDARD:847|B2AI_STANDARD:850]		[B2AI_ORG:103|B2AI_ORG:117]	[B2AI_STANDARD:852]	
B2AI_STANDARD:854	B2AI_STANDARD:BiomedicalStandard	FHIR USCDI v4	Fast Healthcare Interoperability Resources - US Core Data for Interoperability, version 4	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:5]	USCDI is the set of basic healthcare data types expected to supported by other systems. This is the FHIR US Core profile with all elements required by USCDI v4.	True	False	https://build.fhir.org/ig/HL7/US-Core/uscdi.html		True	[B2AI_ORG:40]	[{\"id\": \"B2AI_APP:85\", \"category\": \"B2AI:Application\", \"name\": \"Advanced Clinical AI with Expanded Data Elements\", \"description\": \"FHIR USCDI v4 enables next-generation AI applications with access to expanded data elements including social determinants of health, mental health assessments, substance use information, and care team details. Machine learning models leverage these additional data classes to develop more comprehensive prediction models that account for social, behavioral, and environmental factors affecting health outcomes. AI systems benefit from USCDI v4's enhanced data elements for health equity research, population health modeling, and holistic patient risk assessment that goes beyond traditional clinical variables. The expanded scope supports AI applications in addressing health disparities, improving care coordination, and developing interventions that consider the full spectrum of factors influencing patient health.\", \"used_in_bridge2ai\": false, \"references\": [\"https://www.healthit.gov/isa/united-states-core-data-interoperability-uscdi-v4\"]}]	[Advanced Clinical AI with Expanded Data Elements]	[B2AI_APP:85]	[['https://www.healthit.gov/isa/united-states-core-data-interoperability-uscdi-v4']]	[FHIR USCDI v4 enables next-generation AI applications with access to expanded data elements including social determinants of health, mental health assessments, substance use information, and care team details. Machine learning models leverage these additional data classes to develop more comprehensive prediction models that account for social, behavioral, and environmental factors affecting health outcomes. AI systems benefit from USCDI v4's enhanced data elements for health equity research, population health modeling, and holistic patient risk assessment that goes beyond traditional clinical variables. The expanded scope supports AI applications in addressing health disparities, improving care coordination, and developing interventions that consider the full spectrum of factors influencing patient health.]	[B2AI:Application]	[False]			[B2AI_STANDARD:845|B2AI_STANDARD:846|B2AI_STANDARD:847|B2AI_STANDARD:850]		[B2AI_ORG:103|B2AI_ORG:117]	[B2AI_STANDARD:852]	
B2AI_STANDARD:855	B2AI_STANDARD:SoftwareOrTool	Dedoose	Dedoose app	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[multimodal]	[B2AI_TOPIC:5]	A cross-platform app for analyzing qualitative and mixed methods research	False	True	https://www.dedoose.com/																	2023-03-10
B2AI_STANDARD:856	B2AI_STANDARD:SoftwareOrTool	FAIR Data Station	FAIR Data Station	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A lightweight application written in Java, that aims to support researchers in managing research metadata according to the FAIR principles.	True	False	https://fairbydesign.nl/											doi:10.1093/gigascience/giad014						2023-03-13
B2AI_STANDARD:857	B2AI_STANDARD:BiomedicalStandard	Patient-Led Research Scorecards	Patient-Led Research Scorecards	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:5]	The Council of Medical Specialty Societies (CMSS) and Patient-Led Research Collaborative (PLRC) have developed a sustainable collaborative model of CER based on information from and the expertise of patient communities, researchers, funders, and clinical research organizations. This model takes the form of scorecards which serve to evaluate how effective a patient group and research partner collaboration will be at conducting truly patient-led research.	True	False	https://patientresearchcovid19.com/storage/2023/02/Patient-Led-Research-Scorecards.pdf																	2023-03-14
B2AI_STANDARD:858	B2AI_STANDARD:SoftwareOrTool	Zshot	Zshot	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:32]	A framework for performing Zero and Few shot named entity recognition.	True	False	https://ibm.github.io/zshot/	https://github.com/IBM/zshot														[B2AI_ORG:104]		2023-03-16
B2AI_STANDARD:859	B2AI_STANDARD:Registry	NIH CDE	NIH Common Data Elements Repository	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A registry of standardized, precisely defined questions, paired with sets of allowable responses, used systematically across different sites, studies, or clinical trials to ensure consistent data collection.	True	False	https://cde.nlm.nih.gov/	https://cde.nlm.nih.gov/api														[B2AI_ORG:74]		2023-03-21
B2AI_STANDARD:860	B2AI_STANDARD:Registry	BARTOC	Basic Register of Thesauri, Ontologies & Classifications	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A database of Knowledge Organization Systems and KOS related registries.	True	False	https://bartoc.org/																	2023-03-24
B2AI_STANDARD:861	B2AI_STANDARD:SoftwareOrTool	DuckDB	DuckDB	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A database platform designed for working with tabular data.	True	False	https://duckdb.org/	https://github.com/duckdb/duckdb													[B2AI_SUBSTRATE:9]			2023-03-24
B2AI_STANDARD:862	B2AI_STANDARD:SoftwareOrTool	Polars	Polars library	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	Polars is a high-performance DataFrame library written in Rust with bindings for Python, Node.js, and R, designed as a fast alternative to pandas. The library features a multi-threaded query engine with lazy evaluation, query optimization, and streaming capabilities for processing larger-than-RAM datasets. Polars utilizes Apache Arrow columnar format for zero-copy data sharing and SIMD vectorization for cache-coherent algorithms. It supports both eager and lazy execution modes, with the lazy API enabling automatic query optimization and parallel execution across CPU cores. The library handles diverse data formats including CSV, JSON, Parquet, Delta Lake, AVRO, Excel, and direct database connections to MySQL, PostgreSQL, SQLite, and cloud storage systems. Polars provides an intuitive expression API for data manipulation operations while maintaining minimal dependencies and fast import times (70ms vs 520ms for pandas), making it suitable for data analysis, ETL pipelines, and analytical workloads requiring high performance.	True	False	https://www.pola.rs/	https://github.com/pola-rs/polars/													[B2AI_SUBSTRATE:8]			2023-03-27
B2AI_STANDARD:863	B2AI_STANDARD:Registry	LOV	Linked Open Vocabularies	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A collection of searchable ontologies and vocabularies, spanning multiple fields.	True	False	https://lov.linkeddata.es/	https://github.com/pyvandenbussche/lov																2023-03-27
B2AI_STANDARD:864	B2AI_STANDARD:Registry	PhenX Toolkit	PhenX Toolkit	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4]	A catalog of recommended measurement protocols for biomedical research.	True	False	https://www.phenxtoolkit.org/																	2023-03-27
B2AI_STANDARD:866	B2AI_STANDARD:OntologyOrVocabulary	SALON	Sequence Alignment Ontology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:23]	An OWL2 ontology for representing and semantically annotating pairwise and multiple sequence alignments.	True	False	https://benhid.com/SALON/											doi:10.1186/s12859-023-05190-7						
B2AI_STANDARD:867	B2AI_STANDARD:BiomedicalStandard	GCS	Glasgow Coma Scale	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[diagnosticinstrument]	[B2AI_TOPIC:4]	A neurological assessment tool used to evaluate level of consciousness based on patient responses in three categories: eye-opening, verbal response, and motor response, with a higher score indicating a more favorable neurological status.	True	False	https://www.glasgowcomascale.org/											doi:10.1016/s0140-6736(74)91639-0						2023-05-23
B2AI_STANDARD:868	B2AI_STANDARD:DataStandard	Frictionless	Frictionless data standards	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datamodel]	[B2AI_TOPIC:5]	Frictionless data standards provide a comprehensive suite of lightweight, extensible specifications for describing datasets, data files, and tabular data to enhance FAIR (Findability, Accessibility, Interoperability, Reusability) principles. The core specifications include Data Package for dataset-level metadata and resource collections, Data Resource for individual file descriptions, and Table Schema for tabular data structure definition with field types, constraints, and relationships. These specifications combine to create specialized formats like Tabular Data Packages that integrate CSV/JSON data with JSON Schema-based metadata descriptors. The standards follow a \"small pieces, loosely joined\" philosophy, enabling individual components to be used independently or combined for complex data scenarios. They support cross-technology implementation with human-readable JSON metadata that facilitates machine processing, data validation, and automated discovery. The specifications enable data portability, version control, and collaborative data workflows while maintaining compatibility with existing data formats and tools in the data science ecosystem.	True	False	https://specs.frictionlessdata.io/	https://github.com/frictionlessdata/specs																2023-05-23
B2AI_STANDARD:869	B2AI_STANDARD:SoftwareOrTool	Compass Rose	Epic Compass Rose module	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[implementation_maturity_production]	[B2AI_TOPIC:29]	Care coordination module focused on social determinant of health factors. Part of the Epic EHR platform.	False	True	https://www.epic.com/software/population-health		True		[{\"id\": \"B2AI_APP:87\", \"category\": \"B2AI:Application\", \"name\": \"Population Health Analytics and Risk Stratification\", \"description\": \"Compass Rose (Epic's population health tool) is used in AI applications for large-scale risk stratification, care gap identification, and population-level outcome prediction across Epic's extensive user base. Machine learning models leverage Compass Rose's aggregated clinical data, standardized quality measures, and longitudinal patient tracking to develop predictive algorithms for chronic disease management, preventive care optimization, and resource allocation. AI systems built on this platform can identify high-risk patient populations, predict hospital readmissions, and recommend targeted interventions at scale. The tool's integration with Epic's EHR ecosystem enables real-time AI inference during clinical encounters and automated outreach programs guided by ML-based risk scores.\", \"used_in_bridge2ai\": false}]	[Population Health Analytics and Risk Stratification]	[B2AI_APP:87]		[Compass Rose (Epic's population health tool) is used in AI applications for large-scale risk stratification, care gap identification, and population-level outcome prediction across Epic's extensive user base. Machine learning models leverage Compass Rose's aggregated clinical data, standardized quality measures, and longitudinal patient tracking to develop predictive algorithms for chronic disease management, preventive care optimization, and resource allocation. AI systems built on this platform can identify high-risk patient populations, predict hospital readmissions, and recommend targeted interventions at scale. The tool's integration with Epic's EHR ecosystem enables real-time AI inference during clinical encounters and automated outreach programs guided by ML-based risk scores.]	[B2AI:Application]	[False]					[B2AI_ORG:105|B2AI_ORG:115]		2023-06-20
B2AI_STANDARD:870	B2AI_STANDARD:SoftwareOrTool	GX	Great Expectations platform	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A platform for organizing, testing, and validating data.	True	False	https://greatexpectations.io/	https://github.com/great-expectations/great_expectations																2023-06-20
B2AI_STANDARD:871	B2AI_STANDARD:SoftwareOrTool	Pinecone	Pinecone vector database	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A database platform built around creating vector representations of data. The basic implementation is a managed, cloud-native product, though there is a free tier.	False	True	https://www.pinecone.io/														[B2AI_SUBSTRATE:54|B2AI_SUBSTRATE:55|B2AI_SUBSTRATE:9]			2023-06-20
B2AI_STANDARD:872	B2AI_STANDARD:DataStandard	CLIG	Command Line Interface Guidelines	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:5]	An open-source guide to help with writing command-line programs, based on UNIX principles.	True	False	https://clig.dev/																	2023-06-20
B2AI_STANDARD:873	B2AI_STANDARD:BiomedicalStandard	MINSEQE	Minimum Information about a high-throughput nucleotide SEQuencing Experiment	Harry Caufield	caufieldjh	0000-0001-5705-7831	[minimuminformationschema]	[B2AI_TOPIC:13]	Five elements of experimental description considered essential when making sequencing data available.	True	False	https://www.fged.org/projects/minseqe/	https://zenodo.org/record/5706412																2023-09-25
B2AI_STANDARD:874	B2AI_STANDARD:SoftwareOrTool	GGIR	GGIR accelerometry package	Harry Caufield	caufieldjh	0000-0001-5705-7831		[B2AI_TOPIC:18]	An R package to process multi-day raw accelerometer data for physical activity and sleep research.	True	False	https://cran.r-project.org/web/packages/GGIR/vignettes/GGIR.html	https://github.com/wadpac/GGIR																2023-09-25
B2AI_STANDARD:875	B2AI_STANDARD:BiomedicalStandard	Badawy et al. 2019	Metadata Concepts for Advancing the Use of Digital Health Technologies in Clinical Research	Harry Caufield	caufieldjh	0000-0001-5705-7831		[B2AI_TOPIC:18]	A proposed metadata set for digital health studies.	True	False	https://figshare.com/articles/dataset/Supplementary_Material_for_Metadata_Concepts_for_Advancing_the_Use_of_Digital_Health_Technologies_in_Clinical_Research/9944303											doi:10.1159/000502951						2023-09-25
B2AI_STANDARD:876	B2AI_STANDARD:BiomedicalStandard	GSCID/BRC CMS v1.5	GSCID/BRC Clinical Metadata Standard	Harry Caufield	caufieldjh	0000-0001-5705-7831		[B2AI_TOPIC:5]	A general standard for clinical metadata.	True	False	https://www.niaid.nih.gov/research/clinical-metadata-standard															[B2AI_ORG:118]		2023-09-25
B2AI_STANDARD:877	B2AI_STANDARD:BiomedicalStandard	MI-CLAIM	Minimum information about clinical artificial intelligence modeling	Harry Caufield	caufieldjh	0000-0001-5705-7831	[minimuminformationschema]	[B2AI_TOPIC:5]	MI-CLAIM (Minimum Information about CLinical AI Modeling) is a reporting standard and documentation checklist developed to address transparency and reproducibility challenges in clinical artificial intelligence research, published in Nature Medicine in 2020 by a multidisciplinary team of clinical and data scientists. MI-CLAIM serves two primary purposes - enabling direct assessment of clinical impact including fairness considerations, and allowing rapid replication of the technical design process for clinical AI studies. The standard provides a comprehensive checklist in MS Word table format covering essential reporting elements including study design and data characteristics (patient demographics, inclusion/exclusion criteria, data sources, temporal validation), model development details (feature engineering, architecture selection, hyperparameter tuning, training procedures), performance evaluation (metrics across demographic subgroups, confidence intervals, comparison to clinical standards), clinical implementation considerations (decision thresholds, interpretability mechanisms, failure modes), and ethical aspects (bias assessment, fairness metrics, regulatory status). The repository encourages community feedback through GitHub Issues to continuously improve the standard as the field evolves, promoting best practices for transparent, reproducible, and equitable clinical AI development and deployment across healthcare applications.	True	False	https://github.com/beaunorgeot/MI-CLAIM	https://github.com/beaunorgeot/MI-CLAIM			[{\"id\": \"B2AI_APP:88\", \"category\": \"B2AI:Application\", \"name\": \"Clinical AI Reporting Standards and Model Documentation\", \"description\": \"MI-CLAIM (Minimum Information about Clinical Artificial Intelligence Modeling) checklist is used to standardize reporting of clinical AI studies, ensuring reproducibility, transparency, and appropriate evaluation of machine learning models in healthcare. Researchers leverage MI-CLAIM guidelines to document essential details about model development, validation approaches, and clinical context that enable others to assess reliability and reproduce findings. The standard supports automated model card generation, structured documentation for regulatory submissions, and systematic reviews of clinical AI literature by providing a consistent framework for reporting. MI-CLAIM compliance facilitates responsible AI development by ensuring key information about data provenance, model limitations, and intended use cases is explicitly documented.\", \"used_in_bridge2ai\": false}]	[Clinical AI Reporting Standards and Model Documentation]	[B2AI_APP:88]		[MI-CLAIM (Minimum Information about Clinical Artificial Intelligence Modeling) checklist is used to standardize reporting of clinical AI studies, ensuring reproducibility, transparency, and appropriate evaluation of machine learning models in healthcare. Researchers leverage MI-CLAIM guidelines to document essential details about model development, validation approaches, and clinical context that enable others to assess reliability and reproduce findings. The standard supports automated model card generation, structured documentation for regulatory submissions, and systematic reviews of clinical AI literature by providing a consistent framework for reporting. MI-CLAIM compliance facilitates responsible AI development by ensuring key information about data provenance, model limitations, and intended use cases is explicitly documented.]	[B2AI:Application]	[False]	doi:10.1038/s41591-020-1041-y						2023-09-25
B2AI_STANDARD:878	B2AI_STANDARD:DataStandardOrTool	CSVW	CSV on the Web	Harry Caufield	caufieldjh	0000-0001-5705-7831		[B2AI_TOPIC:5]	A standard for describing and clarifying the content of CSV tables.	True	False	https://csvw.org/	https://w3c.github.io/csvw/syntax/													[B2AI_SUBSTRATE:6]			2023-09-25
B2AI_STANDARD:879	B2AI_STANDARD:DataStandardOrTool	BagIt	BagIt file packaging format	Harry Caufield	caufieldjh	0000-0001-5705-7831		[B2AI_TOPIC:5]	A set of hierarchical file layout conventions for storage and transfer of arbitrary digital content.	True	False	https://datatracker.ietf.org/doc/rfc8493/	https://datatracker.ietf.org/doc/rfc8493/										doi:10.17487/RFC8493						2023-09-25
B2AI_STANDARD:880	B2AI_STANDARD:DataStandardOrTool	Unity Catalog	Unity Catalog	Harry Caufield	caufieldjh	0000-0001-5705-7831		[B2AI_TOPIC:5]	A universal catalog for data and AI. It includes a core set of APIs for tables, unstructured data, and AI assets.	True	False	https://www.unitycatalog.io/	https://github.com/unitycatalog/unitycatalog			[{\"id\": \"B2AI_APP:89\", \"category\": \"B2AI:Application\", \"name\": \"Unified Data and AI Asset Governance\", \"description\": \"Unity Catalog is used in biomedical AI for centralized governance of data assets, ML models, and AI artifacts across multi-cloud and hybrid healthcare IT environments. Healthcare organizations leverage Unity Catalog to implement fine-grained access controls for sensitive patient data used in model training, track lineage from raw clinical data through processed features to trained models, and ensure HIPAA compliance across distributed AI development teams. The catalog provides a single source of truth for data discovery, enables secure data sharing across research and clinical domains, and maintains comprehensive audit logs for regulatory compliance. Unity Catalog's integration with major ML platforms facilitates governed AI development where data scientists can access approved datasets while maintaining security and privacy requirements.\", \"used_in_bridge2ai\": false}]	[Unified Data and AI Asset Governance]	[B2AI_APP:89]		[Unity Catalog is used in biomedical AI for centralized governance of data assets, ML models, and AI artifacts across multi-cloud and hybrid healthcare IT environments. Healthcare organizations leverage Unity Catalog to implement fine-grained access controls for sensitive patient data used in model training, track lineage from raw clinical data through processed features to trained models, and ensure HIPAA compliance across distributed AI development teams. The catalog provides a single source of truth for data discovery, enables secure data sharing across research and clinical domains, and maintains comprehensive audit logs for regulatory compliance. Unity Catalog's integration with major ML platforms facilitates governed AI development where data scientists can access approved datasets while maintaining security and privacy requirements.]	[B2AI:Application]	[False]							2024-11-02
B2AI_STANDARD:881	B2AI_STANDARD:SoftwareOrTool	Senselab	Senselab package	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:36]	A Python package for streamlining the processing and analysis of behavioral data, such as voice and speech patterns, with robust and reproducible methodologies.	True	False	https://sensein.group/senselab/	https://github.com/sensein/senselab																2023-12-06
B2AI_STANDARD:882	B2AI_STANDARD:BiomedicalStandard	CLAIM	Checklist for Artificial Intelligence in Medical Imaging (CLAIM)	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[guidelines]	[B2AI_TOPIC:15]	CLAIM is modeled after the STARD guideline and has been extended to address applications of AI in medical imaging that include classification, image reconstruction, text analysis, and workflow optimization. It is intended to provide a framework for the development and validation of AI algorithms in medical imaging.	True	False	https://doi.org/10.1148/ryai.2020200029											doi:10.1148/ryai.2020200029						2023-12-06
B2AI_STANDARD:883	B2AI_STANDARD:SoftwareOrTool	pydicom	Pydicom software package	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:15]	Pydicom is a Python package for working with DICOM images.	True	False	https://pydicom.github.io/	https://github.com/pydicom/pydicom			[{\"id\": \"B2AI_APP:90\", \"category\": \"B2AI:Application\", \"name\": \"Medical Imaging Data Processing for Deep Learning\", \"description\": \"pydicom is the essential Python library for AI researchers working with medical imaging data, enabling reading, writing, and manipulation of DICOM files in machine learning pipelines. Virtually all medical imaging AI research using Python leverages pydicom to extract pixel data and metadata from DICOM images, convert images to NumPy arrays for neural network input, and create DICOM-compliant outputs for clinical integration. The library handles diverse DICOM transfer syntaxes and encodings, enables efficient batch processing of large imaging datasets, and provides the foundation for medical imaging data loaders in PyTorch and TensorFlow. pydicom's ability to preserve clinical metadata during AI processing ensures that model outputs maintain appropriate associations with patient context and imaging parameters.\", \"used_in_bridge2ai\": false}]	[Medical Imaging Data Processing for Deep Learning]	[B2AI_APP:90]		[pydicom is the essential Python library for AI researchers working with medical imaging data, enabling reading, writing, and manipulation of DICOM files in machine learning pipelines. Virtually all medical imaging AI research using Python leverages pydicom to extract pixel data and metadata from DICOM images, convert images to NumPy arrays for neural network input, and create DICOM-compliant outputs for clinical integration. The library handles diverse DICOM transfer syntaxes and encodings, enables efficient batch processing of large imaging datasets, and provides the foundation for medical imaging data loaders in PyTorch and TensorFlow. pydicom's ability to preserve clinical metadata during AI processing ensures that model outputs maintain appropriate associations with patient context and imaging parameters.]	[B2AI:Application]	[False]		[B2AI_STANDARD:98]					2023-12-06
B2AI_STANDARD:884	B2AI_STANDARD:DataStandard	CSVY	CSVY format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:5]	CSVY is a file format combining CSV with a YAML header.	True	False	https://github.com/leeper/csvy	https://github.com/leeper/csvy																2025-02-16
B2AI_STANDARD:885	B2AI_STANDARD:SoftwareOrTool	openSMILE	openSMILE software	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[implementation_maturity_production]	[B2AI_TOPIC:37]	openSMILE is an open-source audio feature extraction toolkit.	True	False	https://www.audeering.com/research/opensmile/	https://github.com/audeering/opensmile	True													[B2AI_ORG:117]		2025-02-16
B2AI_STANDARD:886	B2AI_STANDARD:SoftwareOrTool	Praat	Praat software	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[implementation_maturity_production]	[B2AI_TOPIC:36]	Praat is software for working with voice data, including speech analysis, segmentation, and synthesis.	True	False	https://www.fon.hum.uva.nl/praat/	https://github.com/praat/praat	True										[B2AI_STANDARD:887]			[B2AI_ORG:117]		2025-02-16
B2AI_STANDARD:887	B2AI_STANDARD:SoftwareOrTool	Parselmouth	Parselmouth software	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[implementation_maturity_production]	[B2AI_TOPIC:36]	Parselmouth is a Python library for working with Praat software. Parselmouth directly accesses Praat's C/C++ code (which means the algorithms and their output are exactly the same as in Praat) and provides efficient access to the program's data, but also provides an interface that looks no different from any other Python library.	True	False	https://parselmouth.readthedocs.io/en/stable/	https://github.com/YannickJadoul/Parselmouth	True										[B2AI_STANDARD:886]			[B2AI_ORG:117]		2025-03-05
B2AI_STANDARD:888	B2AI_STANDARD:DataStandard	BigTIFF	BigTIFF format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[audiovisual|fileformat]	[B2AI_TOPIC:5]	BigTIFF is an image format. It is a variant of the TIFF format that uses 64-bit offsets thereby supporting files up to 18,000 petabytes in size, vastly transcending TIFF's normal 4 GB limit. Since the format also supports all of the normal features and header tags of TIFF_6 and the extended metadata offered by GeoTIFF, it provides good service in the GIS domain, medical imaging, and other applications that employ large scanners or cameras.	True	False	https://www.loc.gov/preservation/digital/formats/fdd/fdd000328.shtml												[B2AI_STANDARD:383]					2025-03-13
B2AI_STANDARD:889	B2AI_STANDARD:SoftwareOrTool	torchaudio	TorchAudio library	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[implementation_maturity_production]	[B2AI_TOPIC:37]	A software library for audio and signal processing with PyTorch.	True	False	https://pytorch.org/audio/stable/index.html		True		[{\"id\": \"B2AI_APP:91\", \"category\": \"B2AI:Application\", \"name\": \"Biomedical Audio Analysis and Speech-Based Diagnostics\", \"description\": \"torchaudio is used in AI applications for processing and analyzing biomedical audio signals including speech patterns for neurological assessment, respiratory sounds for pulmonary diagnosis, and cardiac auscultation for automated heart disease detection. Deep learning models built with torchaudio process audio biomarkers such as cough sounds for COVID-19 screening, voice characteristics for Parkinson's disease detection, and lung sounds for pneumonia classification. The library's preprocessing capabilities, pretrained models, and integration with PyTorch enable researchers to develop audio-based AI diagnostics that can be deployed on mobile devices for remote patient monitoring, telehealth applications, and resource-limited settings where traditional diagnostic equipment is unavailable.\", \"used_in_bridge2ai\": false}]	[Biomedical Audio Analysis and Speech-Based Diagnostics]	[B2AI_APP:91]		[torchaudio is used in AI applications for processing and analyzing biomedical audio signals including speech patterns for neurological assessment, respiratory sounds for pulmonary diagnosis, and cardiac auscultation for automated heart disease detection. Deep learning models built with torchaudio process audio biomarkers such as cough sounds for COVID-19 screening, voice characteristics for Parkinson's disease detection, and lung sounds for pneumonia classification. The library's preprocessing capabilities, pretrained models, and integration with PyTorch enable researchers to develop audio-based AI diagnostics that can be deployed on mobile devices for remote patient monitoring, telehealth applications, and resource-limited settings where traditional diagnostic equipment is unavailable.]	[B2AI:Application]	[False]		[B2AI_STANDARD:816]			[B2AI_ORG:117]		2025-03-13
B2AI_STANDARD:890	B2AI_STANDARD:BiomedicalStandard	RadElement	RadElement Common Data Elements (CDEs) for Radiology	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A set of common data elements for radiology research.	True	False	https://www.radelement.org/																	2025-03-13
B2AI_STANDARD:891	B2AI_STANDARD:BiomedicalStandard	FAIRGenomes	FAIR Genomes Semantic Model	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:5]	A semantic model for describing genomic data in a FAIR manner.	True	False	https://github.com/fairgenomes/fairgenomes-semantic-model	https://github.com/fairgenomes/fairgenomes-semantic-model/blob/main/fair-genomes.yml																2025-03-13
B2AI_STANDARD:893	B2AI_STANDARD:BiomedicalStandard	REMBI	Recommended Metadata for Biological Images (REMBI)	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:15]	Draft metadata guidelines to begin addressing the needs of diverse communities within light and electron microscopy	True	False	https://docs.google.com/spreadsheets/d/1Ck1NeLp-ZN4eMGdNYo2nV6KLEdSfN6oQBKnnWU6Npeo/edit?gid=1023506919#gid=1023506919											doi:10.1038/s41592-021-01166-8						2025-03-13
B2AI_STANDARD:894	B2AI_STANDARD:SoftwareOrTool	PRIDE	PRoteomics IDEntifications Database				[dataregistry]	[B2AI_TOPIC:5|B2AI_TOPIC:23]	The PRIDE PRoteomics IDEntifications (PRIDE) Archive database is a centralized, standards compliant, public data repository for mass spectrometry proteomics data, including protein and peptide identifications and the corresponding expression values, post-translational modifications and supporting mass spectra evidence (both as raw data and peak list files). PRIDE is a core member in the ProteomeXchange (PX) consortium, which provides a standardised way for submitting mass spectrometry based proteomics data to public-domain repositories. Datasets are submitted to ProteomeXchange via PRIDE and are handled by expert bio-curators. All PRIDE public datasets can also be searched in ProteomeCentral, the portal for all ProteomeXchange datasets.	True	False	https://www.ebi.ac.uk/pride/		True		[{\"id\": \"B2AI_APP:92\", \"category\": \"B2AI:Application\", \"name\": \"Proteomics Data Mining and Peptide Identification\", \"description\": \"PRIDE (PRoteomics IDEntifications) database is used in AI applications for training deep learning models on mass spectrometry proteomics data, enabling improved peptide identification, protein quantification, and post-translational modification prediction. Machine learning systems leverage PRIDE's extensive repository of annotated spectra to develop neural networks for de novo peptide sequencing, spectral quality assessment, and cross-linking analysis. AI models trained on PRIDE data improve upon traditional database search algorithms by learning complex patterns in fragmentation spectra, enabling identification of novel peptides, non-canonical modifications, and proteoforms. The database's standardized mzML and mzIdentML formats facilitate reproducible AI research in clinical proteomics, biomarker discovery, and precision medicine applications.\", \"used_in_bridge2ai\": false}]	[Proteomics Data Mining and Peptide Identification]	[B2AI_APP:92]		[PRIDE (PRoteomics IDEntifications) database is used in AI applications for training deep learning models on mass spectrometry proteomics data, enabling improved peptide identification, protein quantification, and post-translational modification prediction. Machine learning systems leverage PRIDE's extensive repository of annotated spectra to develop neural networks for de novo peptide sequencing, spectral quality assessment, and cross-linking analysis. AI models trained on PRIDE data improve upon traditional database search algorithms by learning complex patterns in fragmentation spectra, enabling identification of novel peptides, non-canonical modifications, and proteoforms. The database's standardized mzML and mzIdentML formats facilitate reproducible AI research in clinical proteomics, biomarker discovery, and precision medicine applications.]	[B2AI:Application]	[False]	doi:10.1093/nar/gkae1011				[B2AI_ORG:116]		
B2AI_STANDARD:895	B2AI_STANDARD:DataStandard	ASCII File Format Guidelines for Earth Science Data	American Standard Code for Information Interchange ASCII File Format Guidelines for Earth Science Data				[fileformat|implementation_maturity_production|standards_process_maturity_final]	[B2AI_TOPIC:5]	American Standard Code for Information Interchange (ASCII) file format guidelines for NASA Earth science data.	True	False	https://www.earthdata.nasa.gov/about/esdis/esco/standards-practices/ascii-file-format-guidelines-earth-science-data	https://www.earthdata.nasa.gov/s3fs-public/imported/ESDS-RFC-027v1.1.pdf	True		[{\"id\": \"B2AI_APP:93\", \"category\": \"B2AI:Application\", \"name\": \"Environmental and Climate Health Data Integration\", \"description\": \"ASCII File Format Guidelines for Earth Science Data are used in AI applications that integrate environmental and climate data with biomedical datasets to model health impacts of environmental change, predict disease patterns related to climate factors, and develop early warning systems for climate-sensitive health outcomes. Machine learning models leverage standardized ASCII representations of temperature, precipitation, air quality, and other environmental variables to train on relationships between environmental exposures and health outcomes such as vector-borne disease transmission, heat-related illness, respiratory disease exacerbation, and mental health impacts. The standardized format enables AI systems to integrate NASA Earth observations with clinical and epidemiological data for One Health applications bridging human, animal, and environmental health.\", \"used_in_bridge2ai\": false}]	[Environmental and Climate Health Data Integration]	[B2AI_APP:93]		[ASCII File Format Guidelines for Earth Science Data are used in AI applications that integrate environmental and climate data with biomedical datasets to model health impacts of environmental change, predict disease patterns related to climate factors, and develop early warning systems for climate-sensitive health outcomes. Machine learning models leverage standardized ASCII representations of temperature, precipitation, air quality, and other environmental variables to train on relationships between environmental exposures and health outcomes such as vector-borne disease transmission, heat-related illness, respiratory disease exacerbation, and mental health impacts. The standardized format enables AI systems to integrate NASA Earth observations with clinical and epidemiological data for One Health applications bridging human, animal, and environmental health.]	[B2AI:Application]	[False]					[B2AI_ORG:114]		
B2AI_STANDARD:896	B2AI_STANDARD:SoftwareOrTool	GIS Toolchain	Geographic Informations Systems Toolchain				[datamodel|standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:14]	The GIS toolchain consists of extensions to the OMOP schema, extensions to the OMOP Vocabulary, and GIS-specific software for acquiring and working with geospatial data. Together, these enable researchers to use health-related attributes of the regions where patients live in OHDSI study cohort definitions. For example, you can use the GIS toolchain to define cohorts that include regional data on exposure to toxicants or social deprivation along with EHR data on relevant health outcomes. The toolchain also includes informatics resources that support integration with the main OHDSI tool stack (HADES) and integration with externally supported solutions for geocoding and for finding and deriving relevant data sources from catalogs of available data sources. Importantly, the toolchain allows integrated analysis of geospatial and EHR data without sharing any sensitive patient location data.	True	False	https://ohdsi.github.io/GIS/		True		[{\"id\": \"B2AI_APP:94\", \"category\": \"B2AI:Application\", \"name\": \"Geospatial Health Analytics and Environmental Exposure Modeling\", \"description\": \"GIS Toolchain (from OHDSI) is used in AI applications for integrating geographic information with clinical data to train models that account for environmental exposures, social determinants of health, and spatial disease patterns. Machine learning systems leverage geospatial features derived from this toolchain to develop prediction models for infectious disease spread, environmental health impacts, cancer cluster detection, and health equity analysis. AI applications combine geocoded patient addresses with environmental data layers (air quality, greenspace, food access) to create location-aware risk models that inform population health interventions and resource allocation. The toolchain's integration with OMOP CDM enables spatiotemporal analysis at scale across observational health databases.\", \"used_in_bridge2ai\": false}]	[Geospatial Health Analytics and Environmental Exposure Modeling]	[B2AI_APP:94]		[GIS Toolchain (from OHDSI) is used in AI applications for integrating geographic information with clinical data to train models that account for environmental exposures, social determinants of health, and spatial disease patterns. Machine learning systems leverage geospatial features derived from this toolchain to develop prediction models for infectious disease spread, environmental health impacts, cancer cluster detection, and health equity analysis. AI applications combine geocoded patient addresses with environmental data layers (air quality, greenspace, food access) to create location-aware risk models that inform population health interventions and resource allocation. The toolchain's integration with OMOP CDM enables spatiotemporal analysis at scale across observational health databases.]	[B2AI:Application]	[False]					[B2AI_ORG:76|B2AI_ORG:115]		
B2AI_STANDARD:897	B2AI_STANDARD:BiomedicalStandard	MI-CDM	Medical Imaging Common Data Model				[datamodel|standards_process_maturity_development|implementation_maturity_pilot]	[B2AI_TOPIC:4]	The rapid growth of artificial intelligence (AI) and deep learning techniques require access to large inter-institutional cohorts of data to enable the development of robust models, e.g., targeting the identification of disease biomarkers and quantifying disease progression and treatment efficacy. The Observational Medical Outcomes Partnership Common Data Model (OMOP CDM) has been designed to accommodate a harmonized representation of observational healthcare data. This study proposes the Medical Imaging CDM (MI-CDM) extension, adding two new tables and two vocabularies to the OMOP CDM to address the structural and semantic requirements to support imaging research. The tables provide the capabilities of linking DICOM data sources as well as tracking the provenance of imaging features derived from those images. The implementation of the extension enables phenotype definitions using imaging features and expanding standardized computable imaging biomarkers. This proposal offers a comprehensive and unified approach for conducting imaging research and outcome studies utilizing imaging features.	True	False	https://doi.org/10.1007/s10278-024-00982-6		True		[{\"id\": \"B2AI_APP:95\", \"category\": \"B2AI:Application\", \"name\": \"Medical Imaging AI Data Standardization and Multi-Site Studies\", \"description\": \"MI-CDM (Medical Imaging Common Data Model) is used in AI applications to standardize heterogeneous imaging metadata across institutions, enabling training of robust deep learning models on diverse multi-site imaging datasets. The common data model facilitates AI development by providing consistent representation of imaging protocols, scanner parameters, patient demographics, and clinical annotations across different PACS systems and imaging centers. Machine learning systems leverage MI-CDM to perform federated learning on distributed imaging data without raw image transfer, enable systematic bias detection across sites, and develop models that generalize across different scanner manufacturers and acquisition protocols. The standardization is critical for regulatory-grade AI where model validation requires diverse, well-characterized datasets.\", \"used_in_bridge2ai\": false}]	[Medical Imaging AI Data Standardization and Multi-Site Studies]	[B2AI_APP:95]		[MI-CDM (Medical Imaging Common Data Model) is used in AI applications to standardize heterogeneous imaging metadata across institutions, enabling training of robust deep learning models on diverse multi-site imaging datasets. The common data model facilitates AI development by providing consistent representation of imaging protocols, scanner parameters, patient demographics, and clinical annotations across different PACS systems and imaging centers. Machine learning systems leverage MI-CDM to perform federated learning on distributed imaging data without raw image transfer, enable systematic bias detection across sites, and develop models that generalize across different scanner manufacturers and acquisition protocols. The standardization is critical for regulatory-grade AI where model validation requires diverse, well-characterized datasets.]	[B2AI:Application]	[False]	doi:10.1007/s10278-024-00982-6	[B2AI_STANDARD:98|B2AI_STANDARD:243]			[B2AI_ORG:76|B2AI_ORG:115]		
B2AI_STANDARD:898	B2AI_STANDARD:DataStandard	CX	Cytoscape Exchange				[fileformat|implementation_maturity_production]	[B2AI_TOPIC:21]	Cytoscape Exchange (CX) format is a network exchange format, designed as a flexible structure for transmission of networks. It is designed for flexibility, modularity, and extensibility, and as a message payload in common REST protocols. It is not intended as an in-memory data model for use in applications.	True	False	https://cytoscape.org/cx/		True		[{\"id\": \"B2AI_APP:96\", \"category\": \"B2AI:Application\", \"name\": \"Network Biology and Graph Neural Networks\", \"description\": \"CX (Cytoscape Exchange) format is used in AI applications for representing biological networks that serve as input to graph neural networks for tasks such as protein function prediction, drug-target interaction prediction, and disease gene prioritization. Machine learning models leverage CX's standardized representation of nodes, edges, and network attributes to train graph-based deep learning architectures that capture complex biological relationships. AI systems use CX-encoded networks for multi-omics data integration, pathway analysis, and systems biology modeling where network topology and node features inform predictions. The format enables sharing of network models across platforms and reproducible AI research in network medicine, synthetic biology, and computational drug discovery.\", \"used_in_bridge2ai\": false}]	[Network Biology and Graph Neural Networks]	[B2AI_APP:96]		[CX (Cytoscape Exchange) format is used in AI applications for representing biological networks that serve as input to graph neural networks for tasks such as protein function prediction, drug-target interaction prediction, and disease gene prioritization. Machine learning models leverage CX's standardized representation of nodes, edges, and network attributes to train graph-based deep learning architectures that capture complex biological relationships. AI systems use CX-encoded networks for multi-omics data integration, pathway analysis, and systems biology modeling where network topology and node features inform predictions. The format enables sharing of network models across platforms and reproducible AI research in network medicine, synthetic biology, and computational drug discovery.]	[B2AI:Application]	[False]					[B2AI_ORG:116]		
B2AI_STANDARD:899	B2AI_STANDARD:DataStandard	h5ad	Anndata h5ad format				[fileformat]	[B2AI_TOPIC:5]	h5ad is a format for storing and working with annotated data matrices in memory and on disk. It is produced by the Anndata Python package.	True	False	https://anndata.readthedocs.io/en/stable/	https://github.com/scverse/anndata/blob/main/src/anndata/_io/h5ad.py	True		[{\"id\": \"B2AI_APP:97\", \"category\": \"B2AI:Application\", \"name\": \"Single-Cell Genomics and Deep Learning Integration\", \"description\": \"h5ad format (HDF5-based AnnData) is the standard file format for AI applications in single-cell genomics, enabling efficient storage and access of large-scale single-cell RNA-seq datasets for training deep learning models. AI systems leverage h5ad's structured representation of cell-by-gene expression matrices, cell metadata, and dimensionality reduction embeddings to perform cell type classification, trajectory inference, gene regulatory network inference, and batch effect correction. Deep learning frameworks integrate seamlessly with h5ad through the AnnData API, supporting applications in cancer cell identification, developmental biology, immune profiling, and drug response prediction at single-cell resolution. The format's efficient sparse matrix storage enables training on datasets with millions of cells.\", \"used_in_bridge2ai\": false}]	[Single-Cell Genomics and Deep Learning Integration]	[B2AI_APP:97]		[h5ad format (HDF5-based AnnData) is the standard file format for AI applications in single-cell genomics, enabling efficient storage and access of large-scale single-cell RNA-seq datasets for training deep learning models. AI systems leverage h5ad's structured representation of cell-by-gene expression matrices, cell metadata, and dimensionality reduction embeddings to perform cell type classification, trajectory inference, gene regulatory network inference, and batch effect correction. Deep learning frameworks integrate seamlessly with h5ad through the AnnData API, supporting applications in cancer cell identification, developmental biology, immune profiling, and drug response prediction at single-cell resolution. The format's efficient sparse matrix storage enables training on datasets with millions of cells.]	[B2AI:Application]	[False]					[B2AI_ORG:116]		
B2AI_STANDARD:900	B2AI_STANDARD:BiomedicalStandard	Dataset-JSON	Clinical Data Interchange Standards Consortium (CDISC) Dataset-JavaScript Object Notation	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datamodel|fileformat]	[B2AI_TOPIC:9]	CDISC Dataset-JSON is a JSON-based schema specifically designed for exchanging tabular datasets in clinical studies. It is based on CDISC Dataset-JSON version 1.0 with enhancements, including smaller file sizes, additional metadata, and simpler processing. The format supports file and Application Programming Interface based data exchange, is widely supported across technologies, and can link to Define-XML for additional metadata. Dataset-JSON has the potential to replace Statistical Analysis System (SAS) version 5 XPORT Transport Format (XPT) for submission of electronic study data to regulatory agencies.	True	True	https://www.cdisc.org/standards/data-exchange/dataset-json	https://www.cdisc.org/standards/data-exchange/dataset-json/dataset-json-v1-1		[B2AI_ORG:15]														
B2AI_STANDARD:901	B2AI_STANDARD:BiomedicalStandard	EDF+	European Data Format Plus	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:37]	An extension of the European Data Format (EDF) that maintains EDF compatibility while adding capability to store annotations, events, and stimuli alongside the signal data. It also allows for storing interrupted (non-contiguous) recordings in a single file. EDF+ can save most EEG, PSG, ECG, EMG, and Evoked Potential data that cannot be saved into common hospital information systems. The Persyst universal EEG reader supports this format.	True	False	https://www.edfplus.info/specs/edfplus.html		True									doi:10.1016/S1388-2457(03)00123-8	[B2AI_STANDARD:105]			[B2AI_ORG:115]		
B2AI_STANDARD:902	B2AI_STANDARD:DataStandard	ARK PIDs	Archival Resource Key Persistent Identifiers	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datamodel|standards_process_maturity_draft|implementation_maturity_production]	[B2AI_TOPIC:5]	Archival Resource Key (ARK) identifiers are persistent URLs designed to support long-term access to information objects. ARKs can identify digital objects (documents, databases, images, software), physical objects (books, artifacts), living beings, and intangible objects (concepts, services). ARKs are characterized by their internal \"ark:\" label, their NAAN (Name Assigning Authority Number) identifying the naming organization, and features like metadata access through inflections (adding ? to the URL). They are free to create and use, with no fees to assign or use ARKs, and can be hosted on any web server or through the global N2T.net resolver.	True	False	https://arks.org/about/	https://datatracker.ietf.org/doc/draft-kunze-ark/	True		[{\"id\": \"B2AI_APP:98\", \"category\": \"B2AI:Application\", \"name\": \"Persistent Dataset Identification for ML Reproducibility\", \"description\": \"ARK (Archival Resource Key) persistent identifiers are used in AI applications to create stable, long-term references to training datasets, model artifacts, and research outputs, ensuring reproducibility and provenance tracking in machine learning research. AI systems leverage ARK identifiers to maintain citations to specific versions of datasets used in model training, enabling verification of results and compliance with data governance policies. The identifier scheme's flexibility supports both fine-grained object identification (individual data files) and collections (entire datasets), which is essential for documenting complex ML pipelines. ARK PIDs facilitate data sharing in federated AI research while maintaining clear attribution and enabling auditable access logs for sensitive biomedical data.\", \"used_in_bridge2ai\": false}]	[Persistent Dataset Identification for ML Reproducibility]	[B2AI_APP:98]		[ARK (Archival Resource Key) persistent identifiers are used in AI applications to create stable, long-term references to training datasets, model artifacts, and research outputs, ensuring reproducibility and provenance tracking in machine learning research. AI systems leverage ARK identifiers to maintain citations to specific versions of datasets used in model training, enabling verification of results and compliance with data governance policies. The identifier scheme's flexibility supports both fine-grained object identification (individual data files) and collections (entire datasets), which is essential for documenting complex ML pipelines. ARK PIDs facilitate data sharing in federated AI research while maintaining clear attribution and enabling auditable access logs for sensitive biomedical data.]	[B2AI:Application]	[False]					[B2AI_ORG:116]		
B2AI_STANDARD:903	B2AI_STANDARD:BiomedicalStandard	CDS	Clinical Dataset Structure	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datamodel]	[B2AI_TOPIC:7|B2AI_TOPIC:4]	The Clinical Dataset Structure (CDS) is a standardized way to organize and describe clinical research datasets to make them readily interoperable and easily reusable by humans and machines. It addresses the challenge of integrating multiple data modalities from clinical studies by providing a simple, intuitive file and directory structure. CDS organizes data by datatype at the root level, with each datatype directory structured according to applicable standards or a recommended hierarchy of modality/device/participant directories. The standard includes specifications for metadata files that document the dataset content, structure, and participant information, optimizing datasets for AI-readiness and secondary analysis.	True	False	https://cds-specification.readthedocs.io/	https://github.com/AI-READI/cds-specification	True		[{\"id\": \"B2AI_APP:99\", \"category\": \"B2AI:Application\", \"name\": \"Clinical Decision Support Rule Integration and ML Hybridization\", \"description\": \"CDS (Clinical Decision Support) specification is used in AI applications to integrate machine learning models with traditional rule-based clinical decision support systems, enabling hybrid AI approaches that combine interpretable rules with data-driven predictions. AI systems leverage CDS Hooks and standardized interfaces to deploy ML models as FHIR-based services that provide real-time recommendations during clinical workflows, such as drug-drug interaction checking augmented with personalized risk predictions, or sepsis alerts that combine guideline-based criteria with neural network early warning scores. The specification enables AI applications to surface predictions within EHR user interfaces at appropriate decision points, while maintaining auditability through standardized request/response formats and provenance metadata.\", \"used_in_bridge2ai\": false}]	[Clinical Decision Support Rule Integration and ML Hybridization]	[B2AI_APP:99]		[CDS (Clinical Decision Support) specification is used in AI applications to integrate machine learning models with traditional rule-based clinical decision support systems, enabling hybrid AI approaches that combine interpretable rules with data-driven predictions. AI systems leverage CDS Hooks and standardized interfaces to deploy ML models as FHIR-based services that provide real-time recommendations during clinical workflows, such as drug-drug interaction checking augmented with personalized risk predictions, or sepsis alerts that combine guideline-based criteria with neural network early warning scores. The specification enables AI applications to surface predictions within EHR user interfaces at appropriate decision points, while maintaining auditability through standardized request/response formats and provenance metadata.]	[B2AI:Application]	[False]	doi:10.5281/zenodo.10867040				[B2AI_ORG:114]		2025-05-29
B2AI_STANDARD:904	B2AI_STANDARD:DataStandard	Schema.org	A collaborative, community-developed schema for structured data on the Internet	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[datamodel|standards_process_maturity_final|implementation_maturity_production]	[B2AI_TOPIC:5]	Schema.org is a collaborative, community activity founded by Google, Microsoft, Yahoo, and Yandex with a mission to create, maintain, and promote schemas for structured data on the Internet. It provides a common vocabulary that webmasters can use to mark up their pages in ways that can be understood by major search engines and other applications. The Schema.org vocabulary consists of a set of types (e.g., Person, Event, Organization), properties (e.g., name, location, startDate), and relationships that can be used with many different encoding formats including RDFa, Microdata, and JSON-LD. As of 2024, over 45 million web domains use Schema.org markup with over 450 billion Schema.org objects. The vocabulary is continuously evolving through an open community process managed by the W3C Schema.org Community Group.	True	False	https://schema.org/	https://github.com/schemaorg/schemaorg	True		[{\"id\": \"B2AI_APP:100\", \"category\": \"B2AI:Application\", \"name\": \"Biomedical Knowledge Extraction and Semantic Search\", \"description\": \"Schema.org vocabularies, particularly the biomedical extensions (BioSchemas), are used in AI applications for automated knowledge extraction from web resources, semantic annotation of datasets, and training large language models on structured biomedical information. AI systems leverage Schema.org markup to extract structured data about proteins, genes, diseases, clinical trials, and medical conditions from web pages, enabling automated knowledge base construction and question-answering systems. The vocabulary supports AI-driven dataset discovery, metadata standardization for machine learning pipelines, and training of biomedical language models that understand relationships between biological entities. Schema.org's widespread adoption makes it valuable for web-scale biomedical data mining and federated search applications.\", \"used_in_bridge2ai\": false}]	[Biomedical Knowledge Extraction and Semantic Search]	[B2AI_APP:100]		[Schema.org vocabularies, particularly the biomedical extensions (BioSchemas), are used in AI applications for automated knowledge extraction from web resources, semantic annotation of datasets, and training large language models on structured biomedical information. AI systems leverage Schema.org markup to extract structured data about proteins, genes, diseases, clinical trials, and medical conditions from web pages, enabling automated knowledge base construction and question-answering systems. The vocabulary supports AI-driven dataset discovery, metadata standardization for machine learning pipelines, and training of biomedical language models that understand relationships between biological entities. Schema.org's widespread adoption makes it valuable for web-scale biomedical data mining and federated search applications.]	[B2AI:Application]	[False]					[B2AI_ORG:116]		2025-05-29
B2AI_STANDARD:905	B2AI_STANDARD:BiomedicalStandard	Thermo Fisher RAW	Thermo Fisher RAW mass spectrometry data format	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831	[fileformat]	[B2AI_TOPIC:28]	The Thermo Fisher RAW format is a proprietary file format designed for storing mass spectrometry data generated by Thermo Fisher Scientific instruments. It contains detailed information about mass spectra, chromatograms, instrument parameters, and metadata from experimental runs. The format supports multiple mass spectrometry techniques and is accessed through Thermo's software tools like MSFileReader or more recent RawFileReader. While it's proprietary, various conversion tools allow transformation to open formats like mzML or mzXML for broader compatibility with third-party analysis software. The format is widely used in proteomics, metabolomics, and other mass spectrometry-based research applications where preserving the complete experimental context is essential for data interpretation and analysis.	False	True	https://www.thermofisher.com/us/en/home/industrial/mass-spectrometry.html		True	[B2AI_ORG:123]												[B2AI_ORG:116]		2025-05-29
B2AI_STANDARD:906	B2AI_STANDARD:TrainingProgram	OHDSI 2024 Global Symposium ETL Tutorial	Tutorial: Developing and Evaluating Your Extract, Transform, Load (ETL) Process to the OMOP CDM	Harry Caufield	caufieldjh	ORCID:0000-0001-5705-7831		[B2AI_TOPIC:4|B2AI_TOPIC:52]	In this video tutorial, students will learn about the tools and practices developed by the OHDSI community to support the journey to establish and maintain an ETL to standardize your data to OMOP CDM and enable standardized evidence generation across a data network.	True	False	https://www.youtube.com/watch?v=H69dC7f-edQ															[B2AI_ORG:76]		
