{
  "data_standardortools_collection": [
    {
      "id": "B2AI_STANDARD:1",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": ".ACE format",
      "description": ".ACE format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "The ACE file format is a specification for storing data about genomic contigs. The original ACE format was developed for use with Consed, a program for viewing, editing, and finishing DNA sequence assemblies. ACE files are generated by various assembly programs, including Phrap, CAP3, Newbler, Arachne, AMOS (sequence assembly) (more specifically Minimo) and Tigr Assembler v2.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikipedia.org/wiki/ACE_(genomic_file_format)",
      "formal_specification": "https://web.archive.org/web/20100609072313/http://bcr.musc.edu/manuals/CONSED.txt"
    },
    {
      "id": "B2AI_STANDARD:2",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "DMS",
      "description": "2023 NIH Data Management and Sharing Policy",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "policy"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "NIH has issued the Data Management and Sharing (DMS) policy (effective January 25, 2023) to promote the sharing of scientific data. Sharing scientific data accelerates biomedical research discovery, in part, by enabling validation of research results, providing accessibility to high-value datasets, and promoting data reuse for future research studies. Under the DMS policy, NIH expects that investigators and institutions do the following. Plan and budget for the managing and sharing of data, Submit a DMS plan for review when applying for funding, Comply with the approved DMS plan.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://sharing.nih.gov/data-management-and-sharing-policy/about-data-management-and-sharing-policy/data-management-and-sharing-policy-overview",
      "responsible_organization": [
        "B2AI_ORG:67"
      ]
    },
    {
      "id": "B2AI_STANDARD:3",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "ABCD",
      "description": "Access to Biological Collections Data Schema",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:1",
          "category": "B2AI:Application",
          "name": "DiSSCo Digital Specimen Architecture AI-Assisted Curation",
          "description": "The Digital Specimen Architecture uses FAIR Digital Object type descriptions based on TDWG standards including ABCD. This enables registered AI services to automatically discover digital specimens, execute allowed actions, and attach machine-readable annotations such as automated extraction from images, relation creation for knowledge graphs, and standardization or correction. ABCD defines the object structure and attributes in the FDO, allowing AI services to operate uniformly across collections and propagate outputs to aggregators such as GBIF and GeoCASe.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.3897/biss.7.112678"
          ]
        },
        {
          "id": "B2AI_APP:102",
          "category": "B2AI:Application",
          "name": "Hespi Computer Vision and OCR Pipeline for Herbarium Sheets",
          "description": "Hespi integrates object detection, OCR and handwriting recognition, and a multimodal LLM for post-processing and authority control in herbarium digitization. The pipeline explicitly situates digitization within community standards, citing ABCD alongside Darwin Core. Extracted fields from labels and sheet components are mapped to ABCD and DwC fields to produce interoperable records consumable by downstream AI and analytics systems.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.48550/arxiv.2410.08740"
          ]
        },
        {
          "id": "B2AI_APP:103",
          "category": "B2AI:Application",
          "name": "ML-Ready Benchmark Datasets via ABCD Standardization",
          "description": "Work on improving transcribed digital specimen data highlights ABCD and Darwin Core as the target schema for interoperable outputs and schema-based annotation systems such as AnnoSys. This ecosystem underpins ML-ready benchmark datasets for herbarium images and semi-automated workflows for data cleaning and georeferencing, facilitating training and evaluation of computer vision and NLP models while keeping outputs in ABCD and DwC formats for data exchange.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1093/database/baz129"
          ]
        }
      ],
      "collection": [
        "datamodel"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:1"
      ],
      "purpose_detail": "The Access to Biological Collections Data (ABCD) Schema is an evolving comprehensive standard for the access to and exchange of data about specimens and observations (a.k.a. primary biodiversity data). The ABCD Schema attempts to be comprehensive and highly structured, supporting data from a wide variety of databases. It is compatible with several existing data standards. Parallel structures exist so that either (or both) atomised data and free-text can be accommodated. Version 1.2 is currently in use with the GBIF (Global Biodiversity Information Facility) and BioCASE (Biological Collection Access Service for Europe) networks. Apart from the GBIF and BioCASE networks, the potential for the application of ABCD extends to internal networks, or in-house legacy data access (e.g. datasets from external sources that shall not be converted and integrated into an institution's own data, but be kept separately, though easily accessible). By defining relations between terms, ABCD is a step towards an ontology for biological collections.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://abcd.tdwg.org/",
      "responsible_organization": [
        "B2AI_ORG:93"
      ]
    },
    {
      "id": "B2AI_STANDARD:4",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "AGP",
      "description": "AGP format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "AGP format describes the assembly of a larger sequence object from smaller objects. The large object can be a contig, a scaffold (supercontig), or a chromosome. Each line (row) of the AGP file describes a different piece of the object, and has the column entries defined below. Extended comments follow. It does not serve for either a description of how sequence reads were assembled, or a description of the alignments between components used to construct a larger object. Not all of the information in proprietary assembly files can be represented in the AGP format. It is also not for recording the spans of features like repeats or genes.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.ncbi.nlm.nih.gov/assembly/agp/AGP_Specification/",
      "responsible_organization": [
        "B2AI_ORG:120"
      ]
    },
    {
      "id": "B2AI_STANDARD:5",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "AnIML",
      "description": "Analytical Information Markup Language",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:3"
      ],
      "purpose_detail": "The Analytical Information Markup Language (AnIML) is the emerging ASTM XML standard for analytical chemistry data. It is currently in pre-release form. It is a combination of a highly flexible core schema that defines XML tagging for any kind of analytical information; A set of technique definition documents. These XML files, one per analytical technique, apply tight constraints to the flexible core and in turn are defined by the Technique Schema; Extensions to Technique Definitions are possible to accommodate vendor- and institution-specific data fields. Mission Statement Our goal is to serve as the open-source development platform for a new XML standard for Analytical Chemistry Information. The project is a collaborative effort between many groups and individuals and is sanctioned by the ASTM subcommittee E13.15. http://animl.cvs.sourceforge.net/viewvc/animl/schema/animl-core.xsd",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.animl.org/",
      "responsible_organization": [
        "B2AI_ORG:8"
      ]
    },
    {
      "id": "B2AI_STANDARD:6",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "ARRIVE",
      "description": "Animal Research Reporting In Vivo Experiments",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Guidelines intended to improve the reporting of animal experiments.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://arriveguidelines.org/",
      "publication": "doi:10.1371/journal.pbio.3000411",
      "responsible_organization": [
        "B2AI_ORG:121"
      ]
    },
    {
      "id": "B2AI_STANDARD:7",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "aECG",
      "description": "Annotated ECG standard",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:37"
      ],
      "purpose_detail": "Provides a common means of electronically storing both the ECG wave form and associated annotations.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.hl7.org/implement/standards/product_brief.cfm?product_id=70",
      "formal_specification": "https://www.hl7.org/implement/standards/product_brief.cfm?product_id=70",
      "responsible_organization": [
        "B2AI_ORG:40"
      ]
    },
    {
      "id": "B2AI_STANDARD:8",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "AIM",
      "description": "Annotation and Image Markup schema",
      "related_to": [
        "B2AI_STANDARD:98"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:2",
          "category": "B2AI:Application",
          "name": "PACS-to-AIM Conversion for ML-Ready Label Generation",
          "description": "A deployed workflow converts proprietary vendor DICOM presentation state annotations from commercial PACS systems into AIM XML using the AIM API, enabling standardization of legacy radiologist annotations for machine learning. A Python module matches lesions across longitudinal studies via 3D coordinates, producing AIM files that are imported into ePAD where lesions are linked over time and quantitative metrics are computed. This AIM conversion pipeline unlocks historical radiologist annotations from PACS for supervised training, radiomics pipelines, and generation of high-quality labeled datasets for deep learning, while ensuring interoperability and enabling large-scale analysis across institutions.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1007/s10278-019-00191-6"
          ]
        },
        {
          "id": "B2AI_APP:104",
          "category": "B2AI:Application",
          "name": "ePAD AIM-Based Radiomics and ML Data Platform",
          "description": "ePAD stores annotations and quantitative imaging features in AIM XML and exposes RESTful web services allowing plugins and external tools to retrieve AIM annotations and associated images for downstream analysis. Plugins compute biomarkers and save results back into AIM format, while integrations with external pipelines such as QIFP and pyRadiomics enable feature extraction with outputs persisted in AIM. This AIM-centric architecture facilitates standardized training data management, radiomic feature extraction, and programmatic access for machine learning workflows, cohort analyses, and interoperable analytics environments for quantitative imaging research.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.18383/j.tom.2018.00055"
          ]
        }
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:9",
        "B2AI_TOPIC:15"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:849"
      ],
      "purpose_detail": "The Annotation and Image Markup (AIM) project is the first initiative to propose and create a standard means of adding information and knowledge to medical images in a clinical environment, enabling automatic searching of image content. AIM provides a comprehensive solution to imaging challenges including the lack of agreed-upon syntax for annotation and markup, standardized semantics for annotations, and common formats for annotations and markup. The AIM Model captures descriptive information for images with user-generated graphical symbols into a single common information source. The project includes multiple components: the AIM Template Service (a web service for uploading and downloading AIM templates), the AIM Template Builder (a Java application for creating templates with well-defined questions and answer choices), and reference implementations like AIM on ClearCanvas Workstation. AIM captures results in terms of image regions of interest, semantic descriptions, inferences, calculations, and quantitative features derived by computer programs. It is interoperable with DICOM-SR and HL7-CDA standards while providing unique advantages through an explicit semantic model of imaging results.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/NCIP/annotation-and-image-markup",
      "formal_specification": "https://github.com/NCIP/annotation-and-image-markup",
      "responsible_organization": [
        "B2AI_ORG:71"
      ],
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:11"
      ]
    },
    {
      "id": "B2AI_STANDARD:9",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "ANSI/CTA-2090",
      "description": "ANSI/CTA Standard - The Use of Artificial Intelligence in Health Care Trustworthiness",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:3",
          "category": "B2AI:Application",
          "name": "Pre-Development ML Scoping and Data Provenance Documentation",
          "description": "CTA-2090 recommends concrete developer actions for AI/ML workflows including listing potential use cases before development to properly scope algorithm functionality and limits, documenting whether datasets are raw or pre-processed and what preprocessing was performed, and understanding how original data were collected to identify potential biases. These practices operationalize the standard's emphasis on clear intended use, data lineage, traceability, and documentation requirements for trustworthy ML pipelines in healthcare, ensuring teams establish proper foundations during planning and data preparation stages.",
          "used_in_bridge2ai": false,
          "references": [
            "https://www.fdli.org/wp-content/uploads/2023/01/9-Ross.pdf"
          ]
        },
        {
          "id": "B2AI_APP:105",
          "category": "B2AI:Application",
          "name": "Bias Assessment and Post-Deployment Monitoring",
          "description": "CTA-2090 guidance is applied through explicit testing for racial and other biases including testing on vulnerable populations, contractual diversity and representation benchmarks for training data, and inventories to review, screen, retrain, and prevent bias during development. Post-deployment operationalization includes ongoing real-world testing after approval and continuous review of algorithm results during clinical use. These practices implement the standard's expectations for bias identification, mitigation, representative data, lifecycle monitoring, and real-world performance auditing in healthcare AI systems.",
          "used_in_bridge2ai": false,
          "references": [
            "https://www.fdli.org/wp-content/uploads/2023/01/9-Ross.pdf"
          ]
        },
        {
          "id": "B2AI_APP:106",
          "category": "B2AI:Application",
          "name": "Z-Inspection Stakeholder Co-Design and Trust Assessment",
          "description": "Healthcare AI systems apply CTA-2090 trustworthiness principles through Z-Inspection, an ethically aligned co-design methodology involving interdisciplinary stakeholders to examine ethical, technical, medical, and legal implications during development and deployment. Assessments uncover dataset bias risks, deployment shortcomings such as accent-related accuracy degradation, and protocol effects on ML output accuracy. Recommended practices include building explainability into models, documenting how decisions are generated, and conducting real-world validation that translates the standard's transparency, fairness, and reproducibility dimensions into concrete governance and evaluation activities.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.48550/arxiv.2206.09887"
          ]
        }
      ],
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "This standard outlines key principles for ensuring trustworthiness in AI applications within healthcare, focusing on human trust, technical reliability, and regulatory compliance. It provides a framework for evaluating AI systems in clinical settings, emphasizing transparency, accountability, and ethical considerations.",
      "is_open": false,
      "requires_registration": false,
      "url": "https://shop.cta.tech/products/cta-2090",
      "responsible_organization": [
        "B2AI_ORG:4",
        "B2AI_ORG:122"
      ]
    },
    {
      "id": "B2AI_STANDARD:10",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "AB1",
      "description": "Applied Biosystems sequence read binary format file",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "A binary version of raw DNA sequence reads from Applied Biosystems sequencing analysis software. Also known as ABIF.",
      "is_open": false,
      "requires_registration": false,
      "url": "https://www.thermofisher.com/us/en/home/life-science/sequencing/sanger-sequencing.html",
      "responsible_organization": [
        "B2AI_ORG:123"
      ]
    },
    {
      "id": "B2AI_STANDARD:11",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "ABI",
      "description": "Applied Biosystems sequence read binary format file",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "A binary version of raw DNA sequence reads from Applied Biosystems sequencing analysis software.",
      "is_open": false,
      "requires_registration": false,
      "url": "https://tools.thermofisher.com/content/sfs/manuals/4346366_DNA_Sequenc_Analysis_5_1_UG.pdf",
      "formal_specification": "https://tools.thermofisher.com/content/sfs/manuals/4346366_DNA_Sequenc_Analysis_5_1_UG.pdf",
      "responsible_organization": [
        "B2AI_ORG:123"
      ]
    },
    {
      "id": "B2AI_STANDARD:12",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "ARB",
      "description": "ARB software binary alignment format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "A binary alignment format used by the ARB package.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://www.arb-home.de/documentation.html",
      "publication": "doi:10.1093/nar/gkh293"
    },
    {
      "id": "B2AI_STANDARD:13",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "ADL",
      "description": "Archetype Definition Language",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "ADL (Archetype Definition Language) provides a formal, human-readable syntax for expressing constraint-based models of clinical information structures. It enables the definition of reusable archetypes that constrain generic reference models (such as openEHR's) to represent specific clinical concepts like blood pressure measurements or problem lists. The language consists of cADL (constraint ADL) for structural definitions, dADL (data ADL) for metadata and terminology bindings, and an assertion language for business rules. ADL archetypes support multi-lingual terminology, specialization hierarchies, and versioning, making them suitable for creating maintainable, semantically interoperable health information systems. Tools exist for authoring, compiling, and validating ADL archetypes, with XML exchange formats also supported. The syntax is designed to be accessible to both clinical domain experts and technical implementers.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://specifications.openehr.org/releases/AM/latest/ADL1.4.html",
      "formal_specification": "https://specifications.openehr.org/releases/AM/latest/ADL1.4.html",
      "responsible_organization": [
        "B2AI_ORG:79"
      ]
    },
    {
      "id": "B2AI_STANDARD:14",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "StructureDefinition-argo-careplan",
      "description": "Argonaut Data Query Implementation Guide",
      "related_to": [
        "B2AI_STANDARD:109"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "datamodel"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:845",
        "B2AI_STANDARD:846",
        "B2AI_STANDARD:847",
        "B2AI_STANDARD:850"
      ],
      "purpose_detail": "Specifications for sharing single sets of patient care plans. Based on FHIR R2.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://www.fhir.org/guides/argonaut/r2/StructureDefinition-argo-careplan.html",
      "responsible_organization": [
        "B2AI_ORG:40"
      ]
    },
    {
      "id": "B2AI_STANDARD:15",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "AMIS",
      "description": "Article Minimum Information Standard",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "minimuminformationschema"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:16"
      ],
      "purpose_detail": "The curation process is significantly slowed down by missing information in the articles analyzed. The identity of the clones used to generate ISH probes and the precise sequences tested in reporter assays constituted the most frequent omissions. To help authors ensure in the future that necessary information is present in their article, the Article Minimum Information Standard (AMIS) guidelines have been defined. The guideline describes the mandatory (and useful) information that should be mentioned in literature articles to facilitate the curation process. These guidelines extend the minimal information defined by the MISFISHIE format (Deutsch at al. 2008, Nature Biotechnology).",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.aniseed.fr/aniseed/default/submit_data?module=aniseed&action=default:submit_data#tab-4"
    },
    {
      "id": "B2AI_STANDARD:16",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Axt",
      "description": "Axt Alignment Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "The Axt (Alignment eXTended) format is a simple text-based format developed by UCSC Genome Browser for storing pairwise DNA sequence alignments between two species or sequences. Each alignment block in an Axt file consists of four lines - a summary line containing alignment number, chromosome names, alignment start and end positions for both sequences, strand information, and alignment score, followed by the aligned sequence from the first genome, the aligned sequence from the second genome, and a blank separator line. The format uses zero-based coordinates for the first sequence and supports alignments on either strand, with sequences from the negative strand reverse-complemented. Axt files are particularly useful for representing whole-genome alignments between species (such as human-mouse or human-chimp comparisons) generated by alignment tools like BLASTZ or LASTZ. The format's straightforward structure makes it easy to parse programmatically and convert to other formats. UCSC provides utilities including axtToMaf for converting to MAF (Multiple Alignment Format), axtChain for chaining together alignments, and axtBest for selecting the best alignment for each position. While Axt efficiently represents pairwise alignments, it has been largely superseded by MAF and chain/net formats for more complex multi-way alignments and hierarchical alignment representations in comparative genomics workflows.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://genome.ucsc.edu/goldenPath/help/axt.html",
      "responsible_organization": [
        "B2AI_ORG:119"
      ]
    },
    {
      "id": "B2AI_STANDARD:17",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "BAI",
      "description": "BAM indexing format file",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "A file containing the index for a Binary Alignment Map (BAM) file.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.ncbi.nlm.nih.gov/tools/gbench/tutorial6/"
    },
    {
      "id": "B2AI_STANDARD:18",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "BEDgraph",
      "description": "BEDgraph format",
      "related_to": [
        "B2AI_STANDARD:19",
        "B2AI_STANDARD:20"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "The bedGraph format allows display of continuous-valued data in track format.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://genome.ucsc.edu/goldenPath/help/bedgraph.html",
      "responsible_organization": [
        "B2AI_ORG:119"
      ]
    },
    {
      "id": "B2AI_STANDARD:19",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "bigBed",
      "description": "Big Browser Extensible Data Format",
      "related_to": [
        "B2AI_STANDARD:18",
        "B2AI_STANDARD:20"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "The bigBed format is an indexed binary format for storing genome annotation data, developed by UCSC Genome Browser as a high-performance alternative to text-based BED files for large datasets. BigBed files store annotation items representing either simple genomic features or linked collections of exons, maintaining BED semantics while enabling efficient region-specific data transfer. Files are created using the bedToBigBed utility which converts sorted BED files into compressed binary format with built-in indexing, requiring chromosome sizes files and optionally AutoSql (.as) format definitions for describing standard and custom fields. The format supports BED3 through BED12 plus additional user-defined fields, with features including itemRgb color specification, extra searchable indices via the -extraIndex parameter for track hub item searches, and trackDb settings for mouseOver labels, field filtering, and URL transformations. Only the portions of bigBed files needed for the currently displayed chromosomal region are transferred to the browser, dramatically improving performance compared to full BED file loading. The format supports web-accessible hosting via HTTP, HTTPS, or FTP with sparse file caching, and includes utilities (bigBedToBed, bigBedInfo, bigBedSummary) for extracting and querying data. BigBed files are widely used in UCSC Genome Browser custom tracks, track hubs for consortia data sharing, and integrated with genome analysis workflows requiring scalable annotation storage and rapid genomic region queries.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://genome.ucsc.edu/goldenPath/help/bigBed.html",
      "responsible_organization": [
        "B2AI_ORG:119"
      ]
    },
    {
      "id": "B2AI_STANDARD:20",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "bigWig",
      "description": "Big Wiggle Format",
      "related_to": [
        "B2AI_STANDARD:18",
        "B2AI_STANDARD:19"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "The bigWig format is an indexed binary file format developed by UCSC for efficient storage and visualization of dense, continuous genomic data displayed as graphs in genome browsers. Created from wiggle (wig) or bedGraph files using the wigToBigWig or bedGraphToBigWig utilities, bigWig files enable rapid data access by transferring only the portions needed to display specific genomic regions, rather than entire datasets. This sparse file caching mechanism provides dramatically faster performance than text-based formats when working with large-scale datasets such as ChIP-seq, RNA-seq coverage, methylation levels, or conservation scores. The format supports various visualization options including customizable graph types (bar or points), scaling parameters, smoothing windows, logarithmic transformations, and color schemes. BigWig files can also display sequence logos using the dynseq feature, which scales nucleotide characters by base-resolution scores. The format includes utilities for data extraction (bigWigToBedGraph, bigWigToWig, bigWigSummary, bigWigAverageOverBed) and file inspection (bigWigInfo), making it suitable for both visualization and downstream computational analysis.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://genome.ucsc.edu/goldenPath/help/bigWig.html",
      "responsible_organization": [
        "B2AI_ORG:119"
      ]
    },
    {
      "id": "B2AI_STANDARD:21",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "BAM/CRAM",
      "description": "Binary Alignment Map / Compressed Reference-oriented Alignment Map",
      "related_to": [
        "B2AI_STANDARD:22"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:5",
          "category": "B2AI:Application",
          "name": "DeepVariant Deep Learning Variant Calling",
          "description": "DeepVariant uses BAM/CRAM alignment files as input to train convolutional neural networks that classify candidate genomic variants with high accuracy across multiple sequencing technologies. The model transforms aligned reads into image-like pileup tensors that capture base qualities, mapping qualities, and strand information, enabling the CNN to learn complex patterns that distinguish true genetic variants from sequencing artifacts and alignment errors. This approach achieves state-of-the-art performance on benchmark datasets and generalizes well across Illumina, PacBio, and Oxford Nanopore sequencing platforms without technology-specific parameter tuning.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1038/nbt.4235",
            "https://doi.org/10.1038/s41587-021-01108-x"
          ]
        },
        {
          "id": "B2AI_APP:101",
          "category": "B2AI:Application",
          "name": "Deep Learning for Structural Variant Detection",
          "description": "Deep learning models trained on BAM/CRAM files enable detection of complex structural variants including deletions, duplications, inversions, and translocations that are challenging for traditional callers. Neural networks analyze read depth, split reads, discordant read pairs, and local assembly features from aligned data to identify structural rearrangements, with applications in cancer genomics where somatic structural variants drive tumorigenesis and treatment resistance. These models improve sensitivity for detecting variants in repetitive regions and provide better breakpoint resolution than conventional methods.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1038/s41467-022-28289-w",
            "https://doi.org/10.1093/bioinformatics/btab732"
          ]
        }
      ],
      "collection": [
        "fileformat",
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:13"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:117",
        "B2AI_ORG:116"
      ],
      "purpose_detail": "BAM (Binary Alignment/Map) and CRAM (Compressed Reference-oriented Alignment Map) are binary formats for storing DNA sequence alignments to reference genomes, maintained by the GA4GH Large Scale Genomics work stream. BAM is the binary equivalent of the text-based SAM (Sequence Alignment/Map) format, providing efficient storage and retrieval of aligned sequencing reads with quality scores, alignment positions, CIGAR strings, and optional tags defined in the SAMtags specification. CRAM (currently version 3.x) achieves superior compression ratios by storing differences from a reference sequence rather than full sequence data, using custom compression codecs detailed in the CRAMcodecs specification. Both formats support indexing (BAI for BAM, CRAI for CRAM, and CSI as a more scalable successor) enabling rapid random access to genomic regions. The formats are widely supported by genomics toolkits including samtools, htslib, htsjdk, and GATK, with standard operations for sorting, merging, filtering, and format conversion. CRAM offers significant storage savings particularly important for large-scale projects, while maintaining full compatibility with SAM/BAM workflows. Both formats support the htsget protocol for parallel streaming access and can be wrapped with crypt4gh encryption for secure data sharing.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://samtools.github.io/hts-specs/",
      "responsible_organization": [
        "B2AI_ORG:34"
      ]
    },
    {
      "id": "B2AI_STANDARD:22",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "BAM",
      "description": "Binary Alignment Map format",
      "related_to": [
        "B2AI_STANDARD:21"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "A BAM file (.bam) is the binary version of a SAM file.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikipedia.org/wiki/Binary_Alignment_Map",
      "formal_specification": "https://samtools.github.io/hts-specs/SAMv1.pdf",
      "responsible_organization": [
        "B2AI_ORG:34"
      ]
    },
    {
      "id": "B2AI_STANDARD:23",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "2bit",
      "description": "Binary sequence information Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "A .2bit file stores multiple DNA sequences (up to 4 Gb total) in a compact randomly-accessible format. The file contains masking information as well as the DNA itself.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://genome.ucsc.edu/FAQ/FAQformat.html#format7",
      "responsible_organization": [
        "B2AI_ORG:119"
      ]
    },
    {
      "id": "B2AI_STANDARD:24",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "BCF",
      "description": "Binary variant call format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "A binary version of the variant call format (VCF).",
      "is_open": true,
      "requires_registration": false,
      "url": "https://samtools.github.io/bcftools/bcftools.html"
    },
    {
      "id": "B2AI_STANDARD:25",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "BioCompute",
      "description": "BioCompute Object standard",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:1"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:44"
      ],
      "purpose_detail": "BioCompute Objects (BCOs) are a formal standard for representing bioinformatics computational workflows and analyses, designed to facilitate communication and reproducibility in high-throughput sequencing research. Developed through a collaborative effort and formally recognized as IEEE Standard 2791-2020, BCOs structure critical workflow information into standardized domains including provenance, usability, extension, description, execution, parametric, input/output, and error domains. BCOs are represented in JSON format adhering to JSON schema draft-07, making them both human and machine readable. The standard addresses the challenge of documenting complex bioinformatics methods by providing predictable structure and stability for workflow communication. Upon assignment of a digital etag, three domains become immutable to ensure integrity: the Parametric Domain, Execution Domain, and I/O Domain. BCOs support regulatory compliance including FDA Title 21 CFR Part 11 considerations for electronic records and digital signatures, with hash values and encryption keys generated from execution and parametric domains for validation purposes.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://docs.biocomputeobject.org/user_guide/",
      "publication": "doi:10.5731/pdajpst.2016.006734"
    },
    {
      "id": "B2AI_STANDARD:26",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Biolink",
      "description": "Biolink Model",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:6",
          "category": "B2AI:Application",
          "name": "Translator Knowledge Graph for Drug Repurposing",
          "description": "NCATS Biomedical Data Translator uses Biolink Model to integrate diverse biomedical knowledge sources into a unified knowledge graph supporting AI-driven drug repurposing and mechanism discovery. The standardized Biolink schema enables graph neural networks to perform multi-hop reasoning across chemical-protein-disease relationships, identifying candidate therapeutics by connecting drugs to diseases through intermediate biological entities. The Translator system's reasoning agents leverage Biolink predicates to score and rank hypothesized drug-disease associations based on mechanistic evidence paths learned from integrated data.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1111/cts.13302"
          ]
        }
      ],
      "collection": [
        "datamodel"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:20"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:70"
      ],
      "purpose_detail": "Biolink Model is a comprehensive, high-level data model designed to standardize types and relationships in biological knowledge graphs. It provides a consistent framework for representing biological knowledge across various databases and formats, covering entities such as genes, diseases, chemical substances, organisms, genomics, phenotypes, and pathways. The model incorporates object-oriented classification and graph-oriented features, with a core set of hierarchical, interconnected classes (categories) and relationships (predicates). Biolink Model includes over 400 classes ranging from molecular entities like genes and proteins to clinical concepts like diseases and treatments, plus comprehensive predicates for describing relationships such as \"treats,\" \"causes,\" \"associated_with,\" and \"regulates.\" It supports advanced features like qualifiers for contextualizing relationships, evidence attribution, and knowledge provenance tracking. The model is particularly valuable for translational science applications, enabling integration of data from diverse sources including clinical databases, molecular biology repositories, and literature mining systems. It serves as the foundational schema for knowledge graphs in the Biomedical Data Translator project and other large-scale biomedical data integration efforts.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://biolink.github.io/biolink-model/",
      "publication": "doi:10.1111/cts.13302"
    },
    {
      "id": "B2AI_STANDARD:27",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "BEL",
      "description": "Biological Expression Language",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The Biological Expression Language (BEL) is a domain-specific language for representing scientific findings from life sciences literature in a computable, structured format. BEL captures causal and correlative relationships between biological entities (genes, proteins, complexes, biological processes, pathways) along with their experimental and publication context, enabling systematic knowledge representation and integration. BEL statements are expressed as triples (subject-relationship-object) that can be combined into biological networks and knowledge graphs. Each BEL assertion is packaged as a \"nanopub\" that includes provenance information, experimental conditions, and citations, allowing relationships to be properly evaluated in context. The language supports standard biological nomenclatures (Gene Ontology, HGNC, ChEBI, etc.) through namespace integration and provides a simplified syntax that is more accessible than traditional chemical notation. BEL's computable format enables applications in reverse causal reasoning, heat diffusion algorithms, prior knowledge for machine learning models, and detection of contradictory findings across literature. The standard is maintained by the BEL Language Committee with updates managed through BEL Enhancement Proposals (BEPs).",
      "is_open": true,
      "requires_registration": false,
      "url": "https://bel.bio/",
      "formal_specification": "https://language.bel.bio/"
    },
    {
      "id": "B2AI_STANDARD:28",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "BRIDG Model",
      "description": "Biomedical Research Integrated Domain Group Model",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:15",
        "B2AI_ORG:31",
        "B2AI_ORG:40",
        "B2AI_ORG:71"
      ],
      "purpose_detail": "The Biomedical Research Integrated Domain Group (BRIDG) Model is a collaborative effort engaging stakeholders from the Clinical Data Interchange Standards Consortium (CDISC), the HL7 BRIDG Work Group, the US National Cancer Institute (NCI), and the US Food and Drug Administration (FDA). The goal of the BRIDG Model is to produce a shared view of the dynamic and static semantics for the domain of basic, pre-clinical, clinical, and translational research and its associated regulatory artifacts.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://bridgmodel.nci.nih.gov/",
      "publication": "doi:10.1093/jamia/ocx004",
      "formal_specification": "https://github.com/CBIIT/bridg-model/"
    },
    {
      "id": "B2AI_STANDARD:29",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "BioPAX",
      "description": "BioPAX standard",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:21"
      ],
      "purpose_detail": "BioPAX is a standard language that aims to enable integration, exchange, visualization and analysis of biological pathway data. Specifically, BioPAX supports data exchange between pathway data groups and thus reduces the complexity of interchange between data formats by providing an accepted standard format for pathway data. By offering a standard, with well-defined semantics for pathway representation, BioPAX allows pathway databases and software to interact more efficiently. In addition, BioPAX enables the development of pathway visualization from databases and facilitates analysis of experimentally generated data through combination with prior knowledge. The BioPAX effort is coordinated closely with that of other pathway related standards initiatives namely; PSI-MI, SBML, CellML, and SBGN in order to deliver a compatible standard in the areas where they overlap.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://www.biopax.org/",
      "publication": "doi:10.1038/nbt.1666"
    },
    {
      "id": "B2AI_STANDARD:30",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Bioschemas",
      "description": "Bioschemas",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:1"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:28"
      ],
      "purpose_detail": "Bioschemas is a community-driven initiative that extends Schema.org to improve the findability and discoverability of life sciences resources on the web through structured markup. The project makes two main contributions to the life sciences community - proposing new types and properties to Schema.org specifically for life science resources, and defining usage profiles over existing Schema.org types that specify essential, recommended, and optional properties for consistent markup. Bioschemas profiles significantly simplify the markup process by reducing complex Schema.org types to manageable subsets while ensuring compatibility with search engines like Google Dataset Search. The initiative has achieved major recognition with six Bioschemas types (BioChemEntity, ChemicalSubstance, Gene, MolecularEntity, Protein, Taxon) officially included in Schema.org version 13.0. Endorsed by the European Research Council and serving as a flagship policy of ELIXIR, Bioschemas supports FAIR data principles by enabling automated discovery, collation, and analysis of distributed life sciences resources including datasets, software applications, training materials, and biological entities across the web ecosystem.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://bioschemas.org/",
      "formal_specification": "https://github.com/BioSchemas/specifications"
    },
    {
      "id": "B2AI_STANDARD:31",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "BRISQ",
      "description": "Biospecimen Reporting for Improved Study Quality",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:1"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:72"
      ],
      "purpose_detail": "Human biospecimens are subject to a number of different collection, processing, and storage factors that can significantly alter their molecular composition and consistency. These biospecimen preanalytical factors, in turn, influence experimental outcomes and the ability to reproduce scientific results. Currently, the extent and type of information specific to the biospecimen preanalytical conditions reported in scientific publications and regulatory submissions varies widely. To improve the quality of research utilizing human tissues, it is critical that information regarding the handling of biospecimens be reported in a thorough, accurate, and standardized manner. The Biospecimen Reporting for Improved Study Quality (BRISQ) recommendations outlined herein are intended to apply to any study in which human biospecimens are used. The purpose of reporting these details is to supply others, from researchers to regulators, with more consistent and standardized information to better evaluate, interpret, compare, and reproduce the experimental results. The BRISQ guidelines are proposed as an important and timely resource tool to strengthen communication and publications around biospecimen-related research and help reassure patient contributors and the advocacy community that the contributions are valued and respected",
      "is_open": true,
      "requires_registration": false,
      "url": "https://doi.org/10.1002/cncy.20147",
      "publication": "doi:10.1002/cncy.20147"
    },
    {
      "id": "B2AI_STANDARD:32",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "BioXSD",
      "description": "BioXSD",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:7",
          "category": "B2AI:Application",
          "name": "FreeContact Protein Contact Prediction ML Output Format",
          "description": "FreeContact implements statistical learning-based approaches for protein residue-residue contact prediction using mean-field Direct Coupling Analysis and PSICOV sparse inverse covariance methods. The tool explicitly supports BioXSD as an output format to facilitate integration into bioinformatics workflows and web services, with planned BioXSD input support. This demonstrates BioXSD serving as the standardized exchange format for machine learning tool outputs, enabling incorporation of ML-based contact predictions into interoperable analysis pipelines.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1186/1471-2105-15-85"
          ]
        },
        {
          "id": "B2AI_APP:107",
          "category": "B2AI:Application",
          "name": "OntoDT Datatype Ontology for ML Dataset Repositories",
          "description": "The OntoDT generic ontology of datatypes uses BioXSD to improve representation of basic bioinformatics datatypes in exchange formats, enabling construction of taxonomies for datasets, data mining tasks, generalizations, and algorithms. OntoDT can be used for annotation and querying of machine learning dataset repositories and for constructing data mining workflows. This links BioXSD datatype representations to ML and data mining ontologies, supporting standardized workflow assembly through consistent bioinformatics datatype semantics.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1016/j.ins.2015.08.006"
          ]
        }
      ],
      "collection": [
        "datamodel"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:20"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:28"
      ],
      "purpose_detail": "BioXSD is a unified data model and family of interoperable exchange formats (XML, JSON, YAML, and binary serializations) designed to represent basic bioinformatics data types including biological sequences (DNA, RNA, protein), multiple sequence alignments, annotated feature records with genomic coordinates and associated metadata, database cross-references with accession identifiers, and provenance metadata aligned with the W3C PROV standard for tracking data origins and transformations. The data model is expressed as an XML Schema (currently version 1.1) defining a hierarchical namespace (http://bioxsd.org/BioXSD-1.1) with complex types for sequence records containing raw sequence strings with quality scores, feature annotations with start/end positions and strand information, alignment objects with gap representations and consensus sequences, and rich metadata fields for database identifiers, organism taxonomy, publication references, and analysis provenance. BioXSD is designed to fill the gap between specialized domain-specific XML formats (SBML for systems biology, MAGE-ML for microarrays, PDBML for protein structures, PhyloXML for phylogenetics) by providing a canonical intermediate exchange format that is sufficiently rich to enable lossless conversions among diverse bioinformatics file formats (FASTA, GenBank, EMBL, UniProt, GFF, BED) while remaining simple enough for straightforward implementation across multiple programming languages and web service architectures. The format has been developed through a \"blue-collar\" community-driven approach initiated by the EMBRACE project and 1st DBCLS BioHackathon participants, with data-type definitions semantically annotated using EDAM ontology terms to provide globally defined controlled vocabularies for tool interfaces and service descriptions. BioXSD supports both RESTful HTTP web services and WS-I compliant SOAP web services, interoperating with standard HTTP, SOAP, and XML libraries in common programming languages (Python, Java, Perl, R) without requiring additional infrastructure beyond standard HTTP and XML parsers. The BioXSD|GTrack family developed by ELIXIR Norway offers multiple serialization options with lossless interconversion: tree-structured formats (BioXSD XML, BioJSON, BioYAML) for hierarchical data and web service APIs, tabular GTrack for genome features compatible with tools expecting columnar input, binary BTrack for efficient storage and processing, and GSuite for integrative multi-track analyses, all sharing a common data model with consistent semantics across serializations. BioXSD has been adopted by multiple bioinformatics service providers including CBS Denmark (MaxAlign, ProP, NetNES), University of Bergen Norway (BLAST), IBCP France (ClustalW, GorIV), and TU Munich Germany (FreeContact protein contact prediction), demonstrating practical deployment in production web service infrastructures. The format is particularly valuable for machine learning and computational biology workflows where BioXSD serves as the standardized exchange format for ML tool outputs (FreeContact uses BioXSD for residue-residue contact predictions from mean-field Direct Coupling Analysis and PSICOV sparse inverse covariance methods), enables construction of datatype ontologies for annotating ML dataset repositories and assembling data mining workflows (OntoDT uses BioXSD to improve representation of bioinformatics datatypes for querying algorithm repositories), facilitates integration of diverse tools into automated analysis pipelines through consistent data serialization, supports reproducible computational research by encoding complete provenance chains documenting data transformations and parameter settings, and enables federated data sharing where BioXSD's semantic annotations allow automated discovery and composition of compatible services without requiring custom format converters for each tool pair, thereby reducing the N-squared integration problem to N conversions between native formats and the canonical BioXSD representation.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://bioxsd.org/",
      "publication": "doi:10.1093/bioinformatics/btq391",
      "formal_specification": "https://github.com/bioxsd/bioxsd"
    },
    {
      "id": "B2AI_STANDARD:33",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "BIDS",
      "description": "Brain Imaging Data Structure",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:8",
          "category": "B2AI:Application",
          "name": "BIDS Apps for ML-Ready Preprocessing and Feature Extraction",
          "description": "Containerized BIDS Apps such as fMRIPrep, MRIQC, and MRtrix3 Connectome accept BIDS datasets and emit standardized derivatives including preprocessed time series, image quality metrics, and connectivity matrices that function directly as inputs or labels for ML workflows. The BIDS Apps ecosystem uses containerization to provide reproducible preprocessing pipelines across systems including HPC environments via Singularity, enabling consistent feature extraction for training machine learning models on neuroimaging data.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1371/journal.pcbi.1005209"
          ]
        },
        {
          "id": "B2AI_APP:108",
          "category": "B2AI:Application",
          "name": "PyBIDS and MNE-BIDS Programmatic Dataset Assembly",
          "description": "PyBIDS provides programmatic access to query BIDS datasets and their metadata and to organize derivatives in BIDS-Derivatives format, facilitating reproducible train-validation splits and feature assembly for ML pipelines. MNE-BIDS provides an integration layer for MNE-Python enabling standardized ingestion of EEG, MEG, and iEEG BIDS data into analysis pipelines that feed ML methods. These tools enable automated dataset selection, metadata-driven curation, and scalable learning across neuroimaging studies.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1162/imag_a_00103"
          ]
        },
        {
          "id": "B2AI_APP:109",
          "category": "B2AI:Application",
          "name": "BIDS Derivatives and Connectivity Specifications for ML Features",
          "description": "The BIDS Connectivity extension and Derivatives specifications define interoperable formats for structural and functional connectivity matrices, seed-based maps, tractograms, and tractometry across modalities including sMRI, fMRI, DWI, PET, EEG, iEEG, and MEG. These standardized derivative schemas enable common ML features to be shared and allow benchmarking of ML models on consistent feature representations, supporting reproducible radiomics and connectomics analyses.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1162/imag_a_00103"
          ]
        },
        {
          "id": "B2AI_APP:110",
          "category": "B2AI:Application",
          "name": "XGBoost-Based DICOM to BIDS Automated Conversion",
          "description": "An XGBoost classifier trained on DICOM metadata achieves 99.5% accuracy in classifying MRI acquisition types and automatically transforms unstructured clinical imaging into BIDS datasets with little to no user intervention. This ML-enabled conversion tool reduces manual curation burden, accelerates creation of standardized training corpora, and enables clinical imaging data to be rapidly prepared for downstream machine learning analyses in BIDS format.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1007/s12021-024-09659-5"
          ]
        }
      ],
      "collection": [
        "datamodel"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:22"
      ],
      "purpose_detail": "The Brain Imaging Data Structure (BIDS) is a simple and intuitive way to organize and describe data. This document defines the BIDS specification, which provides many details to help implement the standard. It includes the core specification as well as many extensions to specific brain imaging modalities, and increasingly also to other kinds of data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://bids-specification.readthedocs.io/en/stable/",
      "publication": "doi:10.1038/sdata.2016.44",
      "formal_specification": "https://github.com/bids-standard/bids-specification",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:3"
      ]
    },
    {
      "id": "B2AI_STANDARD:34",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "BFI",
      "description": "Brief Fatigue Inventory",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "diagnosticinstrument"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "The Brief Fatigue Inventory (BFI) is used to rapidly assess the severity and impact of cancer-related fatigue. An increasing focus on cancer-related fatigue emphasized the need for sensitive tools to assess this most frequently reported symptom. The six interference items correlate with standard quality-of-life measures.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.mdanderson.org/research/departments-labs-institutes/departments-divisions/symptom-research/symptom-assessment-tools/brief-fatigue-inventory.html",
      "formal_specification": "http://www.npcrc.org/files/news/brief_fatigue_inventory.pdf"
    },
    {
      "id": "B2AI_STANDARD:35",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "BPI",
      "description": "Brief Pain Inventory",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "diagnosticinstrument"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "The Brief Pain Inventory (BPI) rapidly assesses the severity of pain and its impact on functioning. The BPI has been translated into dozens of languages, and it is widely used in both research and clinical settings.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.mdanderson.org/research/departments-labs-institutes/departments-divisions/symptom-research/symptom-assessment-tools/brief-pain-inventory.html",
      "formal_specification": "http://www.npcrc.org/files/news/briefpain_short.pdf"
    },
    {
      "id": "B2AI_STANDARD:36",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "BED",
      "description": "Browser Extensible Data Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:219",
          "category": "B2AI:Application",
          "name": "StratoMod Variant-Calling Error Prediction with Interpretable ML",
          "description": "StratoMod employs Explainable Boosting Machine (EBM) classifiers to predict sequencing and variant-calling errors (true positives, false positives, false negatives) using BED files as the primary data structure throughout the workflow. VCF-derived variant labels are converted to 0-based BED format with coordinate shifts (start position reduced by 1) to enable accurate intersection with genomic context features, and SNV/INDEL label BED files are merged using bedtools -loj to produce per-variant binary labels (TP/FP/FN mapped to 0/1) for precision and recall models. Assembly-based benchmark regions from DeFrABB/dipcall defining 1:1 haplotype alignments (excluding gaps and large repeats) are represented as BED intervals delimiting evaluation zones where variants are considered for benchmarking. Numerous genomic-context feature tracks including GEM-based low-mappability regions (low-mappabilityall.bed.gz), nonunique mappability intervals (nonunique_l250_*.bed.gz), and SINE/LTR repeat annotations are provided as BED files, converted to binary features by setting interval lengths to 1 or appending a column indicating membership (e.g., \"in hard-to-map region\"), and intersected with variant positions using bedtools intersect -loj to derive feature values for model training. Missing values from non-overlapping intervals are filled with -1 or 0 depending on feature semantics, and the trained EBM model achieves interpretable predictions explaining technology-specific and callset-specific differences in false negative rates, demonstrating BED as the canonical format for labels, evaluation strata, and feature engineering in genomic ML pipelines.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.21203/rs.3.rs-2613736/v1"
          ]
        },
        {
          "id": "B2AI_APP:220",
          "category": "B2AI:Application",
          "name": "Deep Learning Epigenomic Profile Prediction with Peak-Centered Datasets",
          "description": "A comprehensive evaluation framework for deep learning models predicting chromatin accessibility and epigenomic profiles from DNA sequence (including BPNet, Basenji, and other CNN/RNN architectures) uses BED-format IDR (Irreproducible Discovery Rate) peak files from ENCODE as binary classification labels and to construct peak-centered training and evaluation datasets. IDR peak BED files are selected per ATAC-seq experiment, merged across cell lines to create unified peak sets, and used to extract fixed-size sequence windows (e.g., 2 kb) centered on peak summits, ensuring each training sequence contains at least one peak across all cell lines being modeled. This peak-centered dataset construction contrasts with coverage-threshold approaches using bigWig files, enabling systematic comparison between binary peak-based modeling (where BED peaks serve as targets) and quantitative signal regression. Replicate 1 peak BEDs are used to build train/validation/test partitions while replicate 2 provides an experimental performance ceiling, and the resulting models are evaluated on downstream tasks including variant effect prediction and model interpretability analysis using the GOPHER framework, demonstrating BED as the standard format for defining regulatory element locations as binary labels in sequence-to-function deep learning.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1038/s42256-022-00570-9"
          ]
        },
        {
          "id": "B2AI_APP:221",
          "category": "B2AI:Application",
          "name": "Pybedtools ML Feature Engineering and Interval Operations",
          "description": "Pybedtools, a Python wrapper for BEDTools, provides essential genomic interval manipulation capabilities that serve as preprocessing and feature engineering infrastructure for machine learning pipelines operating on BED-format data. The library enables set algebra operations (intersection, subtraction, addition) to extract unique or overlapping genomic features across multiple BED files, facilitating construction of positive/negative training sets by identifying variants overlapping functional annotations versus background regions. The closestBed -d functionality computes distances from query intervals (e.g., SNPs stored as BED) to nearest target features (e.g., gene boundaries from BED annotations), appending distance values as numeric columns that can serve as continuous features or distance-based targets for regression models. Interval objects expose elements by integer index (e.g., [-1] to retrieve appended distance field) and by named core attributes (chromosome, start, end, name, strand), enabling efficient feature extraction and label assignment within Python-based ML workflows. The stream=True option yields Python iterable Interval objects without writing intermediate files, reducing disk I/O and enabling scalable batch processing for large training/evaluation datasets, making pybedtools a foundational utility for genomic ML preprocessing where BED intervals represent labeled regions, feature annotations, or evaluation zones.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1093/bioinformatics/btr539"
          ]
        },
        {
          "id": "B2AI_APP:222",
          "category": "B2AI:Application",
          "name": "LanceOtron Deep Learning Peak Caller with BED-Based Labeling",
          "description": "LanceOtron is a deep learning peak caller for ATAC-seq, ChIP-seq, and DNase-seq that uses convolutional neural networks with enrichment scoring, employing BED format throughout the supervised learning pipeline for candidate peak generation, manual labeling, and model output. Putative peak regions are generated by three methods (MACS2 and two mean-based enrichment approaches) and aggregated within randomly selected 1 Mb windows for labeling, with overlapping candidate peaks from multiple calling permutations resolved using pybedtools (Python implementation of BEDTools) by randomly selecting one overlapping region per cluster for manual visual inspection. Regions sized 50 bp to 2 kb are visually labeled as 'peak' or 'noise' based on clear signal patterns, producing a labeled training set of 736,753 regions (5,016 peaks, 731,737 noise) covering 499 Mb, with region coordinates maintained in BED-compatible interval format. An algorithmic labeling pipeline supplements manual labels by smoothing coverage signals with a 400 bp rolling mean, marking coordinates exceeding 4-fold the chromosome mean as enriched, merging adjacent enriched coordinates, and retaining candidate peaks within the 50 bp2 kb size range (recursively adjusting thresholds for larger regions). The trained deep neural network produces predicted peak coordinates exported as BED files, enabling direct integration with downstream genomic analysis workflows and genome browsers, demonstrating BED as both the labeled training data format and the standard model output format for supervised learning-based peak calling.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1101/2021.01.25.428108"
          ]
        },
        {
          "id": "B2AI_APP:223",
          "category": "B2AI:Application",
          "name": "Tumor-Only Somatic Variant Classification with TabNet and BED-Based Preprocessing",
          "description": "An attentive deep learning tumor-only somatic mutation classifier using TabNet, LightGBM, and XGBoost models to distinguish somatic from germline variants employs BED files for capture kit target definition, genomic blacklist filtering, and tumor mutational burden (TMB) normalization. Exon target BED files for three capture kits (Agilent SureSelect Human All Exon V6, IDT xGen Exome Research Panel V2, Illumina TruSeq Exome) were obtained from manufacturer websites in hg19 coordinates and converted to hg38 using UCSC liftOver tool to ensure consistent coordinate systems across datasets, with resulting target footprints of 33.0, 37.3, and 63.5 megabases computed from the lifted BED intervals. A BED-format genomic blacklist (hg38_simple_repeats.bed from UCSC) is applied as a snpblacklist filter to exclude SNPs in tandem repeat regions that are prone to sequencing errors and alignment artifacts. Target footprint sizes derived from capture kit BED files are used to normalize TMB estimates, computing a patient-weighted average target size of 41 Mb across cohorts to enable fair comparisons of mutation burden between samples sequenced with different capture kits. The TabNet attention-based neural network and gradient boosting models (LightGBM, XGBoost) are trained on variant features to predict somatic versus germline status, achieving high accuracy agnostic of tissue type and capture kit by incorporating BED-based preprocessing that standardizes genomic coordinates, filters problematic regions, and normalizes coverage-dependent features, illustrating BED as essential infrastructure for coordinate system management and quality control in clinical genomic ML applications.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1101/2021.12.07.471513"
          ]
        }
      ],
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "The Browser Extensible Data (BED) format is a flexible, tab-delimited text format designed for defining annotation tracks in genome browsers, particularly the UCSC Genome Browser. BED format provides a standardized way to represent genomic features with positional information, supporting both simple coordinate-based annotations and complex multi-exon structures. The format consists of 12 possible fields, with the first three (chromosome, start position, end position) being mandatory and nine additional optional fields providing detailed feature information including name, score, strand orientation, thick start/end coordinates for coding regions, RGB color values for visualization, and block structures for representing discontinuous features like exons and introns. BED files use zero-based, half-open coordinate system where the start position is inclusive and the end position is exclusive, enabling precise genomic interval representation. The format supports various annotation types from simple genomic intervals to complex gene models, making it essential for genomic data visualization, comparative genomics, and functional annotation workflows in bioinformatics research.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://genome.ucsc.edu/FAQ/FAQformat.html#format1",
      "formal_specification": "https://github.com/samtools/hts-specs/blob/master/BEDv1.pdf",
      "responsible_organization": [
        "B2AI_ORG:119"
      ],
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:53"
      ]
    },
    {
      "id": "B2AI_STANDARD:37",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "BPMN",
      "description": "Business Process Modeling Notation",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:10"
      ],
      "purpose_detail": "A graphical notation that depicts the steps in a business process. BPMN depicts the end-to-end flow of a business process. The notation has been specifically designed to coordinate the sequence of processes and the messages that flow between different process participants in a related set of activities.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.omg.org/bpmn/index.htm"
    },
    {
      "id": "B2AI_STANDARD:38",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "CMMN",
      "description": "Case Management Model and Notation",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:10"
      ],
      "purpose_detail": "Case Management Model and Notation (CMMN) is an Object Management Group (OMG) standard that defines a common meta-model, graphical notation, and XML-based interchange format for modeling and expressing adaptive case management processes, designed to complement BPMN by capturing work methods that involve unpredictable activity sequences, knowledge worker decision-making, and event-driven responses to evolving situations rather than rigidly structured process flows. CMMN centers on the concept of a case as a living information container that aggregates all relevant data, documents, and relationships for a particular matter (patient treatment, legal case, insurance claim, research project), with a case file serving as the central repository that evolves dynamically as the case progresses through various stages and milestones. The notation provides graphical symbols for case plan modeling including discretionary tasks that knowledge workers can choose to perform based on their expertise, required tasks with mandatory completion, event listeners that trigger activities when specific conditions occur, sentries (entry and exit criteria) that guard stage transitions using boolean expressions, case file items representing information assets with lifecycle states, milestones marking significant achievement points, and human tasks requiring user interaction with form-based data collection. CMMN supports the Adaptive Case Management paradigm where humans remain \"firmly in the driver's seat\" making contextualized decisions while the system provides structure, suggestions, and automated responses to predictable patterns, enabling flexibility for complex scenarios like clinical care coordination where treatment paths depend on patient-specific factors, diagnostic results, and expert clinical judgment rather than predetermined protocols. The standard defines an XSD schema for serializing case models in XML format with complete semantic preservation, enabling tool interoperability where case definitions created in one CMMN-compliant platform can be exchanged, imported, and executed in another, supporting collaborative process design across organizational boundaries. CMMN integrates with the OMG \"triple crown\" of process improvement standards: BPMN for structured predictable processes, CMMN for adaptive knowledge-intensive work, and DMN (Decision Model and Notation) for explicit business rule specification, allowing organizations to model hybrid workflows where standardized BPMN subprocesses can be embedded within CMMN cases and decisions can be delegated to DMN decision tables with inputs from the case file. In healthcare and biomedical research, CMMN is particularly valuable for modeling patient care pathways where treatment decisions depend on longitudinal observations and multidisciplinary consultation, clinical trial management where protocol deviations require documented case-by-case assessments, rare disease diagnosis workflows involving iterative testing and specialist referrals, personalized medicine programs tailoring interventions to genetic profiles and biomarker responses, and research data management where data collection, quality control, and analysis activities adapt to preliminary findings and emerging hypotheses. For AI and machine learning applications, CMMN provides a framework where case file data can train predictive models to suggest next-best actions (recommending diagnostic tests based on symptom patterns and prior case outcomes), ML-generated risk scores and classification results can populate case file items triggering sentries that automatically initiate appropriate response protocols, natural language processing can extract structured information from case notes and clinical narratives to enrich the case file, reinforcement learning agents can optimize case routing and resource allocation by learning from historical case resolution patterns, and human-in-the-loop ML systems can embed model predictions as discretionary tasks where domain experts review AI suggestions before taking action, creating audit trails documenting the interplay between automated intelligence and human expertise throughout case lifecycles.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.omg.org/cmmn/"
    },
    {
      "id": "B2AI_STANDARD:39",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "CARE",
      "description": "Case Report guidelines",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "purpose_detail": "The CARE (CAse REport) guidelines are internationally developed reporting standards created by an expert group to improve the accuracy, transparency, and usefulness of medical case reports by providing a structured checklist and framework for documenting individual patient encounters, clinical observations, and treatment outcomes with explicit focus on why particular observations are important in the context of existing knowledge. The CARE checklist comprises 13 items organized across four main sections: Title (indicating case report with key identifying information), Keywords (2-5 words identifying topics), Abstract (structured summary with Introduction, Patient Information, Clinical Findings, Timeline, Diagnostic Assessment, Therapeutic Interventions, Follow-up and Outcomes, Discussion including takeaway lessons, Patient Perspective), and comprehensive Main Text sections including Introduction (brief background and context with literature review), Patient Information (de-identified demographic and clinical characteristics, chief complaints, relevant medical/family/psychosocial history), Clinical Findings (diagnostic tests, assessment methods, imaging, laboratory results), Timeline (chronological table of key dates and events from symptom onset through treatment and follow-up), Diagnostic Assessment (diagnostic reasoning including differential diagnoses, challenges, prognostic characteristics), Therapeutic Intervention (types of intervention with dosages/durations/administration details, changes to interventions with rationales), Follow-up and Outcomes (clinician- and patient-assessed outcomes including adverse events, test results, treatment adherence, tolerability), Discussion (strengths/limitations, relevant medical literature, rationale for conclusions, takeaway lessons including clinical significance), Patient Perspective (when possible, patient's own account of symptoms, diagnosis, treatment experiences), and Informed Consent (documentation of patient consent for publication). The guidelines support the EQUATOR Network's mission to improve health research reporting by helping authors reduce bias risk through systematic documentation requirements, increase transparency through explicit reporting of timeline and decision-making processes, and provide early signals about what interventions work for which patients under which circumstances, particularly valuable for rare conditions, novel treatments, unexpected adverse events, and unusual disease presentations where randomized controlled trials are impractical. CARE guidelines have been endorsed by multiple medical journals and publishers (BMJ Case Reports, Journal of Medical Case Reports, JAMA, Lancet, BMC), translated into multiple languages (Spanish, Portuguese, German, Japanese, Korean, Chinese, Turkish), and integrated into online tools like CARE-writer (an application for organizing, formatting, and writing systematic case reports as preprints or journal submissions). The guidelines benefit multiple healthcare stakeholders: patients reviewing and comparing therapeutic options through transparent outcome reporting, clinicians engaging in peer-to-peer communication at conferences and in communities of practice, researchers developing testable hypotheses from clinical settings (for example, case reports of Zika-associated microcephaly in 2016 led to rapid hypothesis generation and public health responses), educators using real-world systematic case reports for case-based learning curricula, authors simplifying the case report writing process through structured guidance, and journal editors implementing standardized peer review criteria. For AI and machine learning applications, CARE-compliant case reports provide high-quality structured training data where the standardized format enables automated extraction of clinical variables (demographics, symptoms, test results, interventions, outcomes) for populating electronic health record databases and clinical data warehouses, natural language processing models can be trained on the consistent narrative structure to identify entities (diseases, drugs, procedures) and relationships (causal links between interventions and outcomes, temporal sequences from timeline sections), case-based reasoning systems can retrieve similar historical cases for differential diagnosis support by comparing structured patient information and clinical findings across repositories, machine learning classifiers can predict treatment success likelihood by learning patterns from follow-up outcome sections across aggregated case reports, phenotype extraction algorithms can identify rare disease characteristics and novel symptom clusters from standardized clinical findings descriptions, and federated learning approaches can train models across distributed case report databases without pooling sensitive patient data by leveraging the common CARE structure for consistent feature representation, enabling meta-analysis of rare events and personalized medicine insights that would be impossible with traditional aggregate study designs.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.care-statement.org/"
    },
    {
      "id": "B2AI_STANDARD:40",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Define",
      "description": "CDISC Case Report Tabulation Data Definition Specification",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "purpose_detail": "The CDISC Case Report Tabulation Data Definition Specification (define.xml) Version 1.0 reflects changes from a comment period through the Health Level 7 (HL7) Regulated Clinical Research Information Management Technical Committee (RCRIM) in December 2003 (www.hl7.org) and CDISC's website in September 2004 as well as the work done by the define.xml team in conjunction with the CDISC ODM team to add functionality, features, and additional documentation. This document specifies the standard for providing Case Report Tabulations Data Definitions in an XML format for submission to regulatory authorities (e.g., FDA). The XML schema used to define the expected structure for these XML files is based on an extension to the CDISC Operational Data Model (ODM). The 1999 FDA electronic submission (eSub) guidance and the electronic Common Technical Document (eCTD) documents specify that a document describing the content and structure of the included data should be provided within a submission. This document is known as the Data Definition Document (e.g., define.pdf in the 1999 guidance). The Data Definition Document provides a list of the datasets included in the submission along with a detailed description of the contents of each dataset. To increase the level of automation and improve the efficiency of the Regulatory Review process, define.xml can be used to provide the Data Definition Document in a machine-readable format.",
      "is_open": true,
      "requires_registration": true,
      "url": "https://www.cdisc.org/standards/data-exchange/define-xml",
      "responsible_organization": [
        "B2AI_ORG:15"
      ]
    },
    {
      "id": "B2AI_STANDARD:41",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "CDISC ADaM",
      "description": "CDISC Controlled Terminology for Analysis Dataset Model",
      "related_to": [
        "B2AI_STANDARD:42",
        "B2AI_STANDARD:43"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "CDISC ADaM (Analysis Dataset Model) Controlled Terminology is a comprehensive standardized vocabulary maintained by the NCI Enterprise Vocabulary Services (EVS) that defines the semantic concepts, codes, and codelists required for creating analysis-ready datasets in clinical trials following the CDISC ADaM standard, enabling consistent representation of derived variables, analysis parameters, statistical methodologies, and data derivation techniques used in regulatory submissions to FDA, EMA, and other health authorities. The terminology encompasses multiple codelist categories including Derivation Type (DTYPE) defining 30+ imputation and data derivation methods (LOCF last observation carried forward, BOCF baseline observation carried forward, AVERAGE, MAXIMUM, MINIMUM, INTERPOLATION, EXTRAPOLATION, LLOQ lower limit of quantification, PHANTOM record creation for missing visits, ML maximum likelihood), Date and Time Imputation Flags (DATEFL, TIMEFL) indicating the level of imputation for partial dates (Y for year imputed, M for month-day imputed, D for day imputed, H/M/S for time components), Parameter Type (PARAMTYP) distinguishing derived parameters calculated from other parameters versus directly observed measurements, Analysis Stratum (STRATA) defining subgroup variables like age categories, race, sex, tobacco product use status for stratified analyses, Subject Trial Status (SBJTSTAT) tracking completion/discontinuation/ongoing enrollment, and Pool for Integration (POOLINT) identifying combined datasets across multiple studies for integrated safety and efficacy analyses. ADaM terminology includes extensive support for clinical assessment instruments with parameter codes and names for validated scales: APACHE II (Acute Physiology and Chronic Health Evaluation II) with 20+ clinical parameters and total acute physiology score calculations, CHART-SF (Craig Handicap Assessment and Reporting Technique-Short Form) covering physical independence, cognitive independence, mobility, occupation, social integration, and economic self-sufficiency subscores across 19 questionnaire items, GAD-7 (Generalized Anxiety Disorder-7) and GDS (Geriatric Depression Scale Short Form) for mental health assessments, HAMD-17 (Hamilton Depression Rating Scale 17-item), and NEWS2 (National Early Warning Score 2) for clinical deterioration monitoring with trigger thresholds. The terminology provides specialized codelists for tobacco regulatory science including Tobacco Product Category Response (TPCATRS) distinguishing cigarettes, cigars, electronic nicotine delivery systems (ENDS), heated tobacco products, roll-your-own products, smokeless tobacco, pipe tobacco, and waterpipe products, Tobacco Product Use Status Response (TPUSRS) classifying current/former/never use states, Tobacco Use Transition Response (TBUTRS) capturing initiation, cessation, relapse, and product switching behaviors, and Input Parameter (INPRM) terms for population modeling including birth rate, mortality probability, net migration rate, prevalence, transition probability for computational models of public health interventions. Each terminology term is mapped to NCI Thesaurus concept codes (C-codes) providing formal definitions, synonyms, and semantic relationships that enable automated reasoning and cross-standard harmonization with other CDISC terminologies (SDTM Study Data Tabulation Model, Protocol, CDASH Clinical Data Acquisition Standards Harmonization). ADaM terminology is maintained as extensible codelists where sponsor organizations can add study-specific terms for novel assessments or custom analyses while preserving the core standard vocabulary, with updates released quarterly incorporating new clinical scales, analysis methodologies, and regulatory requirements as clinical trial practices evolve. For AI and machine learning applications in clinical trial analytics, ADaM controlled terminology enables standardized feature engineering where derivation type codes inform algorithm selection (choosing appropriate imputation methods for missing data - ML models can learn which DTYPE values historically produced most accurate predictions for specific endpoint patterns), automated quality control by training classifiers to detect inconsistent terminology usage across analysis datasets flagging non-standard codes that may indicate data processing errors, meta-analysis and cross-study learning where standardized parameter codes allow pooling of results across trials (ML models trained on CHART-SF or GAD-7 parameters from thousands of subjects across multiple studies to predict treatment response patterns), natural language processing to extract analysis specifications from statistical analysis plans and automatically generate corresponding ADaM terminology codelists, reinforcement learning for adaptive clinical trial designs where agents select optimal imputation strategies (LOCF vs BOCF vs ML estimation) based on interim analysis results, and federated learning across pharmaceutical companies where ADaM terminology provides the common semantic layer enabling collaborative model training on pooled efficacy/safety data without exposing proprietary study details, all while maintaining regulatory compliance through standardized documentation of every analysis decision and derivation method applied to clinical trial data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://evs.nci.nih.gov/ftp1/CDISC/ADaM/ADaM%20Terminology.html",
      "formal_specification": "https://evs.nci.nih.gov/ftp1/CDISC/ADaM/",
      "responsible_organization": [
        "B2AI_ORG:15"
      ]
    },
    {
      "id": "B2AI_STANDARD:42",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "CDISC Protocol",
      "description": "CDISC Controlled Terminology for Data Collection for Protocol",
      "related_to": [
        "B2AI_STANDARD:41",
        "B2AI_STANDARD:43"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Controlled terminology for biomedical protocols.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://evs.nci.nih.gov/ftp1/CDISC/Protocol/Protocol%20Terminology.html",
      "formal_specification": "https://evs.nci.nih.gov/ftp1/CDISC/Protocol/",
      "responsible_organization": [
        "B2AI_ORG:15"
      ]
    },
    {
      "id": "B2AI_STANDARD:43",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "CDISC TAUGs",
      "description": "CDISC Controlled Terminology for Therapeutic Area Standards",
      "related_to": [
        "B2AI_STANDARD:41",
        "B2AI_STANDARD:42"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:4",
        "B2AI_TOPIC:7"
      ],
      "purpose_detail": "CDISC Therapeutic Area User Guides (TAUGs) are specialized extensions of CDISC foundational standards (SDTM, CDASH, ADaM) that provide disease-specific metadata, controlled terminology, implementation guidance, annotated case report form examples, and data collection recommendations tailored to particular therapeutic areas, enabling consistent, standardized representation of clinical trial data for specific disease domains while supporting global regulatory submissions to agencies including FDA, EMA, and PMDA. TAUGs address the reality that different therapeutic areas present unique clinical concepts, disease-specific assessments, biomarkers, imaging modalities, specialized endpoints, and treatment paradigms that cannot be adequately captured by foundational standards alone, requiring domain-specific supplements that formalize how to model disease progression measures (e.g., ALSFRS-R for ALS, UPDRS for Parkinson's), tumor staging systems (TNM classification for oncology), cardiac imaging parameters (ejection fraction, wall motion abnormalities for cardiovascular trials), respiratory function tests (FEV1, FVC for asthma/COPD), viral load measurements (HIV RNA copies/mL for HIV trials, HCV RNA for hepatitis C), cognitive batteries (ADAS-Cog, MMSE for Alzheimer's disease), pain scales and physical function assessments (WOMAC for osteoarthritis, HAQ-DI for rheumatoid arthritis), and disease-specific adverse events (cytokine release syndrome in CAR-T therapy, infusion reactions in biologics trials, diabetic ketoacidosis in diabetes studies). CDISC has developed TAUGs for over 30 therapeutic areas including Acute Kidney Injury (standardizing creatinine, urine output, RIFLE/AKIN/KDIGO criteria for AKI staging), Alzheimer's Disease (neuropsychological test batteries, MRI volumetrics, amyloid/tau biomarkers, Clinical Dementia Rating), Asthma (spirometry, peak flow, asthma control questionnaires, exacerbation definitions), Breast Cancer (hormone receptor status, HER2 testing, RECIST tumor measurements, pathological complete response), Cardiovascular Disease (ECG intervals, echocardiography parameters, cardiac biomarkers troponin/BNP, MACE endpoints myocardial infarction/stroke/cardiovascular death), CDAD/Clostridioides difficile (stool frequency, Bristol Stool Scale, toxin assays, colonoscopy findings), Colorectal Cancer (microsatellite instability, KRAS/NRAS/BRAF mutation status, CEA tumor marker), COPD (post-bronchodilator spirometry, CAT/SGRQ questionnaires, exacerbation rates, 6-minute walk distance), COVID-19 (SARS-CoV-2 PCR/antigen testing, oxygen saturation, WHO ordinal scale for clinical improvement, inflammatory markers CRP/D-dimer/ferritin), Crohn's Disease (CDAI, endoscopic scoring Harvey-Bradshaw Index, fecal calprotectin, imaging for strictures/fistulas), Diabetes Type 1 and Type 2 (HbA1c, fasting/postprandial glucose, continuous glucose monitoring metrics time-in-range/glucose variability, insulin dosing, hypoglycemia events classified by severity, diabetes-related complications retinopathy/nephropathy/neuropathy), Diabetic Kidney Disease (urine albumin-creatinine ratio, eGFR trajectory, kidney biopsy histology), Duchenne Muscular Dystrophy (6-minute walk test, North Star Ambulatory Assessment, pulmonary function, cardiac MRI for fibrosis, dystrophin expression from muscle biopsy), Dyslipidemia (LDL-C, HDL-C, triglycerides, apolipoprotein B, Lp(a), treatment targets per ATP III/ACC-AHA guidelines), Ebola (viral load quantification, symptom onset and progression, hemorrhagic manifestations, survival outcomes), Heart Failure (NYHA functional class, natriuretic peptides BNP/NT-proBNP, ejection fraction from echocardiography/cardiac MRI, 6-minute walk, heart failure hospitalizations, Kansas City Cardiomyopathy Questionnaire), Hepatitis C (HCV RNA quantification, genotype, liver stiffness measurements FibroScan, sustained virologic response SVR12/SVR24 definitions, liver enzymes ALT/AST), HIV (CD4 count, HIV-1 RNA viral load, antiretroviral resistance testing genotype/phenotype, opportunistic infection prophylaxis and occurrences), Huntington's Disease (CAG repeat length, Unified Huntington's Disease Rating Scale motor/cognitive/behavioral/functional assessments, caudate volume from MRI), Hypertension (ambulatory blood pressure monitoring, home BP measurements, hypertensive urgency/emergency definitions, target organ damage assessment), Influenza (viral culture/PCR, symptom diaries, secondary bacterial infections, hospitalization for respiratory complications), Major Depressive Disorder (Hamilton Depression Rating Scale, MADRS, PHQ-9, remission defined as HAMD-17 7 or MADRS 10, suicidal ideation Columbia Suicide Severity Rating Scale), Multiple Sclerosis (relapse definitions, Expanded Disability Status Scale EDSS, MRI lesion counts T2/gadolinium-enhancing, optical coherence tomography for retinal nerve fiber layer), Non-Small Cell Lung Cancer (PD-L1 expression, EGFR/ALK mutation status, RECIST 1.1, iRECIST for immunotherapy trials, progression-free survival, overall survival), Obesity (BMI, waist circumference, body composition DEXA, metabolic parameters glucose/insulin/lipids, weight loss maintenance), Osteoarthritis (WOMAC pain/stiffness/function subscales, joint space width from radiographs, MRI cartilage morphology, rescue analgesic use), Parkinson's Disease (Unified Parkinson's Disease Rating Scale UPDRS motor/non-motor sections, Hoehn and Yahr staging, levodopa equivalent daily dose, \"on\"/\"off\" time diaries, DaTscan imaging), Prostate Cancer (PSA, Gleason score, TNM staging, bone scan for metastases, androgen deprivation therapy metrics testosterone levels), Psoriasis (Psoriasis Area and Severity Index PASI, body surface area affected, physician's global assessment, DLQI quality of life), Rheumatoid Arthritis (Disease Activity Score DAS28, American College of Rheumatology ACR20/50/70 response criteria, tender/swollen joint counts, CRP/ESR, rheumatoid factor/anti-CCP antibodies, radiographic progression Sharp/van der Heijde scores), Schizophrenia (Positive and Negative Syndrome Scale PANSS, Clinical Global Impression CGI, extrapyramidal symptoms scales, metabolic monitoring glucose/lipids/weight), Solid Tumors (RECIST 1.1 target/non-target lesions, disease control rate, duration of response, circulating tumor DNA, quality of life EORTC QLQ-C30), Transplantation (rejection episodes classified by Banff criteria for kidney/ISHLT for heart/lung, graft function creatinine/ejection fraction/FEV1, immunosuppressant drug levels tacrolimus/cyclosporine, donor-specific antibodies), Tuberculosis (sputum smear microscopy, culture, GeneXpert MTB/RIF, chest radiography, time to culture conversion, treatment outcomes per WHO definitions cure/completed/failed/died/lost-to-follow-up), and Ulcerative Colitis (Mayo Clinic Score, endoscopic appearance, histology, fecal calprotectin, clinical remission definitions). Each TAUG document typically includes scope defining included diseases and study phases, general considerations for therapeutic area-specific trial design and data collection, SDTM implementation guidance showing how to represent disease-specific findings in domains (LB for laboratory, EG for ECG, procedures in PR, questionnaires in QS with appropriate test codes from CDISC controlled terminology or standardized instruments), CDASH recommendations for case report form design with annotated CRF examples demonstrating question text, response options, and mapping to SDTM variables, ADaM considerations for analysis datasets including derivation of composite endpoints (e.g., MACE in cardiovascular, ACR response in rheumatology, sustained virologic response in hepatitis C), disease-specific controlled terminology extensions adding terms not in foundational CDISC CT (e.g., specific tumor histologies, rare adverse events unique to therapeutic area, proprietary assessment scales), and therapeutic area data standards team (TAUG team) roster listing volunteer subject matter experts from pharmaceutical companies, CROs, regulatory agencies, and academia who developed the guidance. TAUGs facilitate consistent data collection and submission practices across sponsors for the same disease enabling regulatory reviewers to more efficiently evaluate trials in a therapeutic area when data follows common standards, support integrated analyses pooling data across multiple trials (meta-analyses, network meta-analyses, indirect treatment comparisons) by ensuring harmonized endpoint definitions and data structures, enable development of therapeutic area-specific data repositories and disease registries where standardized data from multiple sources can be aggregated for real-world evidence generation and post-marketing surveillance, accelerate clinical trial startup by providing sponsors with ready-to-use CRF templates and SDTM implementation conventions reducing need to reinvent standards for each protocol, improve data quality by clarifying ambiguous data collection scenarios through worked examples (e.g., how to represent partial responses in oncology, how to handle missed doses in diabetes insulin trials, how to code cardiovascular events when patient experiences multiple MIs), support machine learning and AI applications in clinical research where standardized therapeutic area data enables training of predictive models across trials (e.g., predicting treatment response from baseline characteristics, early biomarker changes predicting long-term outcomes, natural language processing extracting adverse events from narrative fields when standardized coding applied), facilitate regulatory interactions where sponsors and reviewers can reference agreed-upon TAUG conventions avoiding lengthy discussions about data representation choices, enable global harmonization as TAUGs incorporate perspectives from FDA, EMA, PMDA ensuring data structures support requirements across regulatory regions reducing need for region-specific datasets, and position CDISC standards as living, evolving specifications where TAUGs are updated as therapeutic landscapes change (new biomarkers discovered, novel endpoints validated, emerging therapies like gene therapy/CAR-T requiring new data models). TAUG development follows CDISC's consensus-driven process involving public review periods where stakeholders comment on draft guidance, therapeutic area teams incorporating feedback iteratively, final publication as official CDISC standard with version control, and post-publication maintenance addressing implementation questions through Q&A documents and hosting therapeutic area-focused webinars and workshops. Integration with broader CDISC ecosystem positions TAUGs alongside foundational standards (SDTM, CDASH, ADaM), protocol standards (Protocol Representation Model defining study design elements adaptable to therapeutic area requirements), controlled terminology (CDISC CT with therapeutic area extensions), and Define-XML metadata (documenting therapeutic area-specific dataset structures and derivations), creating comprehensive infrastructure for standardized clinical research data management. Machine learning applications leveraging TAUG-standardized data span predictive modeling for patient stratification where therapeutic area-specific baseline covariates (disease severity scores, biomarkers, imaging parameters) predict treatment response enabling enrichment designs selecting patients most likely to benefit, trial simulation using historical control data from prior TAUG-compliant trials to optimize adaptive designs and sample size calculations, safety signal detection across therapeutic area where standardized adverse event coding and laboratory data enable aggregate analysis identifying rare safety signals not apparent in individual trials, natural language processing training on narrative fields (medical history, adverse event descriptions, concomitant medication indications) mapped to standardized codes improving automated coding accuracy, and real-world evidence generation linking TAUG-compliant clinical trial data to claims databases and electronic health records using common data models (OMOP, Sentinel) with TAUG-defined phenotype algorithms for disease identification and outcome ascertainment ensuring consistency between RCT populations and real-world cohorts. TAUGs exemplify how therapeutic area expertise, clinical research best practices, and data standardization converge to create actionable implementation guidance transforming abstract foundational standards into concrete disease-specific data models that balance rigor of standardization with flexibility to capture unique clinical phenomena, ultimately advancing the goal of making clinical trial data more FAIR (Findable, Accessible, Interoperable, Reusable) through discipline-specific conventions that respect domain knowledge while enabling cross-trial integration and secondary use of clinical research data for regulatory decision-making, evidence synthesis, and translational research accelerating therapeutic development and improving patient outcomes across diverse disease areas.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.cdisc.org/standards/therapeutic-areas",
      "responsible_organization": [
        "B2AI_ORG:15"
      ]
    },
    {
      "id": "B2AI_STANDARD:44",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "SDTM",
      "description": "CDISC Controlled Terminology Standards for Data Aggregation through Study Data Tabulation Model (including QRS, Medical Device and Pharmacogenomics Data)",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "purpose_detail": "A standard for organizing and formatting data to streamline processes in collection, management, analysis and reporting.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.cdisc.org/standards/foundational/sdtm",
      "formal_specification": "https://evs.nci.nih.gov/ftp1/CDISC/SDTM/",
      "responsible_organization": [
        "B2AI_ORG:15"
      ]
    },
    {
      "id": "B2AI_STANDARD:45",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "CDASH",
      "description": "CDISC Controlled Terminology Standards for Data Collection through Clinical Data Acquisition Standards Harmonization",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "purpose_detail": "The Clinical Data Acquisition Standards Harmonization (CDASH) is a CDISC foundational standard that establishes consistent data collection practices across clinical studies and sponsors. CDASH provides a standardized framework for designing case report forms (CRFs) and electronic data capture (eCRF) systems by defining a common set of data collection fields and structures. The standard specifies field names, labels, prompt questions, and controlled terminology for capturing clinical observations, ensuring that data can be directly traced to the Study Data Tabulation Model (SDTM) without extensive post-collection transformation. By harmonizing data collection at the source, CDASH reduces data cleaning efforts, improves data quality, and accelerates the transition from collection to analysis. The standard covers various therapeutic areas and data domains including demographics, vital signs, adverse events, concomitant medications, and laboratory results. CDISC provides a library of ready-to-use, CDASH-compliant annotated eCRFs in multiple formats (PDF, HTML, XML) that can be implemented as-is or customized for specific study needs, facilitating regulatory submission and data review processes.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.cdisc.org/standards/foundational/cdash",
      "formal_specification": "https://evs.nci.nih.gov/ftp1/CDISC/SDTM/",
      "responsible_organization": [
        "B2AI_ORG:15"
      ]
    },
    {
      "id": "B2AI_STANDARD:46",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "CDISC Dataset",
      "description": "CDISC Dataset-XML",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "purpose_detail": "CDISC Dataset-XML, which was released for comment under the name StudyDataSet-XML but was renamed to avoid confusion with the CDISC SDS team, is a new standard used to exchange study datasets in an XML format. The purpose of Dataset-XML is to support the interchange of tabular data for clinical research applications using ODM-based XML technologies. The Dataset-XML model is based on the CDISC Operational Data Model (ODM) standard and should follow the metadata structure defined in the CDISC Define-XML standard. Dataset-XML can represent any tabular dataset including SDTM, ADaM, SEND, or non-standard legacy datasets. Some noteworthy items relating to Dataset-XML v1.0 include alternative to SAS Version 5 Transport (XPT) format for datasets ODM-based model for representation of SEND, SDTM, ADaM or legacy datasets Capable of supporting CDISC regulatory data submissions Based on Define-XML v2 or v1 metadata, easy to reference Dataset-XML supports all language encodings supported by XML.",
      "is_open": true,
      "requires_registration": true,
      "url": "https://www.cdisc.org/standards/data-exchange/dataset-xml",
      "responsible_organization": [
        "B2AI_ORG:15"
      ]
    },
    {
      "id": "B2AI_STANDARD:47",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "CDISC LAB",
      "description": "CDISC Laboratory Data Model",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:9",
          "category": "B2AI:Application",
          "name": "HL7 FHIR to CDISC Laboratory Mapping for ML-Ready EHR Data",
          "description": "The HL7 FHIR to CDISC Joint Mapping Implementation Guide includes mappings for the Laboratory domain that translate FHIR lab resources into CDISC variables, linked with CDISC LB-to-LOINC mapping guidance to leverage real-world data for clinical trials. This harmonization enables EHR laboratory observations to be transformed into standardized, ML-ready lab features with consistent semantic coding via LOINC, supporting downstream machine learning analyses, pooled studies, and data mining across clinical trial and real-world datasets.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.47912/jscdm.162"
          ]
        },
        {
          "id": "B2AI_APP:111",
          "category": "B2AI:Application",
          "name": "Metadata-Driven ETL for Scalable Lab Dataset Construction",
          "description": "CDISC Laboratory Model implementations use metadata-driven ETL approaches to produce standards-compliant lab datasets from source systems, reducing manual data preparation through automated transformation logic that operates at the metadata level rather than requiring code changes. This metadata-driven approach is foundational for building scalable, ML-ready dataset construction pipelines that can consistently process laboratory data across multiple studies and sites, enabling reproducible preprocessing for machine learning workflows.",
          "used_in_bridge2ai": false,
          "references": [
            "https://www.lexjansen.com/pharmasug/2002/proceed/DM/dm01.pdf"
          ]
        },
        {
          "id": "B2AI_APP:112",
          "category": "B2AI:Application",
          "name": "Integrated SDTM Laboratory Data for Pooled ML Analytics",
          "description": "Integration of clinical trial and real-world data using CDISC standards including the Laboratory domain produces harmonized SDTM and ADaM datasets that enable pooled analyses and data mining across multiple studies and sponsors. Standardized laboratory variables with CDISC Controlled Terminology facilitate dataset aggregation, warehousing, and reuse, supporting pooled machine learning analyses and real-world evidence research by providing consistent lab feature representations that serve as inputs for predictive modeling and downstream analytics.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.47912/jscdm.128"
          ]
        }
      ],
      "collection": [
        "datamodel"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "purpose_detail": "LAB provides a standard model for the acquisition and exchange of laboratory data, primarily between labs and sponsors or CROs. The LAB standard was specifically designed for the interchange of lab data acquired in clinical trials.",
      "is_open": true,
      "requires_registration": true,
      "url": "https://www.cdisc.org/standards/data-exchange/lab",
      "responsible_organization": [
        "B2AI_ORG:15"
      ]
    },
    {
      "id": "B2AI_STANDARD:48",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "CDISC ODM",
      "description": "CDISC Operational Data Model",
      "related_to": [
        "B2AI_STANDARD:689"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:10",
          "category": "B2AI:Application",
          "name": "Clinical Trial Data Integration and Predictive Modeling",
          "description": "CDISC ODM (Operational Data Model) is used in AI applications for standardizing clinical trial data exchange, enabling machine learning models to train on multi-study datasets and predict trial outcomes, patient enrollment, and safety events. AI systems leverage ODM's XML-based representation of study metadata, case report forms, and clinical data to automatically harmonize data from different trials, perform cross-study analyses, and develop predictive models for trial design optimization. The standard enables AI applications that forecast patient dropout rates, identify optimal sites for recruitment based on historical data, and detect protocol deviations through anomaly detection. ODM's structured format facilitates automated quality control and regulatory submission preparation through AI-driven validation.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1016/j.cct.2019.105820"
          ]
        }
      ],
      "collection": [
        "datamodel"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "purpose_detail": "The CSDIC ODM is a vendor-neutral, platform-independent format for exchanging and archiving clinical and translational research data, along with their associated metadata, administrative data, reference data, and audit information. ODM facilitates the regulatory-compliant acquisition, archival and exchange of metadata and data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.cdisc.org/standards/data-exchange/odm",
      "responsible_organization": [
        "B2AI_ORG:15"
      ]
    },
    {
      "id": "B2AI_STANDARD:49",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "CDISC PRM",
      "description": "CDISC Protocol Representation Model",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "purpose_detail": "The CDISC Protocol Representation Model Version 1.0 (PRM V1.0) is intended for those involved in the planning and design of a research protocol. The model focuses on the characteristics of a study and the definition and association of activities within the protocols, including arms and epochs. PRM V1.0 also includes the definitions of the roles that participate in those activities. The scope of this model includes protocol content including Study Design, Eligibility Criteria, and the requirements from the ClinicalTrials.gov and World Health Organization (WHO) registries. The majority of business requirements were provided by subject matter experts in clinical trial protocols. PRM V1.0 is based on the BRIDG Release 3.0 Protocol Representation sub-domain. It includes all classes in the BRIDG Protocol Representation sub-domain plus some classes from other BRIDG sub-domains, generally classes required for ClinicalTrials.gov and the WHO registries.",
      "is_open": true,
      "requires_registration": true,
      "url": "https://www.cdisc.org/standards/foundational/protocol",
      "responsible_organization": [
        "B2AI_ORG:15"
      ]
    },
    {
      "id": "B2AI_STANDARD:50",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "SEND",
      "description": "CDISC Standard for the Exchange of Nonclinical Data",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "purpose_detail": "The CDISC SEND is intended to guide the organization, structure, and format of standard nonclinical tabulation datasets for interchange between organizations such as sponsors and CROs and for submission to the US Food and Drug Administration (FDA)",
      "is_open": true,
      "requires_registration": true,
      "url": "https://www.cdisc.org/standards/foundational/send",
      "responsible_organization": [
        "B2AI_ORG:15"
      ]
    },
    {
      "id": "B2AI_STANDARD:51",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "CDISC SDM",
      "description": "CDISC Study Design Model in XML",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "purpose_detail": "The CDISC Study Design Model in XML (SDM-XML) version 1.0 allows organizations to provide rigorous, machine-readable, interchangeable descriptions of the designs of their clinical studies, including treatment plans, eligibility and times and events. As an extension to the existing CDISC Operational Data Model (ODM) specification, SDM-XML affords implementers the ease of leveraging existing ODM concepts and re-using existing ODM definitions. SDM-XML defines three key sub-modules  Structure, Workflow, and Timing  permitting various levels of detail in any representation of a clinical studys design, while allowing a high degree of authoring flexibility. The specification document is available for download as a PDF file. A ZIP file containing the XML Schemas, several examples, and an SDM-XML element and attribute reference also is available.",
      "is_open": true,
      "requires_registration": true,
      "url": "https://www.cdisc.org/standards/data-exchange/sdm-xml",
      "responsible_organization": [
        "B2AI_ORG:15"
      ]
    },
    {
      "id": "B2AI_STANDARD:52",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "CHADO",
      "description": "CHADO XML interchange Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:125"
      ],
      "purpose_detail": "Chado is a modular schema covering many aspects of biology, not just sequence data. Chado-XML has exactly the same scope as the Chado schema.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/GMOD/Chado",
      "publication": "doi:10.1093/bioinformatics/btm189",
      "formal_specification": "https://github.com/GMOD/Chado"
    },
    {
      "id": "B2AI_STANDARD:53",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "chain",
      "description": "Chain Format for pairwise alignment",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "The Chain format is a compact file format developed by UCSC for representing pairwise genomic sequence alignments that permit gaps in both the reference and query sequences simultaneously. Each chain alignment consists of a header line specifying score, chromosome identifiers, sizes, strands, and coordinates for both sequences, followed by alignment data lines describing ungapped blocks and the gaps between them. The format uses zero-based half-open intervals for coordinates and includes support for reverse-complement alignments through strand indicators. Chain files are particularly useful for comparing whole genomes or large genomic regions, representing syntenic relationships, structural variations, and evolutionary rearrangements. They are commonly used in genome browsers and liftOver tools for mapping genomic coordinates between different genome assemblies or between species. The format supports \"snake\" or rearrangement display modes that visualize complex structural variants including inversions, duplications, and translocations. Chain alignments can be produced by tools like BLASTZ and are widely used in comparative genomics workflows.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://genome.ucsc.edu/goldenPath/help/chain.html",
      "responsible_organization": [
        "B2AI_ORG:119"
      ]
    },
    {
      "id": "B2AI_STANDARD:54",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "CARD",
      "description": "CHARMM Card File Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:27"
      ],
      "purpose_detail": "The CARD file format is the standard means in CHARMM for providing a human readable and writable coordinate file.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://charmm-gui.org/charmmdoc/io.html"
    },
    {
      "id": "B2AI_STANDARD:55",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "CML",
      "description": "Chemical Markup Language",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:3"
      ],
      "purpose_detail": "CML (Chemical Markup Language) is an XML language designed to hold most of the central concepts in chemistry. It was the first language to be developed and plays the same role for chemistry as MathML for mathematics and GML for geographical systems. CML covers most mainstream chemistry and especially molecules, reactions, solid-state, computation and spectroscopy. Since it has a special flexible approach to numeric science it also covers a very wide range of chemical properties, parameters and experimental observation. It is particularly concerned with the communication between machines and humans, and machines to machines. It has been heavily informed by the current chemical scholarly literature and chemical databases. XML is a mainstream approach providing semantics for science, such as MathML, SBML/BIOPAX (biology), GML and KML (geo) SVG (graphics) and NLM-DTD, ODT and OOXML (documents). CML provides support for most chemistry, especially molecules, compounds, reactions, spectra, crystals and computational chemistry (compchem). CML has been developed by Peter Murray-Rust and Henry Rzepa since 1995. It is the de facto XML for chemistry, accepted by publishers and with more than 1 million lines of Open Source code supporting it. CML can be validated and built into authoring tools (for example the Chemistry Add-in for Microsoft Word). A list of CML-compliant and CML-aware software can be found on the software page. The infrastructure includes legacy converters, dictionaries and conventions, Semantic Web and Linked Open Data. There are several versions of the CML schema. The most recent schema is schema 3. This essentially relaxes many of the constraints imposed in the previous stable release (schema 2.4), allowing users to put together the elements and attributes in a more flexible manner to fit the data that they want to represent more easily.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.xml-cml.org/"
    },
    {
      "id": "B2AI_STANDARD:56",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "ClinGen Interpretation",
      "description": "ClinGen Interpretation Model",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:11",
          "category": "B2AI:Application",
          "name": "LitGen Semi-Supervised Literature Recommendation and Evidence Classification",
          "description": "LitGen uses semi-supervised deep learning trained on ClinGen VCI-curated papers annotated with ACMG/AMP-aligned evidence types, incorporating curator explanations and unlabeled ClinGen-linked papers to improve proxy labels for literature recommendation and evidence-type classification. The system leverages ClinGen Variant Curation Interface annotations to predict which papers support specific ACMG/AMP evidence criteria, achieving 7.9-12.6% relative performance improvement over supervised-only models and reducing manual curation burden for variant interpretation workflows.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1142/9789811215636_0007"
          ]
        },
        {
          "id": "B2AI_APP:113",
          "category": "B2AI:Application",
          "name": "ACMG/AMP Feature-Based Variant Pathogenicity Classification",
          "description": "Machine learning models explicitly encode ACMG/AMP evidence criteria as features in penalized logistic regression to produce probabilistic pathogenicity scores and rank variants of uncertain significance for prioritization. The approach treats ClinGen/ACMG guideline-based evidence levels as structured ML features, enabling guidelines-informed classification that slightly outperforms some in silico scores on certain VUS datasets and improves prioritization for clinical decision support in hereditary disease gene panels.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1038/s41598-022-06547-3"
          ]
        },
        {
          "id": "B2AI_APP:114",
          "category": "B2AI:Application",
          "name": "ClinGen SVI Calibration of Computational Predictors to PP3/BP4",
          "description": "A probabilistic framework maps continuous scores from missense variant prediction tools to ACMG/AMP PP3 and BP4 evidence strength levels using ClinGen/ACMG standards, establishing score intervals for Supporting, Moderate, and Strong evidence. This calibration enables automated, standardized assignment of computational evidence to ClinGen-aligned interpretation workflows, with most tools achieving Supporting level and several reaching Moderate or Strong evidence thresholds for pathogenic or benign classification.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1016/j.ajhg.2022.10.013"
          ]
        },
        {
          "id": "B2AI_APP:115",
          "category": "B2AI:Application",
          "name": "CGBench LLM Benchmarking for ClinGen Evidence Reasoning",
          "description": "CGBench uses ClinGen VCI and GCI entries with expert explanations as ground truth to evaluate large language models on evidence extraction, strength scoring, and explanation concordance for variant interpretation. The benchmark shows moderate extraction performance with GPT-4o achieving 49% precision and 79% recall, improved explanation concordance with few-shot prompting reaching 70%, but limited strength-change prediction accuracy, revealing both capabilities and gaps in LLM reasoning for ClinGen-aligned genomic curation tasks.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.48550/arxiv.2510.11985"
          ]
        }
      ],
      "collection": [
        "datamodel"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:4",
        "B2AI_TOPIC:35"
      ],
      "purpose_detail": "The ClinGen Interpretation Model is a comprehensive data model and exchange format designed to capture, structure, and communicate the clinical interpretation of genetic variants. It supports the representation of evidence, reasoning, provenance, and contextual information underlying variant pathogenicity assessments, aligning with ACMG/AMP guidelines and community standards. The model enables detailed documentation of what evidence was used, how it was applied, who performed each interpretive act, and how interpretations build upon prior knowledge. It is flexible enough to represent both simple assertions and fully evidence-based interpretations, and is implemented using industry-standard JSON-LD for interoperability. The ClinGen Interpretation Model is closely aligned with the Monarch Initiative's SEPIO ontology, supporting integration with other biomedical data models and facilitating automated reasoning, reproducibility, and data sharing in clinical genomics.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://dataexchange.clinicalgenome.org/interpretation/",
      "formal_specification": "https://github.com/clingen-data-model/clingen-interpretation"
    },
    {
      "id": "B2AI_STANDARD:57",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "CLSI AUTO16",
      "description": "CLSI Next-Generation In Vitro Diagnostic Interface",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Exchange of data about and produced by in vitro diagnostic tests.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://clsi.org/standards/products/automation-and-informatics/documents/auto16/",
      "responsible_organization": [
        "B2AI_ORG:16"
      ]
    },
    {
      "id": "B2AI_STANDARD:58",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MSF",
      "description": "CLUSTAL-W Alignment Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:13",
        "B2AI_TOPIC:26",
        "B2AI_TOPIC:28"
      ],
      "purpose_detail": "The MSF (Multiple Sequence Format) is a standardized file format produced by CLUSTAL-W for representing multiple sequence alignments of proteins or nucleic acids. It includes sequence metadata, alignment length, checksums for data integrity verification, and displays aligned sequences in blocks with position numbering. The format supports gap characters and ambiguity codes, making it widely compatible with bioinformatics tools for phylogenetic analysis, sequence conservation studies, and comparative genomics. MSF files preserve alignment quality information and are human-readable while maintaining machine-parseable structure for automated processing.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://bioinfo.nhri.edu.tw/gcg/doc/11.0/clustalw+.html"
    },
    {
      "id": "B2AI_STANDARD:59",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "DND",
      "description": "CLUSTAL-W Dendrogram Guide File Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:13",
        "B2AI_TOPIC:26",
        "B2AI_TOPIC:28"
      ],
      "purpose_detail": "Format for the tree (or dendrogram) used to guide the a multiple sequence alignment process.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://bioinfo.nhri.edu.tw/gcg/doc/11.0/clustalw+.html"
    },
    {
      "id": "B2AI_STANDARD:60",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "CODE-EHR",
      "description": "CODE-EHR best-practice framework for the use of structured electronic health-care records in clinical research",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "The CODE-EHR (Clinical Outcomes Data in EHR) framework establishes minimum and preferred standards for the design, conduct, analysis, and reporting of clinical research studies using routinely collected electronic health record data, developed through international consensus by the BigData@Heart consortium and European Society of Cardiology with participation from patients, patient advocacy groups, regulators, government agencies, leading medical journals, professional societies, academic institutions, pharmaceutical industry, and payers to enhance transparency, reproducibility, and confidence in EHR-based research findings across the disease spectrum. The framework addresses critical challenges in secondary use of healthcare data including data quality uncertainties, heterogeneous coding systems, variable completeness, linkage complexities, phenotype definition ambiguities, analytical reproducibility gaps, and governance variability that limit comparability across studies and stakeholder confidence in observational research compared to randomized controlled trials. CODE-EHR provides step-by-step guidance across five key domains with minimum standards (essential requirements for methodological rigor) and preferred standards (aspirational best practices for future research): Dataset construction and linkage requires clarifying data sources (single institution EHR, regional health information exchanges, national registries, claims databases), temporal coverage (date ranges for data availability), patient population characteristics (age, sex, demographics), completeness assessment (proportion of expected records present, missingness patterns), and linkage methodology when integrating multiple datasets (deterministic vs. probabilistic matching algorithms, linkage quality metrics including match rates and false positive/negative rates, handling of duplicate records); Data fit for purpose mandates documenting coding systems used (ICD-9/10/11 for diagnoses, CPT/HCPCS for procedures, RxNorm/ATC for medications, LOINC for laboratory tests, SNOMED CT for clinical findings), data transformations and manipulations (unit conversions, date/time standardization, free-text to structured data mapping via natural language processing), data quality assessment including validation against external references or chart review gold standards (positive predictive value, sensitivity, specificity of coded diagnoses), plausibility checks (biologically implausible values, temporal logic violations), and missing data characterization (mechanismsmissing completely at random, missing at random, missing not at randomwith implications for analytical validity); Disease outcome and definitions requires transparent specification of all computable phenotype definitions using explicit code lists and algorithms for patient identification (inclusion/exclusion criteria with specific codes and lookback periods), exposure/treatment definitions (medication exposure windows, dosing requirements, persistence criteria), procedures and interventions (with timing relative to outcomes), comorbidity definitions (Charlson Comorbidity Index, Elixhauser conditions with code mappings), and outcomes (primary and secondary endpoints with validation evidence, composite outcome components, censoring rules, competing risks), enabling other researchers to reproduce cohort selection, validate findings in independent datasets, and iteratively refine definitions; Analysis standards specify analytical approaches with sufficient detail for replication including cohort flow diagrams (CONSORT-style reporting showing number of patients screened, excluded at each step with reasons, final analytical cohort), handling of missing data (complete case analysis, multiple imputation methods and assumptions, inverse probability weighting), confounding control strategies (covariate adjustment, propensity score methodsmatching, stratification, inverse probability of treatment weightinginstrumental variables, difference-in-differences, regression discontinuity), exposure and outcome timing definitions (index date, baseline period for covariate assessment, washout periods, follow-up windows, grace periods for treatment discontinuation), statistical models (regression specifications, survival analysis censoring assumptions, time-varying covariates, model diagnostics), sensitivity analyses exploring robustness to design choices (alternative phenotype definitions, lookback period durations, covariate sets, statistical approaches), and reporting of effect estimates with measures of uncertainty (confidence intervals, p-values with multiple testing corrections if applicable); Ethics and governance domain addresses consent frameworks (broad consent for secondary research, opt-out mechanisms, consent waiver justifications), data privacy safeguards including de-identification approaches (HIPAA Safe Harbor method removing 18 identifier types, expert determination, synthetic data generation, aggregation/suppression of small cells), data access and sharing policies (open data, controlled access via data use agreements, federated query systems preventing data egress, differential privacy mechanisms), institutional review board or ethics committee approvals with protocol registration in public registries (ClinicalTrials.gov, EU Clinical Trials Register) documenting study objectives and methods prior to data access reducing selective reporting and p-hacking, patient and public involvement in research design and interpretation (patient advisory boards, plain language study summaries, return of results to participants), and transparent conflict of interest disclosures from investigators and funders. For artificial intelligence and machine learning applications on EHR data, CODE-EHR principles guide trustworthy model development where dataset construction standards ensure training/validation/test data provenance and quality are documented enabling assessment of dataset shift and domain adaptation needs; data fitness standards support feature engineering transparency (derived variable definitions, temporal feature construction like lookback windows for aggregating lab trends or medication counts, handling of sparse/irregular time-series); outcome definition standards ensure prediction targets have clinical validity (validated phenotypes, ground truth labels with known accuracy, clear clinical utility for the prediction task); analysis standards extend to ML-specific requirements including training procedures (cross-validation strategies, hyperparameter tuning without test set leakage, ensemble methods, regularization), performance metrics beyond AUROC (calibration curves, decision curve analysis quantifying net benefit, sensitivity/specificity at clinically relevant thresholds, performance across patient subgroups assessing fairness), model interpretability and explainability (SHAP values, attention weights, concept activation vectors, counterfactual explanations), uncertainty quantification (conformal prediction, Bayesian credible intervals), external validation on independent healthcare systems or time periods, prospective evaluation measuring real-world clinical impact not just predictive accuracy, and continuous monitoring for model drift requiring retraining; governance standards for ML include algorithmic transparency (model cards documenting intended uses, training data characteristics, performance benchmarks, fairness metrics, limitations), consent considerations for automated decision-making, data sharing enabling independent model validation and adversarial testing, and regulatory pathways (FDA software as medical device, CE marking in Europe). CODE-EHR also informs federated learning on multi-institutional EHR data where standards ensure harmonized phenotype definitions across sites, consistent data quality metrics, coordinated governance frameworks (distributed institutional review boards, federated data use agreements, privacy-preserving analytics), and evaluation of model generalization versus site-specific performance heterogeneity. The framework supports causal inference from observational EHR data by emphasizing directed acyclic graphs documenting assumed causal relationships and confounders, sensitivity analyses for unmeasured confounding (E-values quantifying minimum strength of unmeasured confounders to nullify findings), negative control outcomes (exposures or outcomes with no plausible causal pathway serving as bias indicators), and triangulation with evidence from diverse study designs (comparing EHR findings to randomized trials, Mendelian randomization using genetic instruments, sibling designs controlling familial confounding). Ultimately, CODE-EHR establishes a methodological and reporting standard elevating EHR-based research to scientific rigor comparable to prospective studies, enabling reproducibility through transparent reporting, facilitating evidence synthesis through standardized definitions, supporting regulatory and clinical guideline decision-making through credible observational evidence, and fostering patient and public trust through ethical governance and stakeholder engagement, positioning EHR data as a core research infrastructure for precision medicine, comparative effectiveness research, pharmacovigilance, health services research, and AI-driven clinical decision support.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.bigdata4betterhearts.eu/News/ID/146/More-about-the-CODE-EHR-approach-and-framework",
      "publication": "doi:10.1016/S2589-7500(22)00151-0"
    },
    {
      "id": "B2AI_STANDARD:61",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "CXI",
      "description": "Coherent X-ray Imaging Data Bank format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "purpose_detail": "The CXI format was created as common format for all the data in the Coherent X-ray Imaging Data Bank (CXIDB). Naturally its scope is all experimental data collected during Coherent X-ray Imaging experiments as well as all data generated during the analysis of the experimental data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.cxidb.org/cxi.html"
    },
    {
      "id": "B2AI_STANDARD:62",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "CCPN",
      "description": "Collaborative Computing Project for the NMR community data model",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:12",
          "category": "B2AI:Application",
          "name": "CcpNmr-Integrated Deep Learning for HNCA Backbone Assignment",
          "description": "Deep neural networks integrated with the CcpNmr AnalysisAssign software analyze HNCA spectrum line shapes to yield amino acid type probabilities for automated protein backbone assignment. The networks are trained on synthetic databases with realistic instrumental artifacts and noise, taking 2D slices of pyruvate-patterned HNCA spectra as input and outputting probability tables that combine with primary sequence information for rapid assignment. This ML integration within the CCPN software framework accelerates NMR analysis workflows, though networks require retraining when experimental parameters change.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1126/sciadv.ado0403"
          ]
        }
      ],
      "collection": [
        "datamodel"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The CCPN Data Model for macromolecular NMR is intended to cover all data needed for macromolecular NMR spectroscopy from the initial experimental data to the final validation. It serves for exchange of data between programs, for storage, data harvesting, and database deposition. The data model proper is an abstract description of the relevant data and their relationships - it is implemented in the modeling language UML. From this CCPN autogenerates interfaces (APIs) for various languages, format description and I/O routines, and documentation.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://sites.google.com/site/ccpnwiki/home/documentation/ccpnmr-analysis/core-concepts/the-ccpn-data-model",
      "publication": "doi:10.1002/prot.20449"
    },
    {
      "id": "B2AI_STANDARD:63",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "CFDE C2M2",
      "description": "Common Fund Data Ecosystem Crosscut Metadata Model",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:13",
          "category": "B2AI:Application",
          "name": "CFDE Workbench ML-Ready Data Harmonization and Knowledge Graph Integration",
          "description": "The CFDE Workbench uses C2M2 to harmonize metadata and host processed data in standardized, AI-ready formats that can be loaded as data frames and integrated into ML workflows. C2M2-derived metadata feeds PostgreSQL databases supporting complex queries for ML dataset preparation, while planned conversion to Neo4j knowledge graphs will align database structure with the metadata model to enhance search performance and facilitate ML applications. This harmonization across Common Fund programs reduces barriers to assembling training corpora and interoperable inputs for cross-program AI/ML analyses.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1101/2025.02.04.636535"
          ]
        },
        {
          "id": "B2AI_APP:116",
          "category": "B2AI:Application",
          "name": "LLM-Powered Chatbot with C2M2-Grounded Workflow Automation",
          "description": "The CFDE Workbench integrates an LLM chatbot using OpenAI assistants API that answers Common Fund program and data questions while executing API calls based on standardized workflow specifications. The chatbot leverages C2M2-harmonized resources and codified ETL workflows through the Playbook Workflow Builder to automate discovery and retrieval, using function calling to ensure comprehensive and reproducible results. This direct AI application is enabled by C2M2-backed infrastructure that provides the structured metadata and APIs necessary for the LLM to interact with heterogeneous datasets.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1101/2025.02.04.636535"
          ]
        },
        {
          "id": "B2AI_APP:117",
          "category": "B2AI:Application",
          "name": "FAIRshake Assessment of C2M2-Derived Assets for AI-Readiness",
          "description": "The CFDE Workbench subjects metadata, processed data, and code assets derived from C2M2 submissions to automated FAIR assessments via FAIRshake. These assessments evaluate findability, accessibility, interoperability, and reusability of C2M2-derived assets that support AI/ML tools including the Playbook Workflow Builder, GeneSetCart, DD-KG-UI, and CFDE-GSE. By operationalizing AI-readiness through systematic FAIRness evaluation, this application ensures that C2M2-structured datasets meet the accessibility and interoperability requirements necessary for downstream ML consumption.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1101/2025.02.04.636535"
          ]
        },
        {
          "id": "B2AI_APP:118",
          "category": "B2AI:Application",
          "name": "Ecosystem-Level Metadata Harmonization for Cross-Program ML Dataset Discovery",
          "description": "The CFDE catalog uses C2M2 to ingest diverse Data Coordinating Center metadata into a unified model supporting centralized indexing and search across NIH Common Fund programs. This harmonization with controlled vocabularies and ontologies minimizes barriers to cross-program dataset discovery and reuse, which is prerequisite for assembling training corpora and interoperable inputs for AI/ML analyses. C2M2's evolving model adapts to community standards and emerging AI/ML technologies, enabling effective search and retrieval mechanisms essential for training AI models on biomedical data while promoting FAIR principles.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1101/2021.11.05.467504"
          ]
        }
      ],
      "collection": [
        "datamodel"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The Common Fund Data Ecosystem (CFDE) Crosscut Metadata Model (C2M2) is a flexible, extensible data model designed to harmonize metadata across NIH Common Fund data coordinating centers (DCCs) to enable integrated search, discovery, and analysis. C2M2 provides a standardized framework for describing biomedical research assets including files, biosamples, subjects, and collections with their associated metadata using controlled vocabularies and ontologies. The model consists of core entities (file, biosample, subject, project, collection) connected through relationships that capture data provenance, experimental context, and biological associations. C2M2 submissions include data tables describing these entities along with controlled vocabulary term usage tables that link to standard ontologies for enhanced semantic interoperability. The model supports supra-schematic constraints validated by dedicated tools, and includes ontology reference files for adding display names, descriptions, and synonyms to controlled terms. C2M2 enables the CFDE Workbench portal to provide unified search and browsing across heterogeneous Common Fund datasets, facilitating cross-program data discovery and promoting FAIR (Findable, Accessible, Interoperable, Reusable) principles for biomedical research data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://docs.nih-cfde.org/en/latest/c2m2/draft-C2M2_specification/",
      "formal_specification": "https://osf.io/bq6k9/",
      "responsible_organization": [
        "B2AI_ORG:68"
      ]
    },
    {
      "id": "B2AI_STANDARD:64",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Common Metadata",
      "description": "Common Metadata Elements for Cataloging Biomedical Datasets",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "This dataset outlines a proposed set of core, minimal metadata elements that can be used to describe biomedical datasets, such as those resulting from research funded by the National Institutes of Health. It can inform efforts to better catalog or index such data to improve discoverability. The proposed metadata elements are based on an analysis of the metadata schemas used in a set of NIH-supported data sharing repositories. Common elements from these data repositories were identified, mapped to existing data-specific metadata standards from to existing multidisciplinary data repositories, DataCite and Dryad, and compared with metadata used in MEDLINE records to establish a sustainable and integrated metadata schema.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://figshare.com/articles/dataset/Common_Metadata_Elements_for_Cataloging_Biomedical_Datasets/1496573",
      "formal_specification": "https://figshare.com/articles/dataset/Common_Metadata_Elements_for_Cataloging_Biomedical_Datasets/1496573?file=3377663"
    },
    {
      "id": "B2AI_STANDARD:65",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "FACIT-COST",
      "description": "COmprehensive Score for financial Toxicity A FACIT Measure of Financial Toxicity",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "diagnosticinstrument"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:30"
      ],
      "purpose_detail": "Developed in conjunction with the University of Chicago, the COST is a patient-reported outcome measure that describes the financial distress experienced by cancer patients. Since its initial publication, an additional item from the FACIT System has been included to screen for financial toxicity and to provide a good global summary item for financial toxicity.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.facit.org/measures/FACIT-COST",
      "publication": "doi:10.1002/cncr.30369"
    },
    {
      "id": "B2AI_STANDARD:66",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "C-CDA",
      "description": "Consolidated Clinical Document Architecture",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "A widely-used, XML-based format for electronic health records. Superceded by FHIR document standards.",
      "is_open": true,
      "requires_registration": true,
      "url": "https://www.healthit.gov/topic/standards-technology/consolidated-cda-overview",
      "formal_specification": "http://www.hl7.org/implement/standards/product_brief.cfm?product_id=492",
      "responsible_organization": [
        "B2AI_ORG:40"
      ]
    },
    {
      "id": "B2AI_STANDARD:67",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "COREQ",
      "description": "Consolidated criteria for reporting qualitative research",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "COREQ (Consolidated Criteria for Reporting Qualitative Research) is a comprehensive 32-item reporting checklist specifically designed to improve the transparency, rigor, and completeness of qualitative research reports, particularly those involving interviews and focus groups. This evidence-based guideline addresses three major domains essential for qualitative research reporting: research team and reflexivity (covering the researcher's credentials, occupation, experience, gender, and relationship with participants), study design (including theoretical framework, participant selection, setting, and data collection methods), and analysis and findings (encompassing data analysis approaches, verification procedures, and presentation of findings). COREQ serves as a critical quality assessment tool for researchers, editors, reviewers, and readers, helping ensure that qualitative studies provide sufficient detail for proper evaluation and potential replication. The checklist promotes methodological transparency and supports the credibility and trustworthiness of qualitative health research by standardizing reporting expectations across the research community.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://www.cnfs.net/modules/module2/story_content/external_files/13_COREQ_checklist_000017.pdf",
      "publication": "doi:10.1093/intqhc/mzm042",
      "formal_specification": "http://www.cnfs.net/modules/module2/story_content/external_files/13_COREQ_checklist_000017.pdf"
    },
    {
      "id": "B2AI_STANDARD:68",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "CHEERS",
      "description": "Consolidated Health Economic Evaluation Reporting Standards",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The Consolidated Health Economic Evaluation Reporting Standards (CHEERS) statement is an attempt to consolidate and update previous health economic evaluation guidelines efforts into one current, useful reporting guidance. The primary audiences for the CHEERS statement are researchers reporting economic evaluations and the editors and peer reviewers assessing them for publication.",
      "is_open": true,
      "requires_registration": true,
      "url": "https://www.ispor.org/heor-resources/good-practices/article/consolidated-health-economic-evaluation-reporting-standards-2022-cheers-2022-statement-updated-reporting-guidance-for-health-economic-evaluations",
      "publication": "doi:10.1136/bmj.f1049",
      "formal_specification": "https://www.ispor.org/heor-resources/good-practices/article/consolidated-health-economic-evaluation-reporting-standards-2022-cheers-2022-statement-updated-reporting-guidance-for-health-economic-evaluations"
    },
    {
      "id": "B2AI_STANDARD:69",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "CONSORT",
      "description": "Consolidated Standards of Reporting Trials",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "purpose_detail": "CONSORT (Consolidated Standards of Reporting Trials) is an evidence-based minimum set of recommendations for reporting randomized controlled trial (RCT) results, developed to address inadequate reporting that prevents proper assessment of trial quality and generalizability. The CONSORT 2025 Statement consists of a 30-item checklist and flow diagram covering how trials are designed, analyzed, and interpreted, with detailed recommendations for reporting participant recruitment, randomization procedures, interventions, outcomes, sample size calculations, statistical methods, baseline characteristics, results for each outcome, harms, interpretation, and generalizability. The flow diagram tracks progress of all participants through each trial stage (enrollment, allocation, follow-up, analysis) with numbers and reasons for exclusions and losses. CONSORT is companion to SPIRIT (Standard Protocol Items - Recommendations for Interventional Trials) which provides a 34-item checklist for protocol reporting. Both guidelines emphasize transparent and complete reporting to enable readers to critically appraise, interpret, and apply research findings. CONSORT includes numerous extensions for specific trial designs (cluster randomized, non-inferiority, pragmatic trials), interventions (herbal, acupuncture, non-pharmacological treatment), and data types. The guidelines are supported by Explanation and Elaboration documents with detailed rationale and examples, online tools (COBWEB for report writing, SEPTRE for protocol management), translations in multiple languages, and endorsement by hundreds of medical journals worldwide as part of the EQUATOR Network research reporting standards.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.consort-statement.org/",
      "publication": "doi:10.1136/bmj.c332"
    },
    {
      "id": "B2AI_STANDARD:70",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Continua",
      "description": "Continua Design Guidelines",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The Continua Design Guidelines, developed and maintained by the Personal Connected Health Alliance (PCHA, formerly Continua Health Alliance), comprise a comprehensive suite of interoperability specifications, certification requirements, and implementation guidelines that enable end-to-end data exchange between personal health devices (blood pressure monitors, glucose meters, pulse oximeters, weight scales, activity trackers, thermometers, peak flow meters, medication dispensers), mobile health applications, electronic health record systems, and cloud-based health platforms through standardized communication protocols (Bluetooth Low Energy, USB, NFC, Wi-Fi), data formats (IEEE 11073 Personal Health Device standards for device-to-gateway communication, HL7 FHIR for clinical system integration), and semantic interoperability frameworks (LOINC codes for observations, UCUM units of measure, SNOMED CT for clinical concepts) creating a unified ecosystem where consumers, patients, clinicians, and researchers can reliably exchange personal health data across heterogeneous devices and systems without vendor lock-in or custom integration efforts. Established in 2006 as a non-profit industry consortium bringing together medical device manufacturers (Omron, A&D Medical, Nonin, iHealth), technology companies (Intel, Qualcomm, IBM), healthcare providers (Kaiser Permanente, Mayo Clinic), and standards organizations (HL7, IEEE, Bluetooth SIG, USB-IF), the Continua Guidelines address critical barriers to personal health device interoperability including proliferative proprietary data formats requiring device-specific software, lack of semantic standardization where \"blood pressure\" measurements from different devices use inconsistent units or nomenclature, fragmented connectivity with incompatible wireless protocols, and absent security/privacy frameworks for protecting sensitive health data in home and mobile environments. The Continua framework defines a layered architecture where the Device layer specifies personal health device profiles implementing IEEE 11073-20601 optimized exchange protocol for point-of-care devices with domain information models for specific device types (11073-10407 blood pressure, 11073-10415 weighing scale, 11073-10404 pulse oximeter, 11073-10417 glucose meter defining data structures, measurement units, device status codes), the Transport layer standardizes communication mechanisms (Bluetooth Health Device Profile HDP, USB Personal Healthcare Device Class PHDC, Zigbee Health Care Profile, NFC Forum Type 4 Tag), the Gateway/Aggregation layer describes personal health gateways (smartphones, dedicated hubs, set-top boxes) that collect data from multiple devices and forward to health management services via internet protocols, the Health & Fitness Service Interface layer specifies cloud/server APIs for receiving, storing, and exposing personal health data (RESTful web services, HL7 FHIR endpoints), and the WAN Interface layer defines wide-area network connectivity standards ensuring secure transmission between gateways and remote services (TLS/SSL encryption, OAuth2 authentication, IHE XDS.b document sharing). Continua's certification program validates product conformance where devices, gateways, and backend systems undergo testing against specification requirements (protocol compliance, data format correctness, error handling, security measures) by authorized test labs, earning Continua certification marks signaling to consumers and healthcare organizations that products interoperate seamlessly within the Continua ecosystema blood pressure monitor certified to Continua guidelines will successfully pair with any Continua-certified smartphone app or gateway and transmit readings in standardized IEEE 11073 format interpretable by any Continua-compliant health management platform without custom drivers or configuration. The semantic interoperability layer maps device-generated observations to standardized clinical terminologies where blood pressure readings include LOINC codes (8480-6 for systolic, 8462-4 for diastolic), glucose measurements specify units in mg/dL or mmol/L with UCUM codes, body weight uses standardized units (kg, lb), and qualitative observations reference SNOMED CT concepts (248646004 for irregular pulse detected by blood pressure monitor), enabling clinical decision support systems, population health analytics, and electronic health record integration to consume personal health device data alongside laboratory results, vital signs captured in clinical settings, and patient-reported outcomes with consistent semantic interpretation. For artificial intelligence and machine learning applications, Continua's standardization enables development of device-agnostic predictive models where training datasets aggregate personal health measurements from diverse device manufacturers (Omron blood pressure monitors, Dexcom continuous glucose monitors, Fitbit activity trackers, Withings scales) into unified representations with consistent units, timestamps, and metadata (device type, measurement context like fasting glucose vs. postprandial), supporting supervised learning for hypertension risk stratification (logistic regression, gradient boosting on longitudinal home blood pressure series predicting cardiovascular events), diabetes management (glucose forecasting with LSTMs or attention-based sequence models predicting hypoglycemic episodes from CGM time-series enabling proactive interventions), weight management (time-series clustering identifying weight trajectory phenotypessteady loss, plateaus, reboundsinforming personalized coaching), and heart failure decompensation early warning (multi-task learning combining daily weights, blood pressure trends, activity levels, symptom surveys to predict hospitalizations). Remote patient monitoring programs leverage Continua interoperability where chronic disease management platforms (for diabetes, hypertension, heart failure, COPD) integrate data streams from multiple Continua-certified devices worn or used by patients at home, applying machine learning algorithms for automated anomaly detection (sudden weight gain >2kg over 2 days flagging fluid retention in heart failure, blood glucose readings consistently >180 mg/dL triggering medication titration alerts, oxygen saturation <88% in COPD patients initiating clinical outreach) and personalized feedback generation (reinforcement learning agents trained on longitudinal patient data recommending behavioral modificationsexercise, medication adherence, dietary changestimed to maximize engagement and health outcomes). Clinical trials and real-world evidence studies benefit from Continua standardization where decentralized trial designs distribute Continua-certified devices to participants (blood pressure cuffs, glucometers, spirometers, ECG patches) capturing protocol-defined endpoints remotely with data automatically transmitted to electronic data capture systems in standardized formats, reducing site visit burden, improving adherence, and enabling larger, more diverse study populations; machine learning on trial datasets identifies treatment effect heterogeneity (which patient subgroups benefit most from interventions based on baseline device-measured physiology), predicts dropout risk (time-series models forecasting non-adherence from engagement patterns in device usage), and discovers digital biomarkers (features extracted from continuous device data correlating with clinical outcomesheart rate variability from pulse oximeter predicting medication response, glucose variability metrics predicting complications). Federated learning architectures for personal health devices exploit Continua interoperability where machine learning models train locally on smartphones or gateways receiving standardized device data, then share model updates (not raw data) with central servers, preserving privacy while enabling population-level model improvements; for example, a glucose prediction model fine-tunes on each patient's CGM data (Continua-formatted IEEE 11073 streams) and shares gradients to update global model, or anomaly detection models learn normal patterns from individual home blood pressure trajectories and contribute to ensemble detectors identifying pathological deviations. Digital therapeutics and closed-loop intervention systems integrate Continua devices where therapeutic mobile apps deliver evidence-based interventions (cognitive behavioral therapy for diabetes distress, medication reminders, exercise coaching) and adapt content based on real-time device feedbackan app escalates support when glucometer data shows poor control, a hypertension app adjusts medication reminders when blood pressure readings remain elevated, a heart failure app coordinates care team notifications when smart scale detects weight gain exceeding thresholds, all enabled by Continua's semantic standardization ensuring device readings drive clinical logic reliably across device brands. Population health surveillance and public health monitoring can leverage aggregated, de-identified personal health device data where Continua standardization enables pooling of home blood pressure measurements (identifying community-level hypertension control trends), continuous glucose monitoring data (tracking diabetes management effectiveness across populations), and activity tracker data (assessing physical activity patterns, sedentary behavior prevalence) with machine learning models identifying geographic, demographic, or socioeconomic disparities in chronic disease control, predicting outbreaks or exacerbations (flu activity from increased thermometer readings, asthma exacerbations from decreased peak flow measurements correlated with environmental triggers), and evaluating public health intervention effectiveness, ultimately positioning Continua as the interoperability foundation enabling a scalable, device-diverse personal health data ecosystem where AI-driven analytics, clinical decision support, remote monitoring, digital therapeutics, and precision medicine applications can operate across heterogeneous device landscapes, empowering patients as data generators contributing to their own care and advancing population health through standardized, machine-readable personal health observations.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.pchalliance.org/continua-design-guidelines",
      "formal_specification": "https://members.pchalliance.org/document/dl/2148"
    },
    {
      "id": "B2AI_STANDARD:71",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "CCDEF",
      "description": "Critical care data exchange format",
      "related_to": [
        "B2AI_STANDARD:339"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "The Critical Care Data Exchange Format (CCDEF) is an HDF5-based, self-describing, hierarchical file format specifically designed for efficient storage, exchange, and real-time streaming of high-resolution, multiparametric physiological waveforms and clinical data from intensive care unit (ICU) monitoring systems, addressing critical challenges in critical care informatics including data volume (continuous waveforms sampled at 125-500 Hz generate gigabytes per patient-day), heterogeneity (diverse devices from multiple manufacturers producing incompatible formats), real-time requirements (streaming data for bedside decision support and closed-loop control systems), and interoperability (enabling secondary use for research, quality improvement, and machine learning model development). Developed through collaboration between Carnegie Mellon University's Auton Lab and clinical partners, CCDEF leverages the HDF5 (Hierarchical Data Format version 5) scientific data framework to provide efficient binary storage with transparent compression (gzip, LZF codecs reducing file sizes by 60-90% while maintaining rapid random access), self-describing metadata (attributes documenting units, sampling rates, calibration parameters, device identifiers, alarm thresholds embedded within the file eliminating external documentation dependencies), hierarchical organization (nested groups for Patient/Admission/Device/Parameter structuring data logically with extensible schemas accommodating new measurement modalities without format versioning), and platform-independent access (HDF5 libraries for Python, R, MATLAB, C/C++, Java enabling cross-platform analytics). The CCDEF schema defines a standardized tree structure where the root contains global metadata (format version, institution identifier, de-identification flags), patient-level groups store demographics and admission metadata (age, sex, diagnosis codes, APACHE/SOFA severity scores), time-series datasets represent physiological waveforms (ECG leads I/II/III/V, arterial blood pressure ABP, pulmonary artery pressure PAP, central venous pressure CVP, respiratory plethysmography, SpO2, end-tidal CO2 EtCO2, intracranial pressure ICP) as chunked arrays with attributes specifying sampling frequency (typically 125-500 Hz), units (mmHg, mV, %), calibration coefficients, and acquisition timestamps synchronized to UTC or relative time-since-admission, while derived-parameter datasets store lower-frequency clinical measurements (heart rate, blood pressure systolic/diastolic/mean computed from waveforms, respiratory rate, temperature) and laboratory results (arterial blood gases pH/PaCO2/PaO2, lactate, troponin, BUN/creatinine, complete blood counts) as time-stamped tabular data, and event annotation datasets document clinical interventions (intubation, extubation, vasopressor boluses, fluid resuscitation), alarms (brady-/tachycardia, hypotension, desaturation), and artifact labels (movement, sensor disconnection, calibration periods) essential for supervised machine learning. The format's chunked storage strategy partitions long time-series into fixed-duration segments (e.g., 10-minute chunks) enabling efficient partial reads where analytics workflows query specific time windows without loading entire patient stays (days to weeks of continuous data), supporting distributed computing frameworks (Spark, Dask) that parallelize feature extraction and model inference across chunks, and real-time streaming scenarios where incoming data continuously appends new chunks to open CCDEF files with minimal write overhead, facilitating online monitoring algorithms and closed-loop interventions. CCDEF's real-time streaming capability enables bedside AI applications where machine learning models consume CCDEF streams for early warning score calculation (predicting sepsis onset, hemodynamic decompensation, respiratory failure hours in advance), automated arrhythmia detection (classifying ventricular tachycardia, atrial fibrillation, PVCs from ECG waveforms using CNNs), and alarm management systems (filtering false alarms via multimodal signal fusion reducing alarm fatigue while maintaining high sensitivity for true events), with model predictions written back to CCDEF event annotations creating audit trails for clinical validation and regulatory compliance. For artificial intelligence and machine learning applications, CCDEF provides a standardized data substrate for developing and validating ICU prediction models where large multi-center critical care databases (MIMIC-III/IV waveform matched subsets, eICU Collaborative Research Database, HiRID high-time-resolution ICU dataset) could be distributed in CCDEF format enabling reproducible preprocessing pipelines (artifact removal, signal quality assessment, waveform segmentation), harmonized feature engineering (extracting heart rate variability metrics, pulse pressure variation, respiratory variability, spectral features from Fourier transforms of ECG/ABP/respiratory waveforms, morphological features from QRS complexes and ABP pulse shapes), and consistent train/validation/test splits across studies, addressing reproducibility challenges where custom data formats and preprocessing code create barriers to benchmarking and independent validation. Deep learning architectures for critical care time-series benefit from CCDEF's efficient waveform access where 1D CNNs, LSTMs, and transformer models train on raw or minimally processed physiological signals stored as HDF5 datasets, leveraging GPU-accelerated data loaders that read CCDEF chunks in parallel (TensorFlow tf.data pipelines, PyTorch DataLoader with HDF5 backend) achieving high throughput for training on terabyte-scale waveform corpora, with self-supervised pretraining on unlabeled CCDEF waveforms (contrastive learning, autoencoding) followed by fine-tuning on labeled subsets (mortality, acute kidney injury, ventilator weaning success) improving sample efficiency compared to training from scratch. Multi-modal fusion models leverage CCDEF's unified format to integrate waveforms (high-frequency cardiorespiratory dynamics), laboratory values (intermittent biochemical measurements), clinical notes (extracted via NLP), and interventions (medications, ventilator settings) stored within the same hierarchical structure, training architectures (attention-based fusion, graph neural networks treating modalities as heterogeneous nodes, hierarchical RNNs with separate encoders per modality) that learn complementary information across time-scales (beat-to-beat waveform variability, hourly lab trends, daily disease trajectories) for holistic patient state representations improving prognostic accuracy. Federated learning and privacy-preserving machine learning on critical care data benefit from CCDEF's standardization where multiple hospitals locally train models on their CCDEF-formatted ICU data and share model updates (gradients, model weights) without exchanging patient-level data, with CCDEF's de-identification metadata flags documenting whether timestamps have been shifted, identifiers removed, and rare diagnoses generalized according to HIPAA Safe Harbor guidelines, enabling privacy-preserving consortia to develop and validate AI models on diverse patient populations while respecting data governance constraints. Causal inference and counterfactual reasoning in critical care employ CCDEF's comprehensive capture of interventions and outcomes where researchers train causal models (marginal structural models, G-computation, causal forests, neural causal effect estimators) on observational CCDEF data to estimate treatment effects (vasopressor strategies, ventilator settings, transfusion thresholds) accounting for time-varying confounding, with CCDEF's granular temporal resolution enabling discovery of optimal treatment timing and dosing policies via reinforcement learning (offline RL with batch-constrained Q-learning, fitted Q-iteration) trained on historical CCDEF trajectories, potentially improving clinical decision making through AI-derived precision medicine strategies tailored to individual patient physiology captured in CCDEF waveforms. The format's extensibility future-proofs critical care data infrastructure by accommodating emerging measurement modalities (continuous glucose monitors, microdialysis brain chemistry, wearable sensors, point-of-care ultrasound clips, portable chest X-rays) as new HDF5 datasets or groups without breaking backward compatibility, and genomic data integration (whole exome/genome sequences, polygenic risk scores, pharmacogenomic variants) stored alongside physiological data enables development of integrative models predicting sepsis susceptibility, drug metabolism variability, or recovery trajectories based on both genomic risk and real-time physiology, ultimately positioning CCDEF as the interoperable foundation for data-driven critical care research where standardized, machine-readable, comprehensive capture of ICU patient physiology enables reproducible AI development, multi-center validation, regulatory evaluation, and clinical deployment of intelligent monitoring and decision support systems improving outcomes in the most acutely ill patients.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://ccdef.org/",
      "publication": "doi:10.1088/1361-6579/abfc9b",
      "formal_specification": "https://github.com/autonlab/auviewer",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:16"
      ]
    },
    {
      "id": "B2AI_STANDARD:72",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Crypt4GH",
      "description": "Crypt4GH format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:34"
      ],
      "purpose_detail": "Crypt4GH is a file format specification developed by the Global Alliance for Genomics and Health (GA4GH) for secure storage and transmission of genomic and health-related data. The format provides end-to-end encryption and authentication, allowing sensitive genomic data to be stored in public repositories while maintaining confidentiality. It uses modern cryptographic standards including Curve25519 for key agreement, ChaCha20 for encryption, and Poly1305 for authentication. The format supports multiple recipients through per-recipient encrypted headers, enabling controlled data sharing where different users can decrypt the same file using their own private keys. Crypt4GH is designed for streaming access, allowing applications to decrypt and process data incrementally without loading entire files into memory. The format includes metadata protection and supports selective decryption of file segments, making it suitable for large-scale genomic datasets. Existing bioinformatics tools can be adapted with minimal modifications to read and write Crypt4GH-encrypted data, facilitating adoption in genomics workflows.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://samtools.github.io/hts-specs/crypt4gh.pdf",
      "formal_specification": "https://github.com/samtools/hts-specs"
    },
    {
      "id": "B2AI_STANDARD:73",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "CIF",
      "description": "Crystallographic Information File format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:27"
      ],
      "purpose_detail": "The acronym CIF is used both for the Crystallographic Information File, the data exchange standard file format of Hall, Allen & Brown (1991) (see Documentation), and for the Crystallographic Information Framework, a broader system of exchange protocols based on data dictionaries and relational rules expressible in different machine-readable manifestations, including, but not restricted to, Crystallographic Information File and XML.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikipedia.org/wiki/Crystallographic_Information_File"
    },
    {
      "id": "B2AI_STANDARD:74",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "CPT",
      "description": "Current Procedural Terminology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:263",
          "category": "B2AI:Application",
          "name": "Anesthesiology CPT auto-coding from EHR",
          "description": "Machine learning models for automated prediction of anesthesia Current Procedural Terminology (CPT) codes from perioperative electronic health record data using support vector machines and label-embedding attentive neural networks trained on 1,164,343 procedures from 16 hospitals. Procedure text extracted from EHR served as the most informative input with actual submitted claim data from billing specialists as reference standard. Achieved 87.9% single-best accuracy and 96.8% top-3 accuracy on test set with strong external holdout validation performance. Using tunable confidence threshold, SVM reached 96.4% accuracy for 47.0% of cases and label-embedding model achieved 94.4% accuracy for 62.2% of cases. Demonstrates automated coding value for quality improvement, research, reimbursement, and cost reduction given high manual billing costs ($170-$215 per case) and observed CPT error rates (up to 38%). Supports scalable code assignment reducing administrative burden while improving accuracy and consistency compared to manual specialist coding.",
          "references": [
            "https://doi.org/10.1097/aln.0000000000003150"
          ]
        },
        {
          "id": "B2AI_APP:264",
          "category": "B2AI:Application",
          "name": "Pathology report CPT prediction with NLP",
          "description": "Natural language processing and machine learning pipeline predicting primary CPT codes (88302, 88304, 88305, 88307, 88309) and 38 ancillary CPT codes from pathology reports using corpus of 93,039 reports. Compared XGBoost, support vector machines, and BERT models with advanced topic modeling (20 topics) preprocessing on diagnostic text alone versus all report subfields (gross description, microscopic description, additional comments). XGBoost outperformed BERT when using all report subfields, achieving higher accuracy than prior published work. Model errors tended to occur between CPT codes of similar complexity. Applied model explanation techniques to identify informative report subcomponents. Applications extend beyond auto-coding to detecting mis-billing and fraud/waste, standardizing pathology report text, and estimating physician productivity metrics tied to relative value units (RVUs) for workload assessment and resource allocation.",
          "references": [
            "https://doi.org/10.4103/jpi.jpi_52_21"
          ]
        },
        {
          "id": "B2AI_APP:265",
          "category": "B2AI:Application",
          "name": "Operative note CPT auto-coding comparison",
          "description": "Comparative evaluation of classical machine learning and deep learning approaches for automated CPT code generation from operative note dictations in spine surgery setting. Random Forest model achieved AUC 0.94 and AUPRC 0.85 for CPT prediction and outperformed LSTM deep model (Random Forest ~87% weighted accuracy), demonstrating that classical supervised approaches can be competitive with neural networks given structured operative note text. Systematic evaluation across musculoskeletal operative notes corpus (~126,789 notes) found TF-IDF and classical machine learning methods often outperformed BERT-family transformer models on common CPT codes while offering better interpretability for clinical adoption. Highlights importance of feature engineering and model selection tailored to CPT prediction task characteristics, dataset size, and deployment requirements balancing accuracy with explainability for billing and compliance workflows.",
          "references": [
            "https://doi.org/10.1177/21925682211062831",
            "https://doi.org/10.1101/2022.10.10.22280852"
          ]
        },
        {
          "id": "B2AI_APP:266",
          "category": "B2AI:Application",
          "name": "Cloud-based end-to-end CPT/ICD auto-coding pipeline",
          "description": "Commercial deep learning NLP system for automated medical coding mapping clinical text in electronic health records to standard procedure codes (CPT-4) and diagnosis codes (ICD-10-CM) with cloud-based claim submission, validation, and auditing infrastructure. Pipeline uses trained deep learning models on annotated EHRs with multiple NLP stages for entity recognition, context disambiguation, and code mapping supporting automatic CPT assignment from clinical notes with point-of-care coding suggestions. Real-time claim validation, auditing, and feedback loops leverage cloud infrastructure for HIPAA-compliant deployment. Vendor-reported outcomes include approximately 70% reduction in claim processing time and 23% improvement in coding accuracy at aggregate system level, though per-code performance metrics not publicly disclosed. Demonstrates emerging commercial deployment patterns for AI-driven revenue cycle automation integrating CPT/HCPCS coding with downstream billing workflows and fraud detection capabilities.",
          "references": [
            "https://doi.org/10.63282/3050-922x.ijeret-v4i4p104"
          ]
        },
        {
          "id": "B2AI_APP:267",
          "category": "B2AI:Application",
          "name": "Unsupervised fraud detection on CPT billing patterns",
          "description": "Unsupervised machine learning pipeline applied to Medicare, Medicaid, and outpatient claims data for detecting billing anomalies, CPT code mismatches, and fraudulent procedure coding patterns addressing estimated >$60B annual Medicare/Medicaid losses. Models analyze CPT/HCPCS usage patterns to flag aberrant billing behaviors without labeled fraud examples using anomaly detection on procedure code sequences, frequency distributions, and temporal patterns. Reported substantial improvements in fraud detection rates and significant reduction in manual audit efforts enabling automated prioritization of high-risk claims for investigation. Addresses challenges of data standardization across heterogeneous claims systems, inconsistent terminologies hindering model generalizability, and regulatory compliance requirements (HIPAA, GDPR) for deploying AI in healthcare fraud detection. Emphasizes need for transparency, routine audits for bias, and alignment with CMS program integrity goals ensuring AI-driven fraud detection augments rather than replaces human expertise in complex billing integrity assessments.",
          "references": [
            "https://al-kindipublishers.org/index.php/jcsts/article/download/11010/9926"
          ]
        },
        {
          "id": "B2AI_APP:268",
          "category": "B2AI:Application",
          "name": "CPT AI Taxonomy for coding AI-enabled services",
          "description": "American Medical Association CPT AI Taxonomy framework (Appendix S, effective January 1, 2022) establishing standardized coding methodology for AI-enabled clinical services by categorizing machine work as Assistive (AI assists physician decision-making), Augmentative (AI enhances physician work), or Autonomous (AI performs clinical task independently). Framework provides language describing work performed by machines relative to physician work, requires CPT descriptors align with device technological features and output, characterizes device-physician interaction, and ensures codes are discrete and differentiable supporting correct valuation, coverage determination, and payment policy. Applies to both Category I (established procedures) and Category III (emerging technology) CPT codes with guidance for code change applications including technical specifications justifying taxonomy placement. Example implementation includes autonomous point-of-care diabetic retinal examination (CPT 92229) transitioning descriptor from \"automated\" to \"autonomous\" reflecting independent AI capability. Addresses regulatory context where FDA has reviewed/authorized numerous Software as Medical Device (SaMD) products requiring consistent coding framework enabling integration of AI services into healthcare coverage and reimbursement systems, supporting transparent valuation of machine-performed clinical work.",
          "references": [
            "https://doi.org/10.1038/s41746-022-00723-5"
          ]
        }
      ],
      "collection": [
        "codesystem",
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "Current Procedural Terminology (CPT) is a comprehensive, standardized code set maintained by the American Medical Association (AMA) that provides uniform nomenclature for describing medical, surgical, and diagnostic services performed by physicians and other healthcare providers, serving as the universal language for reporting outpatient and office procedures to payers including Medicare, Medicaid, and private insurance for reimbursement, enabling consistent documentation of clinical services across healthcare systems, supporting quality measurement and performance monitoring, facilitating healthcare research through structured procedure data, and underpinning revenue cycle management where accurate CPT coding directly impacts provider reimbursement (with manual coding costs $170-$215 per case and error rates up to 38% in complex specialties). Originally developed in 1966 by the AMA to standardize reporting of surgical procedures and adopted by CMS in 1983 as part of the Healthcare Common Procedure Coding System (HCPCS Level I), the CPT code set contains over 10,000 five-digit numeric codes organized into three categories where Category I codes describe established procedures and services widely performed across healthcare settings (organized by specialty sections: Evaluation & Management, Anesthesia, Surgery, Radiology, Pathology & Laboratory, Medicine), Category II codes capture performance measurement data for quality reporting and registry submissions (optional supplemental tracking codes documenting evidence-based care components like hemoglobin A1c testing in diabetes management, with alphanumeric format ending in 'F'), and Category III codes designate emerging technologies and procedures under development or investigation (temporary alphanumeric codes ending in 'T' facilitating data collection on new services before sufficient evidence accumulates for Category I inclusion, such as early telehealth services or novel imaging modalities). Each CPT code specifies the work relative value unit (wRVU) reflecting physician time, effort, skill, and intensity required for the procedure, practice expense RVU covering overhead costs, and professional liability insurance RVU, combined with geographic adjustment factors to calculate Medicare reimbursement rates through the Resource-Based Relative Value Scale (RBRVS) system, with private payers negotiating contracted rates often referenced to Medicare fee schedules modified by percentage multipliers. The CPT Editorial Panela multidisciplinary committee including physicians from major medical specialty societies, the Health Care Professionals Advisory Committee, CPT advisors from Blue Cross Blue Shield, America's Health Insurance Plans, CMS, and the American Hospital Associationreviews code change applications quarterly, evaluating proposals for new codes, revisions to existing descriptors, or deletions based on criteria including clinical distinctness (service must be distinct from existing codes), frequency of use (widespread adoption across providers), FDA approval for devices/drugs where applicable, and evidentiary basis (published clinical evidence supporting safety, efficacy, and medical necessity), with approved changes implemented in annual CPT updates released each January following a rigorous consensus process balancing innovation, clinical utility, and billing precision. CPT modifierstwo-character alphanumeric suffixes appended to codes (e.g., -25 for significant separately identifiable evaluation and management service, -59 for distinct procedural service, -RT/LT for right/left anatomical designation, -50 for bilateral procedures)provide additional context about how services were performed, enabling more accurate reimbursement for complex or unusual circumstances without proliferating primary codes. For artificial intelligence and machine learning applications, CPT codes serve multiple critical functions where automated code assignment (auto-coding) leverages supervised natural language processing and machine learning to map clinical documentation (operative notes, pathology reports, anesthesia records, EHR clinical narratives) directly to appropriate CPT codes, with deep learning models (support vector machines, label-embedding neural networks, BERT transformers, XGBoost) trained on large annotated corpora (e.g., 1.16 million procedures, 93,000+ pathology reports, 126,000+ operative notes) achieving 85-96% accuracy depending on specialty, input data completeness, and top-k prediction tolerance, addressing high manual coding costs and error rates while supporting scalable code assignment for quality improvement, research cohort identification, revenue optimization, and compliance monitoring. Machine learning-based fraud, waste, and abuse detection systems apply unsupervised anomaly detection and outlier analysis to claims databases containing CPT/HCPCS billing patterns, identifying aberrant utilization (excessive frequency of high-reimbursement procedures, code mismatches indicating upcoding or unbundling violations, temporally implausible procedure sequences suggesting fabricated claims) without requiring labeled fraud examples, with Medicare/Medicaid program integrity initiatives leveraging these models to flag high-risk claims for investigation addressing estimated >$60 billion annual improper payments, though deployment faces challenges including data standardization across heterogeneous claims systems, inconsistent terminologies across regional carriers, and regulatory requirements for transparency, auditability, and bias monitoring to ensure AI augments rather than replaces human expert judgment in complex fraud assessments. EHR-based phenotyping and outcomes prediction incorporate CPT codes as structured features or prediction targets where supervised models use billed procedure codes alongside diagnosis codes (ICD-10-CM), medication orders (RxNorm), and laboratory results (LOINC) to construct patient phenotypes for epidemiological research, clinical trial recruitment, risk stratification (predicting hospital readmissions, post-operative complications, healthcare utilization), and longitudinal disease progression modeling, with the ubiquity and portability of CPT codes across EHR systems (Epic, Cerner, Allscripts) and claims databases (Medicare, Medicaid, commercial payers) enabling federated learning where multiple institutions collaboratively train models on locally held data without sharing patient-level information, though CPT-based phenotyping confronts limitations including incomplete capture of clinical detail (codes designed for billing may not reflect nuanced clinical presentations), temporal lag between service delivery and claims submission (weeks to months delay reducing utility for real-time predictions), and code selection variability across providers and specialties (physician discretion and billing optimization incentives introduce noise in code distributions). The CPT AI Taxonomy (Appendix S, effective January 2022) establishes a structured framework for coding AI-enabled clinical services by classifying machine work into three categoriesAssistive (AI provides information to support physician decision-making, e.g., computer-aided detection highlighting suspicious regions in mammography with final interpretation by radiologist), Augmentative (AI performs portions of the clinical task with physician oversight and interpretation, e.g., automated coronary artery calcium scoring from CT with physician review and clinical integration), and Autonomous (AI independently performs complete clinical task generating actionable output without real-time physician involvement, e.g., autonomous diabetic retinopathy screening CPT 92229 producing diagnostic recommendations directly from retinal images)requiring CPT code descriptors for AI services to explicitly characterize device technological features, AI-physician interaction model, and discrete outputs supporting accurate valuation through the AMA/Specialty Society RVS Update Committee (RUC) process, coverage policy development by payers determining medical necessity criteria and patient eligibility, and payment rate determination balancing technology costs, clinical value, and budget impact, with the taxonomy addressing the regulatory and reimbursement infrastructure needed to integrate FDA-authorized Software as Medical Device (SaMD) products into routine clinical workflows where transparent CPT coding enables health systems to track AI utilization, measure clinical outcomes, assess return on investment, and satisfy quality reporting requirements. Natural language processing systems for automated CPT coding face deployment challenges including class imbalance (common procedures like routine office visits vastly outnumber complex specialty procedures in training data, requiring oversampling, focal loss functions, or hierarchical classification strategies), code similarity and confusion (many CPT codes describe clinically similar procedures with subtle distinctions in technique, anatomical location, or complexity leading to prediction errors between adjacent codes, e.g., pathology specimen processing codes 88302-88309 differing primarily by tissue complexity and examination extent), generalizability across institutions and specialties (models trained on one hospital's documentation practices, EHR templates, and physician dictation styles may underperform when deployed elsewhere without transfer learning or domain adaptation), explainability and trust (black-box deep learning models producing CPT predictions lack transparency needed for billing compliance and error correction, motivating attention mechanisms, saliency maps, and model-agnostic explainability techniques like SHAP values to surface which clinical text features drive code assignments), integration with clinical workflows (auto-coding systems must interoperate with EHR billing modules, provide real-time point-of-care suggestions during documentation, flag low-confidence predictions for human review, and support iterative feedback loops where corrected codes retrain models, requiring robust software engineering, user interface design, and change management), and regulatory compliance (automated coding tools constitute clinical decision support potentially subject to FDA oversight depending on intended use and risk classification, require validation against coding guidelines published by AMA and specialty societies, and must document audit trails for claims submitted to government payers satisfying anti-fraud regulations and Office of Inspector General scrutiny). CPT codes also serve as labels in supervised learning tasks beyond auto-coding where models predict post-procedure outcomes (complications, readmissions, mortality) conditional on performed procedures represented by CPT codes, estimate procedure duration and operating room utilization for surgical scheduling optimization (random forests predicting case length from CPT codes, patient characteristics, and surgeon experience), forecast healthcare costs and resource consumption for population health management and value-based care contracts (CPT-based procedure costs aggregated with diagnosis-based expected utilization producing risk-adjusted spending predictions), and power clinical natural language generation systems that automatically compose procedure notes, operative summaries, or patient instructions by retrieving templates associated with billed CPT codes then personalizing with patient-specific details extracted from structured EHR data. The interplay between CPT coding accuracy, revenue integrity, and AI/ML applications creates a feedback loop where improved automated coding reduces administrative costs and accelerates reimbursement, the resulting high-quality coded data enables more sophisticated ML models for clinical decision support and operations optimization, and AI-generated insights into coding patterns inform coding guideline refinements and fraud detection strategies, ultimately positioning CPT as not merely a billing code set but a foundational structured vocabulary enabling AI-driven transformation of healthcare delivery, payment models, and quality measurement while demanding ongoing evolution of the code set, taxonomy frameworks, and regulatory policies to keep pace with technological innovation in both clinical procedures and the AI systems that document, analyze, and optimize them.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://www.ama-assn.org/amaone/cpt-current-procedural-terminology",
      "formal_specification": "https://www.cms.gov/Medicare/Fraud-and-Abuse/PhysicianSelfReferral",
      "responsible_organization": [
        "B2AI_ORG:3"
      ]
    },
    {
      "id": "B2AI_STANDARD:75",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "DDI-Lifecycle",
      "description": "Data Documentation Initiative Lifecycle",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The freely available international DDI standard describes data that result from observational methods in the social, behavioral, economic, and health sciences. DDI is used to document data in over 60 countries of the world.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://ddialliance.org/Specification/DDI-Lifecycle/3.3/",
      "responsible_organization": [
        "B2AI_ORG:23"
      ]
    },
    {
      "id": "B2AI_STANDARD:76",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "DOS-DP",
      "description": "Dead simple owl design pattern exchange format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:58"
      ],
      "purpose_detail": "A simple design pattern system that can easily be consumed, whatever your code base, for OWL ontologies.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://oboacademy.github.io/obook/tutorial/dosdp-template/",
      "publication": "doi:10.1186/s13326-017-0126-0",
      "formal_specification": "https://github.com/INCATools/dead_simple_owl_design_patterns"
    },
    {
      "id": "B2AI_STANDARD:77",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "DMN",
      "description": "Decision Model and Notation",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:10"
      ],
      "purpose_detail": "A modeling language and notation for the precise specification of business decisions and business rules.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.omg.org/dmn/"
    },
    {
      "id": "B2AI_STANDARD:78",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "DELTA",
      "description": "Description Language for Taxonomy",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "When taxonomic descriptions are prepared for input to computer programs, the form of the coding is usually dictated by the requirements of a particular program or set of programs. This restricts the type of data that can be represented, and the number of other programs that can use the data. Even when working with a particular program, it is frequently necessary to set up different versions of the same basic data, for example, when using restricted sets of taxa or characters to make special-purpose keys. The potential advantages of automation, especially in connexion with large groups, cannot be realized if the data have to be restructured by hand for every operation. The DELTA (DEscription Language for TAxonomy) system was developed to overcome these problems. It was designed primarily for easy use by people rather than for convenience in computer programming, and is versatile enough to replace the written description as the primary means of recording data. Consequently, it can be used as a shorthand method of recording data, even if computer processing of the data is not envisaged.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.delta-intkey.com/"
    },
    {
      "id": "B2AI_STANDARD:79",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "DICOMDIR",
      "description": "DICOM Part 10 Media Storage and File Format for Media Interchange",
      "subclass_of": [
        "B2AI_STANDARD:98"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "audiovisual",
        "fileformat",
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:849"
      ],
      "purpose_detail": "This Part of the DICOM Standard specifies a general model for the storage of Medical Imaging information on removable media. The purpose of this Part is to provide a framework allowing the interchange of various types of medical images and related information on a broad range of physical storage media.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.dicomstandard.org/current",
      "formal_specification": "http://dicom.nema.org/medical/dicom/current/output/html/part10.html",
      "responsible_organization": [
        "B2AI_ORG:25"
      ],
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:11"
      ]
    },
    {
      "id": "B2AI_STANDARD:80",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "DICOM Part 11",
      "description": "DICOM Part 11 Media Storage Application Profiles",
      "subclass_of": [
        "B2AI_STANDARD:98"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:849"
      ],
      "purpose_detail": "This Part of the DICOM Standard specifies application specific subsets of the DICOM Standard to which an implementation may claim conformance. Such a conformance statement applies to the interoperable interchange of medical images and related information on storage media for specific clinical uses.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.dicomstandard.org/current",
      "formal_specification": "http://dicom.nema.org/medical/dicom/current/output/html/part11.html",
      "responsible_organization": [
        "B2AI_ORG:25"
      ],
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:11"
      ]
    },
    {
      "id": "B2AI_STANDARD:81",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "DICOM Part 12",
      "description": "DICOM Part 12 Media Formats and Physical Media for Media Interchange",
      "subclass_of": [
        "B2AI_STANDARD:98"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:849"
      ],
      "purpose_detail": "This Part of the DICOM Standard facilitates the interchange of information between digital imaging computer systems in medical environments. This interchange will enhance diagnostic imaging and potentially other clinical applications. The multi-part DICOM Standard defines the services and data that shall be supplied to achieve this interchange of information.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.dicomstandard.org/current",
      "formal_specification": "http://dicom.nema.org/medical/dicom/current/output/html/part12.html",
      "responsible_organization": [
        "B2AI_ORG:25"
      ],
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:11"
      ]
    },
    {
      "id": "B2AI_STANDARD:82",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "DICOM Part 14",
      "description": "DICOM Part 14 Grayscale Standard Display Function",
      "subclass_of": [
        "B2AI_STANDARD:98"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:849"
      ],
      "purpose_detail": "PS3.14 specifies a standardized Display Function for display of grayscale images. It provides examples of methods for measuring the Characteristic Curve of a particular Display System for the purpose of either altering the Display System to match the Grayscale Standard Display Function, or for measuring the conformance of a Display System to the Grayscale Standard Display Function. Display Systems include such things as monitors with their associated driving electronics and printers producing films that are placed on light-boxes or alternators.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.dicomstandard.org/current",
      "formal_specification": "http://dicom.nema.org/medical/dicom/current/output/html/part14.html",
      "responsible_organization": [
        "B2AI_ORG:25"
      ],
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:11"
      ]
    },
    {
      "id": "B2AI_STANDARD:83",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "DICOM Part 15",
      "description": "DICOM Part 15 Security and System Management Profiles",
      "subclass_of": [
        "B2AI_STANDARD:98"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:849"
      ],
      "purpose_detail": "This Part of the DICOM Standard specifies Security and System Management Profiles to which implementations may claim conformance. Security and System Management Profiles are defined by referencing externally developed standard protocols, such as TLS, ISCL, DHCP, and LDAP, with attention to their use in a system that uses DICOM Standard protocols for information interchange.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.dicomstandard.org/current",
      "formal_specification": "http://dicom.nema.org/medical/dicom/current/output/html/part15.html",
      "responsible_organization": [
        "B2AI_ORG:25"
      ],
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:11"
      ]
    },
    {
      "id": "B2AI_STANDARD:84",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "DCMR",
      "description": "DICOM Part 16 Content Mapping Resource",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:849"
      ],
      "purpose_detail": "DICOM Part 16 Content Mapping Resource (DCMR) is a comprehensive component of the DICOM standard that provides the foundational vocabulary and semantic structures used throughout medical imaging systems worldwide. DCMR defines the Templates and Context Groups that standardize the representation of clinical concepts, measurements, and observations within DICOM objects, ensuring consistent interpretation of medical imaging data across different systems, vendors, and healthcare institutions. The Content Mapping Resource includes structured templates for various types of medical imaging reports, measurements, and annotations, covering specialties such as radiology, cardiology, ophthalmology, and oncology. These templates define the specific data elements, their relationships, and the controlled terminologies that should be used when creating structured reports and enhanced imaging objects. DCMR plays a crucial role in enabling interoperability, supporting artificial intelligence applications in medical imaging, and facilitating the exchange of semantically rich imaging information in clinical practice and research environments.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.dicomstandard.org/current",
      "formal_specification": "http://dicom.nema.org/medical/dicom/current/output/html/part16.html",
      "responsible_organization": [
        "B2AI_ORG:25"
      ],
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:11"
      ]
    },
    {
      "id": "B2AI_STANDARD:85",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "DICOM Part 17",
      "description": "DICOM Part 17 Explanatory Information",
      "subclass_of": [
        "B2AI_STANDARD:98"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:849"
      ],
      "purpose_detail": "This Part of the DICOM Standard contains explanatory information in the form of Normative and Informative Annexes.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.dicomstandard.org/current",
      "formal_specification": "http://dicom.nema.org/medical/dicom/current/output/html/part17.html",
      "responsible_organization": [
        "B2AI_ORG:25"
      ],
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:11"
      ]
    },
    {
      "id": "B2AI_STANDARD:86",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "DICOMweb",
      "description": "DICOM Part 18 Web Services",
      "subclass_of": [
        "B2AI_STANDARD:98"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:849"
      ],
      "purpose_detail": "PS3.18 specifies web services (using the HTTP family of protocols) for managing and distributing DICOM (Digital Imaging and Communications in Medicine) Information Objects, such as medical images, annotations, reports, etc. to healthcare organizations, providers, and patients. The term DICOMweb is used to designate the RESTful Web Services described here.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.dicomstandard.org/current",
      "formal_specification": "http://dicom.nema.org/medical/dicom/current/output/html/part18.html",
      "responsible_organization": [
        "B2AI_ORG:25"
      ],
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:11"
      ]
    },
    {
      "id": "B2AI_STANDARD:87",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "DICOM Part 19",
      "description": "DICOM Part 19 Application Hosting",
      "subclass_of": [
        "B2AI_STANDARD:98"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:849"
      ],
      "purpose_detail": "This Part of the DICOM Standard defines an interface between two software applications. One application, the Hosting System, provides the second application with data, such as a set of images and related data. The second application, the Hosted Application, analyzes that data, potentially returning the results of that analysis, for example in the form of another set of images and/or structured reports, to the first application. Such an Application Program Interface (API) differs in scope from other portions of the DICOM Standard in that it standardizes the data interchange between software components on the same system, instead of data interchange between different systems.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.dicomstandard.org/current",
      "formal_specification": "http://dicom.nema.org/medical/dicom/current/output/html/part19.html",
      "responsible_organization": [
        "B2AI_ORG:25"
      ],
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:11"
      ]
    },
    {
      "id": "B2AI_STANDARD:88",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "DICOM Part 2",
      "description": "DICOM Part 2 Conformance",
      "subclass_of": [
        "B2AI_STANDARD:98"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:849"
      ],
      "purpose_detail": "An implementation need not employ all the optional components of the DICOM Standard. After meeting the minimum general requirements, a conformant DICOM implementation may utilize whatever SOP Classes, communications protocols, Media Storage Application Profiles, optional (Type 3) Attributes, codes and controlled terminology, etc., needed to accomplish its designed task.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.dicomstandard.org/current",
      "formal_specification": "http://dicom.nema.org/medical/dicom/current/output/html/part02.html",
      "responsible_organization": [
        "B2AI_ORG:25"
      ],
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:11"
      ]
    },
    {
      "id": "B2AI_STANDARD:89",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "DICOM Part 20",
      "description": "DICOM Part 20 Imaging Reports using HL7 Clinical Document Architecture",
      "subclass_of": [
        "B2AI_STANDARD:98"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:40"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:849"
      ],
      "purpose_detail": "This Part of the DICOM Standard specifies templates for the encoding of imaging reports using the HL7 Clinical Document Architecture Release 2 (CDA R2, or simply CDA) Standard. Within this scope are clinical procedure reports for specialties that use imaging for screening, diagnostic, or therapeutic purposes.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.dicomstandard.org/current",
      "formal_specification": "http://dicom.nema.org/medical/dicom/current/output/html/part20.html",
      "responsible_organization": [
        "B2AI_ORG:25"
      ],
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:11"
      ]
    },
    {
      "id": "B2AI_STANDARD:90",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "DICOM Part 21",
      "description": "DICOM Part 21 Transformations between DICOM and other Representations",
      "subclass_of": [
        "B2AI_STANDARD:98"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "concerns_data_topic": [
        "B2AI_TOPIC:15",
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:849"
      ],
      "purpose_detail": "This Part of the DICOM Standard specifies the transformations between DICOM and other representations of the same information.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.dicomstandard.org/current",
      "formal_specification": "http://dicom.nema.org/medical/dicom/current/output/html/part21.html",
      "responsible_organization": [
        "B2AI_ORG:25"
      ],
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:11"
      ]
    },
    {
      "id": "B2AI_STANDARD:91",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "DICOM Part 22",
      "description": "DICOM Part 22 Real-Time Communication",
      "subclass_of": [
        "B2AI_STANDARD:98"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:849"
      ],
      "purpose_detail": "This Part of the DICOM Standard specifies an SMPTE ST 2110-10 based service, relying on RTP, for the real-time transport of DICOM metadata. It provides a mechanism for the transport of DICOM metadata associated with a video or an audio flow based on the SMPTE ST 2110-20 and SMPTE ST 2110-30, respectively.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.dicomstandard.org/current",
      "formal_specification": "http://dicom.nema.org/medical/dicom/current/output/html/part22.html",
      "responsible_organization": [
        "B2AI_ORG:25"
      ],
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:11"
      ]
    },
    {
      "id": "B2AI_STANDARD:92",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "DICOM Part 3",
      "description": "DICOM Part 3 Information Object Definitions",
      "subclass_of": [
        "B2AI_STANDARD:98"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:849"
      ],
      "purpose_detail": "This Part of the DICOM Standard specifies the set of Information Object Definitions (IODs) that provide an abstract definition of real-world objects applicable to communication of digital medical information. For each IOD, this Part specifies any necessary information for the semantic description of the IOD, relationships to associated real-world objects relevant to the IOD, Attributes that describe the characteristics of the IOD.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.dicomstandard.org/current",
      "formal_specification": "http://dicom.nema.org/medical/dicom/current/output/html/part03.html",
      "responsible_organization": [
        "B2AI_ORG:25"
      ],
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:11"
      ]
    },
    {
      "id": "B2AI_STANDARD:93",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "DICOM Part 4",
      "description": "DICOM Part 4 Service Class Specifications",
      "subclass_of": [
        "B2AI_STANDARD:98"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:849"
      ],
      "purpose_detail": "This Part of the DICOM Standard specifies the set of Service Class Definitions that provide an abstract definition of real-world activities applicable to communication of digital medical information.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.dicomstandard.org/current",
      "formal_specification": "http://dicom.nema.org/medical/dicom/current/output/html/part04.html",
      "responsible_organization": [
        "B2AI_ORG:25"
      ],
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:11"
      ]
    },
    {
      "id": "B2AI_STANDARD:94",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "DICOM Part 5",
      "description": "DICOM Part 5 Data Structures and Encoding",
      "subclass_of": [
        "B2AI_STANDARD:98"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:849"
      ],
      "purpose_detail": "In this Part of the Standard the structure and encoding of the Data Set is specified. In the context of Application Entities communicating over a network, a Data Set is that portion of a DICOM Message that conveys information about real world objects being managed over the network. A Data Set may have other contexts in other applications of this Standard; e.g., in media exchange the Data Set translates to file content structure.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.dicomstandard.org/current",
      "formal_specification": "http://dicom.nema.org/medical/dicom/current/output/html/part05.html",
      "responsible_organization": [
        "B2AI_ORG:25"
      ],
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:11"
      ]
    },
    {
      "id": "B2AI_STANDARD:95",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "DICOM Part 6",
      "description": "DICOM Part 6 Data Dictionary",
      "subclass_of": [
        "B2AI_STANDARD:98"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:849"
      ],
      "purpose_detail": "This Part of the DICOM Standard is PS 3.6 of a multi-part standard produced to facilitate the interchange of information between digital imaging computer systems in medical environments. This interchange will enhance diagnostic imaging and potentially other clinical applications. The multi-part DICOM Standard covers the protocols and data that shall be supplied to achieve this interchange of information.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.dicomstandard.org/current",
      "formal_specification": "http://dicom.nema.org/medical/dicom/current/output/html/part06.html",
      "responsible_organization": [
        "B2AI_ORG:25"
      ],
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:11"
      ]
    },
    {
      "id": "B2AI_STANDARD:96",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "DIMSE",
      "description": "DICOM Part 7 Message Exchange",
      "subclass_of": [
        "B2AI_STANDARD:98"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:849"
      ],
      "purpose_detail": "This Part of the DICOM Standard specifies the DICOM Message Service Element (DIMSE). The DIMSE defines an Application Service Element (both the service and protocol) used by peer DICOM Application Entities for the purpose of exchanging medical images and related information.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.dicomstandard.org/current",
      "formal_specification": "http://dicom.nema.org/medical/dicom/current/output/html/part07.html",
      "responsible_organization": [
        "B2AI_ORG:25"
      ],
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:11"
      ]
    },
    {
      "id": "B2AI_STANDARD:97",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "DICOM Part 8",
      "description": "DICOM Part 8 Network Communication Support for Message Exchange",
      "subclass_of": [
        "B2AI_STANDARD:98"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:849"
      ],
      "purpose_detail": "The Communication Protocols specified in this Part of PS3 closely fit the ISO Open Systems Interconnection Basic Reference Model (ISO 7498-1, see Figure 1-1). They relate to the following layers - Physical, Data Link, Network, Transport, Session, Presentation and the Association Control Services (ACSE) of the Application layer. The communication protocols specified by this Part are general purpose communication protocols (TCP/IP) and not specific to this Standard.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.dicomstandard.org/current",
      "formal_specification": "http://dicom.nema.org/medical/dicom/current/output/html/part08.html",
      "responsible_organization": [
        "B2AI_ORG:25"
      ],
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:11"
      ]
    },
    {
      "id": "B2AI_STANDARD:98",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "DICOM",
      "description": "Digital Imaging And Communications In Medicine",
      "related_to": [
        "B2AI_STANDARD:691"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:33",
          "category": "B2AI:Application",
          "name": "NCI Imaging Data Commons FAIR Dataset Harmonization for ML Training",
          "description": "The NCI Imaging Data Commons uses DICOM to harmonize large-scale public imaging collections with AI-derived annotations, supporting transparent cohort selection and reproducible ML development. IDC encodes images and derived artifacts (volumetric segmentations, slice-level annotations, radiomics features) consistently using DICOM Segmentation objects and Structured Reports, enabling FAIR (findable, accessible, interoperable, reusable) datasets. Cloud-ready tools and libraries including highdicom lower barriers for ML developers to produce and consume DICOM-compliant training labels and outputs directly from Python workflows, facilitating dataset quality control and reproducibility for AI model development.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1038/s41597-023-02864-y"
          ]
        },
        {
          "id": "B2AI_APP:119",
          "category": "B2AI:Application",
          "name": "DICOM Segmentation and Structured Report Encoding for Supervised Learning Labels",
          "description": "Research datasets are re-encoded into DICOM annotation objects to make labels directly consumable by ML pipelines. LIDC-IDRI lung nodule annotations were converted to DICOM Segmentation (SEG) objects and DICOM Structured Reports following template TID1500, providing interoperable nodular masks and qualitative/quantitative assessments. Disease-specific datasets like HCC-TACE distribute DICOM-SEG tumor/liver labels for algorithm training and evaluation. DICOM-SEG metadata tags capture algorithm provenance (Segment algorithm type, Segmentation type) essential for data curation, reproducibility, and supervised ML workflows, while software stacks (DCMTK, ITK, dcmqi, pydicom-seg) enable conversion and pipeline integration.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1186/s13244-021-01081-8"
          ]
        },
        {
          "id": "B2AI_APP:120",
          "category": "B2AI:Application",
          "name": "Vendor-Agnostic PACS-Integrated Shadow Testing Pipeline for Near-Real-Time Segmentation",
          "description": "A containerized, DICOM-compatible pipeline integrates PACS with ML inference for near-real-time clinical shadow testing of segmentation algorithms. PACS sends studies via classic DICOM services (C-STORE/C-FIND/C-MOVE) to an on-premises GPU host that performs nnU-Net inference and converts results to DICOM Segmentation objects and Structured Reports encoding volumetry. Results are stored in a DCM4CHEE archive and visualized in OHIF web viewer, enabling clinical evaluation, human-in-the-loop corrections, and dataset curation. This demonstrates concrete PACS-to-inference-to-reporting workflows using established DICOM network protocols for AI integration into radiology operations.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.3389/fmed.2023.1241570"
          ]
        },
        {
          "id": "B2AI_APP:121",
          "category": "B2AI:Application",
          "name": "DICOM Structured Report TID1500 for Machine-Readable AI Results and Radiomics",
          "description": "Quantitative AI outputs including radiomics features and imaging biomarkers are standardized via DICOM Structured Report template TID1500 measurement reports, providing machine-readable, auditable AI results that facilitate downstream research and clinical decision support. DICOM SR encodes semantic measurements with standardized format and lexicon, acting as a big-data container for multimodal patient data integration. DICOM Parametric Map IODs preserve derived pixelwise feature maps (kurtosis, texture) as DICOM objects for reproducible feature extraction. Integration guidance from standards bodies emphasizes encoding AI outputs into SR to scale deployment across vendors and sites.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1186/s13244-021-01081-8",
            "https://doi.org/10.1117/1.jmi.7.1.016502"
          ]
        },
        {
          "id": "B2AI_APP:122",
          "category": "B2AI:Application",
          "name": "DICOMweb RESTful Services for Scalable Cloud and Web-Based ML Pipelines",
          "description": "DICOMweb RESTful services (WADO-RS, QIDO-RS, STOW-RS) connect external ML applications and viewers to enterprise archives over HTTP, enabling scalable web and cloud AI pipelines. DICOMweb client APIs allow external apps to retrieve, query, and store DICOM objects without classic DIMSE protocols, facilitating integration with modern web architectures. Frameworks stream DICOM and metadata from hospital PACS into research compute clusters to power real-time ML and operational analytics. Examples include Niffler for PACS-to-research pipelines with metadata extraction and ML analytics, and EMPAIA decentralized platform for running third-party AI on DICOM data at scale.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1016/j.cmpb.2024.108113"
          ]
        },
        {
          "id": "B2AI_APP:123",
          "category": "B2AI:Application",
          "name": "DICOM-WSI and DICOMweb for Computational Pathology ML Development",
          "description": "DICOM Whole Slide Imaging (DICOM-WSI) and DICOMweb enable interoperable storage, visualization, and annotation of whole-slide images in web viewers for computational pathology. These standards support collecting standardized annotations and displaying AI inference outputs including segmentation masks, heat maps, and image-derived measurements within the DICOM ecosystem. DICOM-WSI brings pathology images into uniform DICOM workflows to facilitate ML model development and validation, while web viewers (e.g., Visilab Viewer) provide platforms for annotating training data and reviewing AI results in digital pathology contexts.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1038/s41597-023-02864-y",
            "https://doi.org/10.1016/j.cmpb.2024.108113"
          ]
        },
        {
          "id": "B2AI_APP:124",
          "category": "B2AI:Application",
          "name": "IODeep Proposed IOD for DNN Model Management and Federated Learning in PACS",
          "description": "The proposed IODeep DICOM Information Object Definition stores deep neural network architectures and weights inside PACS as non-patient objects, enabling model selection via DICOM tag metadata (modality, anatomical region, disease). Inference runs client-side or on dedicated servers proximal to viewers, with predicted ROIs saved as RT Structure Sets for physician validation and retraining. IODeep supports model lifecycle management including fine-tuning with hospital data and federated learning scenarios by sharing model metadata without moving patient data. The design uses Unlimited Text VR for JSON architecture descriptions and weight URIs, with parsers for TensorFlow/PyTorch instantiation, demonstrating an emerging pattern for managing AI models within DICOM-compliant workflows.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1016/j.cmpb.2024.108113"
          ]
        },
        {
          "id": "B2AI_APP:125",
          "category": "B2AI:Application",
          "name": "DICOM GSPS and RT Structure Sets for AI Annotation Presentation and Clinical Feedback",
          "description": "DICOM Grayscale Softcopy Presentation State (GSPS) ensures consistent overlay and display of AI annotations across workstations, while RT Structure Set (RTSS) objects store physician-validated contours after AI suggestion, supporting clinical quality assurance and retraining workflows. RTSS is widely supported for storing ROI contours and labels for model training, enabling human-in-the-loop correction loops where radiologists review AI predictions, validate or correct them, and create ground-truth datasets. Integration of CAD results into PACS using DICOM SR and GSPS, combined with DICOM routers for communication and correction, supports maturity levels from investigational AI display through continuously updated deployed models driven by radiologist interactions.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1117/1.jmi.7.1.016502",
            "https://doi.org/10.1016/j.cmpb.2024.108113"
          ]
        }
      ],
      "collection": [
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:115",
        "B2AI_ORG:114",
        "B2AI_ORG:117"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:849"
      ],
      "purpose_detail": "Digital Imaging and Communications in Medicine (DICOM) is the international standard for medical imaging information and related data. DICOM defines the formats and communication protocols for medical images (CT, MRI, ultrasound, X-ray, etc.) and associated metadata including patient demographics, study information, equipment parameters, and image acquisition details. The standard encompasses file formats, network protocols, and information models that enable interoperability across imaging devices, PACS (Picture Archiving and Communication Systems), workstations, and clinical information systems. DICOM supports structured reporting through DICOM-SR for encoding measurements, annotations, and AI-generated findings in a standardized machine-readable format. The standard includes specialized modules for radiation dose tracking, workflow management, and integration of AI inference results. DICOM's comprehensive metadata structure and pixel data encoding make it essential for training and deploying AI models in medical imaging, while its widespread clinical adoption ensures that AI tools can integrate seamlessly into existing radiological workflows and health IT infrastructure.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.dicomstandard.org/",
      "formal_specification": "https://www.dicomstandard.org/current",
      "responsible_organization": [
        "B2AI_ORG:25"
      ],
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:11",
        "B2AI_SUBSTRATE:19"
      ]
    },
    {
      "id": "B2AI_STANDARD:99",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Direct Standard",
      "description": "Direct Applicability Statement for Secure Health Transport",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:26"
      ],
      "purpose_detail": "Describes how to use SMTP, S/MIME, and X.509 certificates to securely transport health information over the Internet.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://wiki.directproject.org/w/images/e/e6/Applicability_Statement_for_Secure_Health_Transport_v1.2.pdf",
      "formal_specification": "https://wiki.directproject.org/w/images/e/e6/Applicability_Statement_for_Secure_Health_Transport_v1.2.pdf"
    },
    {
      "id": "B2AI_STANDARD:100",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "DAS",
      "description": "Distributed Sequence Annotation System",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "The Distributed Annotation System (DAS) is a client-server protocol and data federation framework enabling decentralized, lightweight integration of heterogeneous biological sequence annotations from multiple independent data sources without requiring centralized data warehousing or schema harmonization, allowing genome browsers and bioinformatics applications to dynamically retrieve and overlay annotations (gene structures, protein domains, sequence variants, regulatory elements, expression data, evolutionary conservation, functional predictions) from distributed annotation servers on-demand as users navigate genomic coordinates or protein sequences. Developed in 2001 by the Wellcome Trust Sanger Institute and European Bioinformatics Institute to address the challenge of integrating the rapidly growing diversity of genome annotation sourcesranging from curated databases (Ensembl, UCSC Genome Browser, UniProt) to experimental datasets (ChIP-seq peaks, RNA-seq coverage, GWAS associations) to computational predictions (transcription factor binding sites, splice variants, structural motifs)without each resource needing to replicate all others' data, DAS defines a simple HTTP-based XML protocol where a reference server provides coordinate systems (genome assemblies like GRCh38, protein sequences like UniProt IDs) establishing the shared addressing scheme, and annotation servers implement a standardized REST API exposing endpoints for querying features within specified coordinate ranges (GET /das/source/features?segment=chr1:1000000,2000000 returns all annotations overlapping that genomic region as XML feature elements with start/end positions, feature type ontology terms, scores, orientation, and links to external resources), enabling genome browsers (Ensembl, IGB, Gbrowse, UCSC Genome Browser via DAS tracks) to federate queries across dozens of annotation sources and render integrated multi-layer visualizations where each data source contributes a separate track without pre-loading all data locally. The DAS architecture specifies a sources endpoint (GET /das/sources) returning a registry of available annotation datasets with metadata (organism, coordinate system, version, capabilities, description), a types endpoint enumerating feature categories available from a source (gene, exon, SNP, binding_site using Sequence Ontology terms for semantic interoperability), a features endpoint retrieving annotations for coordinate ranges with optional filtering by feature type or category, a sequence endpoint allowing reference servers to provide underlying DNA/protein sequences, and a stylesheet endpoint defining visual rendering hints (colors, glyphs, heights) for client-side display customization, collectively creating a lightweight, stateless, RESTful federation model where each server maintains autonomy over its data update schedule, access policies, and internal schemas while presenting a uniform query interface. The decentralized annotation federation paradigm enables community-driven annotation contributions where individual research groups, consortia, or computational prediction pipelines can independently deploy DAS servers exposing their specialized annotations (tissue-specific regulatory elements from ENCODE, disease variants from ClinVar, orthology relationships from Ensembl Compara, mass spectrometry peptide mappings from proteomics experiments, CRISPR guide RNA predictions from design tools) without requiring approval or coordination from central genome databases, democratizing annotation dissemination and accelerating data sharing by reducing barriers (no need to submit to centralized repositories, format conversions, or publication delays), while clients benefit from accessing a comprehensive, continually updated annotation landscape by federating queries across the DAS registry. DAS supports multi-organism comparative genomics where a client application displays syntenic regions across species (human, mouse, zebrafish) by querying DAS servers for each organism's genome with coordinate transformations (liftover between assemblies, orthologous coordinate mapping) handled by specialized alignment servers that expose alignment blocks as DAS features, enabling users to visualize conserved regulatory elements, identify evolutionarily constrained sequences as potential functional elements, and compare gene structures across phylogenetic groups. The system accommodates temporal versioning where DAS sources specify genome assembly versions (GRCh37 vs. GRCh38, mm9 vs. mm10) and annotation release dates, allowing clients to query historical states for reproducibility and longitudinal studies tracking how annotations evolve as evidence accumulates, with provenance metadata documenting annotation methods (experimental vs. computational, manual curation vs. automated prediction) and confidence scores enabling evidence-weighted integration where high-confidence annotations are visually emphasized. For artificial intelligence and machine learning applications, DAS infrastructure enables dynamic feature construction for genomic machine learning models by federating retrieval of multi-modal annotations (sequence conservation PhyloP scores, histone modification ChIP-seq signals, chromatin accessibility DNase-seq peaks, transcription factor binding PWM matches, RNA-seq expression levels, variant population frequencies from gnomAD, pathogenicity predictions from CADD/DANN, tissue-specific enhancer predictions from deep learning models like Enformer) across coordinate ranges corresponding to regulatory elements, gene loci, or variant positions, generating rich feature matrices for training supervised classifiers predicting regulatory activity, variant pathogenicity, or tissue-specific gene expression; the federated architecture avoids data replication by querying annotations on-the-fly during feature engineering pipelines, with caching layers mitigating latency. Deep learning models for functional genomics leverage DAS-federated annotations as training labels where ChIP-seq peaks served via DAS define positive examples of transcription factor binding sites, histone modifications indicate active promoters/enhancers, and CAGE-seq peaks mark transcription start sites, enabling convolutional neural networks to learn sequence motifs predictive of these functional annotations from DNA sequences retrieved via DAS reference servers; model predictions can be published back as new DAS annotation tracks (predicted binding sites, chromatin state segmentations from ChromHMM/Segway, variant effect predictions) completing a cycle where ML-generated annotations become available for federated querying by other researchers and subsequent model iterations. Metagenomics and microbiome research benefit from DAS-like federated annotation where metagenomic contigs assembled from environmental samples are annotated with gene predictions served by one DAS source, taxonomic assignments from another (BLAST hits to RefSeq, marker gene phylogenies), functional annotations from enzyme databases (EC numbers, KEGG pathways), and antibiotic resistance gene predictions from specialized databases (CARD, ResFinder), enabling integrated analysis of community functional potential across multiple annotation dimensions without consolidating heterogeneous databases. Precision medicine applications employ DAS to federate patient variant annotations where a clinical genome browser queries the patient's VCF coordinates against DAS servers providing ClinVar pathogenicity assertions, gnomAD population frequencies, functional predictions (SIFT, PolyPhen-2, CADD scores from separate computational servers), drug-gene interaction annotations from PharmGKB, and literature evidence from PubMed mining, synthesizing a comprehensive variant interpretation without requiring local copies of all databases, with machine learning models trained on federated annotations to predict variant clinical significance (benign, pathogenic, variant of uncertain significance VUS) using ensemble features from multiple DAS sources improving classification accuracy beyond single-source models. The DAS protocol's extensibility supports emerging data types where new feature categories (3D chromatin interactions from Hi-C as arcs between genomic coordinates, RNA structure predictions, nanopore long-read alignments with base modification calls, single-cell ATAC-seq pseudo-bulk tracks aggregated by cell type, spatial transcriptomics histology-linked expression) can be exposed through DAS servers as new sources join the federation, and clients incorporating DAS client libraries (Bio::Das::Lite in Perl, pydas in Python, Java DAS clients) dynamically discover and integrate these novel annotation layers, fostering an open, evolvable ecosystem where innovation in annotation methods (new experimental assays, improved computational predictions, AI-generated annotations) rapidly propagates to end-users through the lightweight DAS federation layer without requiring software updates or data migrations, ultimately enabling AI-driven genomics research where machine learning models training on comprehensive, multi-source genomic annotations benefit from the decentralized, continually updated knowledge encoded in the distributed DAS annotation network spanning experimental databases, computational prediction servers, and AI model inference APIs.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://static-content.springer.com/esm/art%3A10.1186%2F1471-2105-2-7/MediaObjects/12859_2001_8_MOESM1_ESM.pdf",
      "publication": "doi:10.1186/1471-2105-2-7",
      "formal_specification": "https://static-content.springer.com/esm/art%3A10.1186%2F1471-2105-2-7/MediaObjects/12859_2001_8_MOESM1_ESM.pdf"
    },
    {
      "id": "B2AI_STANDARD:101",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "EML",
      "description": "Ecological Metadata Language",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A metadata standard developed for the earth, environmental and ecological sciences.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://eml.ecoinformatics.org/",
      "publication": "doi:10.5063/F11834T2"
    },
    {
      "id": "B2AI_STANDARD:102",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "EBDCS",
      "description": "Economic Botany Data Collection Standard",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:1"
      ],
      "purpose_detail": "This standard provides a system whereby uses of plants (in their cultural context) can be described, using standardised descriptors and terms, and attached to taxonomic data sets. It resulted from discussions at the International Working Group on Taxonomic Databases for Plant Sciences (TDWG) between 1989 and 1992. Users and potential users of the standard include economic botanists and ethnobotanists whose purpose is to record all known information about the uses of a taxon; educationalists, taxonomists, biochemists, anatomists etc. who wish to record plant use, often at a broad level; economic botany collection curators who need to describe accurately the uses and values of specimens in their collections; bibliographers who need to describe plant uses referred to in publications and to apply keywords consistently for ease of data retrieval. While this standard is still in use, it is no longer actively maintained (labelled as prior on the TDWG website).",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.tdwg.org/standards/economic-botany/",
      "responsible_organization": [
        "B2AI_ORG:93"
      ]
    },
    {
      "id": "B2AI_STANDARD:103",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "EC",
      "description": "Enzyme Commission Number",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "codesystem"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:26"
      ],
      "purpose_detail": "The Enzyme Commission (EC) number system provides a hierarchical, internationally standardized nomenclature and classification scheme for enzymes based on the chemical reactions they catalyze rather than their protein structure, sequence homology, or evolutionary relationships, enabling unambiguous identification and cross-referencing of enzymatic functions across biochemistry, molecular biology, genomics, metabolomics, and systems biology research. Established in 1961 by the International Union of Biochemistry and Molecular Biology (IUBMB) and published in the foundational 1965 report \"Enzyme Nomenclature,\" the EC system assigns each characterized enzyme a unique four-level numerical identifier (EC X.Y.Z.W) where the first digit indicates the major reaction class (1=oxidoreductases catalyzing oxidation-reduction, 2=transferases transferring functional groups, 3=hydrolases catalyzing hydrolysis, 4=lyases cleaving bonds by mechanisms other than hydrolysis or oxidation, 5=isomerases catalyzing intramolecular rearrangements, 6=ligases forming bonds with ATP cleavage, 7=translocases moving molecules across membranes), the second digit specifies the subclass refining the reaction type (such as EC 1.1.* for oxidoreductases acting on the CH-OH group of donors, EC 2.7.* for transferases transferring phosphorus-containing groups like kinases), the third digit further narrows substrate specificity or reaction mechanism (EC 2.7.1.* for phosphotransferases with alcohol group as acceptor), and the fourth digit uniquely identifies the individual enzyme (EC 2.7.1.1 for hexokinase catalyzing ATP + D-hexose  ADP + D-hexose 6-phosphate), creating a hierarchical classification framework with ~7,700 entries as of 2025 covering characterized enzymatic activities with recommended systematic names, common synonyms, reaction equations with substrate/product stoichiometry, cofactor requirements, and literature references documenting catalytic mechanisms. The EC system serves as the authoritative enzyme nomenclature standard integrated into major biological databases including UniProt protein knowledgebase (mapping protein sequences to enzymatic functions via EC annotations), KEGG pathway database (representing metabolic and signaling pathways as networks of EC-numbered reactions), BRENDA comprehensive enzyme information system (curating kinetic parameters, substrate specificities, and organism-specific properties for each EC number), MetaCyc/BioCyc metabolic pathway collections (using EC numbers to define reaction steps in organism-specific and reference pathway reconstructions), Reactome pathway knowledgebase (annotating human biochemical reactions with EC classifications), and ENZYME database at ExPASy (the reference repository maintained by Swiss Institute of Bioinformatics mirroring IUBMB nomenclature committee recommendations with monthly updates reflecting newly characterized enzymes and reclassifications). The hierarchical EC structure enables functional annotation where genome sequencing projects assign putative enzymatic functions to predicted protein-coding genes through sequence homology searches against characterized enzymes, with tools like BLAST, HMMer, and InterPro identifying conserved catalytic domains corresponding to specific EC classes; metabolic network reconstruction leverages EC annotations to infer organism-specific biochemical capabilities by mapping genomic enzyme repertoires to known metabolic pathways, predicting auxotrophies, biosynthetic capacities, and catabolic capabilities; and comparative enzymology uses EC-based functional profiles to analyze metabolic evolution, horizontal gene transfer of enzymatic functions, and adaptation to environmental niches by quantifying presence/absence patterns of enzyme classes across phylogenetic groups. For artificial intelligence and machine learning applications, EC numbers provide structured functional labels enabling supervised learning where enzyme function prediction models train on sequence-to-EC mappings using architectures such as convolutional neural networks on amino acid sequences (DeepEC, ECPred), graph neural networks on protein structure representations (predicting catalytic residues and EC class from 3D coordinates), and pre-trained protein language models (ESM, ProtTrans) fine-tuned for multi-label EC classification treating the hierarchical EC system as nested prediction tasks (first predicting reaction class EC X.-.-.-, then subclass, then sub-subclass, then specific enzyme), with evaluation metrics including hierarchical precision/recall accounting for partial correctness when predictions match broader EC categories. Machine learning on EC-annotated enzyme datasets supports metabolic engineering applications where models predict substrate promiscuity and catalytic efficiency for enzyme variants, guiding directed evolution campaigns by forecasting which mutations enhance desired activities (kcat/KM for target substrates) or alter substrate specificity (broadening or narrowing accepted substrates), with training data derived from mutagenesis studies, kinetic assays, and high-throughput screening experiments linked to EC-numbered parent enzymes. Natural language processing extracts enzyme-function relationships from literature by mining PubMed abstracts for sentences mentioning EC numbers alongside organism names, experimental conditions, kinetic parameters, and inhibitors, populating knowledge graphs that connect genes, proteins, EC functions, metabolites, pathways, and phenotypessuch graphs trained using graph embedding methods (node2vec, TransE) or graph neural networks enable link prediction inferring novel enzyme-metabolite associations, pathway gap-filling identifying missing enzymatic steps in incomplete reconstructions, and drug target prioritization by predicting which enzymes are essential for pathogen viability or disease progression. Metabolomics and proteomics data integration benefits from EC-based functional categorization where untargeted metabolomics detects thousands of small molecules, and pathway enrichment analysis tests whether detected metabolites over-represent substrates or products of specific EC classes, implicating corresponding enzymatic activities as dysregulated in disease states; proteomics quantifies enzyme abundances, and flux balance analysis (FBA) of genome-scale metabolic models constrained by EC-annotated reaction stoichiometry predicts metabolic fluxes under different conditions, with machine learning models trained on multi-omics datasets (transcriptomics, proteomics, metabolomics) to predict flux distributions and growth phenotypes. Pharmacology and drug discovery leverage EC classifications where therapeutic enzyme targets (such as EC 3.4.24.* matrix metalloproteinases in cancer metastasis, EC 3.4.25.1 angiotensin-converting enzyme ACE in hypertension, EC 2.7.10.* protein tyrosine kinases in oncogenic signaling) guide rational drug design, with machine learning models predicting compound-enzyme binding affinities and selectivity profiles by training on chemical structure-EC activity datasets from ChEMBL and PubChem BioAssay, enabling virtual screening of compound libraries against specific EC-numbered enzyme families and off-target prediction assessing whether candidate drugs inhibit unintended EC classes causing adverse effects. Metagenomics and microbiome research employ EC-based functional profiling where shotgun metagenomic sequencing of environmental or host-associated microbial communities is followed by gene prediction and EC annotation, yielding community-level enzyme repertoires; tools like HUMAnN (HMP Unified Metabolic Analysis Network) quantify the abundance of EC-numbered pathways, revealing functional shifts in microbiomes across health/disease states or environmental perturbations, with machine learning classifiers trained on EC abundance profiles distinguishing disease phenotypes (inflammatory bowel disease vs. healthy controls, dysbiosis states) and random forests identifying which EC functions (e.g., butyrate-producing enzymes EC 2.8.3.*, bile salt hydrolases EC 3.5.1.24) are most predictive of clinical outcomes. Systems biology integrates EC-annotated metabolic models with gene regulatory networks and signaling pathways, where constraint-based modeling (FBA, flux variability analysis) predicts metabolic fluxes through EC-numbered reactions under genetic or environmental perturbations, and machine learning emulators (neural networks trained on simulation ensembles) accelerate exploration of design spaces for metabolic engineering or predict emergent phenotypes from multi-scale models coupling genome-scale metabolism (EC-defined reactions), transcriptional regulation (transcription factor binding), and environmental sensing. The EC classification system also underpins ontology-based data integration where the Gene Ontology (GO) molecular function terms reference EC numbers to define enzymatic activities (GO:0004396 hexokinase activity is defined with EC 2.7.1.1), enabling semantic reasoning over biological knowledge graphs that link genes annotated with GO/EC terms to pathways, phenotypes, diseases, and drugs, supporting AI-driven hypothesis generation such as identifying candidate enzymes for synthetic biology chassis organisms, predicting metabolic vulnerabilities in cancer cells (synthetic lethality with specific EC-numbered enzyme knockouts), or discovering novel biocatalysts for industrial applications by querying EC-structured enzyme databases for activities matching desired chemical transformations (stereoselectivity, substrate scope, cofactor independence), ultimately providing the standardized functional vocabulary essential for reproducible enzyme annotation, interoperable metabolic databases, and machine learning models that predict, design, and optimize enzymatic functions across biochemistry, biomedicine, biotechnology, and synthetic biology.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://iubmb.qmul.ac.uk/enzyme/rules.html",
      "publication": "doi:10.1126/science.150.3697.719",
      "responsible_organization": [
        "B2AI_ORG:61"
      ]
    },
    {
      "id": "B2AI_STANDARD:104",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "EORTC QLQ",
      "description": "EORTC Quality of Life questionnaires",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "diagnosticinstrument"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "Instruments designed to assess (some of) the different aspects that define the QoL of (a specific group of) cancer patients.",
      "is_open": true,
      "requires_registration": true,
      "url": "https://qol.eortc.org/questionnaires/"
    },
    {
      "id": "B2AI_STANDARD:105",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "EDF",
      "description": "European Data Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:258",
          "category": "B2AI:Application",
          "name": "Sleep-EDF 1D-CNN sleep staging",
          "description": "Automated sleep stage classification using one-dimensional convolutional neural networks (1D-CNN) trained on raw polysomnography (EEG and EOG) signals from Sleep-EDF and Sleep-EDFX databases achieving high multi-class accuracy for 2-6 stage classification. The 1D-CNN architecture processes raw PSG time-series without manual feature engineering, learning characteristic sleep waveforms directly from EDF-formatted recordings. Reported accuracies on Sleep-EDF were 98.06% (2-class), 94.64% (3-class), 92.36% (4-class), 91.22% (5-class), and 91.00% (6-class); on Sleep-EDFX 97.62%, 94.34%, 92.33%, 90.98%, and 89.54% respectively, demonstrating that deep learning on EDF biosignal files achieves near-human performance for automated sleep staging supporting large-scale sleep disorder screening and research.",
          "references": [
            "https://doi.org/10.3390/ijerph16040599"
          ]
        },
        {
          "id": "B2AI_APP:259",
          "category": "B2AI:Application",
          "name": "Sleep-EDF weighted Random Forest + HMM staging",
          "description": "Automated sleep staging using improved weighted Random Forest (WRF) with continuous wavelet packet transform (WPT) features combined with Hidden Markov Model (HMM) temporal smoothing on Sleep-EDF and Sleep-EDF Expanded double-channel EEG (Fpz-Cz and Pz-Oz). Recordings segmented into 30-second epochs then 29 overlapping 2-second subepochs for frequency-domain and statistical feature extraction, with feature selection removing low-variance features and class weighting addressing imbalance, followed by HMM exploiting temporal dependencies between adjacent sleep stages. Achieved subject-non-independent accuracy 93.20% and Cohen's kappa 0.890; subject-independent accuracy 91.97% and kappa 0.874; best double-channel performance 94.34% accuracy and kappa 0.886; best single-subject 96.3% accuracy and kappa 0.912. Demonstrates effective classical ML approach on EDF-formatted PSG with improved N1 stage recognition, providing interpretable feature-based alternative to end-to-end deep learning for clinical sleep medicine.",
          "references": [
            "https://doi.org/10.32604/cmes.2020.08731"
          ]
        },
        {
          "id": "B2AI_APP:260",
          "category": "B2AI:Application",
          "name": "SeizeIT2 wearable EDF seizure detection benchmark",
          "description": "Wearable epilepsy dataset providing behind-the-ear EEG, ECG, EMG, accelerometer, and gyroscope data in EDF format with annotations in JSON/TSV for standardized seizure detection algorithm development. Dataset includes 125 patients with focal epilepsy (80% training split sub-001 to sub-096 with 704 seizures, 20% validation split sub-097 to sub-125 with 182 seizures), recordings aligned with full-scalp video-EEG monitoring, corrupted flat-line segments removed. Provides two baseline pipelines: feature-based ML using SVM (1-25 Hz Butterworth band-pass, 2-second windows 50% overlap, RMS thresholding 13-150 V, 42 features from 2-channel behind-the-ear EEG, 5x background undersampling) and deep learning ChronoNet (convolutional + recurrent layers, 250 Hz resampling, 2-second segments 75% overlap seizures/50% background). Annotations document onset, duration, lateralization, localization, vigilance state, and per-channel visibility. Enables reproducible comparison of wearable seizure detection algorithms on standardized EDF-formatted biosignals supporting ambulatory epilepsy monitoring and closed-loop intervention systems.",
          "references": [
            "https://doi.org/10.1038/s41597-025-05580-x"
          ]
        },
        {
          "id": "B2AI_APP:261",
          "category": "B2AI:Application",
          "name": "EDF-based EEG deep learning survey benchmarks",
          "description": "Comprehensive survey documenting widespread use of EDF-formatted EEG datasets (Sleep-EDF, CHB-MIT pediatric epilepsy with ~969 hours and 173 seizures, TUH EEG Corpus with 60,000+ recordings) underpinning deep learning research for sleep staging, seizure detection, and EEG abnormality detection. Sleep-EDF typical 5-stage classification accuracies reported at 82-88% overall with advanced models (deep CNNs, CNN+BiLSTM, U-Net variants U-Time, temporal convolutional networks, Transformer hybrids, self-supervised pretraining) achieving 87-89% accuracy and Cohen's kappa 0.75-0.82 by learning characteristic sleep waveforms from 30-second epochs. Cascaded RNN pipelines report ~95% on 6-class CAP Sleep. CHB-MIT continuous scalp EEG and TUH/TUSZ corpora enable training CNN, CNN+RNN, and image-based CNN architectures for clinical seizure detection and abnormality identification. Survey highlights that EDF standardization enables comparative evaluation across laboratories, reproducible benchmarking, and transfer learning from public datasets to clinical applications, with deep learning architectures approaching or exceeding human expert performance on EDF-standardized biosignal corpora.",
          "references": [
            "https://doi.org/10.56578/ataiml040304"
          ]
        }
      ],
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:37"
      ],
      "purpose_detail": "The European Data Format (EDF) is a simple and flexible format for exchange and storage of multichannel biological and physical signals, particularly in clinical neurophysiology, sleep medicine, and cardiology. Originally developed in 1990 by European biomedical engineers through the \"Methodology for the Analysis of the Sleep-Wakefulness Continuum\" project funded by the European Community, EDF became the de-facto standard for EEG and PSG recordings. The format is hardware and software independent, supporting various montages, transducers, prefiltering options, and sampling frequencies. EDF enables creation of common databases for sleep records, comparative analysis of algorithms across laboratories, and standardized evaluation of manual and automatic analysis methods. The format has been extended to EDF+, which maintains backward compatibility while adding support for interrupted recordings, annotations, stimuli, events, and analysis results such as deltaplots, QRS parameters, and sleep stages. EDF+ can store any medical recording including EMG, evoked potentials, and ECG data, with stricter specifications that enable automatic electrode localization and calibration. Both formats are freely available and widely adopted in commercial equipment and multicenter research projects.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.edfplus.info/"
    },
    {
      "id": "B2AI_STANDARD:106",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "ELS",
      "description": "European Laryngological Society guidelines",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "diagnosticinstrument"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "A multidimensional set of minimal basic measurements suitable for all common dysphonias is proposed. It includes five different approaches - perception (grade, roughness, breathiness), videostroboscopy (closure, regularity, mucosal wave and symmetry), acoustics (jitter, shimmer, Fo-range and softest intensity), aerodynamics (phonation quotient), and subjective rating by the patient.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://doi.org/10.1007/s004050000299",
      "publication": "doi:10.1007/s004050000299"
    },
    {
      "id": "B2AI_STANDARD:107",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "EQ-5D-3L",
      "description": "EuroQol Five Dimension Three Level descriptive system",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "diagnosticinstrument"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "The EQ-5D-3L descriptive system comprises the following five dimensions - mobility, self-care, usual activities, pain/discomfort and anxiety/depression. Each dimension has 3 levels - no problems, some problems, and extreme problems. The patient is asked to indicate his/her health state by ticking the box next to the most appropriate statement in each of the five dimensions. This decision results into a 1-digit number that expresses the level selected for that dimension. The digits for the five dimensions can be combined into a 5-digit number that describes the patients health state.",
      "is_open": true,
      "requires_registration": true,
      "url": "https://euroqol.org/eq-5d-instruments/eq-5d-3l-about/"
    },
    {
      "id": "B2AI_STANDARD:108",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "FAIR Cookbook",
      "description": "FAIR Cookbook",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:28"
      ],
      "purpose_detail": "An online, open and live resource for the Life Sciences with recipes that help you to make and keep data Findable, Accessible, Interoperable and Reusable; in one word FAIR.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://faircookbook.elixir-europe.org/",
      "publication": "doi:10.5281/zenodo.7156792"
    },
    {
      "id": "B2AI_STANDARD:109",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "FHIR",
      "description": "Fast Healthcare Interoperability Resources",
      "related_to": [
        "B2AI_STANDARD:720",
        "B2AI_STANDARD:688",
        "B2AI_STANDARD:693",
        "B2AI_STANDARD:694",
        "B2AI_STANDARD:697"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:34",
          "category": "B2AI:Application",
          "name": "Cumulus Federated EHR Learning with Bulk FHIR and AI/NLP",
          "description": "The Cumulus platform operationalizes SMART/HL7 Bulk FHIR Access API for standardized data export across multiple healthcare institutions, then applies AI and natural language processing for computable phenotyping to define cohorts and outcomes from both structured and unstructured EHR data. The SMART Text2FHIR pipeline extracts insights from clinical texts and converts them into structured FHIR data elements for analysis. Only aggregate outputs leave each institution, enabling privacy-preserving federated learning across sites for public health monitoring and research while maintaining data sovereignty and interoperability through standardized FHIR exchange.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1101/2024.02.02.24301940"
          ]
        },
        {
          "id": "B2AI_APP:126",
          "category": "B2AI:Application",
          "name": "FHIR Observation Encoding of Radiology AI Findings for Interoperable Integration",
          "description": "A framework models radiology AI outputs (such as pulmonary nodule characteristics from automated detection algorithms) as FHIR Observation resources embedded within DiagnosticReport objects, standardizing AI-generated findings alongside radiologist reports. This FHIR-based encoding enables interoperable downstream use, long-term tracking of imaging findings, and integration with electronic health records for clinical decision-making. The structured FHIR representation facilitates reproducible data management, cohort identification, and longitudinal analysis of AI-detected imaging biomarkers across healthcare systems.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1093/jamia/ocae134"
          ]
        },
        {
          "id": "B2AI_APP:127",
          "category": "B2AI:Application",
          "name": "MyDigiTwin FHIR Harmonization for Federated Learning Cohort Studies",
          "description": "The MyDigiTwin federated learning research infrastructure uses a FHIR-based data harmonization framework to standardize cohort study variables across multiple sites for cardiovascular risk prediction modeling. The pipeline successfully generated approximately 150,000 FHIR bundles from Lifelines cohort variable data, demonstrating practical large-scale FHIR use for multi-center ML. This FHIR harmonization ensures interoperability and facilitates the structuring, standardization, and exchange of healthcare data across federated learning nodes, enabling distributed model training while data remains at local institutions.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.3233/shti240735"
          ]
        },
        {
          "id": "B2AI_APP:128",
          "category": "B2AI:Application",
          "name": "SMART on FHIR Integration for AI-Driven Clinical Decision Support Systems",
          "description": "SMART on FHIR provides a modular framework enabling third-party AI-driven clinical decision support applications to integrate seamlessly into health information systems. Implementations include automated breast cancer diagnosis via ultrasound with FHIR-standardized diagnostic metadata, ECG stream analysis frameworks for cloud-native AI processing, and machine learning-enhanced architectures that use FHIR to standardize clinical and imaging data flow for AI-based risk assessment. FHIR's standardized APIs and resource models enable deployment of predictive models across clinical domains, improving adherence to guidelines and enhancing diagnostic accessibility through interoperable, scalable CDSS implementations.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.20944/preprints202509.0818.v1"
          ]
        },
        {
          "id": "B2AI_APP:129",
          "category": "B2AI:Application",
          "name": "FHIR-Based Analytics Platforms for Clinical Predictive Model Deployment",
          "description": "FHIR data models and APIs support analytics frameworks and deployment of clinical predictive models including sepsis prediction, distributed phenotyping analytics platforms, and personalized medicine applications. Implementations provide patterns for converting FHIR resources to analysis-ready formats, hosting predictive models as web services, and automating management of audit files and structured medical data. Platforms like doc.ai leverage FHIR analytical capabilities for personalized medicine, while distributed phenotyping systems use FHIR to enable clinical decision-making across multiple healthcare sites with standardized data access and model integration.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.3390/healthcare11121729"
          ]
        }
      ],
      "collection": [
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:114",
        "B2AI_ORG:117"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:845",
        "B2AI_STANDARD:846",
        "B2AI_STANDARD:847",
        "B2AI_STANDARD:850"
      ],
      "purpose_detail": "Fast Healthcare Interoperability Resources (FHIR) is a standard for exchanging healthcare information electronically, developed by HL7. FHIR defines a set of modular components called \"resources\" that represent discrete clinical and administrative concepts (patients, observations, medications, procedures, diagnostic reports, etc.) with well-defined data structures, terminology bindings, and RESTful API interactions. Each resource can be retrieved, created, updated, and searched via standard HTTP operations, enabling flexible system integration and mobile/web application development. FHIR supports multiple exchange paradigms including RESTful APIs, messaging, documents, and services, with extensibility mechanisms (profiles, extensions) to adapt to local requirements while maintaining interoperability. The standard leverages modern web technologies (JSON, XML), common terminologies (SNOMED CT, LOINC, RxNorm), and implementation guides to facilitate rapid deployment across EHR systems, clinical decision support tools, patient-facing applications, and research platforms. FHIR's structured, granular data representation and standardized APIs make it particularly suitable for AI/ML applications that require consistent access to longitudinal patient data, clinical observations, and healthcare workflows for training predictive models and deploying real-time decision support at the point of care.",
      "is_open": true,
      "requires_registration": true,
      "url": "https://www.hl7.org/fhir/overview.html",
      "responsible_organization": [
        "B2AI_ORG:40"
      ]
    },
    {
      "id": "B2AI_STANDARD:110",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "FASTA",
      "description": "FASTA Sequence Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "A text-based data format for nucleotide and amino acid sequences.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikipedia.org/wiki/FASTA_format"
    },
    {
      "id": "B2AI_STANDARD:111",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "FASTQ",
      "description": "FASTQ Sequence and Sequence Quality Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:35",
          "category": "B2AI:Application",
          "name": "seqQscorer Automated FASTQ Quality Control Using Machine Learning",
          "description": "seqQscorer computes features from FASTQ files including per-base and per-read quality metrics, overrepresented sequences, k-mer counts, and mapping-derived statistics, then trains tree-based (Random Forest) and deep learning classifiers to automatically flag low- versus high-quality NGS files. The system uses FASTQ-derived quality scores as core predictive features with labels from ENCODE curation, creating generalizable models across assays (RNA-seq, ChIP-seq, DNase-seq, ATAC-seq) and species. This ML-based automated QC provides decision support for curators and researchers, saving resources by identifying poor-quality experiments early in analysis pipelines.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1186/s13059-021-02294-2"
          ]
        },
        {
          "id": "B2AI_APP:130",
          "category": "B2AI:Application",
          "name": "MiniScrub CNN-Based Nanopore Read Error Scrubbing Using FASTQ Quality Pileups",
          "description": "MiniScrub encodes Oxford Nanopore read-to-read overlaps and per-base qualities from FASTQ into image-like pileup channels that capture minimizer matches, base quality scores, and inter-minimizer distances. A convolutional neural network trained on these FASTQ-derived representations identifies and removes low-quality read segments de novo, without requiring reference alignment or hybrid error correction. This CNN-based scrubbing improves downstream assembly quality by reducing misassemblies and indel errors, demonstrating how FASTQ sequence and quality information can be transformed into ML-friendly image representations for quality classification tasks.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1186/s12859-019-3103-z"
          ]
        },
        {
          "id": "B2AI_APP:131",
          "category": "B2AI:Application",
          "name": "MAC-ErrorReads Supervised Classification for Filtering Erroneous NGS Reads",
          "description": "MAC-ErrorReads formulates erroneous-read filtration as supervised binary classification using features extracted from FASTQ sequences via TF-IDF encoding (treating sequence tokens/k-mers like text). Multiple supervised algorithms including Naive Bayes models are trained to distinguish erroneous versus accurate reads, retaining higher-quality reads and improving downstream assembly quality and genomic coverage. This ML approach addresses increasing sequencing throughput by providing automated, scalable read filtering that complements traditional k-mer-based and alignment-based error-handling methods across both short-read and long-read NGS platforms.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1186/s12859-024-05681-1"
          ]
        },
        {
          "id": "B2AI_APP:132",
          "category": "B2AI:Application",
          "name": "SeqTagger CTC-CRF Basecaller for Nanopore Barcode Demultiplexing with Quality Filtering",
          "description": "SeqTagger demultiplexes direct RNA nanopore datasets by training a CTC-CRF DNA basecalling model (using Bonito software) to decode barcode sequences from signal data, producing FASTQ basecalls that are then mapped to reference barcodes. Per-read median base quality from FASTQ is used to filter out potential misassignments, achieving high precision (approximately 99%) and recall (approximately 95%) for demultiplexing. This workflow demonstrates how FASTQ basecalled sequences and their quality scores enable accurate barcode assignment and quality-based filtering in direct RNA sequencing, contrasting with image-based CNN approaches by operating in sequence space.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1101/2024.10.29.620808"
          ]
        },
        {
          "id": "B2AI_APP:133",
          "category": "B2AI:Application",
          "name": "Deep Learning Variant Calling from FASTQ-Aligned Read Pileups",
          "description": "Deep learning variant callers including DeepVariant, Clair3, and DNAscope operate on pileup tensors built from FASTQ-aligned reads, incorporating per-base quality scores as input features alongside base identity. These models transform FASTQ-derived alignments into image-like or tensor representations that preserve sequence context and quality information, then apply CNNs or specialized architectures for genotype prediction. Benchmarks demonstrate that these deep learning approaches outperform traditional algorithms for SNP and indel calling across platforms (Illumina, PacBio HiFi, Oxford Nanopore), with quality scores explicitly used to weight evidence and calibrate predictions through methods like GATK's variant quality score recalibration (VQSR).",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1101/2024.03.15.585313",
            "https://doi.org/10.1002/0471250953.bi1110s43"
          ]
        },
        {
          "id": "B2AI_APP:134",
          "category": "B2AI:Application",
          "name": "CNN-Based eDNA Read Classification and rRNA Detection from FASTQ",
          "description": "Deep learning models for metagenomics and environmental DNA analysis map FASTQ reads directly to taxonomic labels using k-mer embeddings or sequence representations, enabling fast and scalable classification with comparable accuracy to conventional pipelines. CNN-based workflows process raw FASTQ at high throughput for taxonomic assignment, while tools like RiboDetector use bidirectional LSTM models to identify ribosomal RNA reads in short-read data for contaminant removal. These approaches transform FASTQ sequences (and optionally quality-derived features) into token/k-mer embeddings or sequence inputs for classification tasks, demonstrating rapid and accurate read-level identification that reduces misclassification in downstream RNA-seq and metagenomic workflows.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1101/2024.03.15.585313"
          ]
        },
        {
          "id": "B2AI_APP:135",
          "category": "B2AI:Application",
          "name": "fastp FASTQ Preprocessing for ML-Ready Feature Generation",
          "description": "fastp performs ultra-fast all-in-one FASTQ preprocessing including quality control, adapter trimming, quality filtering, UMI processing, and base correction in a single scan, generating standardized outputs used as inputs or targets for ML workflows. The tool produces per-base quality profiles, k-mer occurrence tables, overrepresented-sequence positions, adapter detection results, and paired-end overlap corrections that serve as features for ML quality prediction models, denoising/error-correction training data, and metadata for classifiers. UMI-processed consensus reads reduce noise for ML training, while comprehensive pre- and post-filtering metrics enable automated quality assessment and artifact detection across NGS workflows, facilitating large-scale dataset generation for ML model development.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1093/bioinformatics/bty560"
          ]
        }
      ],
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:13"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:116",
        "B2AI_ORG:114"
      ],
      "purpose_detail": "FASTQ is a text-based file format for storing nucleotide sequences along with their corresponding per-base quality scores, making it the de facto standard for raw sequencing data from high-throughput sequencing platforms (Illumina, PacBio, Oxford Nanopore, etc.). Each sequence record consists of four lines containing a sequence identifier, the raw nucleotide sequence, a separator line, and ASCII-encoded quality scores (Phred scores) representing the confidence of each base call. The quality scores enable downstream tools to weight bases appropriately during alignment, variant calling, and assembly operations. FASTQ supports various quality encoding schemes (Phred+33, Phred+64) and accommodates reads of variable length from different sequencing technologies. The format's simultaneous representation of sequence and quality information is essential for quality control, error correction, read filtering, and AI/ML applications that require confidence metrics for training models on sequencing data quality assessment, base calling improvement, contamination detection, and metagenomic classification tasks.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikipedia.org/wiki/FASTQ_format"
    },
    {
      "id": "B2AI_STANDARD:112",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Common Rule",
      "description": "Federal Policy for the Protection of Human Subjects",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "policy"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:39"
      ],
      "purpose_detail": "The Federal Policy for the Protection of Human Subjects or the Common Rule was published in 1991 and codified in separate regulations by 15 Federal departments and agencies, as listed below. The HHS regulations, 45 CFR part 46, include four subparts - subpart A, also known as the Federal Policy or the Common Rule; subpart B, additional protections for pregnant women, human fetuses, and neonates; subpart C, additional protections for prisoners; and subpart D, additional protections for children. Each agency includes in its chapter of the Code of Federal Regulations [CFR] section numbers and language that are identical to those of the HHS codification at 45 CFR part 46, subpart A. For all participating departments and agencies the Common Rule outlines the basic provisions for IRBs, informed consent, and Assurances of Compliance. Human subject research conducted or supported by each federal department/agency is governed by the regulations of that department/agency.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.hhs.gov/ohrp/regulations-and-policy/regulations/common-rule/index.html"
    },
    {
      "id": "B2AI_STANDARD:113",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "CDMH",
      "description": "FHIR Common Data Models Harmonization",
      "related_to": [
        "B2AI_STANDARD:109"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:845",
        "B2AI_STANDARD:846",
        "B2AI_STANDARD:847",
        "B2AI_STANDARD:850"
      ],
      "purpose_detail": "The Common Data Models Harmonization (CDMH) FHIR Implementation Guide (IG) will focus on mapping and translating observational data extracted for Patient Centered Outcome Research (PCOR) and other purposes into FHIR format. Data is extracted from the different networks each of which may use a different data model for their data representation. The project focuses on the Common Data Models (CDMs) from the following four networks:Patient Centered Outcome Research Network (PCORNet), Informatics for Integrating Biology and Bedside (i2b2) Accrual to Clinical Trials (ACT), also known as i2b2/ACT., Observational Medical Outcomes Partnership (OMOP), and Food and Drug Administrations Sentinel",
      "is_open": true,
      "requires_registration": false,
      "url": "https://build.fhir.org/ig/HL7/cdmh/",
      "formal_specification": "https://build.fhir.org/ig/HL7/cdmh/profiles.html",
      "responsible_organization": [
        "B2AI_ORG:40"
      ]
    },
    {
      "id": "B2AI_STANDARD:114",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Genomics Operations",
      "description": "FHIR Genomics Operations",
      "related_to": [
        "B2AI_STANDARD:109"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:13",
        "B2AI_TOPIC:35"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:845",
        "B2AI_STANDARD:846",
        "B2AI_STANDARD:847",
        "B2AI_STANDARD:850"
      ],
      "purpose_detail": "A standardized suite of genomics operations in FHIR designed to support a wide range of clinical scenarios, such as variant discovery; clinical trial matching; hereditary condition and pharmacogenomic screening; and variant reanalysis.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://build.fhir.org/ig/HL7/genomics-reporting/operations.html",
      "publication": "doi:10.1093/jamia/ocac246",
      "formal_specification": "https://github.com/FHIR/genomics-operations",
      "responsible_organization": [
        "B2AI_ORG:40"
      ]
    },
    {
      "id": "B2AI_STANDARD:115",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Provenance",
      "description": "FHIR Provenance Resource",
      "related_to": [
        "B2AI_STANDARD:109"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:845",
        "B2AI_STANDARD:846",
        "B2AI_STANDARD:847",
        "B2AI_STANDARD:850"
      ],
      "purpose_detail": "\"The FHIR Provenance resource provides comprehensive tracking of information about activities that create, revise, delete, or sign healthcare resources, describing the entities and agents involved in these processes. Based on the W3C Provenance specification, it establishes a critical foundation for assessing authenticity, enabling trust, and allowing reproducibility in healthcare data systems. The resource supports three key components from W3C Provenance: Entity (physical, digital, or conceptual things with fixed aspects), Agent (persons, devices, systems, organizations, or care teams bearing responsibility for activities), and Activity (time-bound processes that act upon entities). Provenance resources serve as record-keeping assertions that capture context information for quality, reliability, and trustworthiness assessments. The standard supports multiple use cases including tracking data lineage in clinical workflows, enabling audit trails for regulatory compliance, supporting clinical decision-making through data origin verification, and facilitating data integration from multiple healthcare systems. It includes digital signature capabilities for integrity verification and non-repudiation, supports versioning and unique identification requirements, and provides mechanisms for recording import and transformation activities in healthcare interoperability scenarios.\"",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.hl7.org/fhir/provenance.html",
      "formal_specification": "https://www.hl7.org/fhir/provenance-definitions.html",
      "responsible_organization": [
        "B2AI_ORG:40"
      ]
    },
    {
      "id": "B2AI_STANDARD:116",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "CodeSystem",
      "description": "FHIR Resource CodeSystem",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:845",
        "B2AI_STANDARD:846",
        "B2AI_STANDARD:847",
        "B2AI_STANDARD:850"
      ],
      "purpose_detail": "The CodeSystem resource is used to declare the existence of and describe a code system or code system supplement and its key properties, and optionally define a part or all of its content.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://hl7.org/fhir/codesystem.html",
      "responsible_organization": [
        "B2AI_ORG:40"
      ]
    },
    {
      "id": "B2AI_STANDARD:117",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "ConceptMap",
      "description": "FHIR Resource ConceptMap",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:4",
        "B2AI_TOPIC:32"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:845",
        "B2AI_STANDARD:846",
        "B2AI_STANDARD:847",
        "B2AI_STANDARD:850"
      ],
      "purpose_detail": "The FHIR ConceptMap resource provides a structured, machine-readable representation of terminology mappings that define relationships between codes from different coding systems (such as SNOMED CT to ICD-10-CM, RxNorm to local hospital formularies, LOINC to institution-specific laboratory test codes, or HL7 v2 table codes to FHIR value sets), enabling semantic translation of clinical concepts across heterogeneous health information systems where different organizations, regions, or time periods have adopted different terminologies for fundamentally equivalent clinical meanings, with ConceptMaps serving as the authoritative specification of how concepts in a source system map to concepts in one or more target systems through explicitly documented equivalence relationships that preserve clinical intent during data exchange, system migration, federated queries across institutional boundaries, real-world evidence studies requiring harmonized phenotype definitions, and AI/ML applications necessitating consistent terminology representation for training data drawn from multiple sources. A ConceptMap resource defines mapping metadata including the canonical URL that serves as the unique, version-independent identifier for the map (enabling unambiguous reference in implementations, like \"http://hl7.org/fhir/ConceptMap/103\" for the standard ICD-9-CM to SNOMED CT map), human-readable name, title, and description providing context on the map's purpose and appropriate usage scenarios, version string and effective date range documenting temporal validity (critical for longitudinal studies where terminology mappings evolve as source or target systems release new versionsICD-10-CM annual updates, SNOMED CT bi-annual releases), publication status indicating lifecycle state (draft maps under development and review, active maps validated and approved for production use, retired maps superseded by newer versions with documented migration paths), publisher and contact information identifying the authoritative source responsible for map curation and maintenance (such as national standards bodies, professional societies, or multi-stakeholder collaboratives like the US National Library of Medicine maintaining UMLS Metathesaurus cross-terminology mappings), copyright and licensing terms governing map reuse (essential for proprietary terminology licenses requiring attribution, usage restrictions, or redistribution controls), jurisdiction and use context specifying appropriate usage scenarios (maps designed for clinical decision support vs billing/reimbursement vs quality measurement vs research analytics may differ in mapping granularity and equivalence semantics), and purpose statement articulating the clinical, administrative, or analytical rationale for the map's creation. The core mapping structure consists of source and target elements defining the conceptual spaces being mapped, where sourceUri or sourceCanonical references the value set or code system providing concepts to be translated (the \"from\" terminology like ICD-10-CM diagnosis codes), targetUri or targetCanonical references the value set or code system providing translation targets (the \"to\" terminology like SNOMED CT concepts), and group elements partition the overall mapping into logical subsets each specifying a narrower source and target scope (enabling efficient lookup and reducing mapping complexity by organizing translations hierarchicallyseparate groups for different ICD-10-CM chapters mapping to different SNOMED CT hierarchies). Within each group, element entries define individual concept mappings with source code specifying the code being translated (like ICD-10-CM \"E11.9\" for Type 2 diabetes mellitus without complications), source display providing human-readable text for the source concept (improving map reviewability and maintenance), and target array enumerating one or more translation targets each with target code (like SNOMED CT \"44054006\" for diabetes mellitus type 2), target display, equivalence indicator specifying the semantic relationship between source and target concepts using standardized codes (equivalent indicating exact semantic match suitable for bidirectional translation, equal for formally identical concepts differing only in representation, wider where target concept subsumes source meaningsource is more specific than target, subsumes synonym for wider, narrower where source concept encompasses targetsource is more general than target, specializes synonym for narrower, inexact for approximate matches where translation loses or adds semantic nuance, unmatched explicitly documenting that no suitable target exists for this source concept, disjoint indicating fundamentally incompatible meanings precluding any translation), comment providing human-readable notes on translation rationale or caveats (documenting clinical judgment, edge cases, or context-dependent validity), and dependsOn elements specifying additional properties or codes that must be considered for context-dependent mappings (such as patient age, anatomical location, or procedure context required to disambiguate translations where a single source code maps to different targets depending on clinical circumstances). ConceptMap resources support bidirectional vs unidirectional mapping specifications where bidirectional maps define reciprocal translations enabling roundtrip conversion (A maps to B AND B maps back to A, critical for system interoperability requiring lossless translation), unidirectional maps define one-way translations where reverse mapping is either undefined or requires a separate ConceptMap (common when mapping from granular source terminology like SNOMED CT 350,000+ active concepts to coarse target terminology like ICD-10-CM ~70,000 codes, where many distinct source concepts map to same target but target cannot unambiguously reverse-map to unique source), and unmapped elements document default behavior when source concepts lack explicit mappings (strategies include returning fixed code indicating \"no map available\", using source code unchanged in target system if permitted by use case, or raising an error to prevent silent data loss). The operational integration of ConceptMaps with FHIR infrastructure occurs through the $translate operation exposed by FHIR terminology servers, where clients submit requests specifying source code, source system, and target system (plus optional context parameters like dependsOn property values, reverse direction flag for bidirectional maps, and scope constraints), the terminology server queries its ConceptMap repository to locate applicable maps matching the source-to-target system pair and temporal validity constraints, executes the mapping logic including evaluating conditional dependencies and equivalence semantics, and returns a Parameters resource containing the translated target code(s), display text, equivalence type, and provenance indicating which ConceptMap and version were appliedthis $translate operation enables real-time, on-demand concept translation within clinical workflows without requiring applications to implement mapping logic internally, supporting batch translation operations that process datasets by translating columns of source codes to target codes in bulk (efficient for ETL pipelines populating analytical databases), and cached translation services that precompute and materialize common translations to optimize performance in high-throughput scenarios like near-real-time clinical decision support or streaming analytics on electronic health record data. For AI and machine learning applications, ConceptMap resources enable foundational data harmonization where federated learning initiatives training models across multiple healthcare institutions use ConceptMaps to translate institution-specific local terminologies to a common reference terminology before model training (ensuring that \"CBC\" at Hospital A, \"complete blood count\" at Hospital B, and LOINC code \"58410-2\" at Hospital C are recognized as equivalent features despite different source representations, preventing vocabulary fragmentation that would degrade model performance), natural language processing pipelines performing clinical entity linking use ConceptMaps to normalize extracted text mentions to preferred vocabularies (mapping free-text medication names to RxNorm, disease mentions to SNOMED CT, laboratory tests to LOINC) with equivalence type metadata guiding confidence scoring (equivalent mappings receiving higher confidence than inexact mappings), clinical decision support systems translate local order codes to knowledge base terminologies to enable guideline checking (mapping hospital formulary drug codes to RxNorm for drug-drug interaction screening, translating local procedure codes to SNOMED CT for appropriateness use criteria evaluation), data warehouse extract-transform-load processes use ConceptMaps to harmonize heterogeneous source systems to unified target schemas like OMOP Common Data Model or PCORnet Common Data Model (systematically translating diagnosis codes, procedure codes, medication codes, laboratory tests to standard_concept_id representations, documenting mapping provenance and equivalence semantics for sensitivity analyses), real-world evidence studies requiring longitudinal observational cohorts spanning multiple health systems leverage ConceptMaps to create harmonized phenotype definitions (defining \"diabetes\" cohort inclusion criteria as UNION of ICD-10-CM codes, SNOMED CT concepts, and local problem list terms after ConceptMap-based semantic alignment ensuring consistent case ascertainment despite source terminology heterogeneity), quality measurement programs use ConceptMaps to translate clinical data to value sets specified in measure definitions (CMS eCQM specifications, HEDIS measures requiring specific code sets with ConceptMaps enabling sites using alternative terminologies to demonstrate compliance), international research collaborations map national terminologies to shared reference standards (NHS Read Codes or SNOMED CT Clinical Terms UK edition to international SNOMED CT for multi-country studies, French CCAM procedure codes to ICD-10-PCS for comparative effectiveness research), interoperability testing frameworks validate roundtrip translation fidelity by applying forward and reverse ConceptMaps and checking that clinical meaning is preserved (detecting lossy or ambiguous mappings that could compromise data quality in health information exchanges), genomic medicine applications linking phenotype terminologies to genotype databases use ConceptMaps to translate clinical diagnosis codes to Human Phenotype Ontology (HPO) terms enabling variant prioritization algorithms that match patient phenotypes to gene-disease associations (mapping ICD-10 \"Q87.0\" Fragile X syndrome to HPO \"HP:0001999\" Abnormal facial shape and related terms), pharmacovigilance systems harmonize adverse event reports from different sources by mapping MedDRA terms, SNOMED CT concepts, and free-text descriptions to common representations supporting signal detection algorithms, and precision medicine data integration platforms use ConceptMaps to align multi-omics entities across measurement platforms (mapping mass spectrometry metabolite identifiers to HMDB codes, proteomics peptide sequences to UniProt identifiers, transcriptomics probe identifiers to HUGO gene symbols) enabling cross-platform statistical analysis and multi-modal machine learning models that integrate heterogeneous molecular phenotypes for treatment response prediction, with ConceptMap-documented semantic relationships providing the essential terminology translation infrastructure that enables AI/ML models to learn from distributed, terminology-diverse healthcare data while maintaining clinical validity and interpretability through explicit, auditable, version-controlled mappings that preserve semantic relationships and document translation limitations, ultimately supporting trustworthy AI where model training data provenance includes terminology harmonization metadata, federated analytics can reason about semantic equivalence across sites, and real-world evidence generation maintains clinical meaning across heterogeneous source terminologies through principled, transparent concept mapping.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://hl7.org/fhir/conceptmap.html",
      "responsible_organization": [
        "B2AI_ORG:40"
      ]
    },
    {
      "id": "B2AI_STANDARD:118",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "NamingSystem",
      "description": "FHIR Resource NamingSystem",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:845",
        "B2AI_STANDARD:846",
        "B2AI_STANDARD:847",
        "B2AI_STANDARD:850"
      ],
      "purpose_detail": "The FHIR NamingSystem resource provides a curated, machine-readable registry of identifier and coding systems used within healthcare information exchange, documenting the namespaces that issue unique symbols for identification of concepts (terminology codes like SNOMED CT concept identifiers, LOINC codes), people (patient medical record numbers, provider National Provider Identifiers NPI, staff employee IDs), devices (medical device unique device identifiers UDI, imaging equipment serial numbers), organizations (facility identifiers, payer organization IDs), and other entities requiring unambiguous identification across distributed healthcare systems, serving as the authoritative metadata layer that enables FHIR Identifier and Coding data types to reference external identifier systems with precise semantics ensuring correct interpretation, validation, and linkage of identifiers across institutional boundaries. A NamingSystem resource defines critical metadata for an identifier/coding system including the system's canonical URI (the authoritative namespace string used in FHIR Identifier.system and Coding.system elements, like \"http://snomed.info/sct\" for SNOMED CT or \"http://hl7.org/fhir/sid/us-npi\" for US National Provider Identifiers), human-readable name and description (contextual information for implementers about the system's purpose, scope, and appropriate usage), uniqueId elements specifying alternative identifiers for the same system (enabling recognition that \"http://snomed.info/sct\" and OID \"2.16.840.1.113883.6.96\" reference the same SNOMED CT system across different contextsURI-based references in modern FHIR implementations, OID-based references in legacy HL7 v2/v3 systems), type classification distinguishing identifier systems (which assign unique identifiers to specific instances like \"patient 12345 at Hospital A\") from codesystem systems (which define concept codes representing classes or types like \"diabetes mellitus Type 2\" as a SNOMED CT concept applicable to many patients), and usage context metadata including responsible organization (identifying issuing authority like College of American Pathologists for SNOMED CT, Regenstrief Institute for LOINC, CMS for NPIs), jurisdiction and preferred usage regions (documenting geographic scope like \"US realm\" for US Core profiles, \"International\" for globally applicable systems), and publication status tracking system lifecycle (draft systems under development, active systems in production use, retired systems being phased out with guidance on migration to successors). NamingSystem resources support identifier validation workflows where applications receiving identifiers check that the namespace portion matches a recognized NamingSystem, verify the identifier format conforms to system-specific rules documented in the resource (NPIs must be 10 digits passing Luhn algorithm checksum, ICD-10-CM codes follow specific alphanumeric patterns), and apply jurisdictional constraints ensuring identifiers are used appropriately (US Realm profiles require US-specific systems, cross-border exchanges may necessitate identifier mapping or alternative identification strategies). The uniqueId elements enable identifier normalization where systems receiving data with OID-based identifiers (common in HL7 v2 messages, CDA documents) translate them to URI-based identifiers for processing in FHIR pipelines, with NamingSystem lookups resolving \"2.16.840.1.113883.6.88\" OID to \"http://www.nlm.nih.gov/research/umls/rxnorm\" RxNorm URI ensuring consistent internal representation regardless of source format. The NamingSystem registry function is crucial for multi-institutional data integration where patient linking requires matching identifiers across institutions (recognizing that different hospitals' MRNs are distinct namespaces requiring explicit NamingSystem declarations like \"http://hospital-a.org/fhir/sid/mrn\" vs \"http://hospital-b.org/fhir/sid/mrn\" preventing accidental conflation of unrelated identifiers), provider directories aggregate credentials from multiple sources (matching NPIs, state medical license numbers, DEA numbers as distinct but co-referent identifiers for same provider using NamingSystem-based identity resolution), and medication reconciliation disambiguates drug identifiers from different systems (RxNorm concept codes, NDC product codes, local formulary IDs requiring explicit namespace documentation for accurate medication matching). For AI/ML applications, NamingSystem resources enable robust data preprocessing pipelines where ETL processes validate identifier formats against NamingSystem-documented patterns before loading data into analytical databases (rejecting malformed identifiers that would corrupt downstream analyses), federated learning initiatives leverage NamingSystem metadata to understand identifier scope and design privacy-preserving record linkage strategies (determining which identifier systems permit cross-institutional matching vs requiring local pseudonymization), natural language processing systems extracting identifiers from clinical text use NamingSystem format specifications to recognize identifier patterns in unstructured notes (detecting \"NPI 1234567890\" as a provider identifier, \"MRN 98765432\" as a patient identifier with appropriate namespace disambiguation), entity resolution models training on linked administrative and clinical data use NamingSystem-based features indicating identifier type and issuing organization to improve matching accuracy (weighting high-trust national identifiers like NPIs more heavily than potentially duplicate local identifiers), synthetic data generation for ML development uses NamingSystem specifications to produce realistic test identifiers following correct format rules and namespace conventions (generating valid-format but non-existent NPIs for testing, synthetic MRNs conforming to institutional patterns), clinical data warehouses implementing OMOP Common Data Model or i2b2 star schema map source system identifiers to standardized domains using NamingSystem-based crosswalks (translating hospital-specific identifiers to common representations while maintaining provenance), quality measurement systems validating data submissions against program requirements use NamingSystem checks ensuring submitted identifiers match expected namespaces (CMS quality reporting requiring specific identifier systems), and blockchain-based health information exchanges leverage NamingSystem resources as smart contract metadata ensuring cryptographically-signed clinical data references identifiers from trusted, auditable namespaces, ultimately supporting trustworthy AI where model training data provenance traces back through properly identified entities, federated analytics respect identifier scope and privacy constraints, and real-world evidence generation maintains identifier integrity across diverse source systems through explicit, machine-actionable namespace documentation.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://hl7.org/fhir/namingsystem.html",
      "responsible_organization": [
        "B2AI_ORG:40"
      ]
    },
    {
      "id": "B2AI_STANDARD:119",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "TerminologyCapabilities",
      "description": "FHIR Resource TerminologyCapabilities",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:845",
        "B2AI_STANDARD:846",
        "B2AI_STANDARD:847",
        "B2AI_STANDARD:850"
      ],
      "purpose_detail": "A TerminologyCapabilities resource documents a set of capabilities (behaviors) of a FHIR Terminology Server that may be used as a statement of actual server functionality or a statement of required or desired server implementation.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://hl7.org/fhir/terminologycapabilities.html",
      "responsible_organization": [
        "B2AI_ORG:40"
      ]
    },
    {
      "id": "B2AI_STANDARD:120",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "ValueSet",
      "description": "FHIR Resource ValueSet",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:845",
        "B2AI_STANDARD:846",
        "B2AI_STANDARD:847",
        "B2AI_STANDARD:850"
      ],
      "purpose_detail": "The FHIR ValueSet resource is a fundamental component of the Fast Healthcare Interoperability Resources (FHIR) standard that defines curated collections of codes drawn from one or more terminology systems (CodeSystems like SNOMED CT, LOINC, RxNorm, ICD-10) intended for use in specific clinical contexts, serving as the mechanism linking standardized medical vocabularies to coded data elements in FHIR resources where terminology binding ensures semantic interoperability, data quality, and consistent interpretation across heterogeneous healthcare information systems. A ValueSet resource instance specifies both intensional definitions (rule-based membership criteria like \"all SNOMED CT concepts descending from '64572001|Disease' excluding neoplasms\") and extensional definitions (explicit enumeration of permitted codes like specific ICD-10 diagnosis codes for diabetes complications), with metadata including unique canonical URL identifier (enabling unambiguous ValueSet references across systems), version information (supporting evolution of code sets over time while maintaining backward compatibility), publication status (draft/active/retired lifecycle states), publisher and contact information (attributing ValueSet authorship to standards organizations like HL7, CDC, WHO, or local healthcare authorities), copyright/licensing terms, and purpose/usage notes explaining clinical context and intended application. ValueSet composition uses include and exclude elements with filters enabling dynamic code selection where \"include\" adds codes matching criteria (concept descendants in hierarchy, codes with specific properties like \"is-a bacterial infection\", codes from particular code system versions), filter operators (is-a for subsumption, descendent-of for transitive closure, regex for pattern matching, in for membership testing against reference sets), and exclude rules remove unwanted codes from included sets, supporting complex value set definitions like \"all cardiovascular procedures (SNOMED codes under 64915003) performed on adults (exclude pediatric-specific procedures) with CPT billing codes mapped via ConceptMap resources.\" ValueSets support expansion operations where FHIR terminology servers resolve intensional definitions into explicit code lists at specific timestamps, returning complete enumerations of valid codes with display names, designations in multiple languages, hierarchical relationships, and additional metadata, with expansions cacheable and versioned to ensure reproducible validation of historical data against time-appropriate code sets. The binding strength mechanism associates ValueSets with FHIR data elements through required bindings (only codes from ValueSet permitted, enforced validation), extensible bindings (ValueSet codes preferred but others allowed if no appropriate match exists, supporting local extensions), preferred bindings (suggested codes but not enforced), and example bindings (illustrative codes for demonstration), enabling graduated interoperability where core clinical concepts like vital signs, medications, procedures require strict standardization (required/extensible bindings to international ValueSets) while local workflow terms, institutional protocols, research-specific classifications permit flexibility (preferred/example bindings). ValueSet supplements and ConceptMap resources extend functionality by adding local designations, translations, or properties to codes (supplements augment base CodeSystems without modifying source terminologies) and mapping between equivalent concepts across terminology systems (ConceptMaps translating ICD-10 diagnoses to SNOMED CT, RxNorm medications to local formulary codes), supporting federated terminology services where institutions contribute local knowledge while maintaining alignment with national/international standards. For AI/ML applications, FHIR ValueSets enable semantic feature engineering where coded clinical data elements (diagnoses, procedures, medications, lab tests) expand from atomic codes into rich semantic features using ValueSet hierarchies and relationships, with NLP systems mapping free-text clinical notes to structured codes then validating against appropriate ValueSets ensuring coded output matches context (problem lists use diagnosis ValueSets, medication statements use RxNorm ValueSets), phenotype algorithms querying FHIR data using ValueSet-based inclusion/exclusion criteria to identify study cohorts (patients with \"Type 2 Diabetes\" ValueSet codes AND \"Metformin\" ValueSet medications AND HbA1c > 7% for real-world evidence studies), machine learning models incorporating ValueSet membership as categorical features or using semantic embeddings where code positions in ValueSet hierarchies inform similarity metrics (SNOMED CT subsumption relationships improving disease classification models), clinical decision support systems validating orders and alerts against ValueSet constraints (medication-allergy checking where allergy ValueSets define contraindications, lab result interpretation using ValueSet-defined normal ranges), automated chart review and quality measurement where NLP extracted codes validate against quality measure ValueSets (CMS Core Measures, HEDIS specifications), federated learning across institutions using standardized ValueSets ensuring consistent phenotype definitions and outcome measurements enabling model training on distributed EHR data without raw data sharing, and clinical trial recruitment where eligibility criteria expressed as FHIR ValueSet queries (OMOP phenotypes, PCORnet Common Data Model queries) identify eligible patients across research networks, supporting precision medicine initiatives requiring semantic interoperability where AI models learn from multi-institutional data consistently coded using shared ValueSets, real-world evidence generation from EHR data where ValueSet-based phenotyping ensures reproducible cohort identification, and healthcare analytics platforms where standardized ValueSets enable accurate cross-system queries, longitudinal patient tracking, and population health surveillance despite underlying terminology heterogeneity in source systems.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://hl7.org/fhir/valueset.html",
      "responsible_organization": [
        "B2AI_ORG:40"
      ]
    },
    {
      "id": "B2AI_STANDARD:121",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "FCS",
      "description": "Flow Cytometry Standard format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:2"
      ],
      "purpose_detail": "The flow cytometry data file standard provides the specifications needed to completely describe flow cytometry data sets within the confines of the file containing the experimental data. In 1984, the first Flow Cytometry Standard format for data files was adopted as FCS 1.0. This standard was modified in 1990 as FCS 2.0 and again in 1997 as FCS 3.0. We report here on the next generation Flow Cytometry Standard data file format. FCS 3.1 is a minor revision based on suggested improvements from the community. The unchanged goal of the Standard is to provide a uniform file format that allows files created by one type of acquisition hardware and software to be analyzed by any other type.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikipedia.org/wiki/Flow_Cytometry_Standard",
      "publication": "doi:10.1002/cyto.a.20825"
    },
    {
      "id": "B2AI_STANDARD:122",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "FORCE11 DC",
      "description": "FORCE11 Data Citation Principles",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The Data Citation Principles cover the purpose, function and attributes of citations. These principles recognize the dual necessity of creating citation practices that are both human understandable and machine-actionable.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://force11.org/info/joint-declaration-of-data-citation-principles-final/",
      "responsible_organization": [
        "B2AI_ORG:32"
      ]
    },
    {
      "id": "B2AI_STANDARD:123",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "FACT-G",
      "description": "Functional Assessment of Cancer Therapy - General",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "diagnosticinstrument"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "The Functional Assessment of Cancer Therapy - General (FACT-G) is a 27-item questionnaire designed to measure four domains of HRQOL in cancer patients - Physical, social, emotional, and functional well-being. Original development and validation involved 854 patients with cancer and 15 oncology specialists. An initial pool of 370 overlapping items for breast, lung, and colorectal cancer was generated by open-ended interviews with patients experienced with the symptoms of cancer and oncology professionals. Using preselected criteria, items were reduced to a 38-item general version. Factor and scaling analyses of these 38 items on 545 patients with mixed cancer diagnoses resulted in the 27-item FACT-General (FACT-G). Coefficients of reliability and validity were uniformly high. The scale's ability to discriminate patients on the basis of stage of disease, performance status rating (PSR), and hospitalization status supports its sensitivity. It has also demonstrated sensitivity to change over time.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.facit.org/measures/FACT-G",
      "responsible_organization": [
        "B2AI_ORG:30"
      ]
    },
    {
      "id": "B2AI_STANDARD:124",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "FACIT-Dyspnea",
      "description": "Functional Assessment of Chronic Illness Therapy - Dyspnea",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "diagnosticinstrument"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "Functional Assessment of Chronic Illness Therapy - Dyspnea (FACIT-Dyspnea-10, also abbreviated FACIT-Dyspnea or FACIT-Dy) is a validated, patient-reported outcome (PRO) instrument consisting of 10 items specifically designed to assess dyspnea (shortness of breath) and its impact on daily functioning and quality of life in patients with chronic respiratory and cardiovascular conditions including chronic obstructive pulmonary disease (COPD), idiopathic pulmonary fibrosis (IPF), pulmonary arterial hypertension (PAH), heart failure, interstitial lung disease, and cancer-related dyspnea, providing standardized, quantitative measurement of breathlessness severity and functional limitations essential for clinical trials evaluating respiratory therapeutics, longitudinal disease monitoring, and patient-centered outcomes research. The instrument employs a 5-point Likert scale (0 = not at all, 1 = a little bit, 2 = somewhat, 3 = quite a bit, 4 = very much) for each item, with patients rating the frequency or severity of dyspnea-related symptoms and activity limitations over the past 7 days, yielding a total score ranging from 0 to 40 where higher scores indicate better functional status (less dyspnea and fewer limitations), with reverse scoring applied so that increasing scores consistently reflect improvement across all items. The 10 items assess multiple dimensions of dyspnea experience including physical symptoms during activities of daily living (shortness of breath when walking on level ground, climbing stairs, performing household chores, bathing or dressing), emotional impact (worry about breathing difficulties, feeling frustrated or frightened by breathlessness), social limitations (avoiding activities due to dyspnea, impact on family or social interactions), and functional capacity (ability to participate in physical activities, overall satisfaction with breathing), capturing the multidimensional burden of respiratory symptoms beyond simple physiological measurements like forced expiratory volume (FEV1) or oxygen saturation which may not correlate closely with subjective symptom severity or quality of life. FACIT-Dyspnea demonstrates strong psychometric properties validated across multiple respiratory disease populations including internal consistency reliability (Cronbach's alpha typically 0.90-0.95 indicating high item correlation), test-retest reliability (intraclass correlation coefficients 0.85-0.92 for stable patients over 2-4 weeks), construct validity (correlations with other respiratory symptom measures like UCSD Shortness of Breath Questionnaire, St. George's Respiratory Questionnaire, modified Medical Research Council dyspnea scale, and 6-minute walk distance following expected patterns), discriminant validity (distinguishing between patients with different disease severities and functional classes), and responsiveness to clinical change (detecting meaningful improvements following interventions like pulmonary rehabilitation, oxygen therapy, pharmacological treatments, with minimal clinically important difference MCID estimates ranging 2-4 points depending on population and context). The instrument is part of the broader FACIT Measurement System maintained by FACIT.org, available in multiple languages with linguistic validation studies ensuring cross-cultural equivalence, used extensively in FDA drug approval trials as a co-primary or secondary endpoint assessing dyspnea relief, and recommended in clinical practice guidelines for standardized symptom monitoring in chronic respiratory disease management. For AI/ML applications, FACIT-Dyspnea scores serve as supervised learning targets where models predict dyspnea trajectories from longitudinal clinical data (spirometry trends, biomarkers like NT-proBNP or KL-6, imaging features from chest CT showing fibrosis extent or emphysema distribution, comorbidity patterns, medication histories) enabling risk stratification identifying patients at high risk for rapid functional decline who may benefit from intensive intervention, natural language processing analyzing free-text clinical notes to extract dyspnea mentions and correlate with formal FACIT-Dyspnea assessments identifying under-recognized symptom burden, digital phenotyping where smartphone accelerometry and step counts correlate with reported dyspnea severity supporting real-time remote monitoring between clinic visits, reinforcement learning optimizing personalized treatment sequences (timing of oxygen therapy initiation, pulmonary rehabilitation referral, medication escalation) to maximize FACIT-Dyspnea improvements while minimizing adverse effects, and causal inference methods estimating treatment effects on dyspnea from observational electronic health record data where FACIT-Dyspnea may be collected as part of routine clinical care enabling pragmatic comparative effectiveness research, supporting precision respiratory medicine where machine learning models integrate multi-modal data to predict individual patient responses to specific interventions (bronchodilators, anti-fibrotic agents, exercise training) based on baseline characteristics and early dyspnea trajectory patterns, clinical trial optimization identifying patient populations most likely to demonstrate FACIT-Dyspnea improvements in Phase II studies to enrich Phase III enrollment, and digital therapeutic development where algorithms deliver adaptive interventions (breathing technique coaching, activity pacing recommendations, anxiety management) personalized to real-time dyspnea patterns captured via mobile apps administering abbreviated FACIT-Dyspnea items daily, ultimately enabling patient-centered outcomes measurement where standardized instruments provide the quantitative framework for AI systems learning optimal symptom management strategies from large-scale real-world data while ensuring that improvements meaningful to patientsnot just physiological biomarkersdrive therapeutic decision-making and clinical research priorities.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.facit.org/measures/facit-dyspnea",
      "publication": "doi:10.1016/j.jval.2010.06.001",
      "responsible_organization": [
        "B2AI_ORG:30"
      ]
    },
    {
      "id": "B2AI_STANDARD:125",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "FANLTC",
      "description": "Functional Assessment of Non-Life Threatening Conditions",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "diagnosticinstrument"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "The Functional Assessment of Non-Life Threatening Conditions (FANLTC) is a 26-item version of the FACT-G designed to be administered to patients with non-life threatening conditions. The item from the FACT-G making reference to anxiety about death has been removed, and the instrument measures four domains of HRQOL - Physical, social/family, emotional and functional well-being.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.facit.org/measures/FANLTC",
      "responsible_organization": [
        "B2AI_ORG:30"
      ]
    },
    {
      "id": "B2AI_STANDARD:126",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Regier2018",
      "description": "Functional equivalence of genome sequencing analysis pipelines enables harmonized variant calling across human genetics projects",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:13",
        "B2AI_TOPIC:35"
      ],
      "purpose_detail": "Regier2018 defines whole genome sequencing (WGS) data processing standards that enable functional equivalence (FE) across different analysis pipelines used by various research groups. These standards establish best practices for alignment, variant calling, and quality control steps in genomic analysis workflows, allowing different computational implementations to produce concordant results while still permitting innovation in pipeline optimization. The framework addresses key challenges in large-scale genomic studies by defining expected outputs, quality metrics, and validation procedures that ensure harmonized variant calling across projects. This standardization is particularly important for collaborative efforts like the Centers for Common Disease Genomics (CCDG), where multiple centers must produce comparable data that can be combined for downstream analysis. The standards cover reference genome usage, read alignment parameters, variant detection algorithms, and filtering criteria, providing a foundation for reproducible genomics research.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/CCDG/Pipeline-Standardization",
      "publication": "doi:10.1038/s41467-018-06159-4",
      "formal_specification": "https://github.com/CCDG/Pipeline-Standardization"
    },
    {
      "id": "B2AI_STANDARD:127",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "FuGE-ML",
      "description": "Functional Genomics Experiment Markup Language",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "Functional genomics experiments present many challenges in data archiving, sharing and querying. As the size and complexity of data generated from such experiments grows, so does the requirement for standard data formats. To address these needs, the Functional Genomics Experiment [Object Model / Markup-Language] (FuGE-OM, FuGE-ML) has been created to facilitate the development of data standards.FuGE is a model of the shared components in different functional genomics domains. FuGE facilitates the development of data standards in functional genomics in two ways. 1. FuGE provides a model of common components in functional genomics investigations, such as materials, data, protocols, equipment and software. These models can be extended to develop modular data formats with consistent structure. 2. FuGE provides a framework for capturing complete laboratory workflows, enabling the integration of pre-existing data formats. In this context, FuGE allows the capture of additional metadata that gives formats a context within the complete workflow. FuGE is available as a UML model and an XML Schema",
      "is_open": true,
      "requires_registration": false,
      "url": "https://fuge.sourceforge.net/index.php"
    },
    {
      "id": "B2AI_STANDARD:128",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "FuGEFlow",
      "description": "Functional Genomics Experiment model for flow cytometry",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:36",
          "category": "B2AI:Application",
          "name": "Functional Genomics Workflow Modeling and Reproducibility",
          "description": "FuGEFlow is used in AI applications for standardizing functional genomics experimental workflows, enabling automated workflow optimization, experimental design prediction, and quality control through machine learning. AI systems leverage FuGEFlow's formal representation of experimental protocols, data transformations, and sample tracking to train models that predict optimal experimental conditions, identify protocol deviations that affect data quality, and automate experimental planning. The standard supports AI-driven meta-analysis across functional genomics studies by ensuring computational reproducibility and enabling models to learn from both successful and failed experiments. Applications include automated protocol generation, prediction of experimental outcomes based on design parameters, and workflow recommendation systems for genomics laboratories.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "datamodel"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "FuGEFlow represents a collaborative effort to develop of flow cytometry experimental workflow description based on the FuGE model. The Functional Genomics Experiment data model (FuGE) describes common aspects of comprehensive, high-throughput experiments. FuGE is an extendable model that provides a basis for creation of new technology-specific data formats, such as FuGEFlow for flow cytometry.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://doi.org/10.1186/1471-2105-10-184",
      "publication": "doi:10.1186/1471-2105-10-184"
    },
    {
      "id": "B2AI_STANDARD:129",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "FAF",
      "description": "Functionality Assessment Flowchart",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "diagnosticinstrument"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "The Functionality Assessment Flowchart (FAF) is a structured clinical assessment tool designed to evaluate cancer patients' performance status through a standardized flowchart-based methodology. FAF provides a systematic approach to measuring functional capacity and activities of daily living in oncology patients, yielding performance status scores with high inter-observer agreement. The flowchart format guides clinicians through a series of binary decision points based on patient capabilities (ability to work, walk, self-care, etc.), resulting in consistent and reproducible assessments. FAF addresses limitations of traditional performance status scales by reducing subjectivity and improving reliability across different observers and clinical settings. The standardized scoring enables comparison of patient functional status over time, supports clinical decision-making for treatment planning, facilitates stratification in clinical trials, and provides structured data suitable for integration into electronic health records and outcomes research databases.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://doi.org/10.1186/s12885-015-1526-0",
      "publication": "doi:10.1186/s12885-015-1526-0"
    },
    {
      "id": "B2AI_STANDARD:130",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "GA4GH",
      "description": "GA4GH metadata model",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "deprecated"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "The Global Alliance for Genomics and Health (GA4GH) metadata model provides a standardized framework for representing and sharing genomic and clinical data across institutions and international borders, developed by a coalition of public and private stakeholders including research institutions, healthcare organizations, and technology companies. The model defines schemas for core genomic data types including variants, reads, references, annotations, and associated clinical phenotypes, using protocol buffers (protobuf) and JSON for interoperable data exchange. GA4GH schemas cover reference genomes and sequences, variant calls and annotations (compatible with VCF), sequencing reads and alignments (supporting BAM/CRAM), RNA quantification, continuous-valued genomic signals, genomic features and annotations, metadata about biosamples and individuals, phenotypic information using ontologies, and provenance tracking. The framework enables federated data access through APIs including hts-get for streaming genomic data, Beacon for discovery queries across datasets, Data Repository Service (DRS) for accessing data objects, Task Execution Service (TES) for running analysis workflows, and Workflow Execution Service (WES) for managing computational pipelines. GA4GH standards facilitate large-scale collaborative research initiatives, support FAIR principles (Findable, Accessible, Interoperable, Reusable), enable secure data sharing with privacy-preserving technologies, and provide the foundation for international genomics research networks, clinical genomics implementations, and precision medicine programs.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/ga4gh-metadata/metadata-schemas?tab=readme-ov-file",
      "formal_specification": "https://github.com/ga4gh-metadata/metadata-schemas",
      "responsible_organization": [
        "B2AI_ORG:34"
      ]
    },
    {
      "id": "B2AI_STANDARD:131",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Gating-ML",
      "description": "Gating-ML specification",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The Gating-ML specification represents a proposal on how to form unambiguous XML-based gate definitions that may be used independently as well as included as one of the components of ACS. Such a description of gates can facilitate the interchange and validation of data between different software packages with the potential of significant increase of hardware and software interoperability. The specification supports rectangular gates in n dimensions (i.e., from one-dimensional range gates up to n-dimensional hyper-rectangular regions), polygon gates in two (and more) dimensions, ellipsoid gates in n dimensions, decision tree structures, and Boolean collections of any of the types of gates. Gates can be uniquely identified and may be ordered into a hierarchical structure to describe a gating strategy. Gates may be applied on parameters as in list mode data files (e.g., FCS files) or on transformed parameters as described by any explicit parameter transformation. Therefore, since version 1.5, parameter transformation and compensation description are included as part of the Gating-ML specification.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://flowcyt.sourceforge.net/gating/"
    },
    {
      "id": "B2AI_STANDARD:132",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "GB",
      "description": "GenBank Sequence Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:26",
        "B2AI_TOPIC:33"
      ],
      "purpose_detail": "The GenBank format is a comprehensive text-based annotation format for nucleotide and protein sequences maintained by NCBI as part of the GenBank sequence database, one of the three components of the International Nucleotide Sequence Database Collaboration (INSDC) alongside EMBL and DDBJ. Each GenBank record contains a sequence along with extensive structured metadata organized into standardized fields including locus name and length, definition line, accession and version numbers, keywords, source organism with taxonomic classification, literature references with PubMed links, and feature table annotations. The feature table uses a hierarchical key-value structure to annotate biological features such as genes, coding sequences (CDS) with translation, regulatory elements, binding sites, variations, and experimental evidence, with controlled qualifiers and ontology terms. GenBank format supports protein translations, codon usage information, cross-references to other databases (UniProt, PDB, Gene, etc.), and provenance tracking of sequence submissions and updates. The rich annotation structure enables integration of genomic context, functional annotations, phylogenetic information, and experimental validation data in a single standardized record, making GenBank essential for comparative genomics, gene annotation pipelines, sequence analysis tools, and machine learning applications that leverage both sequence content and biological metadata for training predictive models of gene structure, function, and regulation.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikipedia.org/wiki/GenBank"
    },
    {
      "id": "B2AI_STANDARD:133",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "GenePred",
      "description": "Gene Prediction File Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12"
      ],
      "purpose_detail": "The genePred (Gene Prediction) format is a flexible tab-delimited table format developed by UCSC Genome Browser for representing gene structure predictions and annotations, widely used for storing transcript models, gene predictions from ab initio tools, and curated gene annotations. The basic genePred format contains 10 required fields - gene name, chromosome, strand, transcription start/end positions, coding sequence (CDS) start/end positions, exon count, comma-separated exon start positions, and comma-separated exon end positions. Extended versions (genePredExt) add fields for unique identifiers, CDS start/end status (complete/incomplete), exon frame information for each coding exon, and gene symbols or descriptions. The format efficiently represents complex transcript structures including alternative splicing isoforms, UTRs (untranslated regions), introns, and partial gene models. GenePred files support various gene prediction algorithms (GeneMark, Augustus, SNAP), RefSeq annotations, Ensembl gene builds, and GENCODE comprehensive gene sets. UCSC provides utilities including genePredToBed for BED format conversion, genePredToGtf for GTF conversion, and genePredToPsl for PSL format output. The format's compact structure and explicit exon coordinates make it ideal for genome browsers, annotation pipelines, and comparative genomics analyses requiring efficient storage and rapid retrieval of gene structure information across whole genomes.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://genome.ucsc.edu/FAQ/FAQformat.html#format9",
      "responsible_organization": [
        "B2AI_ORG:119"
      ]
    },
    {
      "id": "B2AI_STANDARD:134",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "GPAD",
      "description": "Gene Product Annotation Data format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:26",
        "B2AI_TOPIC:33"
      ],
      "purpose_detail": "The Gene Ontology project provides annotations describing attributes of biological entities such as genes and gene products. The Gene Ontology has historically provided annotations via Gene Association Format (GAF), including GAF-1 and GAF-2. Ontologies are distributed separately, using an OWL serialization or OBO format. The use of GAF has some drawbacks. Combined representation of gene/gene product data and annotations leads to redundancy/repetition No way to represent gene/gene product metadata for unannotated genes Requirement to maintain backward compatibility makes it harder to introduce enhancements such as use of an ontology for evidence types GAF formats will continue to be supported, but the need for a way to represent genes/gene products separately from annotations, as well as the need to use the evidence ontology has lead to the creation of the GPAD (Gene Product Annotation Data) and GPI (Gene Product Information) formats, defined here. Whilst GPAD and GPI have been defined for use within the Gene Ontology Consortium for GO annotation, this specification is designed to be reusable for analagous ontology-based annotation - for example, gene phenotype annotation.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://geneontology.org/docs/gene-product-association-data-gpad-format/",
      "responsible_organization": [
        "B2AI_ORG:36"
      ]
    },
    {
      "id": "B2AI_STANDARD:135",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "GTF",
      "description": "Gene Transfer Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12"
      ],
      "purpose_detail": "GTF (Gene Transfer Format, GTF2.2) is an extension to, and backward compatible with, GFF2. The first eight GTF fields are the same as GFF. The feature field is the same as GFF, with the exception that it also includes the following optional values, 5UTR, 3UTR, inter, inter_CNS, and intron_CNS.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://genome.ucsc.edu/FAQ/FAQformat.html#format4",
      "responsible_organization": [
        "B2AI_ORG:119"
      ]
    },
    {
      "id": "B2AI_STANDARD:136",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "GMT",
      "description": "GenePattern GeneSet Table Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12"
      ],
      "purpose_detail": "The GMT (Gene Matrix Transposed) format is a tab-delimited text file format developed by the Broad Institute for representing gene sets used in gene set enrichment analysis (GSEA) and related computational biology applications. Each line in a GMT file represents a single gene set with a simple structure containing the gene set name, description (or URL), and a tab-separated list of gene identifiers (typically gene symbols, Entrez IDs, or other standard identifiers). The format's simplicity and human readability make it ideal for storing and sharing curated gene set collections such as pathway databases (KEGG, Reactome), Gene Ontology term associations, disease gene signatures, transcription factor targets, and experimentally derived co-expression modules. GMT files are widely used by enrichment analysis tools (GSEA, Enrichr, WebGestalt), enable integration of custom gene sets into analysis workflows, support reproducible research through standardized gene set definitions, and facilitate machine learning applications that use gene set membership as features for classification, dimension reduction, and biological interpretation of high-throughput genomic data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.genepattern.org/file-formats-guide#GMT"
    },
    {
      "id": "B2AI_STANDARD:137",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "GFF",
      "description": "General Feature Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "The General Feature Format (GFF) is a tab-delimited text file format developed by the Sequence Ontology Consortium for representing genomic features and annotations. GFF3, the current version, provides a flexible structure for describing genes, transcripts, exons, regulatory elements, and other sequence features with their genomic coordinates, strand orientation, phase information, and hierarchical relationships. Each line represents a single feature with nine standardized fields including sequence ID, source, feature type (from Sequence Ontology terms), start/end positions, score, strand, and attributes. The format supports parent-child relationships through ID and Parent attributes, enabling representation of complex gene structures with multiple transcripts and alternative splicing. GFF3 includes directives for metadata and embedded FASTA sequences. The format is widely used by genome browsers (UCSC, Ensembl, JBrowse), annotation pipelines, and analysis tools for visualizing and processing genomic data. Its human-readable structure and extensive tool support make it a standard choice for genome annotation exchange and long-term data archiving.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/The-Sequence-Ontology/Specifications",
      "formal_specification": "https://github.com/The-Sequence-Ontology/Specifications"
    },
    {
      "id": "B2AI_STANDARD:138",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "GAF",
      "description": "Genome Annotation File",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "The Gene Ontology Annotation File (GAF) format is a standardized, tab-delimited text file specification developed by the Gene Ontology Consortium for representing gene product annotations using GO terms. GAF 2.1 defines 17 fields per line, capturing essential information such as database identifiers, gene symbols, GO terms, evidence codes, references, taxonomic information, and annotation extensions. Each line encodes a single association between a gene product and a GO term, supported by evidence and references, enabling precise tracking of functional, process, and cellular component annotations. The format supports both required and optional fields, allowing for rich annotation detail and interoperability across databases. GAF is widely used for large-scale functional genomics, comparative biology, and integrative bioinformatics, providing a foundation for computational analysis, data sharing, and automated reasoning about gene function across species.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://geneontology.org/docs/go-annotation-file-gaf-format-2.1/",
      "responsible_organization": [
        "B2AI_ORG:36"
      ]
    },
    {
      "id": "B2AI_STANDARD:139",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Beacon",
      "description": "Genome Beacons",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "Beacon v2 is a protocol/specification established by the Global Alliance for Genomics and Health initiative (GA4GH) that defines an open standard for federated discovery of genomic (and phenoclinic) data in biomedical research and clinical applications.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://docs.genomebeacons.org/",
      "publication": "doi:10.1002/humu.24369",
      "formal_specification": "https://github.com/ga4gh-beacon/beacon-v2",
      "responsible_organization": [
        "B2AI_ORG:34"
      ]
    },
    {
      "id": "B2AI_STANDARD:140",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "GVF",
      "description": "Genome Variation Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:13",
        "B2AI_TOPIC:35"
      ],
      "purpose_detail": "The Genome Variation Format (GVF) is a very simple file format for describing sequence alteration features at nucleotide resolution relative to a reference genome.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/The-Sequence-Ontology/Specifications/blob/master/gvf.md",
      "formal_specification": "https://github.com/The-Sequence-Ontology/Specifications/blob/master/gvf.md"
    },
    {
      "id": "B2AI_STANDARD:141",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "GCDML",
      "description": "Genomic Contextual Data Markup Language",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "The Genomic Contextual Data Markup Language (GCDML) is an XML Schema developed by the Genomic Standards Consortium (GSC) to implement the Minimum Information about a Genome Sequence (MIGS), Minimum Information about a Metagenome Sequence (MIMS), and Minimum Information about a MARKer gene Sequence (MIMARKS) specifications. This sample-centric, strongly-typed schema provides a comprehensive set of descriptors for documenting the complete provenance of biological samples, from initial collection and environmental context through sequencing and subsequent analysis. GCDML enables standardized reporting of genomic and metagenomic data by defining required metadata fields that capture sampling conditions, laboratory processing methods, sequencing parameters, and analytical workflows. The schema facilitates data exchange between research groups, repositories, and databases, ensuring that essential contextual information needed to interpret sequence data is preserved and communicated effectively.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://doi.org/10.1089/omi.2008.0A10",
      "publication": "doi:10.1089/omi.2008.0A10",
      "responsible_organization": [
        "B2AI_ORG:38"
      ]
    },
    {
      "id": "B2AI_STANDARD:142",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "GGBN",
      "description": "Global Genome Biodiversity Network Data Standard",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "In order to facilitate exchange of information on genomic samples and their derived data, the Global Genome Biodiversity Network (GGBN) Data Standard is intended to provide a platform based on a documented agreement to promote the efficient sharing and usage of genomic sample material and associated specimen information in a consistent way.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.tdwg.org/standards/ggbn/",
      "publication": "doi:10.1093/database/baw125",
      "responsible_organization": [
        "B2AI_ORG:93"
      ]
    },
    {
      "id": "B2AI_STANDARD:143",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "GlycoCT",
      "description": "GlycoCT encoding scheme",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:20"
      ],
      "purpose_detail": "\"GlycoCT (Glyco Connection Table) format is an advanced, comprehensive encoding scheme specifically designed to describe complex carbohydrate sequences with unprecedented precision and consistency in glycobioinformatics. Developed as version 4 (KIROI) by Stephan Herget and Rene Ranzinger in 2007, this format employs a controlled vocabulary based on IUPAC nomenclature rules to systematically name monosaccharides and utilizes a connection table approach rather than linear encoding, enabling accurate representation of branched and complex glycan structures. GlycoCT incorporates a sophisticated block concept to efficiently handle frequently occurring structural features such as repeating units, which are common in biological carbohydrates. The format exists in two complementary variants: a condensed form optimized for database applications with strict sorting rules ensuring uniqueness for use as primary keys, and a more verbose XML syntax that provides enhanced readability and machine processing capabilities. Released under Creative Commons Attribution 4.0 International License, GlycoCT represents a significant advancement toward achieving a unified and broadly accepted sequence format in glycobioinformatics, encompassing the capabilities of the heterogeneous landscape of existing digital encoding schemata while providing the foundation for developing ontological relationships between glycan structural terms and supporting automated glycan structure analysis.\"",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/glycoinfo/GlycoCT",
      "publication": "doi:10.1016/j.carres.2008.03.011",
      "formal_specification": "https://github.com/glycoinfo/GlycoCT"
    },
    {
      "id": "B2AI_STANDARD:144",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "GTrack",
      "description": "GTrack genomic data format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "GTrack is a tabular format that was developed as part of the Genomic HyperBrowser system to provide a uniform representation of most types of genomic datasets. GTrack is able to replace common formats such as WIG, GFF, BED, FASTA, in addition to represent chromatin capture datasets, such as Hi-C and ChIA-PET.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/gtrack/gtrack",
      "publication": "doi:10.1186/1471-2105-12-494",
      "formal_specification": "https://github.com/gtrack/gtrack"
    },
    {
      "id": "B2AI_STANDARD:145",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "GUID-AS",
      "description": "GUID and Life Sciences Identifiers Applicability Statements",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:20"
      ],
      "purpose_detail": "GUIDs are Globally Unique Identifiers which should be referentially consistent and resolvable in order to support tests of uniqueness and the acquisition of associated metadata. Further, permanent and robust resolution services need to be available. The TDWG Globally Unique Identifiers Task Group (TDWG GUID), after meeting twice in 2006, recommended the use of the Life Sciences Identifiers (LSID) to uniquely identify shared data objects in the biodiversity domain.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://www.tdwg.org/standards/150",
      "formal_specification": "https://github.com/tdwg/guid-as",
      "responsible_organization": [
        "B2AI_ORG:93"
      ]
    },
    {
      "id": "B2AI_STANDARD:146",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "GIATE",
      "description": "Guidelines for Information About Therapy Experiments",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "minimuminformationschema"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "purpose_detail": "The Guidelines for Information About Therapy Experiments (GIATE) is a minimum information checklist developed to establish a consistent framework for transparently reporting the purpose, methods, and results of therapeutic experiments, particularly in preclinical and translational research contexts. GIATE provides structured reporting requirements covering experimental design, intervention details (therapeutic agents, dosing, administration routes, timing), subject characteristics, outcome measures, statistical methods, and results presentation. The guidelines aim to improve reproducibility, enable meta-analyses, facilitate comparison across studies, and enhance the quality of preclinical therapeutic research by ensuring complete documentation of experimental parameters that affect interpretation and translatability. GIATE addresses the need for standardized reporting of therapy experiments to support evidence synthesis, reduce publication bias, and improve the rigor and transparency of preclinical studies that inform clinical trial design and therapeutic development pipelines.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://doi.org/10.1186/1756-0500-5-10",
      "publication": "doi:10.1186/1756-0500-5-10"
    },
    {
      "id": "B2AI_STANDARD:147",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "HIPAA",
      "description": "Health Insurance Portability and Accountability Act of 1996",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "policy"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "The Health Insurance Portability and Accountability Act of 1996 (HIPAA) is a federal law that required the creation of national standards to protect sensitive patient health information from being disclosed without the patient's consent or knowledge. The US Department of Health and Human Services (HHS) issued the HIPAA Privacy Rule to implement the requirements of HIPAA. The HIPAA Security Rule protects a subset of information covered by the Privacy Rule.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.hhs.gov/hipaa/index.html",
      "formal_specification": "https://aspe.hhs.gov/reports/health-insurance-portability-accountability-act-1996",
      "responsible_organization": [
        "B2AI_ORG:39"
      ]
    },
    {
      "id": "B2AI_STANDARD:148",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "HCPCS",
      "description": "Healthcare Common Procedure Coding System",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "codesystem",
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "The Healthcare Common Procedure Coding System (HCPCS) is a standardized code set maintained by the Centers for Medicare & Medicaid Services (CMS) for reporting medical procedures, products, supplies, and services to Medicare, Medicaid, and private health insurers for claims processing and reimbursement. HCPCS consists of two levels - Level I comprises CPT (Current Procedural Terminology) codes maintained by the American Medical Association for physician services and procedures, while Level II contains alphanumeric codes (A through V series) for items and services not covered by CPT including durable medical equipment (DME), prosthetics, orthotics, supplies, ambulance services, drugs administered via methods other than oral, and temporary procedures. Each HCPCS code is accompanied by modifiers that provide additional information about the service or item (laterality, level of service, unusual circumstances). The code set is updated quarterly to reflect new technologies, procedures, and supplies, and includes detailed descriptions, Medicare coverage policies, and pricing information. HCPCS enables standardized billing, facilitates claims adjudication, supports healthcare cost analysis and utilization research, and provides the foundation for healthcare payment systems and revenue cycle management across payers.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.cms.gov/Medicare/Coding/HCPCSReleaseCodeSets",
      "responsible_organization": [
        "B2AI_ORG:17"
      ]
    },
    {
      "id": "B2AI_STANDARD:149",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "HISPID3",
      "description": "Herbarium Information Standards and Protocols for Interchange of Data",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:1"
      ],
      "purpose_detail": "The 'Herbarium Information Standards and Protocols for Interchange of Data' (HISPID) is a standard format for the interchange of electronic herbarium specimen information. HISPID has been developed by a committee of representatives from all major Australian herbaria. This interchange standard was first published in 1989, with a revised version published in 1993./nHISPID3 (version 3) is an accession-based interchange standard. Although many fields refer to attributes of the taxon they should be construed as applying to the specimen represented by the record, not to the taxon per se. The interchange of taxonomic, nomenclatural, bibliographic, typification, rare and endangered plant conservation, and other related information is not dealt with in this standard, unless it specifically refers to a particular accession (record). While this standard is still in use, it is no longer actively maintained (labelled as prior on the TDWG website).",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.tdwg.org/standards/hispid3/",
      "responsible_organization": [
        "B2AI_ORG:93"
      ]
    },
    {
      "id": "B2AI_STANDARD:150",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "HML",
      "description": "Histoimmunogenetics Markup Language",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Histoimmunogenetics Markup Language (HML) is intended as a potentially general-purpose XML format for exchanging genetic typing data. This format supports NGS based genotyping methods, raw sequence reads, registered methodologies, reference data, complete reporting of allele and genotype ambiguity and MIRING compliant reporting.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://bioinformatics.bethematchclinical.org/hla-resources/hml/",
      "formal_specification": "https://github.com/nmdp-bioinformatics/hml"
    },
    {
      "id": "B2AI_STANDARD:151",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Arden",
      "description": "HL7 Arden Syntax",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The Arden Syntax for Medical Logic Systems Version 2.10 is the latest version of a formalism for clinical knowledge representation that can be used by clinicians, knowledge engineers, administrators and others to implement clinical decision support (CDS) solutions to help improve the quality and safety of care. Arden Syntax can be used to create a knowledge base for CDS systems that, when coupled with patient data, can generate patient-specific CDS interventions for improving patient care. The key change in Version 2.10 over Version 2.9 is inclusion of a normative XML representation for Arden Syntax. This was done because the use of XML facilitates the development of tools such as syntax checkers and editors that can help increase the correctness of executable knowledge modules, and this in turn will foster the augmentation of the development and production environments for Arden, thereby increasing its utility.",
      "is_open": true,
      "requires_registration": true,
      "url": "http://www.hl7.org/implement/standards/product_brief.cfm?product_id=372",
      "publication": "doi:10.1016/j.jbi.2012.02.001",
      "formal_specification": "https://www.hl7.org/login/index.cfm?next=/implement/standards/product_brief.cfm?product_id=2",
      "responsible_organization": [
        "B2AI_ORG:40"
      ]
    },
    {
      "id": "B2AI_STANDARD:152",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "HL7 CDA Data Provenance",
      "description": "HL7 Clinical Document Architecture Implementation Guide - Data Provenance",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "The HL7 Clinical Document Architecture (CDA) Data Provenance Implementation Guide provides standardized guidance and templates for capturing and exchanging comprehensive provenance information about clinical and care-related information within CDA documents. The guide addresses the fundamental provenance questions - who created the information (author, organization, device), when it was created (timestamps, versioning), where it was created (facility, location), how it was created (method, procedure, source system), and why it was created (purpose, context). The implementation guide defines structured elements and coded values for documenting data transformations, aggregations, derivations, and chains of custody that are critical for clinical decision-making, regulatory compliance, and legal accountability. Provenance metadata supports trust and transparency in health information exchange, enables auditability and traceability of clinical data across systems and care settings, facilitates detection of data quality issues, and provides essential context for interpreting clinical information in research, quality improvement, and AI/ML applications where understanding data lineage and reliability is paramount.",
      "is_open": true,
      "requires_registration": true,
      "url": "http://www.hl7.org/implement/standards/product_brief.cfm?product_id=420",
      "responsible_organization": [
        "B2AI_ORG:40"
      ]
    },
    {
      "id": "B2AI_STANDARD:153",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "DiagnosticReport",
      "description": "HL7 FHIR Resource DiagnosticReport",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:845",
        "B2AI_STANDARD:846",
        "B2AI_STANDARD:847",
        "B2AI_STANDARD:850"
      ],
      "purpose_detail": "Resource for findings and interpretation of diagnostic tests performed on patients, groups of patients, devices, and locations, and/or specimens derived from these. The report includes clinical context such as requesting and provider information, and some mix of atomic results, images, textual and coded interpretations, and formatted representation of diagnostic reports.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://hl7.org/fhir/diagnosticreport.html",
      "formal_specification": "http://hl7.org/fhir/diagnosticreport-definitions.html",
      "responsible_organization": [
        "B2AI_ORG:40"
      ]
    },
    {
      "id": "B2AI_STANDARD:154",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "GenomicStudy",
      "description": "HL7 FHIR Resource GenomicStudy",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:9",
        "B2AI_TOPIC:13"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:845",
        "B2AI_STANDARD:846",
        "B2AI_STANDARD:847",
        "B2AI_STANDARD:850"
      ],
      "purpose_detail": "GenomicStudy resource aims at delineating relevant information of a genomic study. A genomic study might comprise one or more analyses, each serving a specific purpose. These analyses may vary in method (e.g., karyotyping, CNV, or SNV detection), performer, software, devices used, or regions targeted.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://build.fhir.org/genomicstudy.html",
      "formal_specification": "https://build.fhir.org/genomicstudy-definitions.html",
      "responsible_organization": [
        "B2AI_ORG:40"
      ]
    },
    {
      "id": "B2AI_STANDARD:155",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MolecularSequence",
      "description": "HL7 FHIR Resource MolecularSequence",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:4",
        "B2AI_TOPIC:20"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:845",
        "B2AI_STANDARD:846",
        "B2AI_STANDARD:847",
        "B2AI_STANDARD:850"
      ],
      "purpose_detail": "The FHIR MolecularSequence resource provides a standardized structure for representing raw and processed biological sequence data including DNA, RNA, and amino acid sequences within the FHIR ecosystem, enabling integration of genomic information with clinical data. The resource captures observed sequences, reference sequences, sequence variations (SNPs, indels, structural variants), quality scores, read coverage information, and repository links to external sequence databases (GenBank, EMBL, RefSeq). MolecularSequence supports representation of sequence coordinates using zero-based or one-based numbering systems, strand orientation, and relationships between sequences (such as transcript-to-protein mappings). The resource can reference associated specimens, patients, or other subjects, and links to variant calling results, expression data, and functional annotations. MolecularSequence enables clinical genomics workflows including variant interpretation, pharmacogenomics decision support, molecular diagnostics reporting, and precision medicine applications by providing a FHIR-native way to exchange sequence data alongside phenotypic, diagnostic, and treatment information in electronic health records and clinical information systems.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://hl7.org/implement/standards/fhir/molecularsequence.html",
      "formal_specification": "http://hl7.org/implement/standards/fhir/molecularsequence-definitions.html",
      "responsible_organization": [
        "B2AI_ORG:40"
      ]
    },
    {
      "id": "B2AI_STANDARD:156",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Observation",
      "description": "HL7 FHIR Resource Observation",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:845",
        "B2AI_STANDARD:846",
        "B2AI_STANDARD:847",
        "B2AI_STANDARD:850"
      ],
      "purpose_detail": "The FHIR Observation resource is a central, versatile structure for representing measurements, assessments, and assertions made about a patient, device, location, or other subject in healthcare contexts. Observations encompass a wide range of clinical and research data including vital signs (blood pressure, heart rate, temperature), laboratory results (chemistry, hematology, microbiology), imaging measurements, clinical assessments (pain scores, functional status), device readings (glucose monitors, pulse oximeters), social determinants of health, genomic findings, and quality metrics. The resource uses coded values from standard terminologies (LOINC, SNOMED CT) to identify what was measured, supports numeric values with units (UCUM), coded results, textual findings, and multimedia attachments. Observation includes metadata for effective time, status, performer, method, specimen reference, reference ranges, and interpretation flags. The resource supports panels and components for organizing related observations (complete blood count, metabolic panel), enables longitudinal tracking through sequences and trends, and links to supporting evidence and derivations. Observations are fundamental to AI/ML applications requiring structured clinical phenotypes, time-series analytics, predictive modeling, and integration of diverse data types for clinical decision support and population health analysis.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://hl7.org/implement/standards/fhir/observation.html",
      "formal_specification": "http://hl7.org/implement/standards/fhir/observation-definitions.html",
      "responsible_organization": [
        "B2AI_ORG:40"
      ]
    },
    {
      "id": "B2AI_STANDARD:157",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Patient",
      "description": "HL7 FHIR Resource Patient",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:6",
        "B2AI_TOPIC:9"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:845",
        "B2AI_STANDARD:846",
        "B2AI_STANDARD:847",
        "B2AI_STANDARD:850"
      ],
      "purpose_detail": "The FHIR Patient resource provides a standardized structure for representing demographics and administrative information about individuals or animals receiving care or health-related services. The resource captures core demographic data including name (with multiple name types and use contexts), birth date, gender, address (with structured components and period of validity), telecom contact information (phone, email, with use and priority indicators), and multiple identifiers (medical record numbers, social security numbers, insurance IDs) with system and type codes. Patient supports marital status, multiple languages with proficiency levels, photo attachments, contact persons and their relationships, communication preferences, general practitioners, managing organization, and links to related patients (merged records, see-also relationships). The resource includes deceased indicator, multiple birth information, and animal-specific extensions for species and breed. Patient serves as a central hub linking to all other clinical resources (observations, conditions, procedures, medications) and enables patient matching, record linkage, demographic analytics, cohort identification, and population health management. The standardized demographics are essential for AI/ML applications requiring patient stratification, bias detection, social determinants analysis, and fair representation across diverse populations in clinical prediction models and decision support systems.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://hl7.org/implement/standards/fhir/patient.html",
      "formal_specification": "http://hl7.org/implement/standards/fhir/patient-definitions.html",
      "responsible_organization": [
        "B2AI_ORG:40"
      ]
    },
    {
      "id": "B2AI_STANDARD:158",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Specimen",
      "description": "HL7 FHIR Resource Specimen",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:845",
        "B2AI_STANDARD:846",
        "B2AI_STANDARD:847",
        "B2AI_STANDARD:850"
      ],
      "purpose_detail": "The FHIR Specimen resource provides a standardized structure for representing any material sample taken from a biological entity (living or dead) or from a physical object or the environment, essential for laboratory testing, biobanking, and clinical diagnostics. The resource captures comprehensive specimen metadata including type (blood, tissue, urine, etc.), subject/patient association, collection details (method, body site, quantity, date/time), identifiers (accession number, container ID), processing and handling procedures (centrifugation, fixation, storage), parent-child specimen relationships for aliquots and derivatives, storage conditions (temperature, humidity), and current status (available, unavailable, unsatisfactory, entered-in-error). The resource supports complex laboratory workflows by tracking specimen provenance through collection, transport, processing, and analysis stages, with fields for collection procedure, additive substances, container types, and special handling requirements. Specimen integrates with other FHIR resources including Patient, Practitioner, ServiceRequest, DiagnosticReport, and Observation to create complete clinical laboratory information workflows. The resource enables biobank specimen catalogs, clinical trial sample tracking, public health surveillance specimen management, and research biorepository operations. FHIR Specimen supports interoperability between laboratory information systems (LIS), electronic health records (EHR), biobank management systems, and research databases, facilitating standardized specimen data exchange across healthcare and research ecosystems while maintaining compliance with regulations for specimen handling, consent, and data privacy.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://hl7.org/fhir/specimen.html",
      "formal_specification": "http://hl7.org/fhir/specimen-definitions.html",
      "responsible_organization": [
        "B2AI_ORG:40"
      ]
    },
    {
      "id": "B2AI_STANDARD:159",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "GHP",
      "description": "HL7 Gender Harmony Project",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:6"
      ],
      "purpose_detail": "Currently, it is common that a single data element is used to capture both sex and gender information, often assuming these two items are one unified idea. This specification challenges that notion and proposes that independent consideration of sex and gender, and the assessment of their differences promotes the health of women, men, and people of diverse gender identities of all ages, avoiding systematic errors that generate results with a low validity (if any) in clinical studies. The Gender Harmony model describes an approach that can improve data accuracy for sex and gender information in health care systems.",
      "is_open": true,
      "requires_registration": true,
      "url": "http://www.hl7.org/implement/standards/product_brief.cfm?product_id=564",
      "publication": "doi:10.1093/jamia/ocab196",
      "responsible_organization": [
        "B2AI_ORG:40"
      ]
    },
    {
      "id": "B2AI_STANDARD:160",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "TraML",
      "description": "HUPO-PSI TraML format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:28"
      ],
      "purpose_detail": "The HUPO PSI Mass Spectrometry Standards Working Group (MSS WG) has developed a specification for a standardized format for the exchange and transmission of transition lists for selected reaction monitoring (SRM) experiments.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.psidev.info/traml",
      "responsible_organization": [
        "B2AI_ORG:41"
      ]
    },
    {
      "id": "B2AI_STANDARD:161",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "IEEE P360",
      "description": "IEEE 360-2022 IEEE Standard for Wearable Consumer Electronic Devices",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:18"
      ],
      "purpose_detail": "An overview, terminology, and categorization for Wearable Consumer Electronic Devices (Wearables). It further outlines an architecture for a series of standard specifications that define technical requirements and testing methods for different aspects of Wearables, from basic security and suitableness of wearing to various functional areas like health, fitness, and infotainment, etc.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://standards.ieee.org/ieee/360/6244/",
      "responsible_organization": [
        "B2AI_ORG:44"
      ]
    },
    {
      "id": "B2AI_STANDARD:162",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "BPPC",
      "description": "IHE Basic Patient Privacy Consents",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "A mechanism to record the patient privacy consent(s) and a method for Content Consumers to use to enforce the privacy consent appropriate to the use.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://profiles.ihe.net/ITI/TF/Volume1/ch-19.html",
      "formal_specification": "https://profiles.ihe.net/ITI/TF/Volume1/ch-19.html",
      "responsible_organization": [
        "B2AI_ORG:46"
      ]
    },
    {
      "id": "B2AI_STANDARD:163",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "IHE CDS-OAT",
      "description": "IHE Clinical Decision Support Order Appropriateness Tracking",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Profile for Clinical Decision Support and Appropriate Use Criteria (AUC) information as received from the CDS Mechanism 145 (CDSM) on the order and charge transaction to the revenue cycle application that is responsible to create a claim.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.ihe.net/uploadedFiles/Documents/Radiology/IHE_Rad_Suppl_CDS-OAT.pdf",
      "responsible_organization": [
        "B2AI_ORG:46"
      ]
    },
    {
      "id": "B2AI_STANDARD:164",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "CRD",
      "description": "IHE Clinical Research Document standard",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The Clinical Research Document Profile (CRD) specifies a standard way to generate a clinical research document from EHR data provided in the CDA standard.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.ihe.net/uploadedFiles/Documents/QRPH/IHE_QRPH_Suppl_CRD.pdf",
      "formal_specification": "https://www.ihe.net/uploadedFiles/Documents/QRPH/IHE_QRPH_Suppl_CRD.pdf",
      "responsible_organization": [
        "B2AI_ORG:46"
      ]
    },
    {
      "id": "B2AI_STANDARD:165",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "IHE XCPD",
      "description": "IHE Cross-Community Patient Discovery Profile",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Standardized means to locate communities that hold patient relevant health data and the translation of patient identifiers across communities holding the same patients data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://profiles.ihe.net/ITI/TF/Volume1/ch-27.html",
      "responsible_organization": [
        "B2AI_ORG:46"
      ]
    },
    {
      "id": "B2AI_STANDARD:166",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "IHE ITI",
      "description": "IHE IT Infrastructure Technical Framework",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Technical profiles and definitions for IT use cases, transactions, content, and metadata.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://profiles.ihe.net/ITI/index.html",
      "formal_specification": "https://profiles.ihe.net/ITI/TF/index.html",
      "responsible_organization": [
        "B2AI_ORG:46"
      ]
    },
    {
      "id": "B2AI_STANDARD:167",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "LAW",
      "description": "IHE Laboratory Analytical Workflow",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Profile to support the analytical workflow between analyzers of the clinical laboratory and the IT systems managing their work",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.ihe.net/resources/technical_frameworks/#PaLM",
      "formal_specification": "https://www.ihe.net/uploadedFiles/Documents/PaLM/IHE_PaLM_TF_Vol1.pdf",
      "responsible_organization": [
        "B2AI_ORG:46"
      ]
    },
    {
      "id": "B2AI_STANDARD:168",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "IHE PCD",
      "description": "IHE Patient Care Device Profiles",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "IHE Patient Care Device (PCD) Profiles are standards-based interoperability specifications developed by Integrating the Healthcare Enterprise (IHE) to address medical device communication challenges in clinical settings. The profiles follow IHE's technical framework development process including proposal, supplement development, public comment, trial implementation at Connectathon events, and finalization. Key PCD profiles include ACM (Alert Communication Management) for standardized alarm handling, DEC (Device Enterprise Communication) for device-to-enterprise integration, IDCO (Implantable Device Cardiac Observations) for cardiac device data, IPEC (Infusion Pump Event Communication) for infusion pump safety, PIV (Point of Care Infusion Verification) for medication verification, PCIM (Point of Care Identity Management) for device authentication, RDQ (Retrospective Data Query) for historical data retrieval, RTM (Rosetta Terminology Mapping) for standardized device terminology, and WCM (Waveform Communication Module) for physiological waveform data. These profiles leverage existing standards like HL7, DICOM, and IEEE 11073 to enable vendor-neutral device interoperability, supporting critical use cases such as automated vital signs documentation in electronic health records, integrated alarm management across monitoring systems, and safe medication administration workflows.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://wiki.ihe.net/index.php?title=PCD_Profiles",
      "responsible_organization": [
        "B2AI_ORG:46"
      ]
    },
    {
      "id": "B2AI_STANDARD:169",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "IHE PDQ",
      "description": "IHE Patient Demographics Query Integration Profile",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Standardized ways for multiple distributed applications to query a patient information server for a list of patients, based on user-defined search criteria, and retrieve a patients demographic (and, optionally, visit or visit-related) information directly into the application.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://profiles.ihe.net/ITI/TF/Volume1/ch-8.html",
      "responsible_organization": [
        "B2AI_ORG:46"
      ]
    },
    {
      "id": "B2AI_STANDARD:170",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "RFD",
      "description": "IHE Retrieve Form for Data Capture",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "purpose_detail": "The RFD Profile provides a generic polling mechanism to allow an external agency to indicate issues with data that have been captured and enable the healthcare provider to correct the data. The profile does not dictate the mechanism employed or content required to achieve such corrections.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://profiles.ihe.net/ITI/TF/Volume1/ch-17.html",
      "responsible_organization": [
        "B2AI_ORG:46"
      ]
    },
    {
      "id": "B2AI_STANDARD:171",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "LIVD",
      "description": "IICC Digital Format for Publication of LOINC to Vendor IVD Test Results",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:53"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:848"
      ],
      "purpose_detail": "Industry-defined format to facilitate the publication and exchange of LOINC codes for vendor IVD test results.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://ivdconnectivity.org/livd/"
    },
    {
      "id": "B2AI_STANDARD:172",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "ICS",
      "description": "Image Cytometry Standard",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:19"
      ],
      "purpose_detail": "The Image Cytometry Standard (ICS) is a digital multidimensional image file format used in life sciences microscopy. It stores not only the image data, but also the microscopic parameters describing the optics during the acquisition.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikipedia.org/wiki/Image_Cytometry_Standard",
      "publication": "doi:10.1002/cyto.990110502"
    },
    {
      "id": "B2AI_STANDARD:173",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "imzML",
      "description": "imzML format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15",
        "B2AI_TOPIC:28"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:20"
      ],
      "purpose_detail": "The purpose of imzML is to facilitate the exchange and processing of mass spectrometry imaging data. This website is intended to provide all information neccesary to implement imzML.imzML was developed in the framework of the EU funded project COMPUTIS. The main goals during the development were complete description of MS imaging experiments and efficient storage of (very large) data sets. imzML is it not limited to MS imaging, but is also useful for other MS applications generating large data sets such as LC-FTMS. The current version is mzML 1.1.0. The metadata part of imzML is based on the mzML format by HUPO-PSI",
      "is_open": true,
      "requires_registration": false,
      "url": "https://ms-imaging.org/imzml/",
      "publication": "doi:10.1016/j.jprot.2012.07.026"
    },
    {
      "id": "B2AI_STANDARD:174",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "ICD-10-CM",
      "description": "International Classification of Diseases 10th Revision Clinical Modification",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "codesystem",
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "ICD-10-CM (International Classification of Diseases, 10th Revision, Clinical Modification) is the United States' clinical modification of the WHO's ICD-10 classification system, mandated for reporting diagnoses and health conditions in all healthcare settings for billing, epidemiology, quality measurement, and public health surveillance since October 2015. Maintained by the National Center for Health Statistics (NCHS) in collaboration with CMS, ICD-10-CM provides over 70,000 diagnosis codes with significantly expanded granularity compared to ICD-9-CM, enabling more precise documentation of disease severity, anatomical location, episode of care (initial encounter, subsequent encounter, sequela), and laterality (left, right, bilateral). The coding structure uses alphanumeric format with 3-7 characters where the first character is alphabetic, second is numeric, third through seventh provide increasing specificity of body site, etiology, severity, and clinical details, with optional seventh characters as extensions for encounter context or injury staging. Major chapters include infectious diseases (A00-B99), neoplasms (C00-D49), endocrine and metabolic disorders (E00-E89), mental and behavioral disorders (F01-F99), nervous system (G00-G99), circulatory (I00-I99), respiratory (J00-J99), digestive (K00-K95), musculoskeletal (M00-M99), and external causes of morbidity (V00-Y99). Critical features include combination codes that capture multiple conditions or manifestations in single codes, placeholder 'x' characters to allow for future expansion, and seventh character extensions for obstetric outcomes, fracture healing status, and diabetes complications. ICD-10-CM drives healthcare reimbursement through diagnosis-related groups (DRGs), enables clinical decision support systems through structured diagnosis data, supports population health analytics identifying disease trends and risk factors, powers epidemiological surveillance for CDC reportable conditions tracking disease outbreaks, and provides standardized terminology for electronic health record (EHR) systems ensuring interoperability. Annual updates released October 1st incorporate new diseases, refined definitions, and code revisions reflecting medical advances, essential for medical billing specialists, clinical coders, health informaticians, public health researchers, and healthcare quality officers.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.cdc.gov/nchs/icd/icd-10-cm.htm",
      "responsible_organization": [
        "B2AI_ORG:14"
      ]
    },
    {
      "id": "B2AI_STANDARD:175",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "ICD-11",
      "description": "International Classification of Diseases 11th Revision",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "codesystem",
        "standards_process_maturity_final"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "ICD-11 (International Classification of Diseases, 11th Revision) is the World Health Organization's latest global standard for diagnostic health information, officially adopted by the World Health Assembly in 2019 and effective from January 2022, representing the most comprehensive revision in over three decades with fully digital-native architecture designed for electronic health systems and multilingual implementation. Developed through extensive international collaboration involving thousands of health professionals across 90 countries, ICD-11 contains over 17,000 diagnostic categories with significantly enhanced granularity, clinical detail, and scientific accuracy compared to ICD-10, incorporating advances in medical knowledge including genomic medicine, patient safety, traditional medicine integration, and sexual health classifications. The classification uses a multi-hierarchical structure with foundation component enabling flexible content management, allowing multiple parent categories and post-coordination where complex conditions are composed by combining codes for anatomy, etiology, severity, temporality, and other clinical dimensions. Major structural improvements include entirely new chapters for traditional medicine (validated practices from Chinese, Ayurvedic, and other systems), extension codes for functional assessment using WHO Disability Assessment Schedule 2.0 (WHODAS), detailed antimicrobial resistance documentation, gaming disorder and other emerging conditions, and significantly expanded mental health classifications with dimensional assessments. ICD-11 features built-in multilingual support covering Arabic, Chinese, English, French, Russian, Spanish with machine translation capabilities, linearizations tailored for different use cases (mortality reporting, morbidity statistics, primary care, clinical documentation), and modern URI-based coding enabling semantic web integration and FHIR compatibility. Critical technical features include embedded logical definitions using description logic enabling automated classification and consistency checking, standardized application programming interfaces (APIs) for EHR integration, and continuous online updating mechanism replacing decennial revision cycles with regular incremental updates. ICD-11 supports global health surveillance enabling real-time disease outbreak tracking, international health statistics comparability across countries and regions, research data standardization for epidemiological studies and clinical trials, health service planning through accurate disease burden measurement, and AI/machine learning applications through structured, semantically rich diagnostic data. Adoption varies globally with some countries implementing immediately while others transition gradually, essential for medical informaticians, public health authorities, clinical coders, WHO collaborating centers, and healthcare system planners.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://icd.who.int/en",
      "responsible_organization": [
        "B2AI_ORG:100"
      ]
    },
    {
      "id": "B2AI_STANDARD:176",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "ICD-9-CM",
      "description": "International Classification of Diseases 9th Revision Clinical Modification",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "codesystem"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "ICD-9-CM (International Classification of Diseases, 9th Revision, Clinical Modification) was the official medical classification system used in the United States from 1979 until September 2015 for coding diagnoses and procedures in healthcare billing, epidemiological research, and health statistics, representing the U.S. clinical modification of WHO's ICD-9 with additional detail for morbidity classification. Developed by the National Center for Health Statistics (NCHS) and used by CMS for Medicare/Medicaid reimbursement, ICD-9-CM contained approximately 13,000 diagnosis codes and 3,000 procedure codes, structured with 3-5 digit numeric format where first three digits represent disease category and subsequent digits add specificity for anatomical location, etiology, or manifestations. Major diagnostic chapters included infectious and parasitic diseases (001-139), neoplasms (140-239), endocrine and metabolic diseases (240-279), mental disorders (290-319), nervous system (320-389), circulatory system (390-459), respiratory system (460-519), digestive system (520-579), genitourinary system (580-629), and external causes (E codes E800-E999) documenting injury circumstances. The procedure classification (Volume 3) used 2-4 digit numeric codes organized by anatomical system, covering surgical operations, diagnostic procedures, and therapeutic interventions primarily for inpatient hospital settings. Despite decades of widespread adoption creating massive legacy datasets and establishing institutional coding workflows, ICD-9-CM became outdated due to limited specificity insufficient for modern medicine's diagnostic precision, exhausted code capacity with no room for new diseases or procedures, lack of laterality designation, and outdated medical terminology inconsistent with current clinical practice. The mandated transition to ICD-10-CM/PCS in October 2015 required extensive healthcare industry preparation including EHR system updates, coder retraining, and billing system modifications, yet ICD-9-CM remains critically important for historical medical records analysis, longitudinal epidemiological studies spanning pre-2015 data, retrospective cohort studies, trend analysis requiring crosswalk mappings between ICD-9 and ICD-10, and legacy system maintenance. Applications include historical disease surveillance tracking public health trends, health services research analyzing treatment patterns, medical informatics developing code translation algorithms, and archival medical data management, essential for health informaticians managing legacy data, epidemiologists conducting temporal studies, medical archivists, and researchers working with pre-2015 healthcare datasets.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.cdc.gov/nchs/icd/icd9cm.htm",
      "responsible_organization": [
        "B2AI_ORG:14"
      ]
    },
    {
      "id": "B2AI_STANDARD:177",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "INTRPRT",
      "description": "INTRPRT guidelines for transparent machine learning for medical image analysis",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "purpose_detail": "A design directive for transparent ML systems in medical image analysis. The INTRPRT guideline suggests human-centered design principles, recommending formative user research as the first step to understand user needs and domain requirements.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://doi.org/10.1038/s41746-022-00699-2",
      "publication": "doi:10.1038/s41746-022-00699-2"
    },
    {
      "id": "B2AI_STANDARD:178",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "IPSM-AF",
      "description": "IPSM Alignment Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:21"
      ],
      "purpose_detail": "Alignment can be interpreted as a set of uni-directional mappings for transforming input RDF graph into output RDF graph. It is persisted in IPSM Alignment Format (IPSM-AF) that is based on a well known Alignment API Format.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://inter-iot.readthedocs.io/projects/ipsm/en/latest/Configuration/Alignment-format/IPSM-alignment-format/",
      "formal_specification": "https://github.com/INTER-IoT/ipsm-alignments"
    },
    {
      "id": "B2AI_STANDARD:179",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "ISA-TAB-Nano",
      "description": "ISA-Tab-Nano format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:47"
      ],
      "purpose_detail": "ISA-TAB-Nano specifies the format for representing and sharing information about nanomaterials, small molecules and biological specimens along with their assay characterization data (including metadata, and summary data) using spreadsheet or TAB-delimited files.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://wiki.nci.nih.gov/display/icr/isa-tab-nano",
      "publication": "doi:10.1186/1472-6750-13-2"
    },
    {
      "id": "B2AI_STANDARD:180",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "EN13606",
      "description": "ISO 13606 standard for Electronic Health Record Communication",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:49"
      ],
      "purpose_detail": "A means for communicating part or all of the electronic health record (EHR) of one or more identified subjects of care between EHR systems, or between EHR systems and a centralised EHR data repository. It can also be used for EHR communication between an EHR system or repository and clinical applications or middleware components (such as decision support components), or personal health applications and devices, that need to access or provide EHR data, or as the representation of EHR data within a distributed (federated) record system.",
      "is_open": false,
      "requires_registration": true,
      "url": "http://www.en13606.org/",
      "formal_specification": "https://www.iso.org/standard/67868.html"
    },
    {
      "id": "B2AI_STANDARD:181",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "ITU-T E-health",
      "description": "ITU H.810, H.811, H.812, H.812.5, and H.813",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Standards for medical device communication.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.itu.int/en/ITU-T/studygroups/2013-2016/16/Pages/rm/ehealth.aspx",
      "responsible_organization": [
        "B2AI_ORG:50"
      ]
    },
    {
      "id": "B2AI_STANDARD:182",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "JCAMP-DX",
      "description": "Joint Committee on Atomic and Molecular Physical Data standard",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:3",
        "B2AI_TOPIC:28"
      ],
      "purpose_detail": "The JCAMP-DX was one of the earliest specifications providing a standard file format for data exchange in mass spectrometry. It was initially developed for infrared spectrometry and related chemical and physical information between spectrometer data systems of different manufacture. It was also used later for nuclear magnetic resonance spectroscopy. JCAMP-DX is an ASCII based format and therefore not very compact even though it includes standards for file compression. All data are stored as labeled fields of variable length using printable ASCII characters. JCAMP-DX was officially released in 1988. JCAMP-DX was found impractical for today's large MS data sets, but it is still used for exchanging moderate numbers of spectra. IUPAC is currently in charge of its maintenance and the latest protocol is from 2005.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://iupac.org/what-we-do/digital-standards/jcamp-dx/",
      "responsible_organization": [
        "B2AI_ORG:51"
      ]
    },
    {
      "id": "B2AI_STANDARD:183",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Countgraph",
      "description": "K-mer countgraph",
      "related_to": [
        "B2AI_STANDARD:782"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A format used by the khmer tool to represent k-mer counts and their occurences.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://khmer.readthedocs.io/en/v2.0/dev/binary-file-formats.html#countgraph",
      "publication": "doi:10.12688/f1000research.6924.1"
    },
    {
      "id": "B2AI_STANDARD:184",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "KPS",
      "description": "Karnofsky Performance Scale",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "diagnosticinstrument"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "A standard way of measuring the ability of cancer patients to perform ordinary tasks. The Karnofsky Performance Status scores range from 0 to 100. A higher score means the patient is better able to carry out daily activities. Karnofsky Performance Status may be used to determine a patient's prognosis, to measure changes in a patients ability to function, or to decide if a patient could be included in a clinical trial. Also called KPS.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://www.npcrc.org/files/news/karnofsky_performance_scale.pdf",
      "publication": "doi:10.1200/JCO.1984.2.3.187"
    },
    {
      "id": "B2AI_STANDARD:185",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "KGML",
      "description": "KEGG Markup Language",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:21"
      ],
      "purpose_detail": "The KEGG Markup Language (KGML) is an exchange format of the KEGG pathway maps, which is converted from internally used KGML+ (KGML+SVG) format.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.kegg.jp/kegg/xml/",
      "responsible_organization": [
        "B2AI_ORG:52"
      ]
    },
    {
      "id": "B2AI_STANDARD:186",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "LS-DAM",
      "description": "Life sciences domain analysis model",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "deprecated"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:20"
      ],
      "purpose_detail": "The LS DAM v2.2.1 is comprised of 130 classes and covers several core areas including Experiment, Molecular Biology, Molecular Databases and Specimen. Nearly half of these classes originate from the BRIDG model, emphasizing the semantic harmonization between these models. Validation of the LS DAM against independently derived information models, research scenarios and reference databases supports its general applicability to represent life sciences research.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://doi.org/10.1136/amiajnl-2011-000763",
      "publication": "doi:10.1136/amiajnl-2011-000763"
    },
    {
      "id": "B2AI_STANDARD:187",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "LOINC",
      "description": "Logical Observation Identifier Names and Codes",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:37",
          "category": "B2AI:Application",
          "name": "Automated ML-Based Mapping of Free-Text Laboratory Codes to LOINC",
          "description": "Machine learning classifiers including Random Forest, logistic regression, and ensemble methods automatically map free-text laboratory descriptors (test names, units, panels) from local EHR systems to standardized LOINC codes. Novel encoding methods vectorize heterogeneous lab terminology, and trained models achieve high accuracy (Random Forest approximately 94.5% top-1 accuracy; ensemble approaches up to 99% accuracy) on nationwide oncology EHR data from approximately 280 US clinics. This automation dramatically reduces manual terminologist effort (which typically requires 6-8 hours per 1000 terms) and enables research-ready datasets by harmonizing laboratory data across institutions for downstream ML applications.",
          "used_in_bridge2ai": false,
          "references": [
            "https://pmc.ncbi.nlm.nih.gov/articles/PMC8861721/"
          ]
        },
        {
          "id": "B2AI_APP:136",
          "category": "B2AI:Application",
          "name": "LLM-Based LOINC Standardization Using Pre-Trained T5 Embeddings",
          "description": "Pre-trained T5 large language models with contrastive learning and few-shot fine-tuning retrieve and suggest top-k LOINC code candidates from local laboratory descriptions, supporting human-in-the-loop curation workflows. The approach uses contextual embeddings and data augmentation to handle acronyms, synonyms, misspellings, and missing metadata across LOINC's six dimensions (component, property, time, system, scale, method), generalizing to unseen LOINC targets without retraining. Training and evaluation use source-target pairs from MIMIC-III EHR data, demonstrating improved top-k retrieval performance for scalable cross-site laboratory harmonization given LOINC's large code space (over 80,000 codes).",
          "used_in_bridge2ai": false,
          "references": [
            "https://proceedings.mlr.press/v193/tu22a/tu22a.pdf"
          ]
        },
        {
          "id": "B2AI_APP:137",
          "category": "B2AI:Application",
          "name": "Value-Based Laboratory Code Mapping to LOINC Using Result Distributions",
          "description": "Statistical feature engineering on laboratory test result distributions (mean, median, quartiles, variance, skewness) combined with unit normalization enables AI mapping of in-house codes to LOINC via intermediate standards (JLAC10) without relying on test-name NLP. Applied to the J-DREAMS diabetes database (955,011 entries across 15 facilities, 51 analytes), classifiers achieved at least 70% mapping accuracy for 80.4% of analytes. This value-centric approach is particularly useful where NLP resources or medical corpora are limited, demonstrating that unit and value harmonization tied to LOINC groupings is critical for accurate automated mapping and international data sharing.",
          "used_in_bridge2ai": false,
          "references": [
            "https://pmc.ncbi.nlm.nih.gov/articles/PMC12150744/"
          ]
        },
        {
          "id": "B2AI_APP:138",
          "category": "B2AI:Application",
          "name": "LOINC-Coded Laboratory Features for Improved Multi-Site Predictive Modeling",
          "description": "Using LOINC-standardized laboratory features in predictive models significantly improves performance and cross-site transferability compared to unmapped local codes. In a 13-hospital UPMC heart failure cohort (2008-2012) predicting 30-day readmission, models trained with manually LOINC-mapped lab features consistently achieved significantly higher AUCs despite modest overall performance. LOINC aggregation and grouping procedures materially affect model reproducibility and external validation across institutions, demonstrating that explicit LOINC standardization should be reported as a critical data preprocessing step in multi-site ML studies to ensure interpretability and portability.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1093/jamiaopen/ooy063"
          ]
        },
        {
          "id": "B2AI_APP:139",
          "category": "B2AI:Application",
          "name": "LOINC-to-HPO Semantic Integration for Deep Phenotyping and Biomarker Discovery",
          "description": "A widely adopted pipeline maps LOINC-coded laboratory test results transmitted via FHIR to Human Phenotype Ontology (HPO) terms, enabling computational deep phenotyping and biomarker discovery. The system manually biocurated mappings for 2,923 commonly used LOINC tests, handling numeric, ordinal, and nominal results using standardized FHIR interpretation codes, and leveraging HPO's hierarchical structure to aggregate heterogeneous tests with comparable interpretations. Validated on 15,681 UNC EHR patients with respiratory complaints and identifying known asthma biomarkers, the approach supports association studies, cohort analysis, and was released as a SMART on FHIR application for EHR integration and downstream ML applications requiring ontology-based phenotype embeddings.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1038/s41746-019-0110-4"
          ]
        },
        {
          "id": "B2AI_APP:140",
          "category": "B2AI:Application",
          "name": "N3C Unit and Value Harmonization Using LOINC Concept Grouping for ML-Ready Datasets",
          "description": "The National COVID Cohort Collaborative (N3C) unit harmonization pipeline groups laboratory measurements by LOINC concepts, selects canonical units with UCUM conventions and conversion formulas, and infers missing units using Kolmogorov-Smirnov distributional comparisons across pooled multi-site data. Applied to billions of OMOP-mapped EHR lab records, the pipeline harmonized 88.1% of values and imputed units for 78.2% of records missing units (41% of contributors' records), reclaiming large data fractions otherwise unavailable for analysis. This LOINC-based harmonization directly enables creation of ML-ready datasets for predictive modeling, phenotyping, and analytics across heterogeneous EHR sources.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1093/jamia/ocac054"
          ]
        },
        {
          "id": "B2AI_APP:141",
          "category": "B2AI:Application",
          "name": "High-Throughput LOINC Document Ontology Mapping Using Metadata-Based Methods",
          "description": "A scalable pipeline maps clinical document metadata to LOINC Document Ontology (LOINC DO) codes at large scale using bag-of-words and vector distance methods applied to structured EHR fields (event titles, tags, encounter types) rather than full-text NLP. Applied to University of Missouri Cerner database (over 130 million decompressed notes), the metadata-driven approach achieved 73.4% document coverage and mapped 132 million notes in under 2 hours, claimed to be an order of magnitude more efficient than NLP-based methods. This LOINC DO standardization supports downstream computable phenotyping, documentation quality assessment, and potential ML applications requiring standardized clinical document classification.",
          "used_in_bridge2ai": false,
          "references": [
            "https://pmc.ncbi.nlm.nih.gov/articles/PMC10785913/pdf/1116.pdf"
          ]
        }
      ],
      "collection": [
        "codesystem",
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:114",
        "B2AI_ORG:115"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:848"
      ],
      "purpose_detail": "Tests, observations, diagnostics, and other clinical procedures.",
      "is_open": true,
      "requires_registration": true,
      "url": "loinc.org",
      "responsible_organization": [
        "B2AI_ORG:53"
      ]
    },
    {
      "id": "B2AI_STANDARD:188",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "mmCIF",
      "description": "Macromolecular Crystallographic Information File",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:27"
      ],
      "purpose_detail": "PDBx/mmCIF became the standard PDB archive format in 2014. All PDB data processing and annotation will be performed using PDBx/mmCIF at all wwPDB sites. PDBx/mmCIF consists of categories of information represented as tables and keyword value pairs.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://mmcif.wwpdb.org/",
      "responsible_organization": [
        "B2AI_ORG:82"
      ]
    },
    {
      "id": "B2AI_STANDARD:189",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MSAML",
      "description": "Markup Components for Describing Multiple Sequence Alignments",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:23"
      ],
      "purpose_detail": "MSAML was formulated to make manipulation and extraction of multiple sequence alignment information easier by logically defining the parts of an alignment for use in an XML conformant application.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://xml.coverpages.org/msaml-desc-dec.html"
    },
    {
      "id": "B2AI_STANDARD:190",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MDL",
      "description": "MDL molfile Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:3",
        "B2AI_TOPIC:27"
      ],
      "purpose_detail": "MDL Molfile format (also known as Structure-Data File or SDF when containing multiple structures) is a widely-adopted, human-readable text-based chemical file format developed by MDL Information Systems (now BIOVIA) for storing and exchanging complete molecular structure information including atom coordinates (2D or 3D), bond connectivity, bond types (single, double, triple, aromatic, dative), stereochemistry (tetrahedral chirality at asymmetric carbons, double bond geometry E/Z, atropisomerism), formal charges, isotopic labels, radical states, and molecular properties, serving as the de facto standard for chemical databases, molecular modeling software, virtual screening platforms, and structure-activity relationship (SAR) studies where precise three-dimensional molecular representation enables computational prediction of biological activity, physicochemical properties, and drug-likeness. The Molfile format consists of several distinct blocks including a header (three lines containing molecule name, user/program/timestamp information, and optional comments), a counts line (specifying number of atoms, bonds, chiral flag, and format versionV2000 for legacy compatibility or V3000 for enhanced features like large molecules, complex stereochemistry, polymers), an atom block (one line per atom with XYZ Cartesian coordinates in Angstroms, element symbol, mass difference for isotopes like deuterium, formal charge encoded as integer values 0-7 mapping to charges +3 to -3, stereochemistry parity indicating tetrahedral configuration, hydrogen count, stereo care box, valence, and atom-atom mapping number for reaction tracking), a bond block (one line per bond specifying atom indices it connects, bond type 1-8 encoding single/double/triple/aromatic/single-or-double/single-or-aromatic/double-or-aromatic/any, stereochemistry 0-6 for up/down wedge notation indicating substituents above/below plane, and topology type for substructure queries), and a properties block (key-value pairs with extended annotations like \"M  CHG\" for charge lists, \"M  ISO\" for isotope specifications, \"M  RAD\" for radical indicators, \"M  RGP\" for R-group labels in combinatorial libraries, \"M  STY\" for Sgroup polymer/superatom notations). SDF extends Molfile by concatenating multiple structures with \"$$$$\" delimiters and appending data fields in tagged format (\"> <PROPERTY_NAME>\" followed by values) enabling storage of experimental measurements (IC50 values, solubility, logP), computed descriptors (molecular weight, hydrogen bond donors/acceptors, topological polar surface area, number of rotatable bonds), classification labels (active/inactive in bioassays, toxic/non-toxic), database identifiers (PubChem CID, ChEMBL ID, CAS Registry Number), and literature references, making SDF the primary format for distributing large compound collections from PubChem (100+ million compounds), ChEMBL (2+ million bioactive molecules), ZINC (750+ million purchasable compounds), and vendor catalogs. Cheminformatics software including RDKit, Open Babel, CDK, MOE, Schrdinger Suite, and OpenEye toolkits parse Molfiles to generate molecular graphs (nodes=atoms, edges=bonds) for computational analysis, compute molecular fingerprints (bit vectors encoding substructure presence for similarity searching like ECFP/Morgan fingerprints, MACCS keys, atom pairs), perform 3D conformer generation (sampling low-energy geometries via distance geometry or molecular dynamics), calculate physicochemical descriptors (Lipinski's rule of five parameters, drug-likeness scores, synthetic accessibility), and enable substructure searching (identifying molecules containing query fragments defined via SMARTS patterns). For AI/ML applications, SDF-formatted chemical libraries enable supervised learning where molecular structures serve as inputs to predictive models trained on activity labels quantitative structure-activity relationship (QSAR) models using random forests, gradient boosting, support vector machines on molecular descriptors predict binding affinities, toxicity, ADME properties (absorption, distribution, metabolism, excretion); deep learning architectures including graph convolutional networks (GCNs) operating directly on molecular graphs, message passing neural networks (MPNNs) aggregating information across bonds and atoms, transformers with self-attention over atom sequences, and graph attention networks (GATs) learning to focus on pharmacophoric features relevant to biological targets. Generative models including variational autoencoders (VAEs) learning continuous latent representations of chemical space for interpolation between known drugs, generative adversarial networks (GANs) producing novel molecules optimizing multiple objectives (potency, selectivity, safety), and reinforcement learning agents exploring synthesis pathways to optimize retrosynthetic accessibility rely on SDF datasets providing millions of training examples with associated property measurements. Multi-task learning predicts multiple endpoints simultaneously (hERG liability, CYP inhibition, blood-brain barrier penetration, solubility) from shared molecular representations, transfer learning leverages pre-training on large unlabeled molecule collections (self-supervised learning predicting masked atoms or bond types) before fine-tuning on smaller labeled datasets for specialized targets, and active learning guides experimental campaigns selecting compounds for synthesis and testing that maximize information gain about SAR, supporting drug discovery hit identification (virtual screening of billion-compound libraries docking and ML scoring), lead optimization (navigating chemical modifications improving potency while maintaining drug-like properties), toxicity prediction (identifying structural alerts and off-target liabilities), property optimization (designing molecules with specific solubility, permeability, stability profiles), de novo drug design (generating novel scaffolds for undruggable targets), and materials discovery where similar ML approaches applied to polymers, catalysts, battery materials, and organic semiconductors accelerate development cycles by predicting structure-property relationships from training data systematically encoded in SDF format enabling programmatic access to decades of medicinal chemistry knowledge.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikipedia.org/wiki/Chemical_table_file"
    },
    {
      "id": "B2AI_STANDARD:191",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "RXN",
      "description": "MDL reaction Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:3"
      ],
      "purpose_detail": "MDL reaction format (RXN) is a structured text-based file format developed by MDL Information Systems (now part of BIOVIA) for storing and exchanging complete chemical reaction information including reactants, products, catalysts, solvents, reaction conditions, and atom mapping between corresponding atoms in substrates and products, serving as the primary standard for computational chemistry, cheminformatics databases, and reaction prediction systems where explicit representation of chemical transformations enables automated reaction searching, retrosynthetic analysis, and machine learning model training on large-scale reaction datasets. The RXN format builds upon the MDL Molfile format (also known as SDF or Structure-Data File) for representing individual molecular structures, extending it to capture reaction stoichiometry, multi-component systems, and transformation directionality through a hierarchical structure consisting of a header block (specifying RXN file format version, creation software, timestamp), a counts line (indicating number of reactants and products, typically formatted as \"$RXN\" signature followed by counts like \"3 2\" for 3 reactants yielding 2 products), followed by individual Molfile blocks for each reactant and product with their complete structural definitions including atom coordinates (2D or 3D Cartesian coordinates), bond types (single, double, triple, aromatic), stereochemistry (chiral centers marked with wedge/dash bonds, E/Z double bond geometry), and formal charges. Atom mapping, a critical feature enabling mechanistic understanding and reaction center identification, uses atom map numbers assigned to corresponding atoms across reactants and products (e.g., carbonyl carbon in ketone reactant mapped to same position in alcohol product after reduction), with map numbers specified in the atom block of constituent Molfiles allowing software to track bond breaking/forming patterns, identify reaction centers (atoms whose bonding changes during transformation), and extract local chemical environments (substructures around reactive sites) essential for reaction classification and mechanism elucidation. RXN format supports multi-step reactions through hierarchical nesting where intermediate products from one reaction become reactants for subsequent steps, reaction conditions encoded in extended property fields (temperature ranges, pressure, reaction time, catalyst loading, solvent systems), and reaction role annotations distinguishing reactants from reagents (stoichiometric starting materials vs. catalytic or excess components), products from byproducts (desired vs. waste products), and solvents from additives (reaction medium vs. activating agents). Cheminformatics tools including Open Babel, RDKit, Chemistry Development Kit (CDK), and commercial packages (BIOVIA Pipeline Pilot, ChemAxon) parse RXN files to extract reaction SMILES/SMARTS patterns (linear text representations of reactions like \"CC(=O)C>>CC(O)C\" for ketone reduction), compute reaction fingerprints (binary vectors encoding presence/absence of substructural transformations for similarity searching), generate reaction difference fingerprints (capturing changes in atom environments between reactants and products), and perform substructure-based reaction searching in databases like Reaxys, SciFinder, and USPTO patent reaction collections containing millions of reactions. For AI/ML applications, RXN-formatted reaction datasets enable supervised learning for forward reaction prediction where models learn mappings from reactant structures to product structures (sequence-to-sequence models treating SMILES as natural language, graph neural networks operating on molecular graphs, transformer architectures with multi-head attention over atom/bond features), retrosynthetic planning where models predict likely precursor combinations that could produce a target molecule (single-step retrosynthesis models trained on reversed reactions, multi-step planning algorithms using Monte Carlo tree search or reinforcement learning to explore synthetic routes), reaction condition optimization where models predict optimal temperature/solvent/catalyst combinations from substrate structures and desired transformations (classification models predicting condition categories, regression models predicting yields and selectivities), reaction outcome prediction including regioselectivity (predicting which of multiple reactive sites undergoes transformation), stereoselectivity (predicting stereochemical configurations of products), and competing pathways (ranking alternative products in reactions with multiple possible mechanisms), and reaction mechanism prediction where atom mapping information trains models to identify electron movement patterns and propose step-by-step mechanisms. Large-scale RXN datasets curated from patent literature (USPTO database with ~3.5 million reactions), synthetic chemistry journals (extracted via text mining and manual curation), and proprietary pharmaceutical databases enable transfer learning where models pre-trained on broad reaction corpora fine-tune for specialized reaction classes (organometallic catalysis, heterocycle synthesis, peptide coupling), few-shot learning adapting to novel reaction types from limited examples, and active learning prioritizing experimental validation of model predictions with highest uncertainty, supporting drug discovery workflows requiring rapid exploration of chemical space, process chemistry optimizing routes for manufacturing scale-up, materials science designing synthetic pathways to functional polymers and advanced materials, and green chemistry initiatives identifying more sustainable reaction conditions minimizing waste, energy consumption, and hazardous reagents through AI-guided experiment planning informed by millions of historical reactions systematically encoded in RXN format.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://open-babel.readthedocs.io/en/latest/FileFormats/MDL_RXN_format.html"
    },
    {
      "id": "B2AI_STANDARD:192",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MSGISO",
      "description": "Measuring Sex, Gender Identity, and Sexual Orientation",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:6",
        "B2AI_TOPIC:31"
      ],
      "purpose_detail": "Measuring Sex, Gender Identity, and Sexual Orientation recommends that the National Institutes of Health (NIH) adopt new practices for collecting data on sex, gender, and sexual orientation - including collecting gender data by default, and not conflating gender with sex as a biological variable. The report recommends standardized language to be used in survey questions that ask about a respondent's sex, gender identity, and sexual orientation. Better measurements will improve data quality, as well as the NIH's ability to identify LGBTQI+ populations and understand the challenges they face.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://nap.nationalacademies.org/catalog/26424/measuring-sex-gender-identity-and-sexual-orientation",
      "publication": "doi:10.17226/26424",
      "responsible_organization": [
        "B2AI_ORG:60"
      ]
    },
    {
      "id": "B2AI_STANDARD:193",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MNC",
      "description": "Medical Imaging NetCDF (Minc) format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:255",
          "category": "B2AI:Application",
          "name": "MINC Toolkit for PET Amyloid Classification with Random Forests",
          "description": "The MINC toolkit processes 18F-florbetapir brain PET scans to generate standardized uptake value (SUV) ratios normalized to cerebellar gray matter reference regions, producing quantitative regional features that serve as inputs to random forest machine learning classifiers for automated amyloid positivity classification. In a study of 57 baseline PET scans, MINC toolkit preprocessing and quantification produced SUV ratio data per region of interest, which together with clinical reads provided the feature set and labels for supervised learning, with data split into 37 training and 20 testing cases. The random forest model implemented in MATLAB with 10,000 trees (each tree built using randomly selected cases and feature subsets following standard RF algorithm) classified amyloid PET scans as positive versus negative, achieving 86% sensitivity (correctly identifying true positive amyloid cases), 92% specificity (correctly identifying true negative cases), and 90% overall accuracy on the held-out test set, demonstrating that MINC-derived quantitative imaging features enable robust automated classification matching expert visual interpretation. This workflow exemplifies MINC's role as a preprocessing and quantification layer converting raw PET images into tabular regional SUV features suitable for classical machine learning, where the format's self-documenting metadata (coordinate systems, acquisition parameters, processing history) ensures reproducible feature extraction across multi-center studies and the toolkit's standardized pipeline (registration to anatomical templates, ROI definition, intensity normalization) produces consistent measurements enabling pooling of data from different scanners and sites. The approach supports clinical applications including automated screening for Alzheimer's disease pathology in memory clinics (reducing variability from visual reads and enabling quantitative tracking of amyloid burden over time), research studies correlating regional amyloid deposition with cognitive decline trajectories, and drug trial enrichment strategies using ML-based amyloid classification to identify eligible participants with elevated brain amyloid, demonstrating MINC format's utility in bridging molecular imaging and machine learning for precision neurology diagnostics.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1097/rlu.0000000000002747"
          ]
        },
        {
          "id": "B2AI_APP:256",
          "category": "B2AI:Application",
          "name": "MINC-Based MRI Morphometry Pipeline for ALS Survival Prediction",
          "description": "A multicentre amyotrophic lateral sclerosis (ALS) neuroimaging study implements a comprehensive preprocessing pipeline built on MINC toolkit (minc-tools) and ANTs (Advanced Normalization Tools) to process T1-weighted MRI scans and extract deformation-based morphometry (DBM) features for machine learning-based survival prediction, demonstrating MINC's role in robust, standardized neuroimaging feature engineering for prognostic modeling. The pipeline applies sequential preprocessing steps implemented via MINC tools including denoising (reducing thermal noise and acquisition artifacts), N3/N4 bias field correction (correcting intensity inhomogeneity from RF coil sensitivity profiles), histogram-matching intensity normalization (standardizing intensity distributions across scanners and acquisition protocols enabling multi-site pooling), linear registration to MNI-ICBM152-2009c standard template (aligning brain position and global scale), and high-dimensional nonlinear registration (warping individual anatomy to template space capturing local volume differences), with DBM maps computed as Jacobian determinants of the nonlinear deformation fields quantifying regional volume expansion (Jacobian > 1) or contraction (Jacobian < 1) relative to the population template. Regional mean DBM values are extracted using established brain atlases (CerebrA for 62 bilateral gray matter regions and ventricles, JHU white matter atlas for major tracts, Allen atlas for corpus callosum subregions), converted to w-scores by regressing out age, sex, and scanner site effects and scaling by control group standard deviations to produce standardized measures of regional atrophy, and used as imaging features in elastic-net Cox proportional hazards survival models (combining L1 and L2 regularization penalties to handle correlated features and perform automatic feature selection). The study employs nested cross-validation repeated 100 times to optimize hyperparameters and assess generalization, trains separate models using imaging-only features (DBM w-scores), clinical-only features (ALSFRS-R functional rating, forced vital capacity, disease duration, onset type), and combined feature sets, and evaluates performance with concordance index (C-index measuring discriminative ability to rank survival times), integrated Brier score (IBS quantifying calibration of predicted survival probabilities over time), calibration plots (comparing predicted vs. observed survival), and mean absolute error variants (MAE for survival time predictions). Results demonstrate that imaging features derived from MINC-processed MRI significantly improve survival model performance compared to clinical-only models, with regional atrophy patterns (particularly motor cortex, corticospinal tracts, frontotemporal regions) providing complementary prognostic information enabling more accurate individualized survival predictions. This application showcases MINC's critical role in multi-center neuroimaging ML where the format's flexible metadata accommodates diverse scanner types (Siemens, GE, Philips with varying field strengths and sequences), the toolkit's standardized preprocessing ensures harmonization across sites reducing batch effects, and the self-documenting provenance (embedded processing history in MINC headers) supports reproducibility and quality control, enabling aggregation of data from Canadian ALS neuroimaging consortium sites to achieve sample sizes sufficient for robust survival model development, supporting clinical applications in ALS trial design (patient stratification, enrichment for rapid progressors), individualized counseling (data-driven survival estimates incorporating imaging biomarkers), and therapeutic monitoring (detecting treatment effects on neurodegeneration patterns via longitudinal DBM changes).",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1101/2024.10.04.24314899"
          ]
        },
        {
          "id": "B2AI_APP:257",
          "category": "B2AI:Application",
          "name": "MINC MRSI Metabolite Maps for Glioma Classification",
          "description": "Ultra-high-field 7 Tesla magnetic resonance spectroscopic imaging (MRSI) generates three-dimensional metabolite concentration maps stored in MINC (.mnc) and NIfTI (.nii) formats, which are systematically converted into machine learning-ready tabular databases for automated classification of glioma molecular subtypes (IDH mutation status) and tumor grades using classical machine learning methods including random forests and support vector machines. The workflow processes 3D MRSI metabolite maps representing spatial distributions of key brain metabolites (total creatine tCr, GABA, glutamine Gln, glutamate Glu, glycine Gly, total choline tCho, glutathione GSH, myo-inositol Ins, N-acetylaspartate NAA, N-acetylaspartylglutamate NAAG, total NAA tNAA, serine Ser, taurine Tau) acquired at 7T with high spatial resolution (typical voxel sizes ~5mm enabling detailed intratumoral metabolic heterogeneity mapping), with each metabolite represented as a separate 3D image volume. The database construction pipeline flattens each 3D metabolite map (dimensions resulting in 159,744 voxels per volume) into single-dimensional arrays and creates per-patient CSV files where each row represents one voxel with columns for three-dimensional Cartesian coordinates (X, Y, Z positions in standard space), tissue classification masks (creatine signal mask indicating metabolite detectability, white matter mask, gray matter mask derived from co-registered anatomical MRI), absolute metabolite concentrations (institutional units calibrated to tissue water reference), and computed metabolite ratios (normalized concentrations reducing inter-scanner variability: tCho/tCr, NAA/tCr, Glu/tCr, etc., commonly used clinically). This per-voxel tabular representation enables spatial pattern recognition by machine learning classifiers where intratumoral metabolic heterogeneity (varying metabolite profiles across tumor regions reflecting different degrees of malignancy, proliferation, necrosis) provides discriminative features for IDH mutation prediction (IDH-mutant gliomas showing characteristic 2-hydroxyglutarate accumulation and altered glutamate/glutamine metabolism detectable via specific metabolite ratio patterns) and grade classification (high-grade gliomas exhibiting elevated choline reflecting increased membrane turnover, reduced NAA indicating neuronal loss, lactate accumulation from anaerobic glycolysis). The machine learning pipeline trains random forest classifiers (ensemble of decision trees voting on classification, robust to high-dimensional feature spaces with correlated metabolite measurements) and support vector machines (finding optimal hyperplanes in metabolite feature space separating IDH-mutant from IDH-wildtype cases, low-grade from high-grade tumors, with kernel functions capturing nonlinear metabolite combination patterns) using features extracted from tumor voxels identified via anatomical segmentation and co-registered to metabolite maps. This approach demonstrates MINC format's utility in advanced neuroimaging ML where the format's support for multi-dimensional data (3D spatial dimensions plus spectral dimension encoding chemical shift information) and flexible metadata (storing acquisition parameters: echo time TE, repetition time TR, spectral bandwidth, number of averages essential for quality assessment and inter-study comparison) enables systematic organization of complex MRSI datasets, while conversion to CSV tabular format facilitates integration with standard ML frameworks (scikit-learn, R caret, MATLAB Statistics and ML Toolbox) and enables spatial feature engineering (extracting summary statistics from tumor subregions, computing texture metrics on metabolite maps, modeling spatial gradients between tumor core and periphery). Clinical applications include non-invasive molecular subtyping of gliomas (avoiding biopsy sampling errors by assessing entire tumor metabolic profiles, predicting IDH status with implications for prognosis and treatment selection), personalized treatment planning (identifying aggressive tumor regions via metabolic signatures for targeted biopsy or focused radiotherapy boost volumes), and treatment response monitoring (detecting metabolic changes preceding anatomical tumor size changes on conventional MRI, enabling earlier assessment of therapy effectiveness), supporting precision neuro-oncology where metabolic imaging biomarkers augment genomic profiling and anatomical imaging in guiding personalized management of brain tumors.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.34726/hss.2023.82753"
          ]
        }
      ],
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:22"
      ],
      "purpose_detail": "A system for flexible, self-documenting representation of neuroscientific imaging data with arbitrary orientation and dimensionality.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikibooks.org/wiki/MINC/SoftwareDevelopment/MINC2.0_File_Format_Reference",
      "publication": "doi:10.3389/fninf.2016.00035"
    },
    {
      "id": "B2AI_STANDARD:194",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MS-DRG",
      "description": "Medicare Severity Diagnosis Related Groups codes",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "codesystem"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "Medical cases in the US are classified into Medicare Severity Diagnosis Related Groups (MS-DRGs) for payment based on the following information reported by the hospital - the principal diagnosis, up to 24 additional diagnoses, and up to 25 procedures performed during the stay. In a small number of MS-DRGs, classification is also based on the age, sex, and discharge status of the patient. Effective October 1, 2015, the diagnosis and procedure information is reported by the hospital using codes from the International Classification of Diseases, Tenth Revision, Clinical Modification (ICD-10-CM) and the International Classification of Diseases, Tenth Revision, Procedure Coding System (ICD-10-PCS).",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.cms.gov/Medicare/Medicare-Fee-for-Service-Payment/AcuteInpatientPPS/MS-DRG-Classifications-and-Software",
      "responsible_organization": [
        "B2AI_ORG:17"
      ]
    },
    {
      "id": "B2AI_STANDARD:195",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MEDIN",
      "description": "MEDIN Discovery Metadata Standard",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "datamodel"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:11"
      ],
      "purpose_detail": "MEDIN Discovery Metadata Standard is a UK marine profile of the UK GEMINI (GEo-spatial Metadata Interoperability iNItiative) standard that also complies with the EU INSPIRE Directive and ISO19115. It provides comprehensive guidance for creating discovery metadata that accompanies marine datasets, describing what the data contains, where it was collected, and how to access it. The standard supports both geospatial (v3.1.2) and non-spatial marine data types. MEDIN provides multiple tools including the web-based Discovery Metadata Editor for creating, validating, exporting, and publishing records to the MEDIN Data Discovery Portal, and the standalone Metadata Maestro desktop application with user-friendly interface and offline capability. The standard includes XML Schema Definition (XSD) and Schematron files for validation, enabling systematic quality control of metadata records within organizational workflows.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.medin.org.uk/medin-discovery-metadata-standard",
      "formal_specification": "https://github.com/medin-marine/Discovery-Standard-public-content"
    },
    {
      "id": "B2AI_STANDARD:196",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MSAS",
      "description": "Memorial Symptom Assessment Scale",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "diagnosticinstrument"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "The Memorial Symptom Assessment Scale (MSAS) is a new patient-rated instrument that was developed to provide multidimensional information about a diverse group of common symptoms.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://www.npcrc.org/files/news/memorial_symptom_assessment_scale.pdf",
      "publication": "doi:10.1016/0959-8049(94)90182-1"
    },
    {
      "id": "B2AI_STANDARD:197",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MOD-CO",
      "description": "Meta-omics Data and Collection Objects",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:13",
        "B2AI_TOPIC:28",
        "B2AI_TOPIC:34"
      ],
      "purpose_detail": "Meta-omics Data and Collection Objects (MOD-CO) schema is a comprehensive conceptual and procedural framework designed to standardize the processing, annotation, and exchange of sample data and metadata in meta-omics researchencompassing metagenomics (DNA sequencing of microbial communities), metatranscriptomics (RNA-seq of community gene expression), metaproteomics (mass spectrometry of community proteins), and metabolomics (small molecule profiling)addressing the critical challenge of integrating heterogeneous multi-omics datasets from environmental, host-associated, and synthetic microbial ecosystems where thousands of species and millions of genes generate complex data requiring systematic organization for reproducible analysis and cross-study comparison. Developed through international collaboration coordinating multiple meta-omics initiatives and published in various schema representations (UML class diagrams for conceptual modeling, XML Schema Definitions for data exchange, relational database schemas for implementation, JSON-LD for semantic web integration), MOD-CO provides a generic, technology-agnostic framework adaptable to diverse experimental workflows, sequencing platforms (Illumina, PacBio, Oxford Nanopore), sample types (soil, marine, human gut, industrial bioreactors), and analysis pipelines. The schema defines core data objects organized hierarchically starting with Study (research project with objectives, funding, publications, contacts), Investigation (experimental campaign addressing specific hypotheses), Sample Collection Event (spatial-temporal metadata capturing when, where, and how environmental or biological samples were obtained, including GPS coordinates, depth/altitude, temperature, pH, sampling methodology), Sample Processing (laboratory procedures for DNA/RNA/protein extraction, quality control metrics, storage conditions, contamination controls), through to Sequence Data Objects (raw FASTQ files with quality scores, trimmed/filtered reads, assembled contigs, annotated genes) and derived analytical results (taxonomic profiles, functional annotations, abundance matrices, diversity indices, statistical comparisons). Critical metadata categories include environmental context following MIxS (Minimum Information about any (x) Sequence) standards with environment ontology (ENVO) terms describing biome/feature/material (e.g., freshwater lake sediment with depth strata, sediment grain size, organic carbon content), host-associated metadata for microbiome studies (host taxonomy, age, sex, health status, diet, medications, anatomical sampling site), experimental treatments (perturbations applied to communities: antibiotic exposure, temperature shifts, nutrient amendments with precise concentrations and timing), technical metadata (sequencing platform, library preparation kit, barcode sequences for multiplexing, sequencing depth targets), and quality metrics (DNA concentration, 260/280 absorbance ratios, fragment size distributions, read quality distributions, contamination screening results). MOD-CO explicitly models relationships between objects enabling complex queries like 'retrieve all metatranscriptome samples from human gut with matching metagenomic and metabolomic profiles from the same subjects and timepoints' or 'find marine metagenomes from similar temperature/salinity ranges across different ocean basins,' supporting meta-analyses aggregating hundreds of studies. The schema accommodates multi-omics integration where the same biological samples undergo parallel sequencing (metagenomics revealing community composition) and functional assays (metatranscriptomics showing active metabolic pathways, metaproteomics quantifying enzyme abundances, metabolomics measuring pathway products), with explicit linkage through shared sample identifiers and temporal alignment of measurements enabling systems-level analysis of microbial community structure-function relationships. For machine learning and AI applications, MOD-CO-structured datasets enable supervised learning for microbiome-phenotype associations where taxonomic/functional features predict host disease status (inflammatory bowel disease, obesity, cancer) or environmental conditions (soil fertility, water quality, bioremediation potential), unsupervised clustering discovering novel ecological patterns and community types across global microbiome surveys, time-series modeling forecasting community dynamics and succession patterns in response to environmental change or host development, transfer learning where models trained on well-characterized systems (human gut, mouse models) generalize to understudied environments through shared functional gene representations, multi-modal learning integrating sequence-derived features (gene abundance matrices) with chemical measurements (metabolite concentrations) and environmental variables (temperature, pH, nutrients) in deep neural networks predicting ecosystem functions, causal inference methods identifying keystone taxa and metabolic interactions driving community assembly and stability through interventional experiments and longitudinal observations, and federated learning enabling privacy-preserving analysis of sensitive human microbiome data where MOD-CO-standardized metadata allows model training across institutions without sharing raw sequences, supporting precision medicine applications predicting individualized treatment responses based on microbiome composition, environmental monitoring systems detecting ecosystem disturbances through anomaly detection in microbial community trajectories, and biotechnology optimization where machine learning guides engineering of synthetic microbial consortia for industrial applications including biofuel production, bioremediation, and sustainable agriculture by learning from natural community assembly principles encoded in thousands of standardized meta-omics studies.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.mod-co.net/wiki/Schema_Representations"
    },
    {
      "id": "B2AI_STANDARD:198",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MINiML",
      "description": "MIAME Notation in Markup Language",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831'",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:33"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:74"
      ],
      "purpose_detail": "MINiML (MIAME Notation in Markup Language, pronounced 'minimal') is a data exchange format optimized for microarray gene expression data, as well as many other types of high-throughput molecular abundance data. MINiML assumes only very basic relations between objects - Platform (e.g., array), Sample (e.g., hybridization), and Series (experiment). MINiML captures all components of the MIAME checklist, as well as any additional information that the submitter wants to provide. MINiML uses XML Schema as syntax.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.ncbi.nlm.nih.gov/geo/info/MINiML.html"
    },
    {
      "id": "B2AI_STANDARD:199",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MAGE-ML",
      "description": "MicroArray Gene Expression Markup Language",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:33",
        "B2AI_TOPIC:34"
      ],
      "purpose_detail": "This document is a standard that addresses the representation of gene expression data and relevant annotations, as well as mechanisms for exchanging these data. The field of gene expression experiments has several distinct technologies that a standard must include (e.g., single vs. dual channel experiments, cDNA vs. oligonucleotides). Because of these different technologies and different types of gene expression experiments, it is not expected that all aspects of the standard will be used by all organizations. With the acceptance of XML Metadata Interchange as an OMG standard it is possible to specify a normative UML model using a tool such as Rational Rose that describes the data structures for Gene Expression",
      "is_open": true,
      "requires_registration": false,
      "url": "http://scgap.systemsbiology.net/standards/mage_miame.php"
    },
    {
      "id": "B2AI_STANDARD:200",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MAGE-TAB",
      "description": "MicroArray Gene Expression Markup Language Tab format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:33",
        "B2AI_TOPIC:34"
      ],
      "purpose_detail": "Sharing of microarray data within the research community has been greatly facilitated by the development of the disclosure and communication standards MIAME and MAGE-ML by the FGED Society. However, the complexity of the MAGE-ML format has made its use impractical for laboratories lacking dedicated bioinformatics support. We propose a simple tab-delimited, spreadsheet-based format, MAGE-TAB, which will become a part of the MAGE microarray data standard and can be used for annotating and communicating microarray data in a MIAME compliant fashion. MAGE-TAB will enable laboratories without bioinformatics experience or support to manage, exchange and submit well-annotated microarray data in a standard format using a spreadsheet. The MAGE-TAB format is self-contained, and does not require an understanding of MAGE-ML or XML",
      "is_open": true,
      "requires_registration": false,
      "url": "http://scgap.systemsbiology.net/standards/mage_miame.php"
    },
    {
      "id": "B2AI_STANDARD:201",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MCL",
      "description": "Microbiological Common Language",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:1"
      ],
      "purpose_detail": "MCL is a data exchange standard for microbiological information. In short, MCL defines terms which can be used to reference and describe microorganisms. It is designed to form a simple and generic framework leveraging the electronical exchange of information about microorganisms. MCL is loosely coupled from its actual representation technologies and is currently used to structure XML and RDF files (see examples).",
      "is_open": true,
      "requires_registration": false,
      "url": "https://doi.org/10.1016/j.resmic.2010.02.005",
      "publication": "doi:10.1016/j.resmic.2010.02.005"
    },
    {
      "id": "B2AI_STANDARD:202",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "WFDB Format",
      "description": "MIMIC Waveform Database Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:39",
          "category": "B2AI:Application",
          "name": "MIMIC-BP Curated Dataset for Blood Pressure Estimation Benchmarking",
          "description": "The Python wfdb package reads MIMIC-III waveform signals directly, converts samples to double precision, and preserves signals exactly as recorded for ML workflows. MIMIC-BP organizes 1,524 patients into fixed 30-second segments (30 segments per patient) at 125 Hz with ABP, ECG, PPG, and RESP waveforms stored as NumPy files, providing per-segment median SBP/DBP labels for supervised learning. Pre-defined per-subject train/validation/test splits prevent data leakage under calibration-free protocols. This WFDB-enabled standardized segmentation, labeling, and splitting supports reproducible benchmarking of deep learning (ResNet, LSTM, Transformer) and classical ML (SVR, Random Forest) models for cuff-less blood pressure estimation across approximately 380 hours of ICU data.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1038/s41597-024-04041-1"
          ]
        },
        {
          "id": "B2AI_APP:142",
          "category": "B2AI:Application",
          "name": "PulseDB Large-Scale BP Estimation Corpus Using WFDB Toolbox",
          "description": "The WFDB toolbox retrieves and validates MIMIC-III matched subset records, filtering for simultaneous ECG (lead II), PPG, and ABP channels while removing NaN samples by extracting the longest valid intervals per record (10 seconds to 10 hours). Signals are resampled to 125 Hz and segmented into standardized 10-second windows with beat-level annotations and per-segment SBP/DBP labels, yielding 5.2 million segments. This WFDB-enabled quality control, multi-channel synchronization, and standardized segmentation provides a large benchmarking corpus for training and evaluating ML/DL cuff-less blood pressure estimators with cross-study comparability and reproducible preprocessing pipelines.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.3389/fdgth.2022.1090854"
          ]
        },
        {
          "id": "B2AI_APP:143",
          "category": "B2AI:Application",
          "name": "Intracranial Hypertension Forecasting Using Multi-Scale WFDB Waveforms",
          "description": "MIMIC-III WFDB provides high-frequency waveforms (125 Hz) and derived 1 Hz time series for ML early-warning systems. Specific channels (ICP, CPP, ABP variants, HR, PLETH, RESP, ECG) are selected via WFDB, with cohort filtering enforcing minimum 24-hour recording length and maximum 25% per-channel missing values, reducing to 123 usable segments. Preprocessing produces 1-minute blocks with artifact deletion, and supervised labels are derived from WFDB ICP time series (median ICP greater than 20 mmHg for five consecutive minutes defines intracranial hypertension events). This WFDB-based channel selection, quality filtering, segmentation, and labeling enables training of logistic regression and feature-based predictive models for forecasting intracranial hypertension up to 8 hours ahead.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1088/1361-6579/ab6360"
          ]
        },
        {
          "id": "B2AI_APP:144",
          "category": "B2AI:Application",
          "name": "Deep Learning ABP Waveform Imputation from WFDB Multi-Channel Signals",
          "description": "MIMIC-III waveform records accessed via WFDB provide synchronized ECG (lead II), PPG, and invasive ABP for training deep learning models to impute continuous arterial blood pressure waveforms from non-invasive signals. Preprocessing includes downsampling to 100 Hz, low-pass filtering (16 Hz cutoff), robust per-window scaling, and cross-correlation-based clock drift correction (up to plus or minus 4 seconds). Sliding 32-second windows with 16-second steps are quality-filtered (variance and peak-count thresholds), and labels are derived from invasive ABP medians over 4-second windows. This WFDB-enabled preprocessing, segmentation, inter-signal alignment, and label derivation supports training U-Net-like segmentation models with transfer learning to external (UCLA) cohorts for generalizable waveform-to-waveform prediction.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1038/s41598-021-94913-y"
          ]
        },
        {
          "id": "B2AI_APP:145",
          "category": "B2AI:Application",
          "name": "Comparative Deep Learning for ABP Prediction from WFDB Physiological Signals",
          "description": "MIMIC waveform records provide co-recorded ECG (lead V), PPG, and ABP for comparative evaluation of deep learning architectures including ResNet, WaveNet, and LSTM for blood pressure prediction. WFDB-sourced data undergoes artifact detection and removal (flat lines, missing heartbeats, negative BP values), signal filtering (8th-order Chebyshev bandpass for ECG at 2-59 Hz), and segmentation into 10-minute recordings. Target ABP is normalized with physiological bounds (15-300 mmHg) for model training using RMSE and Huber loss, requiring de-normalization for evaluation. This WFDB-based data selection, preprocessing, segmentation, and target normalization pipeline supports systematic comparison of CNN and RNN architectures for continuous blood pressure estimation from ICU waveforms.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1007/s12559-021-09910-0"
          ]
        }
      ],
      "collection": [
        "fileformat",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:37"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:57",
        "B2AI_ORG:115"
      ],
      "purpose_detail": "Format for MIMIC Waveform Database records.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://wfdb.io/mimic_wfdb_tutorials/mimic/formatting.html",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:9"
      ]
    },
    {
      "id": "B2AI_STANDARD:203",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MINSEQE",
      "description": "Minimal Information about a high throughput SEQuencing Experiment",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "minimuminformationschema"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "Information needed to enable the unambiguous interpretation and facilitate reproduction of the results of a high throughput sequencing experiment.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://zenodo.org/record/5706412",
      "formal_specification": "https://drive.google.com/file/d/1YyvWT02puzMG_UgNmfAEJwVr60-pMvIE/view?usp=sharing"
    },
    {
      "id": "B2AI_STANDARD:204",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MISME",
      "description": "Minimal Information about a Self-Monitoring Experiment",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "minimuminformationschema"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Minimal Information about a Self-Monitoring Experiment (MISME) is a reporting guideline and metadata schema developed to standardize documentation of self-monitoring, personal health tracking, and quantified-self experiments where individuals collect longitudinal data about their own physiology, behavior, environment, or experiences using consumer devices, mobile apps, or manual logging, addressing the critical gap in reproducibility and data quality assessment for n-of-1 studies and personal science investigations that generate rich time-series datasets increasingly valuable for personalized medicine research, digital health validation, and machine learning model development. MISME specifies core metadata elements organized into multiple categories capturing the who (participant demographics including age, sex, health conditions, medications, baseline characteristics), what (measured variables with precise definitions: heart rate in beats per minute, sleep duration in hours, mood ratings on 1-10 scales, step counts from accelerometer thresholds, dietary intake in calories/macronutrients, blood glucose in mg/dL), when (temporal structure with measurement frequency, duration of observation period, time zones, timestamp precision, missing data patterns documenting gaps in collection due to device failures or participant non-adherence), where (geographic location at appropriate granularity respecting privacy, environmental context like indoor/outdoor settings, altitude for activity tracking), why (research question or personal goal motivating the experiment, hypotheses being tested, expected relationships between variables), and how (detailed device and methodology descriptions including manufacturer, model numbers, firmware versions, sensor specifications, calibration procedures, measurement protocols, data export formats, preprocessing steps applied before analysis). The guideline addresses unique challenges of self-experimentation including lack of blinding (participants aware of interventions and measurements, introducing potential placebo effects and confirmation bias), temporal confounding (seasonal variations, life events, concurrent behavioral changes making causal attribution difficult), device heterogeneity (consumer wearables with proprietary algorithms, varying accuracy across brands and body sites), and data ownership/ethics (consent for research use of personal data, privacy protections when sharing sensitive health information, appropriate credit for participant contributions as co-investigators rather than subjects). MISME recommends standardized reporting of experimental design elements including baseline periods (establishing individual norms before interventions), intervention descriptions (dietary changes, exercise regimens, medication adjustments, environmental modifications with precise timing and dosing), washout periods (time between sequential interventions allowing return to baseline), and randomization schemes (if self-randomizing intervention timing, documenting the randomization method and allocation concealment). Data quality metadata includes device validation studies (comparing device outputs against gold-standard measurements like research-grade polysomnography for sleep tracking or doubly-labeled water for energy expenditure), measurement reliability (test-retest correlation coefficients, intra-individual variability quantified through repeated measures), data cleaning procedures (outlier detection thresholds, imputation methods for missing values, smoothing or filtering applied to noisy sensor data), and uncertainty quantification (confidence intervals around estimates, sensitivity analyses examining robustness to methodological choices). For machine learning and AI applications, MISME-compliant datasets enable robust model training where standardized metadata facilitates transfer learning across individuals (identifying which personal characteristics and device configurations affect model generalizability), time-series forecasting of health outcomes (predicting blood glucose excursions from continuous glucose monitor data, sleep quality from activity/heart rate, mood episodes from behavioral digital phenotypes captured via smartphone sensors), anomaly detection for early disease warning (identifying deviations from individual baselines suggesting onset of infection, metabolic dysregulation, or mental health crises), causal inference from observational self-tracking data (applying difference-in-differences, interrupted time series, or instrumental variable methods to estimate intervention effects in the presence of confounding), and federated learning where privacy-sensitive personal health data remains on individual devices while contributing to population-level models learning patterns generalizable across diverse self-tracking contexts, supporting digital health applications including adaptive interventions (AI-driven just-in-time recommendations personalized to real-time physiological and behavioral states), closed-loop systems (automated insulin delivery adjusting doses based on predicted glucose trends, circadian rhythm optimization adjusting light exposure and sleep timing based on actigraphy and performance metrics), and precision diagnostics where longitudinal self-monitoring augments episodic clinical assessments, enabling earlier detection of disease progression and personalized treatment titration in conditions like diabetes, cardiovascular disease, mental health disorders, and sleep disturbances where individual trajectories vary substantially and population-average guidelines provide insufficient guidance for optimal management.",
      "is_open": false,
      "requires_registration": false,
      "url": "https://doi.org/10.3233/978-1-61499-423-7-79",
      "publication": "doi:10.3233/978-1-61499-423-7-79"
    },
    {
      "id": "B2AI_STANDARD:205",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MIRIAM",
      "description": "Minimal Information Required In the Annotation of Models",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "minimuminformationschema"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "MIRIAM (Minimum Information Required In the Annotation of biochemical Models) is a set of guidelines and standards developed by the computational systems biology community to ensure consistent annotation and curation of computational models in biology, particularly those encoded in SBML (Systems Biology Markup Language) and CellML formats. MIRIAM defines requirements for model attribution including reference publications, creator information, creation dates, and modification history to ensure proper provenance tracking and reproducibility. The guidelines mandate the use of controlled vocabularies and standardized identifiers from MIRIAM Resources (identifiers.org) to annotate biological entities such as genes, proteins, metabolites, reactions, and pathways with unambiguous references to external databases like UniProt, ChEBI, Gene Ontology, KEGG, and Reactome. MIRIAM specifies the use of RDF (Resource Description Framework) and qualified annotations to encode biological semantics and relationships between model components and biological knowledge. The standard promotes model reusability by requiring clear licensing information (Creative Commons, etc.), encoded parameter units using standardized ontologies, and comprehensive documentation of model assumptions and limitations. MIRIAM compliance is supported by tools including the MIRIAM annotation editor in systems biology software, libSBML annotation functions, and BioModels Database validation workflows. The guidelines have been widely adopted by model repositories (BioModels, CellML Model Repository), enhancing model discoverability, integration into larger modeling frameworks, and facilitating quantitative comparison of alternative models representing the same biological system.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://co.mbine.org/standards/miriam",
      "responsible_organization": [
        "B2AI_ORG:19"
      ]
    },
    {
      "id": "B2AI_STANDARD:206",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MIFlowCyt",
      "description": "Minimum Information about a Flow Cytometry Experiment",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "minimuminformationschema"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:2"
      ],
      "purpose_detail": "The fundamental tenet of scientific research is that the published results of any study have to be open to independent validation or refutation. The Minimum Information about a Flow Cytometry Experiment (MIFlowCyt) establishes criteria for recording and reporting information about the flow cytometry experiment overview, samples, instrumentation and data analysis. It promotes consistent annotation of clinical, biological and technical issues surrounding a flow cytometry experiment by specifying the requirements for data content and by providing a structured framework for capturing information.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://isac-net.org/page/MIFlowCyt",
      "publication": "doi:10.1002/cyto.a.20623"
    },
    {
      "id": "B2AI_STANDARD:207",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MIAPE",
      "description": "Minimum Information About a Proteomics Experiment",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "minimuminformationschema"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:28"
      ],
      "purpose_detail": "Information from whole proteomics experiments; where samples came from, and how analyses of them were performed.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.psidev.info/miape",
      "responsible_organization": [
        "B2AI_ORG:41"
      ]
    },
    {
      "id": "B2AI_STANDARD:208",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MIARE",
      "description": "Minimum Information About a RNAi Experiment",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "minimuminformationschema"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:33"
      ],
      "purpose_detail": "Minimum Information About an RNAi Experiment (MIARE) is a set of reporting guidelines that describes the minimum information that should be reported about an RNAi experiment to enable the unambiguous interpretation and reproduction of the results. MIARE forms part of a larger effort to develop RNAi data standards that include a data model, data exchange format, controlled vocabulary and supporting software tools.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://miare.sourceforge.net/HomePage"
    },
    {
      "id": "B2AI_STANDARD:209",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MIxS",
      "description": "Minimum information about any sequence",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "minimuminformationschema"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The Minimum Information about any (x) Sequence (MIxS) is a unified standard developed by the Genomic Standards Consortium (GSC) for describing sequence data from diverse environments and organisms. MIxS provides a single point of entry for the scientific community to access and use GSC checklists, which include 11 distinct sequence types covering bacterial/archaeal genomes (MigsBa), eukaryotes (MigsEu), organelles (MigsOrg), plasmids (MigsPl), viruses (MigsVi), marker sequences (MimarksC/S), metagenomes (Mims), metagenome-assembled genomes (Mimag), single amplified genomes (Misag), and uncultivated virus genomes (Miuvig). The standard is complemented by 21 environmental extensions tailored to specific sampling contexts including agriculture, air, built environment, food production, host-associated, human-associated (with specific gut, oral, skin, and vaginal extensions), hydrocarbon resources, microbial mat/biofilm, plant-associated, sediment, soil, symbiont-associated, wastewater/sludge, and water environments. The specification is maintained as a YAML-formatted LinkML schema serving as the authoritative source for generating downstream GSC artifacts.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://genomicsstandardsconsortium.github.io/mixs/",
      "publication": "doi:10.1038/nbt.1823",
      "formal_specification": "https://github.com/GenomicsStandardsConsortium/mixs/",
      "responsible_organization": [
        "B2AI_ORG:38"
      ]
    },
    {
      "id": "B2AI_STANDARD:210",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MIAPPE",
      "description": "Minimum Information About Plant Phenotyping Experiments",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "minimuminformationschema"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A reporting guideline for plant phenotyping experiments. Comprises a checklist, i.e., a list of attributes that may be necessary to fully describe an experiment so that it is understandable and replicable. Should be consulted by people recording and depositing the data. Covers description of the following aspects of plant phenotyping experiment - study, environment, experimental design, sample management, biosource, treatment and phenotype. To read more, please visit http://cropnet.pl/phenotypes",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.miappe.org/"
    },
    {
      "id": "B2AI_STANDARD:211",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MIASM",
      "description": "Minimum Information About Somatic Mutation",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "minimuminformationschema"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:35"
      ],
      "purpose_detail": "MIASM was developed for the collection of somatic variations to promote standards for annotations of somatic variation data, and to promote data integration with other data resources.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://structure.bmc.lu.se/MIASM/miasm.html",
      "publication": "doi:10.1002/humu.20832"
    },
    {
      "id": "B2AI_STANDARD:212",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MIQAS",
      "description": "Minimum Information for QTLs and Association Studies",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "minimuminformationschema"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:35"
      ],
      "purpose_detail": "The MIQAS set of rules accompanied with the standardized XML and tab-delimited file formats will serve two goals - to encourage research groups that wish to publish a QTL paper to provide and submit the necessary information that would make meta-analysis possible and to allow easy interchange of data between different QTL and association analysis databases. Databases that implement the standardized XML format will typically write an import and an export filter to read data from and dump data into that an XML file. This is the same approach as used for the exchange of sequences between NCBI, Ensembl and DDBJ at the early stages of the Human Genome Project.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://miqas.sourceforge.net/"
    },
    {
      "id": "B2AI_STANDARD:213",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MIABIS",
      "description": "Minimum information required to initiate collaborations between biobanks",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "minimuminformationschema"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "MIABIS represents the minimum information required to initiate collaborations between biobanks and to enable the exchange of biological samples and data. The aim is to facilitate the reuse of bio-resources and associated data by harmonizing biobanking and biomedical research.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://doi.org/10.1089/bio.2015.0070",
      "publication": "doi:10.1089/bio.2015.0070"
    },
    {
      "id": "B2AI_STANDARD:214",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MIAPE-Quant",
      "description": "Minimum information required to report the use of quantification techniques in a proteomics experiment",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "minimuminformationschema"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:28"
      ],
      "purpose_detail": "This module identifies the minimum information required to report the use of quantification techniques in a proteomics experiment, sufficient to support both the effective interpretation and assessment of the data and the potential recreation of the results of the data analysis.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.psidev.info/attachments/miape-quant-091-documents",
      "responsible_organization": [
        "B2AI_ORG:41"
      ]
    },
    {
      "id": "B2AI_STANDARD:215",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "ModelCIF",
      "description": "ModelCIF",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:27"
      ],
      "purpose_detail": "An extension of the Protein Data Bank Exchange / macromolecular Crystallographic Information Framework (PDBx/mmCIF); provides an extensible data representation for deposition, archiving, and public dissemination of predicted 3D models of proteins.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://github.com/ihmwg/ModelCIF",
      "publication": "doi:10.1101/2022.12.06.518550",
      "formal_specification": "http://github.com/ihmwg/ModelCIF"
    },
    {
      "id": "B2AI_STANDARD:216",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MLHIM",
      "description": "Multilevel Healthcare Information Modeling specifications",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "multimodal"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The Multilevel Healthcare Information Modeling (MLHIM) specifications enables the exchange of syntactically and semantically interoperable data extracts between distributed, independently developed, biomedical databases and clinical applications, promoting syntactic and semantic integration of Translational Research data. The Semantic MedWeb is an implementation of a MLHIM-based database development platform (open source code available at https://github.com/mlhim/SemanticMedWeb)",
      "is_open": true,
      "requires_registration": false,
      "url": "https://mlhim-specifications.readthedocs.io/en/master/"
    },
    {
      "id": "B2AI_STANDARD:217",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MAF",
      "description": "Multiple Alignment Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:23"
      ],
      "purpose_detail": "The multiple alignment format stores a series of multiple alignments in a format that is easy to parse and relatively easy to read. This format stores multiple alignments at the DNA level between entire genomes. Previously used formats are suitable for multiple alignments of single proteins or regions of DNA without rearrangements, but would require considerable extension to cope with genomic issues such as forward and reverse strand directions, multiple pieces to the alignment, and so forth.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://genome.ucsc.edu/FAQ/FAQformat.html#format5",
      "responsible_organization": [
        "B2AI_ORG:119"
      ]
    },
    {
      "id": "B2AI_STANDARD:218",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "mz5",
      "description": "mz5 format",
      "related_to": [
        "B2AI_STANDARD:339"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The mz5 format is a complete reimplementation of the mzML data model and ontology built on the HDF5 (Hierarchical Data Format 5) storage backend, designed to address performance and scalability limitations of XML-based mass spectrometry formats. mz5 preserves the semantic structure and controlled vocabulary terms of mzML while leveraging HDF5's binary format, chunked storage, and native compression capabilities to achieve significantly faster read/write operations and reduced file sizes compared to mzML. The format organizes mass spectrometry data into hierarchical HDF5 groups and datasets representing spectra, chromatograms, instrument configurations, and metadata, with support for efficient random access to individual spectra and parallel I/O operations. mz5 maintains compatibility with the Proteomics Standards Initiative controlled vocabularies and supports the same rich metadata and data provenance as mzML, while providing superior performance for large-scale proteomics and metabolomics datasets. The HDF5 foundation enables integration with high-performance computing workflows, supports multiple programming languages (C, Python, Java, R), and facilitates efficient processing of high-resolution and high-throughput mass spectrometry data for downstream analysis pipelines and machine learning applications.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://doi.org/10.1074/mcp.O111.011379",
      "publication": "doi:10.1074/mcp.O111.011379",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:16"
      ]
    },
    {
      "id": "B2AI_STANDARD:219",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "mzData",
      "description": "mzData format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:28"
      ],
      "purpose_detail": "mzData is an XML format for representing mass spectrometry data in such a way as to completely describe the instrumental aspects of the experiment. This format is deprecated and has been superseded by mzML.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://psidev.info/mass-spectrometry-workgroup#mzdata",
      "responsible_organization": [
        "B2AI_ORG:41"
      ]
    },
    {
      "id": "B2AI_STANDARD:220",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "mzIndentML",
      "description": "mzIndentML format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:28"
      ],
      "purpose_detail": "A large number of different proteomics search engines are available that produce output in a variety of different formats. It is intended that mzIdentML will provide a common format for the export of identification results from any search engine. The format was originally developed under the name AnalysisXML as a format for several types of computational analyses performed over mass spectra in the proteomics context.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.psidev.info/mzidentml",
      "responsible_organization": [
        "B2AI_ORG:41"
      ]
    },
    {
      "id": "B2AI_STANDARD:221",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "mzML",
      "description": "mzML format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:28"
      ],
      "purpose_detail": "From 2005-2008 there has existed two separate XML formats for encoding raw spectrometer output, mzData developed by the PSI and mzXML developed at the Seattle Proteome Center at the Institute for Systems Biology. It was recognized that the existence of two separate formats for essentially the same thing generated confusion and required extra programming effort. Therefore the PSI, with full participation by ISB, has developed a new format by taking the best aspects of each of the precursor formats to form a single one. It is intended to replace the previous two formats. This new format was originally given a working name of dataXML. The final name is mzML.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://psidev.info/mzML",
      "publication": "doi:10.1007/978-1-60761-444-9_22",
      "formal_specification": "http://www.peptideatlas.org/tmp/mzML1.1.0.html",
      "responsible_organization": [
        "B2AI_ORG:41"
      ]
    },
    {
      "id": "B2AI_STANDARD:222",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "mzQuantML",
      "description": "mzQuantML format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:28"
      ],
      "purpose_detail": "The mzQuantML standard format is intended to store the systematic description of workflows quantifying molecules (principly peptides and proteins) by mass spectrometry. A large number of different software packages are available that produce output in a variety of different formats. It is intended that mzQuantML will provide a common format for the export of identification results from any software package. The format was originally developed under the name AnalysisXML as a format for several types of computational analyses performed over mass spectra in the proteomics context. It has been decided to split development into two formats, mzIdentML for peptide and protein identification and mzQuantML (described here), covering quantitative proteomic data derived from MS. The development of mzQuantML is driven by some general principles, specific use cases and the goal of supporting specific techniques, as listed below. These were discussed and agreed at the development meeting in Tubingen in July 2011.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.psidev.info/mzquantml",
      "responsible_organization": [
        "B2AI_ORG:41"
      ]
    },
    {
      "id": "B2AI_STANDARD:223",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "mzXML",
      "description": "mzXML format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "deprecated",
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:28"
      ],
      "purpose_detail": "mzXML was a pioneering open, generic XML (extensible markup language) representation specifically developed for mass spectrometry (MS) data exchange and storage in proteomics research. As one of the first standardized formats for MS data, mzXML provided a vendor-neutral way to represent complex mass spectrometry information including mass-to-charge ratios, intensity values, scan parameters, and instrument metadata in a structured, machine-readable format. The format supported both profile and centroid data representation modes and could accommodate various types of MS experiments including MS/MS fragmentation spectra. Despite its historical significance in establishing data standardization practices in proteomics, mzXML is now deprecated and has been superseded by the more comprehensive mzML format, which offers enhanced features, better compression, controlled vocabularies, and broader community support. While legacy systems may still encounter mzXML files, current best practices recommend migration to mzML for new data processing workflows and long-term data preservation in mass spectrometry applications.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://doi.org/10.1038/nbt1031",
      "publication": "doi:10.1038/nbt1031"
    },
    {
      "id": "B2AI_STANDARD:224",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "NCI EVS",
      "description": "National Cancer Institute Enterprise Vocabulary Service",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Terminology content, tools, and services to accurately code, analyze and share cancer and biomedical research, clinical and public health information. Includes NCI Thesaurus and Metathesaurus.",
      "is_open": true,
      "requires_registration": true,
      "url": "https://evs.nci.nih.gov/",
      "responsible_organization": [
        "B2AI_ORG:71"
      ]
    },
    {
      "id": "B2AI_STANDARD:225",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "NEMSIS",
      "description": "National Emergency Medical Services Information System",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:66"
      ],
      "purpose_detail": "Standard for the collection and transmission of emergency medical services (EMS) operations and patient care data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://nemsis.org/technical-resources/"
    },
    {
      "id": "B2AI_STANDARD:226",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "PCORNet CDM",
      "description": "National Patient-Centered Clinical Research Network Common Data Model",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "purpose_detail": "This guiding principle is expressed in the CDM design through prioritization of analytic functionality, and a parsimonious approach based upon analytic utility. At times, this results in decisions that are not based in relational database modeling principles such as normalization. The model is designed to facilitate routine and rapid execution of distributed complex analytics. To meet this design requirement, some fields are duplicated across multiple tables to support faster analytic operations for distributed querying. The PCORnet CDM is based on the FDA Mini-Sentinel CDM. This allows PCORnet to more easily leverage the large array of analytic tools and expertise developed for the MSCDM v4.0, including data characterization approaches and the various tools for complex distributed analytics.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://pcornet.org/pcornet-common-data-model/",
      "formal_specification": "https://pcornet.org/wp-content/uploads/2022/01/PCORnet-Common-Data-Model-v60-2020_10_221.pdf",
      "responsible_organization": [
        "B2AI_ORG:81"
      ]
    },
    {
      "id": "B2AI_STANDARD:227",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "NCD",
      "description": "Natural Collections Description standard",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:1"
      ],
      "purpose_detail": "Natural Collections Description (NCD) (A data standard for exchanging data describing natural history collections) is a proposed data standard for describing collections of natural history materials at the collection level; one NCD record describes one entire collection. Collection descriptions are electronic records that document the holdings of an organisation as groups of items, which complement the more traditional item-level records such as are produced for a single specimen or a library book. NCD is tailored to natural history. It lies between general resource discovery standards such as Dublin Core (DC) and rich collection description standards such as the Encoded Archival Description (EAD). The NCD standard covers all types of natural history collections, such as specimens, original artwork, archives, observations, library materials, datasets, photographs or mixed collections such as those that result from expeditions and voyages of discovery.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.tdwg.org/standards/ncd/",
      "responsible_organization": [
        "B2AI_ORG:93"
      ]
    },
    {
      "id": "B2AI_STANDARD:228",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "BioProject Schema",
      "description": "NCBI BioProject XML Schema",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:40",
          "category": "B2AI:Application",
          "name": "Research Project Metadata Mining and Dataset Discovery",
          "description": "BioProject schema is used in AI applications for mining metadata about biological research projects, enabling automated discovery of relevant datasets, prediction of experimental outcomes, and analysis of research trends. Machine learning systems parse BioProject records to identify related studies for meta-analyses, recommend similar projects to researchers, and predict which experimental approaches are likely to succeed based on project descriptions. AI applications leverage structured project metadata to train models that classify research projects by methodology, extract experimental design features, and identify collaborative networks in biological research. The schema enables large-scale scientometric analyses and AI-driven research prioritization based on historical project outcomes and resource allocation.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "datamodel"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:1"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:74"
      ],
      "purpose_detail": "This is a XML Schema specification of BioProject data. A BioProject is a collection of biological data related to a single initiative, originating from a single organization or from a consortium. A BioProject record provides users a single place to find links to the diverse data types generated for that project.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.ncbi.nlm.nih.gov/data_specs/schema/other/bioproject/",
      "publication": "doi:10.1093/nar/gkr1163",
      "formal_specification": "https://www.ncbi.nlm.nih.gov/data_specs/schema/other/bioproject/"
    },
    {
      "id": "B2AI_STANDARD:229",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "NCPDP F&B",
      "description": "NCPDP Formulary and Benefit Standard",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The NCPDP Formulary and Benefit Standard provides a standardized means for pharmacy benefit payers, including health plans and Pharmacy Benefit Managers (PBMs), to electronically communicate formulary and benefit information to prescribers via technology vendor systems. This standard enables real-time access to critical medication coverage information at the point of prescribing, including patient eligibility verification, product coverage status, benefit financial details (copayments, deductibles, out-of-pocket costs), coverage restrictions such as prior authorization requirements, step therapy protocols, quantity limits, and therapeutic alternatives when restrictions exist. By standardizing this data exchange, the standard reduces prescription abandonment, improves medication adherence, decreases prior authorization processing time, and enhances clinical decision-making by providing prescribers with transparent, actionable benefit information. The standard supports integration with Electronic Health Record (EHR) systems and e-prescribing platforms in accordance with HIPAA, MMA, HITECH, and Meaningful Use requirements, ultimately contributing to reduced healthcare costs and improved patient safety through more informed prescribing decisions.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://standards.ncpdp.org/Access-to-Standards.aspx",
      "responsible_organization": [
        "B2AI_ORG:63"
      ]
    },
    {
      "id": "B2AI_STANDARD:230",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "NESTcc DQF",
      "description": "NESTcc Data Quality Framework",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "purpose_detail": "Guiding principles and a foundation for the capture and use of high-quality data for post-market evaluation of medical devices",
      "is_open": true,
      "requires_registration": true,
      "url": "https://nestcc.org/data-quality-and-methods/",
      "formal_specification": "https://mdic.org/wp-content/uploads/2020/02/NESTcc-Data-Quality-Framework.pdf",
      "responsible_organization": [
        "B2AI_ORG:64"
      ]
    },
    {
      "id": "B2AI_STANDARD:231",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "NWB",
      "description": "Neurodata Without Borders",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Neurodata Without Borders (NWB) is a comprehensive data standard and software ecosystem for neurophysiology and behavioral data, developed by a collaborative team of neuroscientists and software developers to break down barriers to data sharing in neuroscience. NWB supports diverse neurophysiology data types including intracellular and extracellular electrophysiology (single-unit recordings, local field potentials, patch-clamp), optical physiology (calcium imaging, two-photon microscopy, optogenetics), behavioral tracking, stimulus presentation metadata, and experimental trial structures. The format is built on HDF5, providing efficient storage and access to large-scale datasets while maintaining human-readable metadata. NWB's extensibility mechanism through neurodata extensions allows researchers to adapt the standard for novel recording modalities and experimental paradigms without breaking compatibility. The ecosystem includes PyNWB and MatNWB APIs for data creation and access, validation tools, and integration with the DANDI Archive for public data sharing. NWB adoption is growing across major neuroscience initiatives with support from Allen Institute, HHMI, Kavli Foundation, Simons Foundation, and INCF, facilitating reproducible research, cross-laboratory data integration, and development of standardized analysis pipelines across cellular, systems, and computational neuroscience.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.nwb.org/",
      "publication": "doi:10.1101/523035",
      "formal_specification": "https://github.com/NeurodataWithoutBorders"
    },
    {
      "id": "B2AI_STANDARD:232",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "NIDM",
      "description": "Neuroimaging Data Model",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:41",
          "category": "B2AI:Application",
          "name": "Neuroimaging Results Sharing and Meta-Analytic AI",
          "description": "NIDM (NeuroImaging Data Model) is used in AI applications for sharing and aggregating neuroimaging analysis results across studies, enabling meta-analytic machine learning and improving reproducibility in computational neuroscience. AI systems leverage NIDM's semantic representation of imaging workflows, statistical maps, and analysis provenance to train models that learn from aggregated results rather than raw images, respecting data sharing constraints while enabling large-scale analyses. The model supports AI applications that perform automated quality assessment of imaging studies, detect inconsistencies in reported results, and synthesize findings across diverse analysis pipelines. NIDM enables machine learning systems to understand the complete analytical context of neuroimaging results, improving reproducibility and meta-analytic power.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "datamodel"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:22"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:99"
      ],
      "purpose_detail": "The Neuroimaging Data Model (NIDM) is a collection of specification documents and examples that outline a domain specific extension to the W3C Provenance Data Model (PROV-DM) for the exchange and sharing of human brain imaging data. The goal of the data model is to capture data, information about the data and processes that generated the data (i.e. provenance). This information can be converted to RDF and therefore queried using SPARQL. This representation allows machine accessible representations of brain imaging data and will provide links to related resources such as publications, virtual machines, people and funding agencies.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://nidm.nidash.org/"
    },
    {
      "id": "B2AI_STANDARD:233",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "NIFTI",
      "description": "Neuroimaging Informatics Technology Initiative file format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:22"
      ],
      "purpose_detail": "The Neuroimaging Informatics Technology Initiative (nifti) file format was envisioned about a decade ago as a replacement to the then widespread, yet problematic, analyze 7.5 file format. The main problem with the previous format was perhaps the lack of adequate information about orientation in space, such that the stored data could not be unambiguously interpreted. Although the file was used by many different imaging software, the lack of adequate information on orientation obliged some, most notably spm, to include, for every analyze file, an accompanying file describing the orientation, such as a file with extension .mat.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://nifti.nimh.nih.gov/",
      "formal_specification": "https://nifti.nimh.nih.gov/nifti-2"
    },
    {
      "id": "B2AI_STANDARD:234",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "NeuroML",
      "description": "NeuroML",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "NeuroML is a model description language developed in XML (extensible Markup Language) that was created to facilitate data archiving, data and model exchange, database creation, and model publication in the neurosciences. One of the goals of the NeuroML project is to develop standards for model specification that will allow for greater simulator interoperability and model exchange.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://neuroml.org/"
    },
    {
      "id": "B2AI_STANDARD:235",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "NDF",
      "description": "Neurophysiology Data Translation Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:48"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:11"
      ],
      "purpose_detail": "The purpose of the Neurophysiology Data Translation Format (NDF) is to provide a means of sharing neurophysiology experimental data and derived data between services and tools developed within the CARMEN project (www.carmen.org.uk). This document specifes the NDF. The specification supports the types of data that are currently used by members of the CARMEN consortium and provides a capability to support future data types. It is capable of accommodating external data file formats as well as metadata such as user defined experimental descriptions and the history (provenance) of derived data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://doi.org/10.3389/conf.fnins.2010.13.00118",
      "publication": "doi:10.3389/conf.fnins.2010.13.00118"
    },
    {
      "id": "B2AI_STANDARD:236",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "NHX",
      "description": "New Hampshire eXtended Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:26"
      ],
      "purpose_detail": "NHX is based on the New Hampshire (NH) standard (also called Newick tree format).",
      "is_open": true,
      "requires_registration": false,
      "url": "http://www.phylosoft.org/NHX/"
    },
    {
      "id": "B2AI_STANDARD:237",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Newick",
      "description": "Newick tree Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:26"
      ],
      "purpose_detail": "The Newick Standard for representing trees in computer-readable form makes use of the correspondence between trees and nested parentheses, noticed in 1857 by the famous English mathematician Arthur Cayley.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://evolution.genetics.washington.edu/phylip/newicktree.html"
    },
    {
      "id": "B2AI_STANDARD:238",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "NeML",
      "description": "NeXML format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:25",
        "B2AI_TOPIC:26"
      ],
      "purpose_detail": "To facilitate interoperability in evolutionary comparative analysis, we present NeXML, an XML standard (inspired by the current standard, NEXUS) that supports exchange of richly annotated comparative data. NeXML defines syntax for operational taxonomic units, character-state matrices, and phylogenetic trees and networks. Documents can be validated unambiguously. Importantly, any data element can be annotated, to an arbitrary degree of richness, using a system that is both flexible and rigorous. We describe how the use of NeXML by the TreeBASE and Phenoscape projects satisfies user needs that cannot be satisfied with other available file formats",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/nexml/nexml",
      "formal_specification": "https://github.com/nexml/nexml"
    },
    {
      "id": "B2AI_STANDARD:239",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "nib",
      "description": "Nibble sequence format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12"
      ],
      "purpose_detail": "The nibble (.nib) format is a compact binary file format for storing DNA sequences, originally developed for the UCSC Genome Browser and BLAT alignment tool. The format achieves 4-fold compression by packing two nucleotide bases per byte using 2-bit encoding (A=00, C=01, G=10, T=11), significantly reducing storage requirements compared to text-based FASTA format. Each .nib file contains a single sequence record with a simple header specifying sequence length and format version, followed by the packed sequence data. The format stores sequences in a form optimized for rapid access by genome browsers and alignment algorithms, supporting efficient memory mapping for large-scale genomic analyses. While .nib files provide space-efficient storage, they lack the flexibility of indexed formats like 2bit (which can store multiple sequences) and have been largely superseded by more modern compressed formats. The format remains in use for legacy applications and continues to be supported by UCSC Genome Browser utilities including nibFrag for sequence extraction and faToNib/nibToFa for format conversion, primarily for maintaining compatibility with older genome browser implementations and analysis pipelines.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://genomebrowser.wustl.edu/goldenPath/help/blatSpec.html"
    },
    {
      "id": "B2AI_STANDARD:240",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "NMR-STAR",
      "description": "NMR Self-defining Text Archive and Retrieval format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:3"
      ],
      "purpose_detail": "Format and ontology used to represent experiments, spectral and derived data, and supporting metadata.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://bmrb.io/standards/",
      "publication": "doi:10.1007/s10858-018-0220-3",
      "responsible_organization": [
        "B2AI_ORG:9"
      ]
    },
    {
      "id": "B2AI_STANDARD:241",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "nmrML",
      "description": "nmrML",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:17"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:21"
      ],
      "purpose_detail": "nmrML is an open mark-up language for NMR data. It is currently under heavy development and is not yet ready for public use. The development of this standard is coordinated by Workpackage 2 of the COSMOS - COordination Of Standards In MetabOlomicS Project. COSMOS is a global effort to enable free and open sharing of metabolomics data. Coordinated by Dr Christoph Steinbeck of the EMBL-European Bioinformatics Institute, COSMOS brings together European data providers to set and promote community standards that will make it easier to disseminate metabolomics data through life science e-infrastructures. This Coordination Action has been financed with 2 million by the European Commission's Seventh Framework Programme. The nmrML data standard will be approved by the Metabolomics Standards Initiative and was derived from an earlier nmrML that was developed by the Metabolomics Innovation Centre (TMIC).",
      "is_open": true,
      "requires_registration": false,
      "url": "https://nmrml.org/"
    },
    {
      "id": "B2AI_STANDARD:242",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Observ-Tab",
      "description": "Observ-Tab format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:25"
      ],
      "purpose_detail": "Observ-Tab is a simple spreadsheet format to represent and exchange phenotype data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://doi.org/10.1002/humu.22070",
      "publication": "doi:10.1002/humu.22070"
    },
    {
      "id": "B2AI_STANDARD:243",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "OMOP CDM",
      "description": "Observational Medical Outcomes Partnership Common Data Model",
      "related_to": [
        "B2AI_STANDARD:733",
        "B2AI_STANDARD:692",
        "B2AI_STANDARD:695",
        "B2AI_STANDARD:698",
        "B2AI_STANDARD:703"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:42",
          "category": "B2AI:Application",
          "name": "Multi-Database Patient-Level Risk Prediction with External Validation",
          "description": "OMOP-standardized features and the OHDSI Patient-Level Prediction (PLP) pipeline enabled development and extensive external validation of clinical risk models across international sites. A Lasso logistic regression model predicting symptomatic hemorrhagic transformation after ischemic stroke was developed on OMOP-mapped EHR and externally validated across 10 databases spanning the US, Europe, and Asia (internal AUC 0.75; mean external AUC approximately 0.71, range 0.60-0.78). OMOP's standardized covariate definitions and PLP tooling enabled identical feature extraction and cross-site code portability for reproducible, multi-database risk modeling.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1371/journal.pone.0226718"
          ]
        },
        {
          "id": "B2AI_APP:146",
          "category": "B2AI:Application",
          "name": "Federated Diabetes Heart Failure Risk Modeling",
          "description": "OMOP CDM enabled federated patient-level prediction for 1-year incident heart failure risk in type 2 diabetes patients across five US databases. Shared cohort and covariate definitions in OMOP, combined with OHDSI tools, enabled reproducible distributed model development and validation using multiple classifiers (Lasso logistic regression, Random Forest, Gradient Boosting, XGBoost), achieving external AUCs of approximately 0.72-0.80 across validation sites. OMOP's standardization facilitated consistent model portability without patient-level data sharing.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1371/journal.pone.0226718"
          ]
        },
        {
          "id": "B2AI_APP:147",
          "category": "B2AI:Application",
          "name": "Hospital Length-of-Stay Prediction with Explainable AI",
          "description": "OMOP v5.3-standardized features from condition, drug, procedure, and measurement tables supported operational length-of-stay prediction for planned hospital admissions using gradient-boosting methods (XGBoost, LightGBM) with SHAP explainability. A single-site OMOP implementation (South Korea) achieved internal AUROC up to 0.891, with external validation at a separate OMOP-mapped hospital yielding AUROC approximately 0.804. OMOP's standardized feature representation allowed reproducible training and external testing for operational forecasting.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1101/2024.08.23.24311950"
          ]
        },
        {
          "id": "B2AI_APP:148",
          "category": "B2AI:Application",
          "name": "Portable NLP-Based Clinical Phenotyping",
          "description": "A portable NLP phenotyping system stored NLP outputs and rule artifacts in OMOP tables (notes and annotation mapping) to enable cross-institutional reuse. The system combined rule-based extractions with statistical machine learning classifiers for phenotype identification (e.g., obesity and comorbidities) and demonstrated competitive performance on i2b2 challenge discharge summaries. OMOP's common schema for text-derived concepts enabled portable NLP pipelines and downstream machine learning for cohort discovery and trial recruitment across multiple sites.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1016/j.jbi.2023.104343"
          ]
        },
        {
          "id": "B2AI_APP:149",
          "category": "B2AI:Application",
          "name": "Clinical Knowledge Graphs for Explainable AI",
          "description": "OMOP CDM served as the data source for constructing clinical knowledge graphs in FHIR RDF format (FHIR-Ontop-OMOP) to support explainable AI workflows. OMOP's standardized vocabularies and relational structure enabled consistent semantic linking and query over patient-level data, transforming OMOP tables into RDF knowledge graphs that provide semantic features and enable explainable AI by linking standardized clinical concepts through ontological relationships.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1101/2024.08.23.24311950"
          ]
        },
        {
          "id": "B2AI_APP:150",
          "category": "B2AI:Application",
          "name": "Process Mining of Clinical Workflows",
          "description": "Methods to derive event logs from OMOP tables (visit, procedure, measurement) enabled process mining of inpatient, outpatient, and emergency workflows and patient care pathways. Real-world surgical cases at a tertiary hospital were analyzed to construct clinical pathway models, and artificial neural networks were demonstrated for pathway variance prediction. OMOP CDM provided a reproducible source for process-aware analytics and downstream predictive tasks from standardized event sequences.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1101/2024.08.23.24311950"
          ]
        },
        {
          "id": "B2AI_APP:151",
          "category": "B2AI:Application",
          "name": "Imaging AI Enablement via MI-CDM Extension",
          "description": "The Medical Imaging CDM (MI-CDM) extends OMOP with imaging metadata and feature-provenance tables to link DICOM data and imaging-derived biomarkers with clinical OMOP data, enabling multimodal phenotyping and imaging AI workflows. A prototype use case demonstrated longitudinal CT lung nodule tracking, and implementations for prostate cancer research (ProCAncer-I) captured imaging metadata and curation processes. MI-CDM makes image-derived features computable within OMOP, supporting reproducible imaging AI pipelines and phenotype definitions that include imaging biomarkers.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1007/s10278-024-00982-6"
          ]
        },
        {
          "id": "B2AI_APP:152",
          "category": "B2AI:Application",
          "name": "Cross-Country Model Generalizability and Feature Selection",
          "description": "OMOP-standardized features from EHRs mapped across the US, UK, Finland, and Korea enabled cross-site feature evaluation to improve external validity of prolonged opioid use prediction after surgery. Independent cross-site feature selection workflows using Lasso logistic regression improved generalizability, with local AUROC approximately 0.75 and averaged external AUROC approximately 0.69 after cross-site feature selection. OMOP's consistent feature representation enabled generalizable machine learning across countries and healthcare systems.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1101/2024.08.23.24311950"
          ]
        },
        {
          "id": "B2AI_APP:153",
          "category": "B2AI:Application",
          "name": "Oncology-Specific AI with Genomic and Imaging Vocabularies",
          "description": "OMOP oncology extensions incorporating genomic vocabularies (ClinVar, CIVic, OncoKB), HemOnc chemotherapy regimen vocabularies, and radiology CDM (R-CDM) with RadLex-mapped imaging tables enable cancer-specific AI applications. Use cases include case identification from clinical notes using support vector machines and tree-based models, predictive modeling with the ATLAS Patient-Level Prediction module on genomically-enriched OMOP data, and standardized imaging-AI workflows. OMOP's oncology-specific vocabularies and modules facilitate AI model development for precision oncology across harmonized multicenter datasets.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.3390/ijms231911834",
            "https://doi.org/10.1101/2024.08.23.24311950"
          ]
        }
      ],
      "collection": [
        "datamodel",
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:115",
        "B2AI_ORG:114"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:844"
      ],
      "purpose_detail": "The Observational Medical Outcomes Partnership (OMOP) Common Data Model is an open community data standard designed to standardize the structure and content of observational healthcare data from electronic health records, administrative claims, registries, and other sources to enable efficient, large-scale analyses that produce reliable evidence. OMOP CDM organizes patient-level data into standardized tables including person, visit, condition occurrence, drug exposure, procedure occurrence, measurement, observation, and device exposure, with relationships linking events to patients and care episodes. The model uses standardized vocabularies (SNOMED CT, RxNorm, LOINC, etc.) mapped through the OHDSI Vocabulary for semantic interoperability, ensuring consistent representation of clinical concepts across diverse healthcare systems. OMOP CDM supports longitudinal patient histories, enables reproducible network studies where the same analytic code runs on data from multiple institutions without sharing patient-level information, and provides the foundation for the OHDSI open-science community's analytical tools including cohort definition, characterization, population-level effect estimation, and patient-level prediction. The standardized schema facilitates federated learning, multi-site clinical research, comparative effectiveness studies, pharmacovigilance, and machine learning applications requiring harmonized features across heterogeneous EHR systems for training predictive models, phenotyping algorithms, and decision support tools.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://ohdsi.github.io/CommonDataModel/",
      "publication": "doi:10.3233/978-1-61499-564-7-574",
      "formal_specification": "https://github.com/OHDSI/CommonDataModel",
      "responsible_organization": [
        "B2AI_ORG:76"
      ]
    },
    {
      "id": "B2AI_STANDARD:244",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "OMOP CEM",
      "description": "Observational Medical Outcomes Partnership Common Evidence Model",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:43",
          "category": "B2AI:Application",
          "name": "Causal Inference and Observational Study AI",
          "description": "OMOP Common Evidence Model is used in AI applications for standardizing the representation of evidence from observational studies, enabling machine learning models to learn causal relationships from real-world data and generate reliable evidence for treatment effectiveness. AI systems leverage CEM's structured representation of study designs, populations, exposures, and outcomes to train models that estimate treatment effects from observational data, adjust for confounding using propensity score methods, and validate predictions through negative controls. The model enables AI applications in comparative effectiveness research, pharmacovigilance, and evidence synthesis where distinguishing correlation from causation is critical. Machine learning approaches use CEM to automate evidence quality assessment and synthesize findings across heterogeneous observational studies.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "datamodel"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:844"
      ],
      "purpose_detail": "An improved replacement to the previously reported LAERTES system. One of the initial uses of CEM has been its use in generating lists of negative control concepts to be used in empirical calibration.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/OHDSI/CommonEvidenceModel/wiki/Postprocessing-Negative-Controls",
      "publication": "doi:10.1007/s40264-014-0189-0",
      "formal_specification": "https://github.com/OHDSI/CommonEvidenceModel",
      "responsible_organization": [
        "B2AI_ORG:76"
      ]
    },
    {
      "id": "B2AI_STANDARD:245",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "OBO",
      "description": "Open Biomedical Ontology Flat File Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The Open Biomedical Ontology (OBO) Flat File Format is a human-readable, line-oriented syntax for representing biomedical ontologies that originated with the Gene Ontology and has become a widely adopted standard within the OBO Foundry community. OBO format provides a simplified, tag-value structure for defining ontology terms, their hierarchical relationships (is_a), logical definitions, synonyms, cross-references, and metadata, with each term enclosed in a [Term] stanza containing fields like id, name, namespace, def (text definition with citations), and relationship tags. The format has a defined mapping to OWL (Web Ontology Language), allowing OBO files to be converted to OWL/RDF for semantic web applications while maintaining a more accessible syntax for biologists and curators. OBO format supports rich relationship types beyond simple hierarchies (part_of, regulates, develops_from), structured synonym types (exact, broad, narrow, related), obsoletion workflows with replaced_by and consider tags, and cross-references to external databases. The format is designed for version control systems (line-oriented changes), supports modular ontology development through import statements, and enables community-driven collaborative ontology construction. OBO format files are processed by standard tools (ROBOT, Owltools, OWL API with OBO parser) and underpin hundreds of biomedical ontologies including GO, Uberon, ChEBI, Disease Ontology, and Cell Ontology. The format balances human readability for manual curation with machine-parseability for computational workflows, semantic reasoning, data annotation, and integration into knowledge graphs supporting AI/ML applications in biomedicine.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://owlcollab.github.io/oboformat/doc/GO.format.obo-1_4.html",
      "formal_specification": "https://owlcollab.github.io/oboformat/doc/obo-syntax.html"
    },
    {
      "id": "B2AI_STANDARD:246",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Open mHealth",
      "description": "Open mHealth",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:44",
          "category": "B2AI:Application",
          "name": "Mobile Health Data Integration for Behavioral AI",
          "description": "Open mHealth schemas are used in AI applications for standardizing and integrating data from wearable devices, mobile health apps, and patient-generated health data streams for behavioral pattern recognition, activity classification, and health prediction models. Machine learning systems leverage Open mHealth's JSON-based data schemas to process heterogeneous data from fitness trackers, sleep monitors, medication adherence apps, and symptom tracking tools, enabling AI models for real-time health monitoring, early disease detection, and personalized intervention recommendations. The standardized format facilitates training of deep learning models on multi-modal time-series data including heart rate, physical activity, sleep patterns, and self-reported symptoms, supporting applications in chronic disease management, mental health monitoring, and precision behavioral medicine.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:18"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:114"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:845",
        "B2AI_STANDARD:846",
        "B2AI_STANDARD:847",
        "B2AI_STANDARD:850"
      ],
      "purpose_detail": "Open mHealth (OMH) is a comprehensive ecosystem of data standards, schemas, and tools designed to make patient-generated health data from mobile devices, wearables, and sensors interoperable, meaningful, and actionable across healthcare and research applications. The OMH data standards provide JSON-based schemas for over 75 common health data types including physical activity (steps, distance, calories burned), vital signs (heart rate, blood pressure, body temperature), sleep metrics, body composition, medications, survey responses, and environmental data. Each schema includes metadata for data provenance (acquisition source, temporal relationship, user notes) enabling proper contextualization and interpretation. OMH tools include Shimmer for integrating data from diverse sources (Fitbit, Apple HealthKit, Google Fit, Withings, iHealth), libraries for data validation and storage, visualization components for detecting health patterns, and FHIR integration capabilities for aligning mobile health data with electronic health records. The platform supports diverse use cases including randomized controlled trials with standardized mobile data collection, remote patient monitoring programs, n-of-1 clinical trial analyses, machine learning algorithm development on normalized health datasets, and patient-reported outcomes integrated with biometric data. OMH is used by over 6,000 developers and health organizations including Cornell Tech, Kaiser Permanente, Stanford Medicine, UCSF, and Copenhagen Center for Health Technology, enabling research reproducibility and clinical care integration by transforming heterogeneous mobile health data into a unified, queryable format.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.openmhealth.org/",
      "formal_specification": "https://github.com/openmhealth"
    },
    {
      "id": "B2AI_STANDARD:247",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "OME-TIFF",
      "description": "Open Microscopy Environment TIFF specification",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:19"
      ],
      "purpose_detail": "OME-TIFF (Open Microscopy Environment TIFF) is a file format specification that combines standard TIFF image files with OME-XML metadata embedded in the TIFF header, enabling storage of complex multi-dimensional microscopy datasets including multi-channel fluorescence, z-stacks, time series, and multi-position acquisitions. Each OME-TIFF file contains one or more standard TIFF images with rich microscopy metadata encoded as OME-XML in the ImageDescription tag of the first IFD (Image File Directory), describing acquisition parameters, instrument settings, channel information, dimensions, physical pixel sizes, timestamps, and experimental context. The format supports large datasets by allowing data to span multiple TIFF files while maintaining metadata consistency through the OME-XML master file that references all constituent files. OME-TIFF balances the advantages of widespread TIFF support in image processing software with the semantic richness of OME-XML metadata, making microscopy data accessible to both OME-aware applications (Bio-Formats, OMERO, ImageJ/Fiji) and standard image viewers. This dual compatibility facilitates data sharing, long-term archiving, interoperability across microscopy platforms, and integration into computational workflows including machine learning pipelines for image analysis, segmentation, and feature extraction in biological imaging applications.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://docs.openmicroscopy.org/ome-model/5.6.3/ome-tiff/#",
      "responsible_organization": [
        "B2AI_ORG:77"
      ]
    },
    {
      "id": "B2AI_STANDARD:248",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "OME-XML",
      "description": "Open Microscopy Environment XML format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:19"
      ],
      "purpose_detail": "OME-XML (Open Microscopy Environment XML) is a comprehensive XML-based file format for storing both microscopy image pixels and associated metadata using the OME Data Model, a rich schema that describes biological imaging experiments with semantic precision. The format encodes multi-dimensional image data (x, y, z, channel, time) as Base64-encoded binary pixel arrays within XML elements, alongside extensive metadata including instrument configuration (microscope, objectives, detectors, light sources), acquisition parameters (exposure times, wavelengths, filters), experimental context (annotations, regions of interest, overlays), specimen information, and structured annotations. OME-XML uses a controlled vocabulary and hierarchical schema (defined by XSD) that ensures consistent representation of microscopy concepts across diverse imaging modalities including widefield, confocal, super-resolution, high-content screening, and light-sheet microscopy. The format serves as the foundation for OME-TIFF and is the native format for Bio-Formats library, enabling interoperability across proprietary microscope file formats through standardized conversion. OME-XML supports FAIR principles by providing rich, machine-readable metadata essential for data sharing, long-term preservation, reproducible analysis, and integration into computational workflows including image processing pipelines and machine learning applications requiring comprehensive contextual information about imaging experiments.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://docs.openmicroscopy.org/ome-model/5.6.3/ome-xml/",
      "publication": "doi:10.1186/gb-2005-6-5-r47",
      "responsible_organization": [
        "B2AI_ORG:77"
      ]
    },
    {
      "id": "B2AI_STANDARD:249",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "OMEX",
      "description": "Open Modeling EXchange format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:27"
      ],
      "purpose_detail": "OMEX (Open Modeling EXchange format) is a ZIP-based container file format developed by the COMBINE (Computational Modeling in Biology Network) community for bundling all components of a computational modeling and simulation experiment into a single, self-contained, exchangeable archive. An OMEX archive contains model descriptions (SBML, CellML, NeuroML, etc.), simulation experiment specifications (SED-ML for defining simulation protocols, analysis steps, and visualization), associated data files (initial conditions, parameters, experimental observations), metadata (OMEX Metadata describing provenance, authorship, annotations using RDF), and a manifest file that catalogs all contents with their roles and formats. The format ensures reproducibility by packaging model structure, simulation configuration, analysis workflows, and contextual information together, enabling complete recreation of computational experiments across different simulation tools and platforms. OMEX supports FAIR principles for computational models through standardized packaging, facilitates model exchange and reuse across the systems biology and computational physiology communities, enables archiving in model repositories (BioModels, PMR), and provides the foundation for reproducible in silico experiments essential for model validation, parameter estimation, and integration into larger computational workflows including systems biology pipelines and machine learning applications for biological modeling.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/combine-org/combine-specifications/blob/main/specifications/omex.md",
      "formal_specification": "http://co.mbine.org/specifications/omex.version-1.pdf",
      "responsible_organization": [
        "B2AI_ORG:19"
      ]
    },
    {
      "id": "B2AI_STANDARD:250",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "OpenICE",
      "description": "Open-Source Integrated Clinical Environment Standard",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "purpose_detail": "OpenICE is an initiative to create a community implementation of an Integrated Clinical Environment. The initiative encompasses not only software implementation but also an architecture for a wider clinical ecosystem to enable new avenues of clinical research. OpenICE seeks to integrate an inclusive framework of healthcare devices and clinical applications to existing Healthcare IT ecosystems.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.openice.info/",
      "formal_specification": "https://github.com/mdpnp/mdpnp",
      "responsible_organization": [
        "B2AI_ORG:54"
      ]
    },
    {
      "id": "B2AI_STANDARD:251",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "AOM14",
      "description": "openEHR Archetype Object Model",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:45",
          "category": "B2AI:Application",
          "name": "Clinical Archetypes and Semantic Interoperability for AI",
          "description": "AOM14 (Archetype Object Model) is used in AI applications for defining reusable clinical information models that enable semantic interoperability of health data across systems and support consistent feature extraction for machine learning. AI systems leverage archetypes to understand the clinical meaning and constraints of health data elements, enabling models to learn from data collected using openEHR archetypes across different implementations. The standardized clinical models support AI applications that require portable feature definitions, enable transfer learning across healthcare systems using archetypes, and ensure that AI models interpret clinical concepts consistently regardless of underlying database structures. Archetypes provide machine-readable clinical semantics that improve AI model interpretability and facilitate automated validation of model inputs.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "datamodel"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "This document contains the definitive statement of archetype semantics, in the form of an object model for archetypes. The AOM provides a comprehensive formal specification enabling software to process archetypes independent of their persistent representation (ADL, XML, or other formats). The model defines constraint-based domain entities through hierarchical structures alternating between object and attribute constraints (C_COMPLEX_OBJECT, C_ATTRIBUTE, C_PRIMITIVE_OBJECT classes). It supports archetype specialization with explicit depth tracking, composition through archetype slots with assertion-based constraints, and domain-specific extensions via C_DOMAIN_TYPE classes. The AOM includes comprehensive primitive type constraints (C_DATE, C_TIME, C_STRING, C_INTEGER), reference mechanisms (ARCHETYPE_INTERNAL_REF, CONSTRAINT_REF), and assertion capabilities using first-order predicate logic. It provides the semantic foundation for archetype-enabled kernels, ADL parsers, and archetype validation systems in clinical information modeling, enabling programmatic manipulation of constraint structures and serving as the API specification for archetype-based software applications.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://specifications.openehr.org/releases/AM/latest/AOM1.4.html",
      "formal_specification": "https://specifications.openehr.org/releases/AM/latest/AOM1.4.html",
      "responsible_organization": [
        "B2AI_ORG:79"
      ]
    },
    {
      "id": "B2AI_STANDARD:252",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "openEHR",
      "description": "openEHR Architecture",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "openEHR is the name of a technology for e-health, consisting of open specifications, clinical models and software that can be used to create standards, and build information and interoperability solutions for healthcare. The various artefacts of openEHR are produced by the openEHR community and managed by openEHR International, an international non-profit organisation originally established in 2003 and previously managed by the openEHR Foundation.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.openehr.org/about/what_is_openehr",
      "formal_specification": "https://specifications.openehr.org/releases/BASE/latest/architecture_overview.html",
      "responsible_organization": [
        "B2AI_ORG:79"
      ]
    },
    {
      "id": "B2AI_STANDARD:253",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "ORION",
      "description": "Outbreak Reports and Intervention Studies Of Nosocomial infection",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The quality of research in hospital epidemiology (infection control) must be improved to be robust enough to influence policy and practice. In order to raise the standards of research and publication, a CONSORT equivalent for these largely quasi-experimental studies has been prepared by the authors of two relevant systematic reviews undertaken for the HTA and the Cochrane Collaboration. The statement was revised following widespread consultation with learned societies, editors of journals and researchers. It consists of a 22 item checklist, and a summary table. The emphasis is on transparency to improve the quality of reporting and on the use of appropriate statistical techniques.The statement has been endorsed and welcomed by a number of professional special interest groups and societies including the Association of Medical Microbiologists (AMM), Bristish Society for Antimicrobial Chemotherapy (BSAC) and the Infection Control Nurses' Association (ICNA) Research and Development Group. Like CONSORT, ORION considers itself a work in progress, which requires ongoing dialogue for successful promotion and dissemination. The statement is therefore offered for further public discussion and journals are encouraged to trial it as part of their reviewing and editing process and feedback to the authors.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.ucl.ac.uk/antimicrobial-resistance/reporting-guidelines/orion-statement-consort-equivalent-infection-control-intervention-studies",
      "publication": "doi:10.1016/S1473-3099(07)70082-8",
      "formal_specification": "https://www.ucl.ac.uk/drupal/site_antimicrobial-resistance/sites/antimicrobial-resistance/files/checklist_authors.pdf"
    },
    {
      "id": "B2AI_STANDARD:254",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "ICE",
      "description": "Patient-Centric Integrated Clinical Environment Standard",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "purpose_detail": "This standard specifies the characteristics necessary for the safe integration of MEDICAL DEVICES and other equipment, via an electronic interface, from different MANUFACTURERS into a single medical system for the care of a single high acuity PATIENT. This standard establishes requirements for a medical system that is intended to have greater error resistance and improved PATIENT safety, treatment efficacy and workflow efficiency than can be achieved with independently used MEDICAL DEVICES.",
      "is_open": false,
      "requires_registration": false,
      "url": "https://mdpnp.org/mdice.html",
      "formal_specification": "https://www.astm.org/f2761-09r13.html",
      "responsible_organization": [
        "B2AI_ORG:54"
      ]
    },
    {
      "id": "B2AI_STANDARD:255",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "HMMER Format",
      "description": "Pfam / HMMER Profile file format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:26"
      ],
      "purpose_detail": "The profile HMM calculated from multiple sequnce alignment data in this service is stored in Profile HMM save format (usually with .hmm extension). It is an ASCII file containing a lot of header and descriptive records followed by large numerical matrix which holds probabilistic model of the motif. The file of this format is useful to search against sequnce databases to find out other proteins which share the same motif. This HMM file should not be edited manually (especially the matrix part) because it contains consistent numerical model as a whole.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://hmmer.org/",
      "formal_specification": "https://www.genome.jp/tools/motif/hmmformat.htm"
    },
    {
      "id": "B2AI_STANDARD:256",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Phenopackets",
      "description": "Phenopackets schema",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:46",
          "category": "B2AI:Application",
          "name": "ML-Ready Rare Disease Corpus for Benchmarking and Training",
          "description": "A curated corpus of 4,916 case-level phenopackets spanning 277 Mendelian and chromosomal diseases was released explicitly as an analysis-ready, AI-ready dataset to enable machine learning analyses of clinical phenotype data. The corpus supports gene and disease prioritization pipelines, patient stratification studies, and genotype-phenotype correlation analyses by providing standardized HPO-encoded phenotypes linked to genomic diagnoses. Phenopackets' uniform format enables benchmarking of diagnostic software performance, testing and tuning of algorithms on standardized inputs, and evaluation of ML models for rare disease genomic diagnostics across diverse disease presentations.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1101/2024.05.29.24308104"
          ]
        },
        {
          "id": "B2AI_APP:154",
          "category": "B2AI:Application",
          "name": "Phenotype-Driven Diagnostic Tools Integration",
          "description": "Widely used phenotype-driven diagnostic and gene/variant prioritization tools including Exomiser, LIRICAL, Phen2Gene, and CADA accept Phenopackets as standardized input files, enabling integration of patient-level HPO-encoded phenotypes with genomic data for AI-enabled diagnostics. Phenopackets facilitate computational pipeline integration by providing a consistent format for representing clinical observations, supporting aggregation across sites, and enabling interoperability with electronic health record (EHR) systems and rare disease registries. This standardization allows diagnostic AI tools to process patient data from diverse sources using a common schema, improving diagnostic accuracy and enabling federated diagnostic workflows.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1101/2021.11.27.21266944",
            "https://doi.org/10.1038/s41587-022-01357-4"
          ]
        },
        {
          "id": "B2AI_APP:155",
          "category": "B2AI:Application",
          "name": "Clinical Annotation System with Enhanced ML Performance",
          "description": "SAMS (Symptom Annotation Made Simple), an HPO-integrated clinical annotation system that imports and exports Phenopackets, demonstrated measurable improvements in data quality that benefit downstream AI/ML pipelines. SAMS reported a 10% increase in recall for scientific publication annotations and a 20% increase in recall for EHR-derived annotations through improved entity linking and standardized symptom representation. These quality improvements in structured phenotype data directly enhance the performance of machine learning models that rely on high-quality, standardized clinical phenotypes for training and inference in diagnostic and research applications.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1093/nar/gkad1005"
          ]
        },
        {
          "id": "B2AI_APP:156",
          "category": "B2AI:Application",
          "name": "Large-Scale Federated Rare Disease Data Sharing",
          "description": "Within the Solve-RD consortium, Phenopackets support standardized clinical data sharing for large federated cohorts with 11,349+ individuals (growing to over 19,000), facilitating AI/ML-ready data integration for rare disease diagnostics and research. The European Joint Programme on Rare Diseases (EJP RD) utilizes Phenopackets for federated data discovery across rare disease resources while adhering to FAIR principles. Phenopackets enable distributed analyses, patient matchmaking services, and cohort identification across international sites without requiring patient-level data sharing, providing a computable substrate for training and validating machine learning models on multi-institutional rare disease datasets.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1101/2021.11.27.21266944"
          ]
        },
        {
          "id": "B2AI_APP:157",
          "category": "B2AI:Application",
          "name": "Genotype-Phenotype Correlation Analytics",
          "description": "The GPSEA (GenoPheno Statistical Evidence Assessment) framework uses Phenopackets to represent adverse phenotype data and compute genotype-phenotype correlations across 6,613 individuals spanning 85 cohorts. Phenopackets' standardized representation of patient-level phenotypes, variants, and biosample information enables systematic computational analysis of genotype-phenotype relationships, supporting downstream AI/ML analytics for variant interpretation, phenotype prediction from genotypes, and discovery of novel genotype-phenotype associations. The format facilitates aggregation of case-level data from diverse sources into analysis-ready datasets that machine learning models can use to learn patterns between genetic variants and clinical presentations.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1101/2024.05.29.24308104"
          ]
        },
        {
          "id": "B2AI_APP:158",
          "category": "B2AI:Application",
          "name": "Patient Matchmaking and Cohort Discovery Systems",
          "description": "Phenopackets serve as the computable representation underlying data-driven patient matchmaking services that connect individuals with similar phenotypes across international rare disease networks for clinical research, trial recruitment, and collaborative diagnosis. The standardized format enables AI-powered differential diagnosis systems and automated cohort identification tools that match patient phenotypes to disease profiles, identify suitable clinical trial candidates, and discover similar cases in distributed databases. Phenopackets' consistent encoding of clinical observations using HPO terms allows machine learning algorithms to compute phenotypic similarity scores, cluster patients by presentation, and support precision medicine initiatives through data-driven patient stratification across global rare disease registries.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.17863/cam.87963",
            "https://doi.org/10.1038/s41587-022-01357-4"
          ]
        }
      ],
      "collection": [
        "datamodel"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:58"
      ],
      "purpose_detail": "Phenopackets is a schema for exchanging computable representations of patient clinical phenotypes, diseases, genomic information, and associated metadata in a standardized, machine-readable format. The schema uses Protocol Buffers (protobuf) and JSON to encode clinical observations as Human Phenotype Ontology (HPO) terms, diseases as OMIM/MONDO identifiers, age and temporal information, biosample details, genomic interpretations (variants, genes), pedigree information, measurements, medical actions, and provenance metadata. Phenopackets supports multiple use cases including individual patient records, family pedigrees, and cohort descriptions, enabling seamless exchange of phenotypic data between clinical systems, research databases, and diagnostic platforms. The format bridges clinical phenotyping with genomics by linking patient phenotypes to genetic variants, facilitating variant interpretation, genotype-phenotype correlation studies, and rare disease diagnosis. Phenopackets promotes FAIR principles for clinical data through standardized vocabularies (HPO, LOINC, UCUM), supports federated analysis across rare disease registries without sharing raw patient data, enables AI/ML applications for automated phenotyping and diagnostic support, and provides the foundation for international data sharing initiatives including matchmaking services that connect patients with similar phenotypes for clinical research and trial recruitment.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://phenopackets.org/",
      "formal_specification": "https://github.com/phenopackets/phenopacket-schema",
      "responsible_organization": [
        "B2AI_ORG:34"
      ]
    },
    {
      "id": "B2AI_STANDARD:257",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "PHIN Guide",
      "description": "PHIN Messaging Guide for Syndromic Surveillance Emergency Department, Urgent Care, Inpatient and Ambulatory Care Settings",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:40"
      ],
      "purpose_detail": "An HL7 messaging and content reference standard for national, syndromic surveillance electronic health record technology certification; A basis for local and state syndromic surveillance messaging implementation guides; A resource for planning for the increasing use of electronic health record technology and for providing details on health data elements that may become a part of future public health syndromic surveillance messaging requirements; Optional elements of interest for adding laboratory results to syndromic surveillance messages using ORU^R01 message structure (see details in the PHIN messaging Standard, National Condition Reporting case Notification, ORU^R01 message Structure Specification profile, Version 2.1, 2014)",
      "is_open": true,
      "requires_registration": false,
      "url": "https://knowledgerepository.syndromicsurveillance.org/hl7-version-251-phin-messaging-guide-syndromic-surveillance-emergency-department-urgent-care-and",
      "formal_specification": "https://www.cdc.gov/nssp/documents/guides/syndrsurvmessagguide2_messagingguide_phn.pdf"
    },
    {
      "id": "B2AI_STANDARD:258",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "phyloXML",
      "description": "phyloXML",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5",
        "B2AI_TOPIC:12"
      ],
      "purpose_detail": "phyloXML is a specialized XML-based language for representing phylogenetic trees and networks, along with rich associated metadata. It provides standardized elements for encoding taxonomic information, gene names, sequence identifiers, branch lengths, support values, gene duplication and speciation events, and other evolutionary attributes. The extensible structure of phyloXML enables interoperability between evolutionary biology and comparative genomics tools, supporting both simple and complex tree annotations. Its schema allows for domain-specific extensions and integration with other bioinformatics resources, making it a widely adopted standard for sharing, visualizing, and analyzing phylogenetic data in research and database applications.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://www.phyloxml.org/",
      "publication": "doi:10.1186/1471-2105-10-356"
    },
    {
      "id": "B2AI_STANDARD:259",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "ANSI/CTA-2056",
      "description": "Physical Activity Monitoring for Fitness Wearables Step Counting",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:18"
      ],
      "purpose_detail": "ANSI/CTA-2056 is a voluntary consensus standard developed by the Consumer Technology Association (CTA) that establishes standardized definitions, testing methodologies, and minimum performance criteria for measuring step counting accuracy on consumer wearable devices and smartphone applications used for physical activity monitoring and health tracking. The standard defines a \"step\" as a single stride during human locomotion and specifies validation protocols using controlled laboratory testing on treadmills at various speeds (slow walking 2.0 mph, normal walking 3.0 mph, brisk walking 4.0 mph) as well as free-living wear tests in real-world conditions to evaluate device performance across diverse user populations and activity patterns. ANSI/CTA-2056 requires manufacturers to report step counting accuracy as mean absolute percentage error (MAPE) and specifies acceptable error thresholds - devices should achieve less than 10% error for controlled walking tests and less than 20% error for free-living conditions to meet the standard's performance benchmarks. The standard addresses measurement challenges including false positive step detection from upper body movements, vehicle vibrations, and daily living activities, as well as false negative errors from slow shuffling gaits or irregular walking patterns. ANSI/CTA-2056 promotes transparency by requiring clear disclosure of test conditions, participant demographics, reference measurement systems (ActiGraph accelerometers, manual observation), and statistical analysis methods used for validation. The standard facilitates comparability across commercial fitness trackers, smartwatches, and pedometer applications from manufacturers like Fitbit, Apple Watch, Garmin, and Samsung, supporting consumer informed decision-making, clinical research applications requiring validated activity metrics, and workplace wellness programs utilizing step count goals for employee health initiatives.",
      "is_open": true,
      "requires_registration": true,
      "url": "https://webstore.ansi.org/standards/ansi/cta20562016ansi",
      "responsible_organization": [
        "B2AI_ORG:4"
      ]
    },
    {
      "id": "B2AI_STANDARD:260",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "PEP",
      "description": "Portable Encapsulated Project specification",
      "related_to": [
        "B2AI_STANDARD:761"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:1"
      ],
      "purpose_detail": "The Portable Encapsulated Project (PEP) specification is a standardized, machine-readable format for organizing and describing biological sample metadata and associated computational analysis configurations in data-intensive bioinformatics projects. PEP uses a YAML-based configuration file (project_config.yaml) that points to a sample annotation table (typically CSV or TSV) containing sample attributes, along with optional subsample tables for complex hierarchical relationships. The specification defines a formal structure for representing sample metadata with arbitrary attributes, derived attributes computed from other columns, sample modifiers for conditional processing, and implied attributes inferred from organizational context. PEP supports amendments for alternative project configurations, subanotations for linking multiple data files to single samples, and project-level metadata including descriptions, keywords, and namespace information. The format is designed to make projects portable across compute environments, reproducible through version-controlled configurations, and interoperable across analysis tools through a growing ecosystem of PEP-compatible software (looper, pypiper, pepr, geofetch, pephub). PEP's structured metadata representation enables systematic queries across sample collections, facilitates batch processing and parallelization of analyses, supports complex experimental designs with multiple data types per sample, and provides a foundation for FAIR data practices in genomics and multi-omics research. The specification is particularly valuable for managing large-scale projects with hundreds or thousands of samples, enabling metadata-driven workflow execution, automated data retrieval, and standardized documentation that supports reproducible computational research and machine learning applications requiring well-annotated training datasets.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://pep.databio.org/",
      "publication": "doi:10.1093/gigascience/giab077",
      "formal_specification": "https://github.com/pepkit/pepspec"
    },
    {
      "id": "B2AI_STANDARD:261",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "PFB",
      "description": "Portable Format for Bioinformatics",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:1"
      ],
      "purpose_detail": "A self-describing serialized format for bulk biomedical data called the Portable Format for Biomedical (PFB) data. The Portable Format for Biomedical data is based upon Avro and encapsulates a data model, a data dictionary, the data itself, and pointers to third party controlled vocabularies. In general, each data element in the data dictionary is associated with a third party controlled vocabulary to make it easier for applications to harmonize two or more PFB files.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://anvilproject.org/ncpi/technologies#portable-format-for-bioinformatics-pfb",
      "publication": "doi:10.1101/2022.07.19.500678",
      "formal_specification": "https://github.com/uc-cdis/pypfb"
    },
    {
      "id": "B2AI_STANDARD:262",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "PRISMA",
      "description": "Preferred Reporting Items for Systematic Reviews and Meta-Analyses",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "An evidence-based minimum set of items for reporting in systematic reviews and meta-analyses.The aim of the PRISMA Statement is to help authors improve the reporting of systematic reviews and meta-analyses. We have focused on randomized trials, but PRISMA can also be used as a basis for reporting systematic reviews of other types of research, particularly evaluations of interventions. PRISMA may also be useful for critical appraisal of published systematic reviews, although it is not a quality assessment instrument to gauge the quality of a systematic review.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.prisma-statement.org/",
      "publication": "doi:10.1136/bmj.n71"
    },
    {
      "id": "B2AI_STANDARD:263",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "PDB",
      "description": "Protein Data Bank Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:27"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:82"
      ],
      "purpose_detail": "An exchange format for reporting experimentally determined three-dimensional structures of biological macromolecules that serves a global community of researchers, educators, and students. The data contained in the archive include atomic coordinates, bibliographic citations, primary and secondary structure, information, and crystallographic structure factors and NMR experimental data",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.cgl.ucsf.edu/chimera/docs/UsersGuide/tutorials/pdbintro.html"
    },
    {
      "id": "B2AI_STANDARD:264",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "PSI-PAR",
      "description": "Proteomics Standards Initiative Protein Affinity Reagent format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:26"
      ],
      "purpose_detail": "The work on PSI-PAR was initiated as part of the ProteomeBinders project and carried out by EMBL-EBI and the PSI-MI work group. The Proteomics Standards Initiative (PSI) aims to define community standards for data representation in proteomics to facilitate data comparison, exchange and verification. For detailed information on all PSI activities, please see PSI Home Page. The PSI-PAR format is a standardized means of representing protein affinity reagent data and is designed to facilitate the exchange of information between different databases and/or LIMS systems. PSI-PAR is not a proposed database structure. The PSI-PAR format consists of the PSI-MI XML2.5 schema (originally designed for molecular interactions) and the PSI-PAR controlled vocabulary. In addition, PSI-PAR documentation and examples are available on this web page. The scope of PSI-PAR is PAR and target protein production and characterization.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.psidev.info/psi-par",
      "publication": "doi:10.1074/mcp.M900185-MCP200",
      "responsible_organization": [
        "B2AI_ORG:41"
      ]
    },
    {
      "id": "B2AI_STANDARD:265",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "PSI GelML",
      "description": "PSI GelML",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:26"
      ],
      "purpose_detail": "GelML is a data exchange format for describing the results of gel electrophoresis experiments. GelML is developed as a HUPO-PSI working group.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.psidev.info/gelml/1.0",
      "responsible_organization": [
        "B2AI_ORG:41"
      ]
    },
    {
      "id": "B2AI_STANDARD:266",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "PSI-MI XML",
      "description": "PSI-MI XML",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:28"
      ],
      "purpose_detail": "The PSI-MI XML 2.5 is a community standard for molecular interactions which has been jointly developed by major data providers (BIND, CellZome, DIP, GSK, HPRD, Hybrigenics, IntAct, MINT, MIPS, Serono, U. Bielefeld, U. Bordeaux, U. Cambridge, and others).This format is stable and used for several years now - published in October 2007 (Broadening the Horizon  Level 2.5 of the HUPO-PSI Format for Molecular Interactions; Samuel Kerrien et al. BioMed Central. 2007.), it has been adapted for many different usages. It can be used for storing any kind of molecular interaction data - complexes and binary interactions not only protein-protein interactions, can describe nucleic acids interactions and others hierarchical complexes modelling by using interactionRef in participants instead of an interactor Data representation in PSI-MI 2.5 XML relies heavily on the use of controlled vocabularies. They can be accessed easily via the Ontology Lookup Service, PSI-MI, PSI-MOD.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.psidev.info/mif",
      "responsible_organization": [
        "B2AI_ORG:41"
      ]
    },
    {
      "id": "B2AI_STANDARD:267",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "qcML",
      "description": "qcML format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:28"
      ],
      "purpose_detail": "An XML format for quality-related data of mass spectrometry and other high-throughput experiments. Quality control is increasingly recognized as a crucial aspect of mass spectrometry based proteomics. Several recent papers discuss relevant parameters for quality control and present applications to extract these from the instrumental raw data. What has been missing, however, is a standard data exchange format for reporting these performance metrics. We therefore developed the qcML format, an XML-based standard that follows the design principles of the related mzML, mzIdentML, mzQuantML, and TraML standards from the HUPO-PSI (Proteomics Standards Initiative). In addition to the XML format, we also provide tools for the calculation of a wide range of quality metrics as well as a database format and interconversion tools, so that existing LIMS systems can easily add relational storage of the quality control data to their existing schema. We here describe the qcML specification, along with possible use cases and an illustrative example of the subsequent analysis possibilities. All information about qcML is available at http://code.google.com/p/qcml",
      "is_open": true,
      "requires_registration": false,
      "url": "https://doi.org/10.1074/mcp.M113.035907",
      "publication": "doi:10.1074/mcp.M113.035907"
    },
    {
      "id": "B2AI_STANDARD:268",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "RDML",
      "description": "Real-time PCR Data Markup Language",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:33"
      ],
      "purpose_detail": "The RDML file format is developed by the RDML consortium (http://www.rdml.org) and can be used free of charge. The RDML file format was created to encourage the exchange, publication, revision and re-analysis of raw qPCR data. The core of an RDML file is an experiment, not a PCR run. Therefore all the information is collected which is required to understand an experiment. The structure of the file format was inspired by a database structure. In the file are several master elements, which are then referred to in other parts of the file. This structure allows to reduce the amount of redundant information and encourages the user to provide useful information. The Real-time PCR Data Markup Language (RDML) is a structured and universal data standard for exchanging quantitative PCR (qPCR) data. The data standard should contain sufficient information to understand the experimental setup, re-analyse the data and interpret the results. The data standard is a compressed text file in Extensible Markup Language (XML) and enables transparent exchange of annotated qPCR data between instrument software and third-party data analysis packages, between colleagues and collaborators, and between authors, peer reviewers, journals and readers. To support the public acceptance of this standard, both an on-line RDML file generator is available for end users, as well as RDML software libraries to be used by software developers, enabling import and export of RDML data files.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://www.rdml.org"
    },
    {
      "id": "B2AI_STANDARD:269",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "REFLECT",
      "description": "Reporting guidelines for randomized controlled trials for livestock and food safety",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "REFLECT (Reporting guidElines For randomized controLled trials for livEstoCk and food safeTy) is a specialized, evidence-based minimum set of 22 reporting items specifically designed to improve the transparency, reproducibility, and quality of randomized controlled trials in livestock production, animal health, and food safety research. This comprehensive reporting guideline addresses the unique challenges and complexities inherent in agricultural and veterinary research, including both field trials conducted in real-world farm settings and controlled challenge studies in laboratory environments. REFLECT covers essential aspects of trial design, implementation, and reporting including study population characteristics, randomization procedures, intervention details, outcome measurements, statistical analyses, and results presentation. The guideline is specifically tailored for trials evaluating therapeutic or preventive interventions that impact production outcomes, animal health parameters, and food safety measures. Available in both MS Word and PDF formats, REFLECT serves as a dynamic, evolving document that is periodically updated as new evidence emerges, helping researchers, editors, reviewers, and regulatory agencies ensure that livestock and food safety trials are reported with sufficient detail for proper scientific evaluation and evidence-based decision making.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://meridian.cvm.iastate.edu/reflect/",
      "publication": "doi:10.4315/0362-028x-73.3.579"
    },
    {
      "id": "B2AI_STANDARD:270",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "REMARK",
      "description": "Reporting recommendations for tumour Marker prognostic studies",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Despite years of research and hundreds of reports on tumor markers in oncology, the number of markers that have emerged as clinically useful is pitifully small. Often initially reported studies of a marker show great promise, but subsequent studies on the same or related markers yield inconsistent conclusions or stand in direct contradiction to the promising results. It is imperative that we attempt to understand the reasons why multiple studies of the same marker lead to differing conclusions. A variety of methodological problems have been cited to explain these discrepancies. Unfortunately, many tumor marker studies have not been reported in a rigorous fashion, and published articles often lack sufficient information to allow adequate assessment of the quality of the study or the generalizability of study results. The development of guidelines for the reporting of tumor marker studies was a major recommendation of the National Cancer Institute-European Organisation for Research and Treatment of Cancer (NCI-EORTC) First International Meeting on Cancer Diagnostics in 2000. As for the successful CONSORT initiative for randomized trials and for the STARD statement for diagnostic studies, we suggest guidelines to provide relevant information about the study design, preplanned hypotheses, patient and specimen characteristics, assay methods, and statistical analysis methods. In addition, the guidelines provide helpful suggestions on how to present data and important elements to include in discussions. The goal of these guidelines is to encourage transparent and complete reporting so that the relevant information will be available to others to help them to judge the usefulness of the data and understand the context in which the conclusions apply.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.equator-network.org/reporting-guidelines/reporting-recommendations-for-tumour-marker-prognostic-studies-remark/",
      "publication": "doi:10.1038/sj.bjc.6602678",
      "formal_specification": "https://www.equator-network.org/wp-content/uploads/2016/10/REMARK-checklist-for-EQUATOR-website-002.docx"
    },
    {
      "id": "B2AI_STANDARD:271",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "ReproSchema",
      "description": "ReproSchema",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:31"
      ],
      "purpose_detail": "A common schema that encodes how the different elements of assessment data and / or the metadata relate to one another.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.repronim.org/reproschema/",
      "formal_specification": "https://github.com/ReproNim/reproschema"
    },
    {
      "id": "B2AI_STANDARD:272",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "SCDM",
      "description": "Sentinel Common Data Model",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:47",
          "category": "B2AI:Application",
          "name": "FDA Sentinel System and Distributed AI for Drug Safety",
          "description": "Sentinel Common Data Model is used in AI applications for active surveillance of medical product safety across distributed healthcare databases, enabling privacy-preserving machine learning for adverse event detection and risk assessment. AI systems leverage SCDM's standardization of claims data, electronic health records, and registries to develop models that identify safety signals, predict adverse events, and characterize treatment patterns across the FDA Sentinel System network. The common data model enables federated learning where AI algorithms execute locally at each data partner without sharing patient-level data, supporting rapid assessment of emerging safety concerns. Machine learning applications use SCDM to train models on massive populations for rare adverse event detection and comparative safety analysis.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "datamodel"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "A standard data structure that allows Sentinel Data Partners to quickly execute distributed programs against local data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.sentinelinitiative.org/methods-data-tools/sentinel-common-data-model",
      "formal_specification": "https://dev.sentinelsystem.org/projects/SCDM/repos/sentinel_common_data_model/browse",
      "responsible_organization": [
        "B2AI_ORG:89"
      ]
    },
    {
      "id": "B2AI_STANDARD:273",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "SAM",
      "description": "Sequence Alignment/Map Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The Sequence Alignment/Map (SAM) format is a TAB-delimited text format for storing sequence alignments against reference sequences, developed as part of the samtools project and maintained by the GA4GH Large Scale Genomics work stream. SAM consists of an optional header section (beginning with @ symbols) containing metadata about reference sequences, read groups, programs, and comments, followed by an alignment section with one line per aligned read. Each alignment line contains 11 mandatory fields including query name (QNAME), bitwise FLAG encoding mapping properties, reference sequence name (RNAME), 1-based leftmost mapping position (POS), mapping quality (MAPQ), CIGAR string describing alignment operations, mate pair information (RNEXT, PNEXT, TLEN), sequence (SEQ), and ASCII-encoded base quality scores (QUAL). The format supports optional fields as TAG:TYPE:VALUE triplets for storing additional information such as edit distance, alternative alignments, and aligner-specific metadata. SAM serves as the text-based companion to the binary BAM and compressed CRAM formats, with bidirectional conversion tools (samtools view) enabling interoperability. The format accommodates various alignment types including mapped, unmapped, secondary, supplementary, and chimeric alignments with proper-pair relationships. SAM files are human-readable for debugging and inspection, widely supported by aligners (BWA, Bowtie2, STAR), variant callers, and genomics toolkits, forming the foundation for standardized sequencing data exchange in next-generation sequencing workflows.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://samtools.sourceforge.net/",
      "publication": "doi:10.1093/bioinformatics/btp352",
      "responsible_organization": [
        "B2AI_ORG:34"
      ]
    },
    {
      "id": "B2AI_STANDARD:274",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "SRA-XML",
      "description": "Sequence Read Archive Metadata XML",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:48",
          "category": "B2AI:Application",
          "name": "Sequence Data Metadata Mining and Dataset Discovery",
          "description": "SRA-XML (Sequence Read Archive metadata format) is used in AI applications for automated mining of experimental metadata from genomic studies, enabling dataset discovery, quality assessment, and training of models that learn from experimental design information. Machine learning systems parse SRA metadata to identify relevant datasets for specific research questions, predict data quality issues before download, and extract experimental conditions that inform downstream analysis. AI applications leverage structured metadata to train models for automated experiment type classification, sample relationship inference, and detection of metadata quality issues. The format enables large-scale meta-analyses where AI systems integrate findings across thousands of sequencing experiments by understanding their experimental context.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "datamodel"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:13",
        "B2AI_TOPIC:33"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:74"
      ],
      "purpose_detail": "SRA-XML is an XML-based metadata format used by the Sequence Read Archive (SRA) and the European Nucleotide Archive (ENA) to describe and exchange metadata for raw sequencing data submissions from next-generation sequencing platforms. The format captures detailed information about sequencing experiments, sample attributes, library preparation, instrument models, and data files, enabling standardized data submission, validation, and integration across international repositories. SRA-XML supports the reproducibility and discoverability of sequencing datasets by providing a structured, machine-readable representation of experimental context and provenance, facilitating large-scale genomics research and data sharing.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.ncbi.nlm.nih.gov/sra/docs/submitmeta/",
      "publication": "doi:10.1093/nar/gkq1019"
    },
    {
      "id": "B2AI_STANDARD:275",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "SOFT",
      "description": "Simple Omnibus Format in Text",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Simple Omnibus Format in Text (SOFT) is a line-based, plain text format originally developed by NCBI Gene Expression Omnibus (GEO) for batch submission and download of genomic data. Though GEO discontinued accepting SOFT submissions in early 2024, all records remain available for download in SOFT format. The format uses four line types distinguished by first-character markers - caret lines (^) indicate entity types (PLATFORM, SAMPLE, SERIES), bang lines (!) specify entity attributes as label-value pairs, hash lines (#) describe data table headers, and data lines contain tab-delimited table rows. A single SOFT file can concatenate multiple Platform (GPL), Sample (GSM), and Series (GSE) records with their data tables and descriptive metadata. Platform tables require unique row identifiers and trackable sequence identifiers (GenBank/RefSeq accessions, clone IDs, oligo sequences) with standard headers for sequences, organism source, and various accession types. Sample tables must include ID_REF columns matching Platform identifiers and VALUE columns containing normalized, comparable measurements (scaled signals for single-channel or log ratios for dual-channel data). The format is compatible with common spreadsheet and database applications and supports MIAME standards for comprehensive data interpretation.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.ncbi.nlm.nih.gov/geo/info/soft-seq.html"
    },
    {
      "id": "B2AI_STANDARD:276",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "SMILES",
      "description": "Simplified Molecular Input Line Entry Specification Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:3"
      ],
      "purpose_detail": "A typographical line notation for specifying chemical structure.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://opensmiles.org/",
      "formal_specification": "http://opensmiles.org/opensmiles.html"
    },
    {
      "id": "B2AI_STANDARD:277",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "SED-ML",
      "description": "Simulation Experiment Description Markup Language",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "SED-ML is an XML-based format for encoding simulation setups, to ensure exchangeability and reproducibility of simulation experiments. It follows the requirements defined in the MIASE guidelines.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://sed-ml.org/"
    },
    {
      "id": "B2AI_STANDARD:278",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "SPDI",
      "description": "SPDI data model",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:49",
          "category": "B2AI:Application",
          "name": "Unambiguous Variant Representation for Clinical AI",
          "description": "SPDI (Sequence-Position-Deletion-Insertion) notation is used in AI applications for creating precise, unambiguous representations of genetic variants that eliminate nomenclature inconsistencies affecting model training and clinical interpretation. AI systems leverage SPDI's standardized four-element format to normalize variant representations from diverse sources (clinical labs, research databases, literature), enabling consistent feature engineering for machine learning models predicting variant pathogenicity. The notation resolves ambiguities in variant left-normalization and reference sequence specification that can cause the same biological variant to appear as different features to AI models, improving model accuracy and reproducibility. SPDI is particularly valuable for clinical AI systems that must reconcile variants reported in different formats across laboratories.",
          "used_in_bridge2ai": false,
          "references": [
            "https://www.ncbi.nlm.nih.gov/variation/notation/"
          ]
        }
      ],
      "collection": [
        "datamodel"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:13",
        "B2AI_TOPIC:35"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:74"
      ],
      "purpose_detail": "Sequence variant data model. Represents all variants as a sequence of four operations. Start at the boundary before the first nucleotide in the sequence S, advance P nucleotides, delete D nucleotides, then Insert the nucleotides in the string.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://doi.org/10.1093/bioinformatics/btz856",
      "publication": "doi:10.1093/bioinformatics/btz856"
    },
    {
      "id": "B2AI_STANDARD:279",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "SEDS",
      "description": "Standard EEG Data Structure",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:37"
      ],
      "purpose_detail": "A new and more flexible data structure, named the Standard EEG Data Structure (SEDS), was proposed to meet the needs of both small-scale EEG data batch processing in single-site studies and large-scale EEG data sharing and analysis in single-/multisite studies (especially on cloud platforms).",
      "is_open": false,
      "requires_registration": false,
      "url": "https://doi.org/10.1016/j.softx.2021.100933",
      "publication": "doi:10.1016/j.softx.2021.100933"
    },
    {
      "id": "B2AI_STANDARD:280",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "SPREC",
      "description": "Standard PREanalytical Code",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "codesystem"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "The Standard PREanalytical Code (SPREC) is a comprehensive coding system developed by the ISBER Biospecimen Science Working Group to standardize documentation of pre-analytical variables that affect biospecimen quality. SPREC uses a seven-element alphanumeric code to capture critical factors including specimen type, primary container, time to processing (warm and cold ischemia), centrifugation parameters (speed and temperature), and long-term storage conditions. This systematic encoding enables biobanks and biorepositories to consistently track and communicate how specimens were collected, processed, and preserved, which directly impacts molecular analyte stability and experimental reproducibility. By providing a standardized language for pre-analytical variation, SPREC facilitates data harmonization across biobanks, enables quality comparisons between specimen collections, supports regulatory compliance, and enhances the value of biological samples for translational research. The code is particularly valuable for multi-center studies where specimen provenance must be tracked and controlled. SPREC version 3.0 covers various specimen types including blood, tissue, urine, and other biological fluids, with specific codes for derivatives like DNA, RNA, and proteins.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.isber.org/page/SPREC",
      "publication": "doi:10.1089/bio.2017.0109",
      "responsible_organization": [
        "B2AI_ORG:48"
      ]
    },
    {
      "id": "B2AI_STANDARD:281",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "STARD",
      "description": "Standards for Reporting of Diagnostic Accuracy Studies",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The objective of the STARD initiative is to improve the accuracy and completeness of reporting of studies of diagnostic accuracy, to allow readers to assess the potential for bias in the study (internal validity) and to evaluate its generalisability (external validity).The STARD statement consist of a checklist of 25 items and recommends the use of a flow diagram which describe the design of the study and the flow of patients.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.equator-network.org/reporting-guidelines/stard/",
      "publication": "doi:10.1136/bmjopen-2016-012799"
    },
    {
      "id": "B2AI_STANDARD:282",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Stockholm",
      "description": "Stockholm Multiple Alignment Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:26"
      ],
      "purpose_detail": "The Stockholm format is a comprehensive system for marking up and annotating features in multiple sequence alignments, widely used by HMMER, Pfam, and Belvu bioinformatics tools. The format supports detailed annotation of secondary structure (SS), surface accessibility (SA), transmembrane regions (TM), posterior probability (PP), ligand binding sites (LI), active sites (AS), and intron positions (IN). The format includes structured headers with STOCKHOLM 1.0 identifier, sequence alignment blocks with name/start-end notation, and comprehensive markup capabilities supporting database references, organism classification, phylogenetic trees in New Hampshire format, and various structural and functional annotations. It provides flexible annotation of aligned sequences with exact one-character-per-column markup for positional features and free-text annotations for sequence and file-level metadata.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://sonnhammer.sbc.su.se/Stockholm.html"
    },
    {
      "id": "B2AI_STANDARD:283",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "STREGA",
      "description": "Strengthening the Reporting of Genetic Association studies",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Making sense of rapidly evolving evidence on genetic associations is crucial to making genuine advances in human genomics and the eventual integration of this information in the practice of medicine and public health. Assessment of the strengths and weaknesses of this evidence, and hence the ability to synthesize it, has been limited by inadequate reporting of results. The STrengthening the REporting of Genetic Association studies (STREGA) initiative builds on the Strengthening the Reporting of Observational Studies in Epidemiology (STROBE) Statement and provides additions to 12 of the 22 items on the STROBE checklist. The additions concern population stratification, genotyping errors, modelling haplotype variation, Hardy-Weinberg equilibrium, replication, selection of participants, rationale for choice of genes and variants, treatment effects in studying quantitative traits, statistical methods, relatedness, reporting of descriptive and outcome data, and the volume of data issues that are important to consider in genetic association studies. The STREGA recommendations do not prescribe or dictate how a genetic association study should be designed but seek to enhance the transparency of its reporting, regardless of choices made during design, conduct, or analysis.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.equator-network.org/reporting-guidelines/strobe-strega/",
      "publication": "doi:10.1002/gepi.20410"
    },
    {
      "id": "B2AI_STANDARD:284",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "STROBE",
      "description": "Strengthening the Reporting of Observational studies in Epidemiology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "STROBE stands for an international, collaborative initiative of epidemiologists, methodologists, statisticians, researchers and journal editors involved in the conduct and dissemination of observational studies, with the common aim of STrengthening the Reporting of OBservational studies in Epidemiology. The STROBE Statement is being endorsed by a growing number of biomedical journals. Incomplete and inadequate reporting of research hampers the assessment of the strengths and weaknesses of the studies reported in the medical literature. Readers need to know what was planned (and what was not), what was done, what was found, and what the results mean. Recommendations on the reporting of studies that are endorsed by leading medical journals can improve the quality of reporting.Observational research comprises several study designs and many topic areas. We aimed to establish a checklist of items that should be included in articles reporting such research - the STROBE Statement. We considered it reasonable to initially restrict the recommendations to the three main analytical designs that are used in observational research - cohort, case-control, and cross-sectional studies. We want to provide guidance on how to report observational research well. Our recommendations are not prescriptions for designing or conducting studies. Also, the checklist is not an instrument to evaluate the quality of observational research.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.strobe-statement.org/",
      "publication": "doi:10.1016/j.jclinepi.2007.11.008",
      "responsible_organization": [
        "B2AI_ORG:91"
      ]
    },
    {
      "id": "B2AI_STANDARD:285",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "SDF",
      "description": "Structure Data Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:27"
      ],
      "purpose_detail": "SDF is one of a family of chemical-data file formats developed by MDL; it is intended especially for structural information. SDF stands for structure-data file, and SDF files actually wrap the molfile (MDL Molfile) format.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikipedia.org/wiki/Chemical_table_file#SDF"
    },
    {
      "id": "B2AI_STANDARD:286",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "SummarizedExperiment",
      "description": "SummarizedExperiment container",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "The SummarizedExperiment container contains one or more assays, each represented by a matrix-like object of numeric or other mode. The rows typically represent genomic ranges of interest and the columns represent samples.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://bioconductor.org/packages/release/bioc/html/SummarizedExperiment.html",
      "formal_specification": "https://github.com/Bioconductor/SummarizedExperiment",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:40"
      ]
    },
    {
      "id": "B2AI_STANDARD:287",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "SBOL",
      "description": "Synthetic Biology Open Language",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:1"
      ],
      "purpose_detail": "Standard for in silico representation of genetic designs. SBOL is designed to allow synthetic biologists and genetic engineers to electronically exchange designs . Send and receive genetic designs to and from biofabrication centers. Facilitate storage of genetic designs in repositories Embed genetic designs in publications.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://sbolstandard.org/",
      "formal_specification": "https://sbolstandard.org/#specifications"
    },
    {
      "id": "B2AI_STANDARD:288",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "SBGN",
      "description": "Systems Biology Graphical Notation",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:1"
      ],
      "purpose_detail": "Systems Biology Graphical Notation (SBGN) is a standardized visual language for representing biological processes and relationships in maps. SBGN defines the precise meaning of all graphical symbols through three complementary languages - Process Description (PD) for biochemical reactions and molecular interactions, Activity Flow (AF) for the flow of information between biochemical entities, and Entity Relationship (ER) for relationships between biological entities independent of temporal aspects. Each language has specific symbols, syntax rules, and semantic definitions to ensure unambiguous interpretation across different tools and users. SBGN-ML, an XML-based exchange format, enables storage and transfer of SBGN diagrams between software applications, supported by the standard LibSBGN library. The notation is widely adopted in major biological databases including Reactome, PANTHER Pathway, BioModels, and Pathway Commons. Multiple software tools support SBGN diagram creation and editing (CellDesigner, Newt Editor, VANTED/SBGN-ED, PathVisio, yEd), format conversion from KEGG, BioPAX, and SBML, and visualization of pathway models, facilitating standardized communication of complex biological knowledge across the systems biology community.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://sbgn.github.io/",
      "publication": "doi:10.1038/nbt.1558",
      "formal_specification": "https://sbgn.github.io/downloads/specifications/pd_level1_version2.pdf"
    },
    {
      "id": "B2AI_STANDARD:289",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "SBML",
      "description": "Systems Biology Markup Language",
      "related_to": [
        "B2AI_STANDARD:829"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:1"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:19"
      ],
      "purpose_detail": "The Systems Biology Markup Language (SBML) is a comprehensive XML-based exchange format specifically designed for representing computational models of biological processes in a machine-readable, standardized form. Developed and maintained by the international SBML community, SBML enables interoperability between diverse systems biology software tools by providing a common model representation language that eliminates translation errors and ensures consistent model interpretation across different platforms. While SBML excels at representing biochemical reaction networks, metabolic pathways, gene regulatory networks, and signal transduction cascades, its flexible architecture supports modeling of various biological phenomena from molecular to cellular scales. The format includes sophisticated features for describing reaction kinetics, species concentrations, compartmentalization, parameter sensitivity, and dynamic behaviors through mathematical expressions and differential equations. SBML's modular design supports extensions for specialized modeling requirements including spatial modeling, flux balance analysis, and multi-scale simulations. Supported by over 270 software tools, SBML serves as the de facto standard for systems biology model exchange, enabling reproducible research, model sharing, database integration, and collaborative development in computational systems biology.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://sbml.org/",
      "publication": "doi:10.1515/jib-2017-0081"
    },
    {
      "id": "B2AI_STANDARD:290",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "SBRML",
      "description": "Systems Biology Results Markup Language",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:1"
      ],
      "purpose_detail": "Systems Biology has benefited tremendously from the development and use of SBML, which is a markup language to specify models composed of molecular species, and their interactions (including reactions). SBML is a common format that many systems biology software understand and thus it has become the way in which models are shared and communicated. Despite the popularity of SBML, that has resulted in many models being available in electronic format, there is currently no standard way of communicating the results of the operations carried out with such models (e.g. simulations). Here we propose a new markup language which is complementary to SBML and which is intended to specify results from operations carried out on models SBRML. In fact, this markup language is useful also to communicate experimental data as long as it is possible to express the data in terms of a reference SBML model. Thus SBRML is a means of specifying quantitative results in the context of a systems biology model.",
      "is_open": false,
      "requires_registration": false,
      "url": "https://sbrml.sourceforge.net/SBRML/Welcome.html"
    },
    {
      "id": "B2AI_STANDARD:291",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Tabix",
      "description": "Tabix index file format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "A tab-delimited genome position index file format. The format can handle individual chromosomes up to 512 Mbp (2^29 bases) in length.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://www.htslib.org/doc/tabix.html",
      "publication": "doi:10.1093/bioinformatics/btq671",
      "formal_specification": "https://samtools.github.io/hts-specs/tabix.pdf"
    },
    {
      "id": "B2AI_STANDARD:292",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "TCS",
      "description": "Taxonomic Concept Transfer Schema",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:50",
          "category": "B2AI:Application",
          "name": "Taxonomic Data Integration and Species Classification AI",
          "description": "TCS (Taxonomic Concept Schema) is used in AI applications for managing taxonomic concepts and classifications, enabling machine learning models to understand species relationships, resolve taxonomic synonyms, and integrate biodiversity data across different classification systems. AI systems leverage TCS to train models that automatically map species names across taxonomies, predict taxonomic placement of newly discovered organisms, and reconcile conflicting classifications. The schema supports natural language processing applications that extract taxonomic information from scientific literature, automated quality control of species occurrence records, and machine learning approaches to phylogenetic inference. TCS enables AI systems to reason about taxonomic hierarchies and evolutionary relationships when analyzing biological data.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "datamodel"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:1"
      ],
      "purpose_detail": "The development of an abstract model for a taxonomic concept, which can capture the various models represented and understood by the various data providers, is central to this project. This model is presented as an XML schema document that is proposed as a standard to allow exchange of data between different data models. It aims to capture data as understood by the data owners without distortion, and facilitate the query of different data resources according to the common schema model. The TCS schema was conceived to allow the representation of taxonomic concepts as defined in published taxonomic classifications, revisions and databases. As such, it specifies the structure for XML documents to be used for the transfer of defined concepts. Valid transfer documents may either explicitly detail the defining components of taxon concepts, transfer GUIDs referring to defined taxon concepts (if and when these are available) or a mixture of the two.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://www.tdwg.org/standards/117",
      "formal_specification": "https://github.com/tdwg/tcs",
      "responsible_organization": [
        "B2AI_ORG:93"
      ]
    },
    {
      "id": "B2AI_STANDARD:293",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "TDWG SDS",
      "description": "Taxonomic Databases Working Group Standards Documentation Standard",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "This document defines how TDWG standards should be presented. Each standard is a logical directory or folder containing two or more files - a cover page outlining basic meta data for the standard and one or more normative files specifying the standard itself. Rules are specified for the naming of standards and files. Human readable files should be in English, follow basic layout principles and be marked up in XHTML. The legal statements that all documents must contain are defined.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://www.tdwg.org/standards/147",
      "responsible_organization": [
        "B2AI_ORG:93"
      ]
    },
    {
      "id": "B2AI_STANDARD:294",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "ToxML",
      "description": "ToxML standard",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:3"
      ],
      "purpose_detail": "ToxML is an open data exchange standard that allows the representation and communication of toxicological and related data in a well-structured electronic format.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://doi.org/10.1080/1062936X.2013.783506",
      "publication": "doi:10.1080/1062936X.2013.783506"
    },
    {
      "id": "B2AI_STANDARD:295",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "TREND",
      "description": "Transparent Reporting of Evaluations with Nonrandomized Designs",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Evidence-based public health decisions are based on evaluations of intervention studies with randomized and nonrandomized designs. Transparent reporting is crucial for assessing the validity and efficacy of these intervention studies, and, it facilitates synthesis of the findings for evidence-based recommendations. Therefore, the mission of the Transparent Reporting of Evaluations with Nonrandomized Designs (TREND) group is to improve the reporting standards of nonrandomized evaluations of behavioral and public health intervention",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.cdc.gov/trendstatement/index.html",
      "publication": "doi:10.2105/ajph.94.3.361",
      "formal_specification": "https://www.cdc.gov/trendstatement/pdf/trendstatement_TREND_Checklist.pdf",
      "responsible_organization": [
        "B2AI_ORG:12"
      ]
    },
    {
      "id": "B2AI_STANDARD:296",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "TIM+",
      "description": "Trusted Instant Messaging Plus Applicability Statement",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:26"
      ],
      "purpose_detail": "Trusted Instant Messaging (TIM+) defines a protocol that facilitates real-time communication and incorporates secure messaging concepts to ensure information is transmitted securely between known, trusted entities both within and across enterprises. TIM+ will determine the availability or presence of trusted endpoints and support text-based communication and file transfers.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://directtrust.app.box.com/s/p3vp3g4bv52cyi4nbpyfpdal6t7z2qdz",
      "formal_specification": "https://directtrust.app.box.com/s/p3vp3g4bv52cyi4nbpyfpdal6t7z2qdz"
    },
    {
      "id": "B2AI_STANDARD:297",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "TumorML",
      "description": "TumorML standard",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "purpose_detail": "Originally developed as part of the FP7 Transatlantic Tumor Model Repositories project, TumorML has been developed as an XML-based domain-specific vocabulary that includes elements from existing vocabularies, to deal with storing and transmitting existing cancer models among research communities.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://doi.org/10.1145/2544063.2544064",
      "publication": "doi:10.1145/2544063.2544064"
    },
    {
      "id": "B2AI_STANDARD:298",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "UNII",
      "description": "Unique Ingredient Identifiers",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "codesystem"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:3"
      ],
      "purpose_detail": "Unique Ingredient Identifiers (UNIIs) are alphanumeric codes generated by the FDA's Global Substance Registration System (GSRS) to uniquely identify substances based on their scientific identity characteristics using ISO 11238 data elements. Each UNII is derived from a substance's molecular structure and/or descriptive information, ensuring that the same substance always receives the same identifier regardless of when or where it is registered in the regulatory lifecycle. UNIIs enable efficient and accurate exchange of substance information across regulatory submissions, product labeling, adverse event reporting, and drug databases without ambiguity from varying nomenclature or trade names. The system covers diverse substance types including chemical compounds, proteins, nucleic acids, polymers, mixtures, structurally diverse materials, and specified substances (salts, stereoisomers, defined mixtures). UNIIs are generated at any time in the regulatory process and do not imply FDA review or approval. The GSRS database provides searchable access to UNIIs along with associated substance names, codes (CAS, INN, EC), structural representations, and attribute data. UNIIs are extensively used in SPL (Structured Product Labeling) documents, NDC (National Drug Code) directory, FAERS (FDA Adverse Event Reporting System), and international regulatory systems, facilitating interoperability between FDA databases and harmonization with global substance identification standards.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://precision.fda.gov/uniisearch",
      "responsible_organization": [
        "B2AI_ORG:31"
      ]
    },
    {
      "id": "B2AI_STANDARD:299",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "VCF",
      "description": "Variant Call Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:51",
          "category": "B2AI:Application",
          "name": "Variant Pathogenicity Prediction and Genomic AI",
          "description": "VCF format is essential for AI applications in genomics, particularly for training machine learning models to predict variant pathogenicity, disease associations, and functional impacts. Deep learning models parse VCF files to extract genetic variants and their annotations for tasks such as rare disease diagnosis, cancer genome interpretation, and pharmacogenomic prediction. AI systems leverage VCF's structured representation of genomic variants, quality scores, and population frequencies to build models that integrate variant-level data with clinical phenotypes, enabling precision medicine applications and automated variant classification according to ACMG guidelines.",
          "used_in_bridge2ai": false
        },
        {
          "id": "B2AI_APP:248",
          "category": "B2AI:Application",
          "name": "DeepVariant and Cohort Joint-Calling Pipelines",
          "description": "DeepVariant, a convolutional neural network-based variant caller using Inception-derived image classification architecture, produces genomic Variant Call Format (gVCF) files as primary output for per-sample variant calling, with gVCFs serving as inputs to GLnexus for scalable cohort joint-calling that merges single-sample gVCFs into population-scale cohort VCF files enabling downstream phasing (Eagle2), imputation (Beagle 5.0), and population genetics analyses. DeepVariant's gVCF quantization scheme yields substantially smaller file sizes compared to GATK HaplotypeCaller gVCFs (reducing storage costs and I/O bottlenecks in large-scale sequencing projects), while maintaining high accuracy with genotype quality (GQ) scores stored in VCF FORMAT fields guiding downstream filtering and quality control. The cohort VCF outputs serve as inputs to machine-learning-based variant quality score recalibration (GATK VQSR) which applies Gaussian mixture models to distinguish true variants from technical artifacts based on annotation features extracted from VCF INFO fields (mapping quality, strand bias, depth, allele balance), demonstrating VCF as the central data structure connecting deep learning variant calling, statistical recalibration, cohort aggregation, and reference panel construction. Phased cohort VCFs are converted into reference panels for genotype imputation enabling cost-effective genome-wide association studies where low-coverage or array-genotyped samples are imputed to high-density variant catalogs, with imputation accuracy evaluated by concordance metrics comparing imputed VCF genotypes against high-coverage truth VCFs. This workflow demonstrates VCF's role as the canonical interchange format linking deep learning inference (DeepVariant CNN processing aligned reads to emit probabilistic genotype calls), scalable merging algorithms (GLnexus joint-calling across thousands of samples), ML-driven quality filtering (VQSR iterative refinement), and population genetics tooling (phasing/imputation pipelines), supporting large biobanks (UK Biobank, All of Us, gnomAD) where millions of samples require consistent VCF-based variant representation enabling cross-study meta-analyses and polygenic risk score development.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1101/2020.02.10.942086"
          ]
        },
        {
          "id": "B2AI_APP:249",
          "category": "B2AI:Application",
          "name": "DeepTrio Family-Based Variant Calling with Mendelian Correction",
          "description": "The dv-trio pipeline implements family-based (trio) variant calling using DeepVariant deep neural network outputs, consuming per-sample gVCF files produced by DeepVariant and assembling them into trio VCF files with joint genotype calls for parent-offspring trios, then applying Mendelian-error-aware correction to refine genotypes and reduce false positives by leveraging inheritance constraints. The pipeline uses VCF genotype (GT) and genotype quality (GQ) fields to compute Mendelian errors (offspring genotypes inconsistent with parental genotypes under Mendelian inheritance), filters candidate errors based on GQ thresholds, and adjusts genotype calls or marks low-confidence sites, achieving approximately 60% reduction in Mendelian error rate compared to standard DeepVariant single-sample calling without compromising sensitivity for true de novo variants. An alternative approach (dv-gatk) creates DeepVariant gVCFs for each family member and merges them using GATK4 GenotypeGVCFs to produce co-called trio VCFs, demonstrating flexibility in combining deep learning variant calling with traditional joint-calling methods via the standardized gVCF intermediate format. Benchmarking is performed using Illumina hap.py tool comparing trio VCF outputs against GIAB (Genome in a Bottle) truth sets, with precision/recall/F1 metrics computed separately for SNVs and indels, and Mendelian concordance rates reported across multiple trios (validation on Ashkenazi Jewish trio, Chinese trio, and synthetic pedigrees). The workflow highlights VCF's role as the data structure enabling integration of deep learning inference (DeepVariant CNN-based genotyping), family-based correction logic (Mendelian filtering operating on VCF genotype fields), joint-calling algorithms (GATK or GLnexus merging gVCFs), and standardized benchmarking (hap.py comparing VCF outputs to truth VCFs), supporting clinical genetics applications where accurate de novo variant detection is critical for diagnosing rare genetic disorders in parent-offspring trios and enabling trio-based research studies investigating germline mutation rates and inheritance patterns.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1093/bioinformatics/btaa116"
          ]
        },
        {
          "id": "B2AI_APP:250",
          "category": "B2AI:Application",
          "name": "Deep Learning Variant Callers Producing VCF Outputs",
          "description": "Multiple deep learning architectures for variant calling from sequencing reads produce VCF files as standardized output format, enabling interoperability with downstream analysis pipelines and demonstrating VCF's role as the canonical representation for ML-predicted genetic variants. HELLO implements problem-specific deep neural networks tailored for small-variant calling (substitutions and indels) with architectures smaller than standard Inception-based models, achieving improved indel accuracy while producing VCF records with allele-specific annotations and quality scores. DeepSV applies deep convolutional neural networks to structural variant detection, specifically focusing on deletion calling, where the model visualizes aligned reads in fixed-size windows, trains CNNs to distinguish true deletions from alignment artifacts, and outputs VCF records with structural variant annotations (SVTYPE=DEL, SVLEN specifying deletion length, END position) conforming to VCF 4.2 structural variant specification, reducing false-positive rates compared to traditional read-depth and split-read callers. Clairvoyante (and its successor Clair) implements multi-task convolutional neural networks predicting variant type, zygosity, and alternate alleles simultaneously across multiple sequencing technologies (Illumina short reads, PacBio/Nanopore long reads), outputting VCF files with platform-appropriate quality annotations and supporting both small variants and complex events, with model architecture processing read pileup images and outputting per-position variant probabilities encoded as VCF genotype likelihoods (PL field). These tools collectively demonstrate convergence on VCF as the standard output format regardless of underlying neural architecture (problem-specific networks, CNNs, multi-task models), variant type (SNVs, indels, structural variants), or sequencing technology (short-read, long-read, single-molecule), with VCF's flexible INFO and FORMAT fields accommodating model-specific annotations (e.g., neural network confidence scores, alternative allele rankings, read support metrics) while maintaining compatibility with standard VCF parsers and downstream tools (variant annotation via VEP/ANNOVAR, variant filtering, genotype merging, benchmarking against truth sets), supporting the genomics community's need for consistent data interchange formats enabling fair comparison of ML-based callers, ensemble calling strategies combining predictions from multiple models, and integration into clinical sequencing pipelines requiring validated VCF outputs for diagnostic reporting.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1186/s40246-022-00396-x"
          ]
        },
        {
          "id": "B2AI_APP:251",
          "category": "B2AI:Application",
          "name": "DeepPVP Phenotype-Driven Variant Prioritization",
          "description": "DeepPVP implements a deep neural network for phenotype-driven prioritization of candidate causative variants, accepting an individual's whole-genome or whole-exome VCF file together with Human Phenotype Ontology (HPO) terms describing the patient's clinical phenotype and mode of inheritance (autosomal dominant/recessive, X-linked), then ranking all variants in the VCF by predicted causality and returning a prioritized list with the top candidate(s) for diagnostic follow-up. The model was trained and evaluated using synthetic patient VCFs created by inserting known ClinVar pathogenic variants into 1000 Genomes Project population VCF backgrounds (creating artificial cases with known causative variants at various allele frequencies), with evaluation also performed on a real whole-genome VCF from the Personal Genome Project containing 4,120,185 variants, demonstrating scalability to large VCF files. Performance benchmarking shows DeepPVP processing the whole-genome VCF in approximately 85 minutes (~1.3 milliseconds per variant) compared to 189 minutes for Exomiser (without CADD precomputation) and 800 minutes for Exomiser with CADD, highlighting computational efficiency advantages of the deep learning approach. The deep neural network architecture outperforms a random forest baseline (DeepPVP-RF) and existing tools (PVP, Exomiser, CADD, DANN, GWAVA) on metrics including top-hit recovery rate, top-10 recovery rate, ROC AUC, and precision-recall AUC across different inheritance modes and variant frequencies, with detailed performance breakdowns reported for de novo, autosomal recessive homozygous/compound heterozygous, and autosomal dominant variants. The system processes VCF variant records by extracting genomic context (gene annotations, variant consequences from VEP-like predictors), integrating pathogenicity scores (CADD, conservation metrics), mapping variants to affected genes and proteins, computing phenotype-gene associations using HPO semantic similarity and disease-gene knowledge graphs, and feeding multidimensional feature vectors into the deep neural network which learns nonlinear relationships between variant properties, gene function, and phenotypic outcomes. This application demonstrates VCF serving as the primary input to ML-driven clinical diagnostics, where the standardized variant representation enables integration of sequence-level information (allele frequencies, quality scores, genotype calls), functional annotations (consequence predictions, pathogenicity scores), and phenotype-driven prioritization, supporting rare disease diagnosis workflows where clinicians upload patient VCFs and phenotype descriptions to receive ranked candidate variants for confirmatory testing and therapeutic decision-making.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1186/s12859-019-2633-8"
          ]
        },
        {
          "id": "B2AI_APP:252",
          "category": "B2AI:Application",
          "name": "Neural Network Variant Filtering and Classification",
          "description": "Post-calling variant filtering and classification tools apply neural networks to VCF files for distinguishing true genetic variants from technical artifacts, processing VCF records in batch mode to evaluate call quality and emit filtered VCF outputs or annotated reports. Intelli-NGS implements artificial neural networks (ANNs) to process VCF files generated from Ion Torrent sequencing, classifying variants as high-confidence or low-confidence calls based on learned patterns in VCF annotation fields (depth of coverage, strand bias, mapping quality, base quality scores, allele balance), and providing Human Genome Variation Society (HGVS) nomenclature codes for all variants in the output, demonstrating integration of ML-based quality control with standardized variant nomenclature. GARFIELD-NGS extends this approach with deeper neural networks (DNNs) combined with multi-layer perceptrons (MLPs) for variant filtering and feature engineering, consuming VCF files from Illumina platforms and applying learned classification models to distinguish true positives from false positives based on multivariate patterns in sequencing quality metrics that traditional hard-filter approaches (fixed thresholds on individual metrics) fail to capture, with the system outputting filtered VCF files or Excel reports with variant classifications and confidence scores. Both systems operate on the principle that neural networks can learn complex, nonlinear decision boundaries in high-dimensional feature spaces defined by VCF annotation fields, capturing interactions between quality metrics that indicate systematic sequencing errors (e.g., low mapping quality correlated with high strand bias suggesting alignment artifacts, or low base quality at homopolymer runs indicating sequencing chemistry issues), and generalizing across samples to provide consistent quality filtering without manual threshold tuning for each dataset. The tools demonstrate VCF's dual role as input to ML models (where INFO and FORMAT fields provide rich feature vectors for classification) and as output format (where filtered variants are written back to VCF maintaining compatibility with downstream analysis tools), supporting clinical sequencing laboratories implementing ML-based quality control pipelines to reduce manual variant review burden while maintaining high sensitivity for true variants, research studies requiring standardized variant filtering across large cohorts to minimize batch effects, and comparative evaluations of variant calling pipelines where ML-based filters can be consistently applied to VCF outputs from different callers (GATK, FreeBayes, Strelka, DeepVariant) to harmonize quality standards and improve cross-study reproducibility.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1186/s40246-022-00396-x",
            "https://doi.org/10.48048/tis.2025.9149"
          ]
        },
        {
          "id": "B2AI_APP:253",
          "category": "B2AI:Application",
          "name": "VariantSpark Distributed ML on VCF Genotype Matrices",
          "description": "VariantSpark provides a scalable interface between Apache Spark distributed computing framework with MLlib machine learning library and standard VCF files, enabling population-scale machine learning on genotype data by extracting genotype matrices from multi-sample VCF files (where rows represent samples and columns represent variant sites with genotype encodings 0/1/2 for reference homozygous/heterozygous/alternate homozygous) and applying distributed ML algorithms for clustering, genome-wide association, and variant importance ranking. The system reads VCF files using Spark's distributed file I/O, parses genotype fields (GT format field with phased or unphased alleles) across thousands to millions of samples in parallel, converts categorical genotypes into numerical feature matrices suitable for ML algorithms, and distributes computation across Spark worker nodes to handle datasets exceeding single-machine memory capacity (supporting biobank-scale VCF files with 100,000+ samples and 10+ million variants). VariantSpark implements random forest-based importance analysis identifying variants most predictive of phenotypic outcomes, principal component analysis (PCA) for population structure inference detecting ancestry-related clustering in genotype space, and k-means clustering discovering subpopulations with shared genetic ancestry, all operating directly on VCF-derived genotype matrices without requiring intermediate file format conversions. The tool demonstrates VCF's role as the source format for large-scale ML feature engineering in genomics, where standardized genotype representation enables systematic extraction of predictor variables (variant genotypes) and their integration with phenotype labels (case/control status, quantitative traits, drug response outcomes) for supervised learning, while distributed processing addresses computational challenges of modern biobanks where VCF files reach terabyte scale and traditional single-machine analysis becomes infeasible. Applications include genome-wide association studies (GWAS) using ML classifiers more flexible than linear regression (capturing epistatic interactions and nonlinear genetic architectures), polygenic risk score development through feature selection identifying variants with strongest predictive power, admixture mapping in diverse populations where ML-based clustering refines ancestry assignments beyond principal components, and rare variant association testing where ML aggregation methods combine signals across multiple low-frequency variants in genes or regulatory regions, supporting precision medicine initiatives requiring integration of genomic variation (encoded in VCF) with clinical outcomes through scalable, reproducible ML pipelines operating on industry-standard big data infrastructure.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1186/s40246-022-00396-x"
          ]
        },
        {
          "id": "B2AI_APP:254",
          "category": "B2AI:Application",
          "name": "GWAS-VCF Format for ML-Ready Summary Statistics",
          "description": "GWAS-VCF adapts the Variant Call Format to store genome-wide association study (GWAS) summary statistics by repurposing VCF's SAMPLE/FORMAT fields to hold per-variant association metrics (effect size ES, standard error SE, -log10 p-value LP) while using INFO fields for allele frequencies, functional annotations, and harmonized metadata, creating an indexed, queryable data container for GWAS results that supports downstream machine learning workflows requiring harmonized variant-level features and rapid genomic queries. The format uses VCF header lines to store rich study-level metadata (trait descriptions, sample size, analysis methods, population ancestry, genome build, phenotype definitions), enabling automated provenance tracking and reproducible analyses, while each variant record captures CHROM/POS/REF/ALT allele definitions with left-alignment and normalization (via vgraph allele matching) ensuring consistent variant representation across studies for meta-analysis. GWAS-VCF files are indexed with tabix for fast positional queries (extracting all associations in a genomic region in milliseconds without loading the entire file) and with rsidx for dbSNP rsID-based lookups, supporting interactive exploration and programmatic access patterns common in ML feature engineering pipelines where specific loci or candidate gene regions are repeatedly queried. The gwas2vcf Python tool provides conversion from standard GWAS summary statistic formats (tab-delimited files with varied column naming conventions) to standardized GWAS-VCF with automated allele harmonization (flipping effect directions for strand mismatches, left-aligning indels, resolving ambiguous A/T and G/C SNPs using allele frequency priors), quality control filtering (removing variants with mismatched allele frequencies, impossible effect size/standard error combinations, palindromic SNPs without clear strand assignment), and metadata validation ensuring completeness for downstream analyses. Ecosystem integration enables reading GWAS-VCF files into R (VariantAnnotation package) and Python (pysam, cyvcf2) for statistical analysis and ML modeling, command-line processing with bcftools (filtering, annotation, format conversion), GATK (variant manipulation, liftover between genome builds), bedtools (intersection with genomic features), and plink (integration with individual-level genotype data in VCF format for two-sample Mendelian randomization and colocalization analyses). For machine learning applications, GWAS-VCF provides ML-ready input data where each variant's effect size and significance serve as response variables or feature labels for polygenic score construction (selecting variants and computing weighted sums of effect sizes), trans-ethnic meta-analysis models (learning ancestry-specific effect size patterns and predicting cross-population transferability), fine-mapping algorithms (combining association statistics with linkage disequilibrium matrices from reference VCFs to identify causal variants in associated loci), and colocalization methods (assessing shared causal variants between GWAS traits or between GWAS and molecular QTLs using effect size correlation patterns). The format addresses practical ML workflow requirements including rapid feature extraction (querying association statistics for gene sets or functional annotations without full file scans), consistent variant identifiers across datasets (standardized CHROM:POS:REF:ALT notation enabling joins between GWAS-VCF files and genotype VCFs or annotation databases), and streaming I/O for cloud computing (block GZIP compression with tabix indexing enables range requests extracting chromosome-specific subsets without transferring entire files, reducing bandwidth costs in cloud-based ML pipelines analyzing hundreds of GWAS traits), supporting large-scale genomics ML projects including UK Biobank PheWAS (phenome-wide association studies correlating thousands of traits with millions of variants), polygenic risk score repositories (aggregating GWAS-VCF files across traits for multi-trait prediction models), and AI-driven drug target identification (integrating GWAS-VCF associations with expression QTLs and protein-protein interaction networks to prioritize therapeutic targets via causal inference and graph neural networks).",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1186/s13059-020-02248-0"
          ]
        }
      ],
      "collection": [
        "fileformat",
        "markuplanguage",
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:13",
        "B2AI_TOPIC:35"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:117"
      ],
      "purpose_detail": "Variant Call Format (VCF) is a text file format (most likely stored in a compressed manner). It contains meta-information lines, a header line, and then data lines each containing information about a position in the genome.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikipedia.org/wiki/Variant_Call_Format",
      "formal_specification": "https://github.com/samtools/hts-specs",
      "responsible_organization": [
        "B2AI_ORG:34"
      ]
    },
    {
      "id": "B2AI_STANDARD:300",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "VEP",
      "description": "Variant Effect Predictor format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:29"
      ],
      "purpose_detail": "Variant Effect Predictor (VEP) format is a tab-delimited text format devised by Ensembl for representing functional annotations and predicted effects of genetic variants on genes, transcripts, proteins, and regulatory regions, serving as the primary output format for Ensembl VEPthe widely-adopted variant annotation tool that processes millions of variants from whole-genome and exome sequencing studies to predict biological consequences ranging from synonymous changes to loss-of-function mutations, splice site disruptions, and regulatory element modifications. The format consists of a header section with metadata (VEP version, command-line options, annotation sources, database versions, analysis timestamp) followed by variant records in rows with dozens of tab-separated columns, with a default minimal set including Uploaded_variation (original variant identifier from input VCF), Location (chromosome:position), Allele (alternate allele), Gene (Ensembl stable gene ID like ENSG00000139618 for BRCA2), Feature (transcript ID like ENST00000380152), Feature_type (Transcript/RegulatoryFeature/MotifFeature), Consequence (standardized Sequence Ontology terms like missense_variant, stop_gained, splice_donor_variant), cDNA_position (position in transcript cDNA), CDS_position (position in coding sequence), Protein_position (position in protein), Amino_acids (reference/alternate amino acids), Codons (reference/alternate codons), Existing_variation (dbSNP rsIDs, ClinVar/COSMIC identifiers for known variants), and Extra column containing semicolon-separated key=value pairs for additional annotations. VEP's extensibility allows augmentation with plugin-based annotations including SIFT scores (predicting whether amino acid substitutions affect protein function, threshold <0.05 indicating deleterious), PolyPhen-2 scores (predicting pathogenicity with benign/possibly damaging/probably damaging classifications), CADD scores (Combined Annotation Dependent Depletion ranking variant deleteriousness on phred-scaled 0-99 scale where >20 suggests top 1% most deleterious variants), gnomAD allele frequencies (population frequencies from 125,748 exomes and 71,702 genomes enabling filtering of common benign variants), ClinVar clinical significance (pathogenic/likely pathogenic/benign/likely benign/uncertain significance classifications from expert-curated database), conservation scores (PhyloP/PhastCons measuring evolutionary conservation across species), splicing predictions (dbscSNV/MaxEntScan/SpliceAI scores identifying cryptic splice sites and branchpoint disruptions), regulatory annotations (chromatin states, transcription factor binding sites, enhancer/promoter overlaps from ENCODE/Roadmap Epigenomics), and custom annotations from user-provided VCF/BED/GFF files or remote databases. VEP format supports multi-allelic sites where each alternate allele receives a separate row with consequences calculated independently, accommodates multiple transcript annotations per variant (showing effects across canonical transcript, MANE Select transcript, Ensembl/RefSeq transcripts with varying severities), and handles complex consequences with comma-separated lists (e.g., \"missense_variant,splice_region_variant\" when variant affects both coding and splicing). The format integrates with standard genomic workflows VEP input accepts VCF, JSON, or Ensembl region notation; output VEP format can be converted back to VCF with consequence annotations in INFO/CSQ field using --vcf flag, exported to JSON with --json flag for programmatic parsing, or consumed by downstream tools including Ensembl Variant Recoder, VAtools for precision oncology, OpenCRAVAT for clinical interpretation, and custom parsers in R/Python for machine learning feature extraction. For AI/ML applications, VEP format serves as rich feature representation for variant effect prediction models training supervised classifiers (random forests, gradient boosting, deep neural networks) on labeled pathogenic/benign variants using VEP consequence types, conservation scores, population frequencies, and in silico predictions as features to build ensemble models outperforming individual predictors; variant prioritization pipelines ranking candidate disease-causing variants by aggregating VEP annotation scores with inheritance patterns and phenotype matching; splice variant analysis models using VEP's splice predictions combined with RNA-seq junction reads to train deep learning models identifying cryptic splicing events; cancer driver identification algorithms integrating VEP consequences, COSMIC mutation frequencies, and protein domain annotations to distinguish driver mutations from passenger variants in tumor sequencing; and pharmacogenomics models correlating VEP-annotated variants in drug metabolism genes (CYP2D6, CYP2C19, TPMT) with clinical outcomes to predict drug response, enabling precision medicine applications where genetic variants inform treatment decisions and supporting large-scale variant interpretation in clinical genomics, population genetics, and functional genomics research where systematic annotation of millions of variants requires standardized, comprehensive, machine-readable output capturing diverse biological effects and predictive scores.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://useast.ensembl.org/info/docs/tools/vep/vep_formats.html",
      "responsible_organization": [
        "B2AI_ORG:124"
      ]
    },
    {
      "id": "B2AI_STANDARD:301",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "VRS",
      "description": "Variation Representation Specification",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:52",
          "category": "B2AI:Application",
          "name": "Standardized Variant Representation for ML Interoperability",
          "description": "VRS (Variation Representation Specification) is used in AI applications to create unambiguous, computationally accessible representations of genetic variants that enable machine learning model interoperability across different genomic databases and variant calling pipelines. AI systems leverage VRS to normalize variant representations, resolve ambiguities in variant nomenclature, and create consistent feature representations for training models that predict variant effects, interpret clinical significance, or perform variant prioritization. VRS's precise coordinate system and allele representation enable AI models to correctly match variants across different genome builds, integrate data from multiple sources, and generate reproducible predictions that can be shared across institutions and validated against standardized variant benchmarks.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:117"
      ],
      "purpose_detail": "The Variation Representation Specification (VRS, pronounced verse) is a standard developed by the Global Alliance for Genomics and Health (GA4GH) to facilitate and improve sharing of genetic information. The Specification consists of a JSON Schema for representing many classes of genetic variation, conventions to maximize the utility of the schema, and a Python implementation that promotes adoption of the standard.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://vrs.ga4gh.org/en/latest/index.html",
      "publication": "doi:10.1016/j.xgen.2021.100027",
      "responsible_organization": [
        "B2AI_ORG:34"
      ]
    },
    {
      "id": "B2AI_STANDARD:302",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "VarioML",
      "description": "VarioML format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:35"
      ],
      "purpose_detail": "A Locus-specific Database (LSDB) describes the variants discovered on a single gene or members of a gene family and other related functional elements. LSDBs are curated by experts on their respective loci, and as such are typically the best resources of such information available. But LSDBs vary widely in format and completeness, making data integration and exchange among them difficult and time-consuming. To address these difficulties, the VarioML format has been developed for the full range of variation data use-cases, providing semantically well-defined components which can be easily composed to fit specific needs. Using VarioML, data owners can now efficiently enable the integration, federation, and exchange of their variant data. The discoverabiliaty, extensibility, and quality of variation data is immediately enhanced. Critical new avenues of research and knowledge discovery are opened, as data using the VarioML standard can be integrated with the global library of purely genetic data. VarioML is a central prerequisite for effective modelling of phenotype data and genotype-to-phenotype relationships. It removes the long-standing technical obstacles to the effective passing of variant data from discovery laboratories into the biomedical database world. Now all that is needed is the broad participation of the genotype-to-phenotype research community.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/VarioML/VarioML",
      "formal_specification": "https://github.com/VarioML/VarioML"
    },
    {
      "id": "B2AI_STANDARD:303",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "VMS",
      "description": "Vocabulary Maintenance Standard",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:93"
      ],
      "purpose_detail": "The Vocabulary Maintenance Standard (VMS) is a TDWG (Biodiversity Information Standards) specification that defines formal procedures for maintaining and evolving controlled vocabularies used in biodiversity informatics. Unlike traditional standards that remain relatively static, VMS recognizes that vocabulary standards must adapt incrementally to meet evolving community needs without requiring the full standards ratification process for every change. The specification categorizes types of vocabulary modifications (additions, modifications, deprecations) and establishes governance mechanisms for implementing these changes through designated vocabulary maintenance groups. VMS defines roles and responsibilities for stakeholders including vocabulary maintainers, Interest Groups, and the broader TDWG community, specifying how change proposals are submitted, reviewed, and approved. The standard ensures that vocabulary evolution remains transparent, traceable, and backward-compatible where possible, while maintaining the stability needed for production systems. VMS applies to major TDWG vocabularies including Darwin Core terms and is essential for maintaining semantic interoperability across biodiversity data systems as scientific understanding and data practices evolve.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.tdwg.org/standards/vms/"
    },
    {
      "id": "B2AI_STANDARD:304",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "VHI",
      "description": "Voice Handicap Index",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "diagnosticinstrument"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "A statistically robust Voice Handicap Index (VHI). An 85-item version of this instrument was administered to 65 consecutive patients seen in the Voice Clinic at Henry Ford Hospital. The data were subjected to measures of internal consistency reliability and the initial 85-item version was reduced to a 30-item final version. This final version was administered to 63 consecutive patients on two occasions in an attempt to assess test-retest stability, which proved to be strong. The findings of the latter analysis demonstrated that a change between two administrations of 18 points represents a significant shift in psychosocial function.",
      "is_open": false,
      "requires_registration": false,
      "url": "https://doi.org/10.1044/1058-0360.0603.66",
      "publication": "doi:10.1044/1058-0360.0603.66"
    },
    {
      "id": "B2AI_STANDARD:305",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "HCLS",
      "description": "W3C HCLS Dataset Description",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Access to consistent, high-quality metadata is critical to finding, understanding, and reusing scientific data. This document describes a consensus among participating stakeholders in the Health Care and the Life Sciences domain on the description of datasets using the Resource Description Framework (RDF). This specification meets key functional requirements, reuses existing vocabularies to the extent that it is possible, and addresses elements of data description, versioning, provenance, discovery, exchange, query, and retrieval.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.w3.org/TR/hcls-dataset/",
      "responsible_organization": [
        "B2AI_ORG:99"
      ]
    },
    {
      "id": "B2AI_STANDARD:306",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "WIG",
      "description": "Wiggle Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "Wiggle files and its bedgraph variant allow you to plot quantitative data as either shades of color (dense mode) or bars of varying height (full and pack mode) on the genome.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://genome.ucsc.edu/goldenPath/help/wiggle.html",
      "responsible_organization": [
        "B2AI_ORG:119"
      ]
    },
    {
      "id": "B2AI_STANDARD:307",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "LSM",
      "description": "Zeiss LSM series confocal microscope image",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "audiovisual",
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:19"
      ],
      "purpose_detail": "A proprietary image format based on TIFF.",
      "is_open": false,
      "requires_registration": false,
      "url": "https://openwetware.org/wiki/Dissecting_LSM_files",
      "responsible_organization": [
        "B2AI_ORG:101"
      ],
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:19"
      ]
    },
    {
      "id": "B2AI_STANDARD:308",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "AMR",
      "description": "Adaptive Multi-Rate Speech Codec",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "audiovisual"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:37"
      ],
      "purpose_detail": "Audio data compression scheme optimized for speech coding, adopted in October 1998 as the standard speech codec by 3GPP (3d Generation Partnership Project) and now widely used in GSM (Global System for Mobile Communications).",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.loc.gov/preservation/digital/formats/fdd/fdd000254.shtml",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:49"
      ]
    },
    {
      "id": "B2AI_STANDARD:309",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "WIFF",
      "description": "Analyst native acquisition file format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:28"
      ],
      "purpose_detail": "Mass spectra output format used by AB SCIEX intstruments.",
      "is_open": false,
      "requires_registration": false,
      "url": "https://doi.org/10.1074/mcp.R112.019695",
      "publication": "doi:10.1074/mcp.R112.019695"
    },
    {
      "id": "B2AI_STANDARD:310",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "HDR/IMG",
      "description": "Analyze file format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "purpose_detail": "Analyze 7.5 can store voxel-based volumes and consists of two files - One file with the actual data in a binary format with the filename extension .img and another file (header with filename extension .hdr) with information about the data such as voxel size and number of voxel in each dimension.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://analyzedirect.com/documents/AD_AnalyzeImage75_File_Format.pdf",
      "formal_specification": "https://analyzedirect.com/documents/AD_AnalyzeImage75_File_Format.pdf"
    },
    {
      "id": "B2AI_STANDARD:311",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "Arrow",
      "description": "Apache Arrow",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:5"
      ],
      "purpose_detail": "A language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware like CPUs and GPUs. The Arrow memory format also supports zero-copy reads for lightning-fast data access without serialization overhead.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://arrow.apache.org/",
      "formal_specification": "https://github.com/apache/arrow"
    },
    {
      "id": "B2AI_STANDARD:312",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "Avro",
      "description": "Apache Avro",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:5"
      ],
      "purpose_detail": "Avro is a row-oriented remote procedure call and data serialization framework developed within Apache's Hadoop project. It uses JSON for defining data types and protocols, and serializes data in a compact binary format.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://avro.apache.org/",
      "formal_specification": "https://github.com/apache/avro"
    },
    {
      "id": "B2AI_STANDARD:313",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "ADMS",
      "description": "Asset Description Metadata Schema",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:99"
      ],
      "purpose_detail": "ADMS is a profile of DCAT, used to describe semantic assets (or just 'Assets'), defined as highly reusable metadata (e.g. xml schemata, generic data models) and reference data (e.g. code lists, taxonomies, dictionaries, vocabularies) that are used for eGovernment system development.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.w3.org/TR/vocab-adms/"
    },
    {
      "id": "B2AI_STANDARD:314",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "AIFF",
      "description": "Audio Interchange File Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "audiovisual",
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:37"
      ],
      "purpose_detail": "File format for sound that wraps various sound bitstreams, ranging from uncompressed waveform to MIDI.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.loc.gov/preservation/digital/formats/fdd/fdd000005.shtml",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:49"
      ]
    },
    {
      "id": "B2AI_STANDARD:315",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "AVI",
      "description": "Audio Video Interleave digital multimedia container format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "audiovisual",
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15",
        "B2AI_TOPIC:37"
      ],
      "purpose_detail": "AVI files can contain both audio and video data in a file container that allows synchronous audio-with-video playback.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikipedia.org/wiki/Audio_Video_Interleave",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:49"
      ]
    },
    {
      "id": "B2AI_STANDARD:316",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "BMP",
      "description": "Bitmap format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "audiovisual",
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "purpose_detail": "BMP (Bitmap) is a raster graphics file format developed by Microsoft for storing device-independent bitmaps (DIBs) in Windows and OS/2 systems. The format consists of a file header, DIB header (with multiple versions: BITMAPCOREHEADER, BITMAPINFOHEADER, BITMAPV4HEADER, BITMAPV5HEADER), optional color palette, and pixel array storing uncompressed or RLE-compressed bitmap data. BMP supports 1, 4, 8, 16, 24, and 32 bits per pixel, accommodating indexed color (palettes), RGB color, and alpha channels for transparency. The format uses little-endian byte ordering and stores pixel rows bottom-to-top by default, with each row padded to 4-byte boundaries. BMP files can contain color profiles (ICC) for color management and support various compression methods including BI_RGB (uncompressed), BI_RLE4/BI_RLE8 (run-length encoding), and BI_BITFIELDS (custom RGB bit masks). The simplicity, widespread familiarity, and open format specification make BMP common in Windows applications, image processing software, and scientific imaging. While BMP files are typically large due to minimal compression, they compress efficiently with lossless algorithms (ZIP, RAR). BMP is used in GDI (Graphics Device Interface) subsystems, icons (ICO), cursors (CUR), and as an intermediate format for image processing. Software support is extensive: Adobe Photoshop, GIMP, Microsoft Office, browsers (Chrome, Edge), and programming libraries across platforms. In AI/ML imaging applications, BMP serves as a raw, lossless format for medical imaging (preserving pixel accuracy), training data preparation (avoiding compression artifacts), image annotation workflows, and intermediate processing steps where format simplicity facilitates pixel-level manipulation for computer vision tasks, though typically converted to compressed formats for efficient storage and transmission.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikipedia.org/wiki/BMP_file_format",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:19"
      ]
    },
    {
      "id": "B2AI_STANDARD:317",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "Cap'n'Proto",
      "description": "Cap'n'Proto",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Capn Proto is an insanely fast data interchange format and capability-based RPC system. Think JSON, except binary. Or think Protocol Buffers, except faster.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://capnproto.org/",
      "formal_specification": "https://github.com/capnproto/capnproto"
    },
    {
      "id": "B2AI_STANDARD:318",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "CellML",
      "description": "CellML language",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "CellML language is an open standard based on the XML markup language. CellML is being developed by the Auckland Bioengineering Institute at the University of Auckland and affiliated research groups. The purpose of CellML is to store and exchange computer-based mathematical models. CellML allows scientists to share models even if they are using different model-building software. It also enables them to reuse components from one model in another, thus accelerating model building.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.cellml.org/",
      "publication": "doi:10.1186/1471-2105-11-178"
    },
    {
      "id": "B2AI_STANDARD:319",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "CFF",
      "description": "citation-file-format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A file format for providing citation metadata for software or datasets in plaintext files that are easy to read by both humans and machines.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://citation-file-format.github.io/",
      "formal_specification": "https://github.com/citation-file-format/citation-file-format"
    },
    {
      "id": "B2AI_STANDARD:320",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "CWL",
      "description": "Common Workflow Language",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "workflowlanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "specification for describing data analysis workflows and tools",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.commonwl.org/",
      "publication": "doi:10.48550/arXiv.2105.07028",
      "formal_specification": "https://github.com/common-workflow-language/common-workflow-language"
    },
    {
      "id": "B2AI_STANDARD:321",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "CURATE(D)",
      "description": "CURATE(D) Checklists",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The CURATE(D) steps are a teaching and representation tool. This model is useful for onboarding data curators and orienting researchers preparing to share their data. It serves as a demonstration for the type of work involved in robust data curation, and was created to fit within institution-specific data repository workflows.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://datacurationnetwork.org/outputs/workflows/",
      "formal_specification": "https://docs.google.com/document/d/1RWt2obXOOeJRRFmVo9VAkl4h41cL33Zm5YYny3hbPZ8/edit"
    },
    {
      "id": "B2AI_STANDARD:322",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "DAMA-DMBOK",
      "description": "DAMA Data Management Body of Knowledge",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:22"
      ],
      "purpose_detail": "Reference guide for processes, best practices, and principles in data management.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://www.dama.org/cpages/body-of-knowledge"
    },
    {
      "id": "B2AI_STANDARD:323",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "DCAT",
      "description": "Data Catalog Vocabulary",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:99"
      ],
      "purpose_detail": "An RDF vocabulary designed to facilitate interoperability between data catalogs published on the Web.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.w3.org/TR/vocab-dcat-1/"
    },
    {
      "id": "B2AI_STANDARD:324",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "DataCite",
      "description": "DataCite Metadata Schema",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The DataCite Metadata Schema is a list of core metadata properties chosen for the accurate and consistent identification of a resource for citation and retrieval purposes, along with recommended use instructions. The resource that is being identified can be of any kind, but it is typically a dataset. We use the term dataset in its broadest sense. We mean it to include not only numerical data, but any other research data outputs.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://schema.datacite.org/",
      "formal_specification": "https://schema.datacite.org/meta/kernel-4.3/metadata.xsd"
    },
    {
      "id": "B2AI_STANDARD:325",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "Dataset Cards",
      "description": "Dataset Cards",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Dataset Cards are structured documentation templates for describing machine learning datasets, introduced by Google and adopted widely in the ML community, particularly through Hugging Face's Datasets library. Dataset Cards provide standardized metadata capturing dataset purpose, structure, creation methodology, intended uses, limitations, and ethical considerations. The documentation includes: dataset summary and description; language coverage and data sources; supported ML tasks and features; dataset structure (splits, configurations, data fields); creation process (annotations, quality control, collection methodology); considerations for using the data (social impact, biases, privacy concerns); additional resources (papers, repositories, licenses). Dataset Cards promote responsible AI by making dataset characteristics transparent, helping practitioners assess fitness for purpose, understand potential biases, and evaluate dataset limitations before use. The structured format enables dataset discovery on ML platforms, facilitates reproducibility by documenting provenance, and encourages accountability in dataset creation and curation. Hugging Face hosts 100,000+ dataset cards in their Hub, standardizing documentation for NLP, computer vision, audio, and multimodal datasets. Dataset Cards complement Model Cards by extending documentation principles to training data, addressing data quality, collection practices, annotation procedures, and data ethics. In AI/ML pipelines, Dataset Cards support informed dataset selection, bias mitigation through transparency, reproducible research by documenting data versions, and regulatory compliance (AI Act, GDPR) by clarifying data provenance, consent, and usage restrictions.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://huggingface.co/docs/datasets/dataset_card"
    },
    {
      "id": "B2AI_STANDARD:326",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "Datasheets",
      "description": "Datasheets for Datasets",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "datasheets"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "...we propose that every dataset be accompanied with a datasheet that documents its motivation, composition, collection process, recommended uses, and so on.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://arxiv.org/abs/1803.09010",
      "publication": "https://arxiv.org/abs/1803.09010"
    },
    {
      "id": "B2AI_STANDARD:327",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "XGMML",
      "description": "eXtensible Graph Markup and Modeling Language",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:21"
      ],
      "purpose_detail": "eXtensible Graph Markup and Modeling Language is an XML application based on GML which is used for graph description. XGMML uses tags to describe nodes and edges of a graph. The purpose of XGMML is to make possible the exchange of graphs between differents authoring and browsing tools for graphs.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://xml.coverpages.org/xgmml.html"
    },
    {
      "id": "B2AI_STANDARD:328",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "FAIR4RS",
      "description": "FAIR Principles for Research Software",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:83"
      ],
      "purpose_detail": "Simple and research software appropriate goalposts to inform those who publish and/or preserve research software.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.rd-alliance.org/group/fair-research-software-fair4rs-wg/outcomes/fair-principles-research-software-fair4rs",
      "publication": "doi:10.1038/s41597-022-01710-x"
    },
    {
      "id": "B2AI_STANDARD:329",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "FIPS",
      "description": "Federal Information Processing System Codes for States and Counties",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "codesystem",
        "deprecated"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:6"
      ],
      "purpose_detail": "FIPS codes are numbers which uniquely identify geographic areas. As of database version 55, FIPS has been merged with the Geographic Names Information System (GNIS).",
      "is_open": true,
      "requires_registration": false,
      "url": "https://transition.fcc.gov/oet/info/maps/census/fips/fips.txt",
      "formal_specification": "https://transition.fcc.gov/oet/info/maps/census/fips/fips.txt"
    },
    {
      "id": "B2AI_STANDARD:330",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "FieldML",
      "description": "FieldML",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "FieldML is a declarative language for building hierarchical models represented by generalized mathematical fields. FieldML is developed as a data model and accompanying API. Find out more about the FieldML API, where to get the latest release and how to contribute to its development. FieldML is a declarative language for representing hierarchical models using generalized mathematical fields. FieldML can be used to represent the dynamic 3D geometry and solution fields from computational models of cells, tissues and organs. It enables model interchange for the bioengineering and general engineering analysis communities. Example uses are models of tissue structure, the distribution of proteins and other biochemical compounds, anatomical annotation, and other biological annotation.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://doi.org/10.1007/s11517-013-1097-7",
      "publication": "doi:10.1007/s11517-013-1097-7"
    },
    {
      "id": "B2AI_STANDARD:331",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "FLAC",
      "description": "Free Lossless Audio Codec",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "audiovisual",
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:37"
      ],
      "purpose_detail": "FLAC (Free Lossless Audio Codec) is an audio coding format for lossless compression of digital audio.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikipedia.org/wiki/FLAC",
      "formal_specification": "https://xiph.org/flac/format.html",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:49"
      ]
    },
    {
      "id": "B2AI_STANDARD:332",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "Frictionless",
      "description": "Frictionless Data Package",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Frictionless Data Package is a specification for packaging, describing, and sharing datasets in a lightweight, standardized container format developed by Open Knowledge Foundation. A Data Package consists of a descriptor (datapackage.json) containing metadata about the package and its resources, along with the data files themselves (local or remote). The specification defines comprehensive metadata including: package title, description, licenses, version, sources, contributors, and keywords; resource information for each data file (path/URL, format, schema, encoding); Table Schema for tabular data defining field names, types, constraints, and relationships; and optional goodtables validation rules. Frictionless supports FAIR data principles by ensuring datasets are Findable (rich metadata), Accessible (standard formats, clear paths), Interoperable (JSON-based descriptor, Table Schema), and Reusable (license, provenance, structure documentation). The ecosystem includes libraries for Python, JavaScript, R, Ruby, PHP, and command-line tools for validation, conversion, and publishing. Frictionless integrates with data portals (CKAN, Dataverse), notebooks (Jupyter), and analysis tools, enabling seamless dataset exchange and processing. Extensions support specialized data types (geospatial, time series, budget data) and validation frameworks. In AI/ML workflows, Frictionless Data Packages standardize dataset distribution for reproducible research, provide machine-readable schemas for automated data validation and ingestion, enable data versioning and provenance tracking for ML pipelines, and facilitate dataset documentation complementing model training by ensuring data quality, consistency, and interpretability across collaborative ML projects.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://specs.frictionlessdata.io/data-package/",
      "formal_specification": "https://github.com/frictionlessdata/specs"
    },
    {
      "id": "B2AI_STANDARD:333",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "serviceinfo",
      "description": "GA4GH serviceinfo",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:34"
      ],
      "purpose_detail": "Provides a way for an API to expose a set of metadata to help discovery and aggregation of services via computational methods.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/ga4gh-discovery/ga4gh-service-info"
    },
    {
      "id": "B2AI_STANDARD:334",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "gxformat2",
      "description": "Galaxy workflow Format 2",
      "related_to": [
        "B2AI_STANDARD:766"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "workflowlanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:20"
      ],
      "purpose_detail": "A schema moving Galaxy's workflow description language toward standards such as the Common Workflow Language.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/galaxyproject/gxformat2",
      "formal_specification": "https://github.com/galaxyproject/gxformat2"
    },
    {
      "id": "B2AI_STANDARD:335",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "GNIS ID",
      "description": "Geographic Names Information System Feature IDs",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:14"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:98"
      ],
      "purpose_detail": "The Geographic Names Information System (GNIS) is the Federal and national standard for geographic nomenclature. The U.S. Geological Survey's National Geospatial Program developed the GNIS in support of the U.S. Board on Geographic Names as the official repository of domestic geographic names data, the official vehicle for geographic names use by all departments of the Federal Government, and the source for applying geographic names to Federal electronic and printed products.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.usgs.gov/us-board-on-geographic-names/domestic-names",
      "formal_specification": "https://www.usgs.gov/u.s.-board-on-geographic-names/download-gnis-data"
    },
    {
      "id": "B2AI_STANDARD:336",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "GENC",
      "description": "Geopolitical Entities, Names, and Codes",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "codesystem"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:6"
      ],
      "purpose_detail": "The GENC Standard specifies a profile of ISO 3166 codes for the representation of names of countries and their subdivisions.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.dni.gov/index.php/who-we-are/organizations/ic-cio/ic-cio-related-menus/ic-cio-related-links/ic-technical-specifications/geopolitical-entities-names-and-codes",
      "formal_specification": "https://evs.nci.nih.gov/ftp1/GENC/"
    },
    {
      "id": "B2AI_STANDARD:337",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "GraphML",
      "description": "Graph Markup Language",
      "related_to": [
        "B2AI_STANDARD:829"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat",
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:21"
      ],
      "purpose_detail": "An XML file format and language for describing graphs.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://graphml.graphdrawing.org/specification.html"
    },
    {
      "id": "B2AI_STANDARD:338",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "GIF",
      "description": "Graphics Interchange Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "purpose_detail": "Graphics Interchange Format (GIF) is a bitmap image format developed by CompuServe in 1987 that supports lossless LZW compression, up to 256 colors from a 24-bit RGB palette, transparency, and multi-frame animation with per-frame delay controls. GIF became widely adopted on the early web for its efficient compression and cross-platform compatibility, and remains ubiquitous for short animated loops and simple graphics with well-defined edges. The format supports two versions (GIF87a and GIF89a), with the latter adding animation delays, transparent backgrounds, and application-specific metadata. While the LZW patent controversy drove development of PNG as an alternative, GIF's animation capabilities and \"hot\" data accessibility (frames stored sequentially without random access penalties) maintain its relevance for social media reactions, educational demonstrations, and web UI elements. Modern applications leverage GIF's deterministic looping (via Netscape Application Block extension) and universal browser support, though video formats like WebP and MP4 increasingly replace GIF for better compression ratios. The format's 256-color limitation makes it suitable for logos, diagrams, and pixel art but less appropriate for photographs or gradients, where dithering techniques are often applied. GIF files consist of a logical screen descriptor, optional global color table, image descriptors with optional local color tables, and LZW-encoded pixel data stored in sub-blocks. For AI/ML workflows, GIF serves as a compact format for visualizing model predictions over time series, displaying attention mechanisms frame-by-frame, and sharing animated training/validation metrics without requiring video codec dependencies.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikipedia.org/wiki/GIF"
    },
    {
      "id": "B2AI_STANDARD:339",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "HDF5",
      "description": "HDF5 format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "fileformat",
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:116"
      ],
      "purpose_detail": "HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data. HDF5 is portable and is extensible, allowing applications to evolve in their use of HDF5. The HDF5 Technology suite includes tools and applications for managing, manipulating, viewing, and analyzing data in the HDF5 format.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.hdfgroup.org/solutions/hdf5/",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:16"
      ]
    },
    {
      "id": "B2AI_STANDARD:340",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "HDMF",
      "description": "Hierarchical Data Modeling Framework for Modern Science Data Standards",
      "related_to": [
        "B2AI_STANDARD:339",
        "B2AI_STANDARD:379"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2025-05-29",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5",
        "B2AI_TOPIC:48"
      ],
      "purpose_detail": "HDMF is a hierarchical data modeling framework for modern science data standards. It separates data standardization into three main components: (1) data modeling and specification, (2) data I/O and storage, and (3) data interaction and data APIs. HDMF provides object mapping infrastructure to insulate and integrate these components, supporting flexible development of data standards and extensions, optimized storage backends, and data APIs. It offers advanced data I/O functionality for iterative data write, lazy data load, parallel I/O, and modular data storage. HDMF is particularly used to design NWB 2.0, a data standard for neurophysiology data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://hdmf-common-schema.readthedocs.io/en/latest/format.html",
      "publication": "doi:10.1109/bigdata47090.2019.9005648",
      "formal_specification": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8500680/",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:16"
      ]
    },
    {
      "id": "B2AI_STANDARD:341",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "ISA-Tab",
      "description": "ISA-Tab format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:47"
      ],
      "purpose_detail": "General-purpose ISA-Tab file format - an extensible, hierarchical structure that focuses on the description of the experimental metadata (i.e. sample characteristics, technology and measurement types, sample-to-data relationships).",
      "is_open": true,
      "requires_registration": false,
      "url": "https://isa-specs.readthedocs.io/en/latest/isatab.html",
      "formal_specification": "https://github.com/ISA-tools/ISAdatasets"
    },
    {
      "id": "B2AI_STANDARD:342",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "ISO 3166",
      "description": "ISO 3166 Country Codes",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "codesystem"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:6"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:49"
      ],
      "purpose_detail": "The purpose of ISO 3166 is to define internationally recognized codes of letters and/or numbers that we can use when we refer to countries and their subdivisions. However, it does not define the names of countries  this information comes from United Nations sources (Terminology Bulletin Country Names and the Country and Region Codes for Statistical Use maintained by the United Nations Statistics Divisions).",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.iso.org/iso-3166-country-codes.html",
      "formal_specification": "https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes"
    },
    {
      "id": "B2AI_STANDARD:343",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "ISO 8601",
      "description": "ISO 8601 Date and time format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:49"
      ],
      "purpose_detail": "A way of presenting dates and times that is clearly defined and understandable to both people and machines.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.iso.org/iso-8601-date-and-time-format.html"
    },
    {
      "id": "B2AI_STANDARD:344",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "JPEG",
      "description": "Joint Photographic Experts Group Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:53",
          "category": "B2AI:Application",
          "name": "Medical Image Compression and Transfer Learning",
          "description": "JPEG format is used in AI applications for efficient storage and transmission of medical images, particularly in telepathology, dermatology AI, and mobile diagnostic applications where bandwidth and storage constraints are critical. Deep learning models are trained on JPEG-compressed whole slide images, dermoscopic images, and fundus photographs to perform tasks such as cancer detection, skin lesion classification, and diabetic retinopathy screening. While DICOM remains the standard for radiology, JPEG enables AI deployment in resource-constrained settings and supports transfer learning from natural image datasets (ImageNet) to medical imaging domains. AI researchers must account for JPEG compression artifacts when training models, and recent work explores AI-optimized compression techniques that preserve diagnostically relevant features.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "audiovisual",
        "fileformat",
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:116"
      ],
      "purpose_detail": "A method of lossy compression for digital images.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikipedia.org/wiki/JPEG",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:19"
      ]
    },
    {
      "id": "B2AI_STANDARD:345",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "JSON-schema",
      "description": "JSON-schema",
      "related_to": [
        "B2AI_STANDARD:761"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "JSON Schema is a vocabulary for annotating and validating JSON documents, providing a declarative format for describing the structure, constraints, and semantics of JSON data. As an IETF standard (draft specifications progressing toward RFC status), JSON Schema defines a JSON-based format for specifying the expected shape of JSON data, including data types, required properties, value ranges, string patterns, array constraints, and object structures. The schema itself is expressed in JSON, enabling meta-schema validation and recursive definitions. JSON Schema supports multiple vocabularies including Core, Validation, Hyper-Schema for hypermedia, and Format for semantic validation. Widely adopted across industries with 60+ million weekly downloads, it powers API documentation (OpenAPI/Swagger), configuration validation, code generation, form generation in UIs, and data interchange contracts. Major implementations exist in JavaScript, Python, Java, Go, and 40+ other languages, with extensive tooling ecosystem including validators, generators, linters, and editors. JSON Schema enables confident JSON data handling through streamlined testing, seamless data exchange via shared understanding, and clear documentation for developer collaboration. In AI/ML contexts, JSON Schema validates training data structures, API request/response payloads, configuration files for ML pipelines, and ensures data quality for machine learning workflows by catching inconsistencies and schema violations at ingestion time.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://json-schema.org/",
      "formal_specification": "https://github.com/json-schema-org/json-schema-spec"
    },
    {
      "id": "B2AI_STANDARD:346",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "KGX",
      "description": "Knowledge Graph Exchange",
      "related_to": [
        "B2AI_STANDARD:783"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:21"
      ],
      "purpose_detail": "Knowledge Graph Exchange (KGX) is a standardized graph-oriented data format for exchanging knowledge graphs, developed by the Biolink community to facilitate interoperability between biomedical knowledge bases. KGX provides both a formal specification and a Python toolkit (kgx library) for transforming, validating, and exchanging knowledge graphs conforming to the Biolink Model. The format represents graphs as nodes (entities with identifiers, categories, and properties) and edges (relationships with predicates, subject/object references, and provenance), serialized in JSON, TSV, or RDF formats. KGX ensures semantic alignment by enforcing Biolink Model categories (e.g., Gene, Disease, Pathway) and predicates (e.g., causes, treats, interacts_with), enabling consistent cross-database integration. The toolkit supports graph transformations (merging, filtering, mapping), format conversions (Neo4j, RDF, GraphML), and validation against Biolink Model constraints. KGX is foundational for Translator knowledge graphs, integrating data from 150+ biomedical sources including Monarch Initiative, NCATS Biomedical Data Translator, and Clinical Data Commons. In AI/ML applications, KGX-formatted knowledge graphs power link prediction for drug repurposing, knowledge graph embeddings for biomedical entity representation learning, reasoning algorithms for hypothesis generation, and multi-modal knowledge integration combining genomics, phenotypes, pathways, and clinical data to support precision medicine and systems biology research.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/biolink/kgx/blob/master/specification/kgx-format.md",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:21"
      ]
    },
    {
      "id": "B2AI_STANDARD:347",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "LinkML",
      "description": "LinkML",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:54",
          "category": "B2AI:Application",
          "name": "AI-Ready Data Schema Design and Validation",
          "description": "LinkML is used in AI applications to define machine-readable data schemas that enable automated data validation, transformation, and integration for machine learning pipelines. AI systems leverage LinkML schemas to ensure data quality and consistency across heterogeneous biomedical datasets, automatically generate data loaders and validators for ML frameworks, and create semantic mappings that allow AI models to understand relationships between data elements. LinkML's ability to compile to multiple formats (JSON Schema, SHACL, SQL DDL) makes it particularly valuable for building reproducible AI/ML workflows where data provenance and validation are critical.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "markuplanguage",
        "standards_process_maturity_development",
        "implementation_maturity_pilot"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:116"
      ],
      "purpose_detail": "LinkML is a flexible modeling language that allows you to author schemas in YAML that describe the structure of your data. Additionally, it is a framework for working with and validating data in a variety of formats (JSON, RDF, TSV), with generators for compiling LinkML schemas to other frameworks.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://linkml.io/linkml",
      "formal_specification": "https://github.com/linkml/linkml",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:41",
        "B2AI_SUBSTRATE:6"
      ]
    },
    {
      "id": "B2AI_STANDARD:348",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "MathML",
      "description": "MathML",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:99"
      ],
      "purpose_detail": "A product of the W3C Math Working Group, MathML is a low-level specification for describing mathematics as a basis for machine to machine communication which provides a much needed foundation for the inclusion of mathematical expressions in Web pages. It is also important in publishing workflows for science and technology and wherever mathematics has to be handled by software.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.w3.org/Math/whatIsMathML.html"
    },
    {
      "id": "B2AI_STANDARD:349",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "xlsx",
      "description": "Microsoft Excel spreadsheet container file",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:56"
      ],
      "purpose_detail": "Format used by Microsoft Excel spreadsheet software.",
      "is_open": false,
      "requires_registration": false,
      "url": "https://www.iso.org/standard/71691.html",
      "formal_specification": "https://www.iso.org/standard/71691.html"
    },
    {
      "id": "B2AI_STANDARD:350",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "Model Cards",
      "description": "Model Cards",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "modelcards"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:37"
      ],
      "purpose_detail": "Structured documentation detailing performance characteristics of machine learning models.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://modelcards.withgoogle.com/about",
      "publication": "https://arxiv.org/abs/1810.03993",
      "formal_specification": "https://github.com/tensorflow/model-card-toolkit/blob/master/model_card_toolkit/schema/v0.0.2/model_card.schema.json"
    },
    {
      "id": "B2AI_STANDARD:351",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "MP3",
      "description": "MPEG-1 Audio Layer 3 | MPEG-2 Audio Layer 3",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "audiovisual",
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:37"
      ],
      "purpose_detail": "MP3 (formally MPEG-1 Audio Layer III or MPEG-2 Audio Layer III) is a coding format for digital audio.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikipedia.org/wiki/MP3",
      "formal_specification": "https://www.iso.org/standard/26797.html",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:49"
      ]
    },
    {
      "id": "B2AI_STANDARD:352",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "MPEG-4",
      "description": "MPEG-4 Part 14 digital multimedia container format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "audiovisual",
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15",
        "B2AI_TOPIC:37"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:49"
      ],
      "purpose_detail": "A digital multimedia container format most commonly used to store video and audio, but it can also be used to store other data such as subtitles and still images. Like most modern container formats, it allows streaming over the Internet.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikipedia.org/wiki/MP4_file_format",
      "formal_specification": "https://www.loc.gov/preservation/digital/formats/fdd/fdd000155.shtml",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:49"
      ]
    },
    {
      "id": "B2AI_STANDARD:353",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "netCDF",
      "description": "Network Common Data Form",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:3",
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:8"
      ],
      "purpose_detail": "A standardized format for chromatographic data representation.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.unidata.ucar.edu/software/netcdf/",
      "formal_specification": "https://www.astm.org/e1947-98r14.html"
    },
    {
      "id": "B2AI_STANDARD:354",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "NNEF",
      "description": "Neural Network Exchange Format",
      "related_to": [
        "B2AI_STANDARD:816",
        "B2AI_STANDARD:831",
        "B2AI_STANDARD:834"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:55",
          "category": "B2AI:Application",
          "name": "Neural Network Exchange for Medical Device AI",
          "description": "NNEF (Neural Network Exchange Format) is used in biomedical AI for deploying models on medical devices and embedded systems where hardware-specific optimization and standardized model representation are critical. Medical device manufacturers leverage NNEF to ensure neural network models can be optimized for diverse hardware accelerators (DSPs, NPUs, custom ASICs) commonly used in portable medical equipment, bedside monitors, and point-of-care devices. The format enables vendor-independent model deployment, facilitates regulatory approval by providing clear model specifications, and supports hardware efficiency optimizations necessary for real-time inference in resource-constrained medical devices. NNEF is particularly valuable for AI-enabled medical devices where power consumption, latency, and deterministic behavior are critical requirements.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "An exchange format for neural network models produced using Torch, Caffe, TensorFlow, Theano, Chainer, Caffe2, PyTorch, or MXNet.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.khronos.org/nnef",
      "formal_specification": "https://github.com/KhronosGroup/NNEF-Tools",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:27",
        "B2AI_SUBSTRATE:33",
        "B2AI_SUBSTRATE:42"
      ]
    },
    {
      "id": "B2AI_STANDARD:355",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "OMXML",
      "description": "OGC and ISO Observations and Measurements standard in XML",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:49"
      ],
      "purpose_detail": "This standard specifies an XML implementation for the OGC and ISO Observations and Measurements (O&M) conceptual model (OGC Observations and Measurements v2.0 also published as ISO/DIS 19156), including a schema for Sampling Features. This encoding is an essential dependency for the OGC Sensor Observation Service (SOS) Interface Standard. More specifically, this standard defines XML schemas for observations, and for features involved in sampling when making observations. These provide document models for the exchange of information describing observation acts and their results, both within and between different scientific and technical communities.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.ogc.org/standards/om"
    },
    {
      "id": "B2AI_STANDARD:356",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "OGG Speex",
      "description": "Ogg Speex Audio Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "audiovisual",
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:37"
      ],
      "purpose_detail": "File format and bitstream encoding for for spoken content, targeted at a wide range of devices other than mobile phones.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://speex.org/docs/manual/speex-manual/node8.html",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:49"
      ]
    },
    {
      "id": "B2AI_STANDARD:357",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "ONNX",
      "description": "Open Neural Network Exchange",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:56",
          "category": "B2AI:Application",
          "name": "Cross-Platform Medical AI Model Interoperability",
          "description": "ONNX (Open Neural Network Exchange) is used in biomedical AI for creating framework-agnostic models that can be trained in PyTorch, TensorFlow, or other frameworks and deployed across diverse clinical platforms and hardware accelerators. Healthcare AI developers leverage ONNX to ensure vendor independence, enabling models to run on different hospital IT systems, edge devices, and specialized medical hardware regardless of training framework. The standard facilitates regulatory submissions by providing a stable model representation, supports hardware optimization through ONNX Runtime's performance tuning for CPUs, GPUs, and custom accelerators, and enables model sharing across research institutions without requiring framework dependencies. ONNX is particularly valuable for clinical AI products that must support multiple deployment environments.",
          "used_in_bridge2ai": false,
          "references": [
            "https://github.com/microsoft/onnxruntime"
          ]
        }
      ],
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "ONNX is an open format built to represent machine learning models.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://onnx.ai/",
      "formal_specification": "https://github.com/onnx/onnx/blob/main/docs/IR.md",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:28"
      ]
    },
    {
      "id": "B2AI_STANDARD:358",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "OpenAPI",
      "description": "OpenAPI Specification",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Standard for describing program interfaces.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://spec.openapis.org/oas/latest.html",
      "formal_specification": "https://github.com/OAI/OpenAPI-Specification/"
    },
    {
      "id": "B2AI_STANDARD:359",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "parquet",
      "description": "Parquet",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:57",
          "category": "B2AI:Application",
          "name": "Efficient Large-Scale ML Data Storage and Processing",
          "description": "Apache Parquet is widely adopted in AI/ML pipelines for efficient storage and processing of large-scale biomedical datasets, particularly for training deep learning models on tabular clinical data, multi-omics datasets, and imaging metadata. The columnar storage format enables high-performance data loading during model training, reduces storage costs through efficient compression, and supports predicate pushdown for selective feature reading. AI frameworks like TensorFlow, PyTorch, and scikit-learn leverage Parquet's integration with Apache Arrow for zero-copy data transfer, enabling faster iteration during hyperparameter tuning and model development. Parquet is particularly valuable for storing processed features from EHR data, genomic variant annotations, and large-scale biobank datasets where query performance and storage efficiency are critical.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "fileformat",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:114"
      ],
      "purpose_detail": "Apache Parquet is a free and open-source column-oriented data storage format in the Apache Hadoop ecosystem.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://parquet.apache.org/",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:30"
      ]
    },
    {
      "id": "B2AI_STANDARD:360",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "PURL",
      "description": "Persistent Uniform Resource Locator",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:43"
      ],
      "purpose_detail": "PURLs are Web addresses or Uniform Resource Locators (URLs) that act as permanent identifiers in the face of a dynamic and changing Web infrastructure. Instead of resolving directly to Web resources (documents, data, services, people, etc.) PURLs provide a level of indirection that allows the underlying Web addresses of resources to change over time without negatively affecting systems that depend on them. This capability provides continuity of references to network resources that may migrate from machine to machine for business, social or technical reasons.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://sites.google.com/site/persistenturls/",
      "formal_specification": "https://code.google.com/archive/p/persistenturls/"
    },
    {
      "id": "B2AI_STANDARD:361",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "PDF",
      "description": "Portable Document Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15",
        "B2AI_TOPIC:32"
      ],
      "purpose_detail": "A file format to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikipedia.org/wiki/PDF"
    },
    {
      "id": "B2AI_STANDARD:362",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "PNG",
      "description": "Portable Network Graphics",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "audiovisual",
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "purpose_detail": "A raster-graphics file format that supports lossless data compression.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikipedia.org/wiki/Portable_Network_Graphics",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:19"
      ]
    },
    {
      "id": "B2AI_STANDARD:363",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "PS",
      "description": "Postscript Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15",
        "B2AI_TOPIC:32"
      ],
      "purpose_detail": "PostScript (PS) is a Turing-complete, stack-based page description language and programming language developed by Adobe Systems (John Warnock, Chuck Geschke, Doug Brotz, Ed Taft, Bill Paxton) and released in 1984, revolutionizing desktop publishing by providing device-independent representation of documents combining text, vector graphics, and raster images. PostScript uses reverse Polish notation and an interpreted execution model where documents are programs that, when executed by a PostScript interpreter (Raster Image Processor or RIP), render pages at the target device's resolution. The language describes graphics using vector primitives (straight lines, cubic Bzier curves) enabling arbitrary scaling, rotation, and transformation without quality loss, crucial for professional typography and technical illustrations. PostScript's sophisticated font system uses outline fonts with font hinting to maintain glyph quality at low resolutions, standardized through Type 1, Type 2, and Type 3 font formats that influenced modern font technologies like TrueType and OpenType. Three major versions exist: PostScript Level 1 (1984) introducing basic page description capabilities, PostScript Level 2 (1991) adding improved speed, image decompression (JPEG support), composite fonts, color separation, and form caching, and PostScript 3 (1997) providing enhanced color handling with up to 4096 gray levels, smooth shading operations, DeviceN color space for spot colors, and better filtering. PostScript powered the Apple LaserWriter (1985), triggering the desktop publishing revolution by enabling WYSIWYG document creation on Macintosh with PageMaker software. The language became the de facto standard for electronic prepress systems, high-end typesetters (Linotronic), and professional printing workflows throughout the 1980s-1990s. PostScript's imaging model directly influenced PDF (Portable Document Format), Adobe's 1993 successor that simplified PostScript for document distribution by removing general-purpose programming features while retaining the imaging model, making PDF documents static data structures rather than executable programs. PostScript remains common in high-end printers, professional publishing, and scientific visualization where precise vector graphics control is required. Open-source implementations like Ghostscript enable PostScript rendering on devices lacking native PostScript support. Scientific applications include generation of publication-quality figures from computational analysis software, precise technical diagrams, and device-independent archival documents.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikipedia.org/wiki/PostScript"
    },
    {
      "id": "B2AI_STANDARD:364",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "PMML",
      "description": "Predictive Model Markup Language",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "markuplanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "PMML (Predictive Model Markup Language) uses XML to represent mining models. The structure of the models is described by an XML Schema. One or more mining models can be contained in a PMML document. A PMML document is an XML document with a root element of type PMML",
      "is_open": true,
      "requires_registration": false,
      "url": "https://dmg.org/pmml/v4-4-1/GeneralStructure.html",
      "formal_specification": "https://dmg.org/pmml/v4-4-1/GeneralStructure.html"
    },
    {
      "id": "B2AI_STANDARD:365",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "protobuf",
      "description": "Protocol Buffers",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:37"
      ],
      "purpose_detail": "Protocol Buffers (protobuf) is Google's open-source, language-neutral, platform-neutral, extensible mechanism for serializing structured data, providing compact binary encoding (typically 3-10 smaller than XML/JSON), fast parsing (20-100 faster than XML), and strongly-typed schema definitions that enable backward and forward compatibility, making it the de facto standard for high-performance RPC systems (gRPC), distributed machine learning model serving (TensorFlow Serving), and large-scale data pipelines requiring efficient cross-language interoperability. Developed internally at Google in 2001 and open-sourced in 2008, protobuf defines data structures in .proto schema files using an interface definition language (IDL) specifying messages (analogous to structs or classes) with typed fields (int32, int64, float, double, string, bytes, bool, enum, nested messages, repeated fields for arrays), field numbers (integer tags used in wire format for backward compatibility), and optional/required/repeated modifiers controlling field cardinality, with schema compilation via protoc compiler generating language-specific code (C++, Java, Python, Go, C#, JavaScript, Ruby, PHP, Objective-C, Dart, Rust) providing type-safe serialization/deserialization APIs that eliminate manual parsing and reduce bugs from schema mismatches. The wire format employs tag-length-value encoding where each field is identified by its field number (1-byte to 5-byte varint tags) enabling schema evolution field numbers are permanently reserved once assigned, new fields can be added without breaking existing code (old parsers ignore unknown fields), and fields can be deprecated by marking as reserved, ensuring long-term compatibility in distributed systems where client and server versions may diverge, critical for AI/ML production environments where model inference services must handle requests from diverse client SDKs with varying feature sets. Protobuf's compact encoding uses variable-length integers (varints) for small numbers (1-byte for 0-127, scaling to 5 bytes for large values), zigzag encoding for signed integers, length-prefixed strings, and packed repeated fields (arrays stored contiguously without per-element tags), achieving space efficiency essential for mobile/edge ML applications where model metadata, feature vectors, and predictions are serialized for transmission over bandwidth-constrained networks, and for large-scale data lakes storing billions of training examples where 10 compression translates to significant storage cost reductions. Integration with gRPC (Google's high-performance RPC framework) makes protobuf the standard for microservices communication in ML serving architectures where feature extraction services, model inference endpoints, and post-processing pipelines exchange structured requests/responses with sub-millisecond latency, with protobuf schemas defining service interfaces (methods with input/output message types) compiled into client/server stubs supporting streaming RPCs (server-side streaming for batch inference, bidirectional streaming for online learning feedback loops). TensorFlow uses protobuf extensively for serializing computation graphs (GraphDef protocol buffers), model checkpoints (SavedModel protobuf format), and TensorBoard event logs, enabling cross-platform model deployment where models trained in Python are served via C++ TensorFlow Serving with Java/Go clients consuming predictions through protobuf-defined APIs, demonstrating protobuf's role in ML operationalization pipelines. Extensions and well-known types support common patterns google.protobuf.Any enables polymorphic message embedding (useful for heterogeneous training example types), google.protobuf.Duration/Timestamp provide standardized temporal representations, google.protobuf.Struct represents JSON-like dynamic structures when schema is not known at compile time (accommodating variable-length feature vectors in online learning), and oneof constructs enable tagged unions (e.g., model prediction can be classification probabilities OR regression value, serialized efficiently with single tag). Schema documentation and tooling include proto3 syntax (simplified modern variant with implicit optional fields and JSON mapping), protobuf-to-JSON converters enabling web API gateways that accept JSON and translate to binary protobuf for backend services, and rich IDE support (IntelliJ, VSCode extensions) with syntax highlighting, error checking, and schema navigation. For AI/ML applications, protobuf enables efficient feature storage where training datasets serialize as sequences of Example protobuf messages containing feature dictionaries (feature name  tensor value), model serving where prediction requests/responses are strongly typed (preventing silent errors from schema drift), distributed training where parameter server updates exchange gradient protobuf messages, and model monitoring where inference logs capture predictions and ground-truth labels in compact binary format suitable for large-scale analysis and retraining pipelines.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://developers.google.com/protocol-buffers/",
      "formal_specification": "https://github.com/protocolbuffers/protobuf"
    },
    {
      "id": "B2AI_STANDARD:366",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "PROV",
      "description": "Provenance",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:99"
      ],
      "purpose_detail": "The PROV Family of Documents defines a model, corresponding serializations and other supporting definitions to enable the inter-operable interchange of provenance information in heterogeneous environments such as the Web.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.w3.org/TR/prov-overview/"
    },
    {
      "id": "B2AI_STANDARD:367",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "pickle",
      "description": "Python pickle format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Serialization format for a Python object structure. Pickling is the process whereby a Python object hierarchy is converted into a byte stream, and unpickling is the inverse operation, whereby a byte stream (from a binary file or bytes-like object) is converted back into an object hierarchy.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://docs.python.org/3.8/library/pickle.html",
      "formal_specification": "https://github.com/python/cpython/blob/3.8/Lib/pickle.py"
    },
    {
      "id": "B2AI_STANDARD:368",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "QuDEx",
      "description": "Qualitative Data Exchange Schema",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:97"
      ],
      "purpose_detail": "The Qualitative Data Exchange Schema (QuDEx) allows users to discover, find, retrieve and cite complex qualitative data collections in context.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.data-archive.ac.uk/managing-data/standards-and-procedures/metadata-standards/qudex/",
      "formal_specification": "https://dam.data-archive.ac.uk/standards/qudex_v03_01.xsd"
    },
    {
      "id": "B2AI_STANDARD:369",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "RDA 10 Things",
      "description": "RDA CURE-FAIR 10 Things for Curating Reproducible and FAIR Research",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:83"
      ],
      "purpose_detail": "A framework for implementing effective curation workflows for achieving greater FAIR-ness and long-term usability of research data and code.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.rd-alliance.org/group/cure-fair-wg/outcomes/10-things-curating-reproducible-and-fair-research",
      "formal_specification": "https://curating4reproducibility.org/10things/"
    },
    {
      "id": "B2AI_STANDARD:370",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "RDFS",
      "description": "RDF Schema",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:99"
      ],
      "purpose_detail": "RDF Schema (RDFS) is the foundational vocabulary description language that extends the basic RDF vocabulary to provide essential data modeling capabilities for the Semantic Web and linked data applications. As a W3C Recommendation and semantic extension of RDF, RDFS enables the description of groups of related resources and relationships between them by defining classes, properties, and their hierarchical structures. Unlike traditional object-oriented programming models that define classes in terms of their instance properties, RDFS takes a property-centric approach where properties are described in terms of the classes they apply to through domain and range mechanisms. This design philosophy promotes the extensibility principle of the Web, allowing anyone to define additional properties for existing resources without requiring modification of original class definitions. RDFS provides core vocabulary elements including rdfs:Class, rdfs:Resource, rdfs:Property, rdfs:subClassOf, rdfs:subPropertyOf, rdfs:domain, rdfs:range, rdfs:label, and rdfs:comment, which form the basis for more sophisticated ontology languages like OWL. The schema supports the development of machine-readable vocabularies that can be processed automatically, enabling applications to discover and reason about resource relationships, making it an essential component of the Semantic Web infrastructure.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.w3.org/TR/rdf-schema/"
    },
    {
      "id": "B2AI_STANDARD:371",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "RIF-CS",
      "description": "Registry Interchange Format - Collections and Services schema",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:6"
      ],
      "purpose_detail": "The Registry Interchange Format - Collections and Services (RIF-CS) is an XML-based metadata schema developed by the Australian National Data Service (ANDS) for describing and exchanging information about research collections, services, parties (people and organizations), and activities. RIF-CS serves as the foundational data interchange format for Research Data Australia and enables institutions to contribute metadata about their research assets to national and international discovery services. The schema organizes metadata into four core entity types with rich relationship modeling capabilities - collections (datasets, databases, repositories), services (software tools, web services, facilities), parties (researchers, institutions, funders), and activities (projects, programs, events). Each entity supports comprehensive descriptive metadata including identifiers, names, descriptions, locations, dates, subjects, and crucially, relationships to other entities that create a connected graph of research infrastructure. RIF-CS enables automated harvesting and aggregation of research metadata across institutions, supporting research discovery, collaboration, and compliance with research data management policies.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://services.ands.org.au/documentation/rifcs/1.2.0/guidelines/rif-cs.html"
    },
    {
      "id": "B2AI_STANDARD:372",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "RO-CRATE",
      "description": "Research Object Crate",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:243",
          "category": "B2AI:Application",
          "name": "Workflow Run RO-Crate for ML Digital Pathology",
          "description": "Workflow Run RO-Crate profiles are applied to capture execution provenance for machine learning workflows that train and evaluate deep learning models to detect carcinoma cells in high-resolution digital images of magnified human prostate tissue. The Process Run Crate and CPM RO-Crate profiles bundle workflow inputs (training/testing image datasets), outputs (trained models, classification results, evaluation metrics), scripts implementing preprocessing/training/evaluation steps, execution logs, and explicit parameter mappings connecting workflow-level parameters to tool-level parameters (e.g., linking workflow parameter to \"extract_tissue.cwl tool\"), enabling programmatic discovery of which tools are affected by workflow-level configurations and providing insight into workflow internal mechanics. The same CWL workflow can be executed with different workflow management systems (cwltool converting to Provenance Run Crate via runcrate tool, StreamFlow WMS) generating fully interoperable RO-Crates with consistent metadata structures recording four actions (workflow, two tissue extraction tool runs, tumour classification tool run) including schema.org instrument links, start/end times, inputs/outputs, and parameter value mappings, demonstrating cross-WMS provenance interoperability. Reported benefits include traceability of parameters across execution layers, improved reproducibility via properties like schema.org alternateName documenting alternative tool identifiers, integration of distributed provenance via the CPM extension, and cohesive packing of data/metadata/provenance enabling rerun of computations from recorded CWL input parameter mappings, supporting quality assessment and validation of ML-based diagnostic outputs in digital pathology research.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1371/journal.pone.0309210"
          ]
        },
        {
          "id": "B2AI_APP:244",
          "category": "B2AI:Application",
          "name": "MLentory Model Registry with RO-Crate and FAIR Signposting",
          "description": "MLentory uses RO-Crate plus FAIR Signposting to implement a lightweight FAIR Digital Object registry for machine learning model metadata, treating the metadata (rather than model binaries) as the primary research object. Each model's metadataincluding name, description, provenance, licensing, associated datasets, and publication referencesis encapsulated as an RO-Crate JSON-LD document dynamically generated via server-side rendering and exposed through a backend API, with direct download links available from each model's landing page. FAIR Signposting is implemented via HTML <link> elements and HTTP Link headers connecting landing pages to RO-Crates and machine-readable metadata, with each RO-Crate assigned a resolvable persistent identifier via w3id.org following FDO Configuration Type 2 (PID resolution to landing page, which links to metadata). The system improves FAIRness of ML model metadata by making it machine-actionable (enabling automated harvesting and integration), discoverable (through structured Signposting links and Schema.org markup), reusable (with explicit licensing and provenance), and interoperable (via JSON-LD serialization supporting integration with knowledge graphs and semantic web tools). Current limitations include absence of hosted model artifact binaries (registry focuses on metadata aggregation from external repositories), reliance on simulated PIDs for some artifacts where permanent identifiers are unavailable, and RO-Crates linking to downloadable artifacts via landing pages rather than embedding permanent archives (with plans to offer temporary archive downloads), demonstrating a pragmatic approach to FAIR model metadata management that balances lightweight implementation with evolving FDO principles while improving transparency and reproducibility of ML model documentation.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.18420/inf2025_104",
            "https://doi.org/10.52825/ocp.v5i.1188"
          ]
        },
        {
          "id": "B2AI_APP:245",
          "category": "B2AI:Application",
          "name": "FAIRSCAPE AI-Readiness Platform with RO-Crate Packaging",
          "description": "FAIRSCAPE implements RO-Crate as the core packaging format for an AI-readiness framework targeting biomedical datasets and software, where users collect required metadata (producing JSON-LD shown in web interface), package it into ZIP files, and upload RO-Crate bundles to FAIRSCAPE server instances built on FastAPI with MinIO S3-compliant object storage for digital objects, MongoDB for metadata management, and REDIS message broker. The server receives, catalogs, indexes, extracts, and registers uploaded RO-Crate packages with their component metadata, exposing CLI and Web GUI clients for upload/view/download/publish operations. Each RO-Crate landing page displays a table summary of required JSON-LD metadata, serializations in multiple formats (JSON-LD for Schema.org interoperability, RDF/Turtle for semantic web integration), and D3 visualizations of evidence graphs (via jsonld-vis) revealing provenance relationships among datasets, computations, and software components. RO-Crates can be published directly to Dataverse repository instances for long-term preservation and institutional compliance. Near-term development targets explicit AI-readiness through planned expansion of metadata to include Bridge2AI Model Cards (documenting intended use, known limitations, bias assessments, performance benchmarks) and Datasheets for Datasets (capturing data collection methodology, annotation processes, ethical considerations), support for statistical characterization of input datasets (enabling automated data quality assessment and feature distribution analysis), integration with PEP (Portable Encapsulated Projects) for genomics workflows, and support for Bridge2AI CHoRUS challenge requirements. Reported benefits include deep provenance capture supporting explainability and auditability (critical for clinical AI regulatory compliance), FAIR-compliant metadata enabling automated discovery and integration, controlled sharing and publishing workflows (supporting collaborative research with governance controls), and pre-model explainability where data quality, feature engineering, and transformation provenance are documented before model training, supporting responsible AI development practices in biomedical research contexts.",
          "used_in_bridge2ai": true,
          "references": [
            "https://doi.org/10.1101/2024.12.23.629818"
          ]
        },
        {
          "id": "B2AI_APP:246",
          "category": "B2AI:Application",
          "name": "HPC Provenance Capture for ML-at-Scale Workflows",
          "description": "COMPSs runtime system implements automatic RO-Crate generation for HPC workflows, capturing provenance of large-scale computational tasks including distributed machine learning training and inference runs common in high-performance computing environments. The approach is lightweight and efficient the COMPSs runtime records file accesses during execution in dataprovenance.log files, then generates RO-Crate packages post-run via generate_COMPSs_RO-Crate.py using ro-crate-py library (version 0.6.1) to avoid runtime overheads that would degrade performance on time-critical HPC jobs, enabling scalability for large workflows with thousands of task nodes and files. Generated RO-Crates (named COMPSs_RO-Crate_[uuid]/) contain ro-crate-metadata.json with machine-readable JSON-LD adhering to Workflow RO-Crate profile, application source code files annotated with types (File, SoftwareSourceCode, ComputationalWorkflow), workflow visualization images (e.g., complete_graph.pdf showing task dependencies), command-line arguments capturing execution configuration, and input/output file lists with provenance relationships. The system integrates with WorkflowHub for workflow discovery and sharing, supporting FAIR principles by making provenance findable (through workflow registries), accessible (via standard RO-Crate format), interoperable (JSON-LD serialization compatible with semantic web tools), and reusable (bundling code, parameters, and artifacts needed for reproduction). Design goals achieved include automatic provenance registration requiring no user annotations (reducing burden on computational scientists), efficiency through post-execution generation minimizing impact on HPC resource utilization, and scalability proven on production workflows processing large scientific datasets. For AI/ML applications, this approach enables transparent documentation of distributed training runs (recording which GPUs/nodes processed which data partitions, parameter server update patterns, checkpoint schedules), model hyperparameter sweeps (capturing parameter combinations and resulting validation metrics across hundreds of parallel trials), and large-scale inference workflows (documenting model versions, input data batches, and output predictions with lineage), supporting reproducibility, debugging, and auditing requirements for production machine learning systems deployed on supercomputing infrastructure.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1109/works56498.2022.00006"
          ]
        },
        {
          "id": "B2AI_APP:247",
          "category": "B2AI:Application",
          "name": "Workflow Packaging and Testing Ecosystems for ML Pipelines",
          "description": "Community-developed RO-Crate profiles for computational workflows and workflow testing, maintained by WorkflowHub and adopted by Galaxy, LifeMonitor, and other platforms, provide standardized packaging and automated quality assurance mechanisms frequently leveraged for machine learning pipelines requiring reproducibility and interoperability across heterogeneous execution environments. Workflow RO-Crate profile snapshots multi-file workflows (e.g., Common Workflow Language definitions in GitHub repositories, Galaxy workflow XML, Nextflow scripts) into single RO-Crate ZIP archives for archival preservation and assignment of persistent identifiers, with formal metadata documenting workflow structure (main workflow file, subworkflows, tool dependencies), input/output specifications with types and formats, software dependencies with versions, and authorship/licensing information following Schema.org and Bioschemas conventions. Workflow Testing RO-Crate extension adds formal testing components integrated with LifeMonitor, enabling specification of test instances (example inputs with expected outputs), automated health checks (periodic execution to verify workflow functionality), and continuous integration workflows, ensuring that packaged ML pipelines maintain correctness as underlying tools evolve. RO-Crates serve as typed, machine-actionable metadata templates enabling workflow discovery through registries (WorkflowHub search and filtering), PID resolution and FAIR Signposting (linking DOIs to workflow metadata and executable files), and CRUD operations via standardized APIs (GA4GH TRS API for programmatic workflow retrieval and execution). Profiles have been extended to encode provenance at multiple levels (researchers/authorship, computational resources/infrastructure, activities/executions, measurements/results) and combined with machine-actionable Data Management Plans, with a tailored profile existing for electronic lab notebook protocols supporting experimental workflow documentation. Reported benefits include improved machine-actionability (enabling automated workflow execution and composition in platforms like Galaxy), FAIRness through findability (registry integration), accessibility (PID resolution), interoperability (Schema.org/Bioschemas vocabularies compatible across domains), and reusability (licensing and frozen snapshots preventing version drift), portability via self-contained packaging (workflows travel with dependency specifications and example data), reproducibility through testing infrastructure (automated validation that workflows produce consistent outputs), and interoperability demonstrated by cross-platform workflow sharing (CWL workflows developed in one environment executed in Galaxy, Nextflow workflows shared via WorkflowHub consumed by diverse execution backends), supporting collaborative ML research where pipeline components are shared, validated, and reused across institutions and computational infrastructures.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.3897/rio.8.e93937"
          ]
        }
      ],
      "collection": [
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:116"
      ],
      "purpose_detail": "Research Object Crate (RO-Crate) is a community-developed, lightweight standard for packaging research data, code, workflows, and metadata into self-describing, FAIR-compliant digital objects using Schema.org annotations serialized as JSON-LD, making formal metadata description accessible and practical across diverse research contexts from individual datasets to large-scale computational experiments. Emerging from the Research Object movement and endorsed by communities including WorkflowHub, Australian Research Data Commons (ARDC), and European Open Science Cloud (EOSC), RO-Crate addresses the reproducibility crisis by bundling computational artifacts with their contextual metadataincluding authorship, provenance, licensing, computational environment specifications, and relationships among componentsin a machine-readable format that enables automated validation, discovery, and reuse without requiring specialized knowledge representation tools. The specification defines a structured directory containing data files, software, documentation, and a standardized ro-crate-metadata.json file at the root that describes the crate's contents using Schema.org vocabulary (Person, Organization, Dataset, SoftwareApplication, File, CreativeWork) extended with domain-specific types for computational workflows (ComputationalWorkflow, FormalParameter, WorkflowDescription Language), enabling rich representation of research artifacts as connected graphs where entities link to authors, funding agencies, publications, input/output data, and execution environments. RO-Crate supports hierarchical composition where complex research objects nest sub-cratesfor example, a parent crate describing a machine learning experiment contains child crates for training data, validation data, model checkpoints, inference code, and evaluation resultseach with its own metadata while the parent ro-crate-metadata.json aggregates relationships among components, supporting modular reuse where individual datasets or code components can be extracted and cited independently while maintaining their connection to the larger research context. The format's lightweight design requires only JSON-LD understanding (widely supported across programming languages) rather than specialized triple stores or OWL reasoners, with human-readable JSON structure enabling researchers to manually create or inspect metadata using text editors while automated tools (ro-crate-py Python library, ro-crate-js JavaScript package, ro-crate-ruby) provide programmatic creation, validation, and parsing capabilities integrating with workflow management systems (Galaxy, Nextflow, Snakemake), electronic lab notebooks (RSpace), data repositories (Zenodo, OSF, institutional repositories), and model registries (Hugging Face, MLflow). Integration with FAIR principles is explicit Findable through required persistent identifiers (DOIs, ORCIDs, ROR for organizations) and rich descriptive metadata; Accessible via standard web protocols with human-readable HTML landing pages generated from JSON-LD metadata; Interoperable through Schema.org's ubiquitous vocabulary and JSON-LD's linked data semantics enabling merging and querying across crates; Reusable through explicit licensing (Creative Commons, software licenses), versioning (schema.org/version), and detailed provenance including software versions, parameter settings, and computational environment specifications (Docker images, Conda environments, language versions). RO-Crate profiles enable domain-specific metadata requirements: Workflow RO-Crate for computational workflows with explicit input/output specifications and tool dependencies; ML RO-Crate for machine learning models bundling training data, model artifacts, performance metrics, and deployment requirements; Bioschemas profiles for life sciences data integrating biological annotations. WorkflowHub (workflowhub.eu), a major adopter, uses RO-Crate to package computational workflows from diverse systems (Galaxy, Nextflow, CWL, Snakemake) with consistent metadata enabling cross-platform workflow discovery and reuse, while FAIRSCAPE implements RO-Crate for AI-readiness packaging combining datasets, evidence graphs (EVI provenance), data dictionaries, and validation results in portable bundles supporting explainable AI requirements. For AI/ML applications, RO-Crate enables reproducible model packaging where training datasets, preprocessing scripts, model weights (Safetensors, PyTorch checkpoints), evaluation notebooks, and computational environment specifications travel together with metadata documenting intended use cases, known limitations, bias assessments, and performance benchmarks, supporting regulatory compliance in clinical AI, transparent reporting in publications, and confident model reuse by downstream researchers who can verify exact software versions, hyperparameters, and data provenance before adapting models to new domains.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.researchobject.org/ro-crate/",
      "publication": "doi:10.3233/DS-210053",
      "formal_specification": "https://www.researchobject.org/ro-crate/specification.html"
    },
    {
      "id": "B2AI_STANDARD:373",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "RDF",
      "description": "Resource Description Framework",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:99"
      ],
      "purpose_detail": "Resource Description Framework (RDF) is a W3C standard framework for representing information about resources on the Web using subject-predicate-object triples. RDF provides a graph-based data model where resources are identified by IRIs (Internationalized Resource Identifiers) and relationships form a directed, labeled graph. The framework enables decentralized knowledge representation, allowing anyone to make statements about any resource and merge distributed data seamlessly. RDF Schema (RDFS) extends RDF with vocabulary for describing classes, properties, domain, range, and hierarchical relationships, enabling semantic reasoning and inference. RDF supports multiple serialization formats including Turtle (human-readable), RDF/XML (verbose XML), JSON-LD (JSON-based), and N-Triples (line-oriented). The semantic web stack builds upon RDF: OWL for rich ontologies, SPARQL for querying RDF graphs, SHACL for shape validation. RDF powers linked open data initiatives (DBpedia, Wikidata), biomedical ontologies (OBO Foundry, Bio2RDF), knowledge graphs (Google Knowledge Graph principles), and enterprise knowledge management. The framework enables data integration across heterogeneous sources by providing common vocabularies (Dublin Core, FOAF, Schema.org) and federated querying. In AI/ML contexts, RDF graphs serve as structured knowledge bases for knowledge graph embeddings (TransE, DistMult, ComplEx), semantic reasoning for inference rules, ontology-guided feature engineering, and multi-relational learning where symbolic knowledge augments statistical learning, enabling explainable AI and knowledge-driven machine learning systems.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.w3.org/TR/rdf-schema/"
    },
    {
      "id": "B2AI_STANDARD:374",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "Safetensors",
      "description": "Safetensors",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Safetensors is a secure, simple, and fast file format for storing machine learning model weights and tensors, designed by Hugging Face to eliminate critical security vulnerabilities inherent in Python's pickle serialization while maintaining zero-copy memory-mapped loading performance essential for rapid model deployment and inference in production environments. Unlike pickle-based formats (PyTorch's .pt/.pth, TensorFlow's SavedModel with pickled components) that execute arbitrary Python code during deserializationenabling attackers to embed malicious code in model files that executes upon loading, compromising systems through remote code execution, data exfiltration, or cryptominingSafetensors provides a pure data format with no executable code, consisting of a JSON header specifying tensor metadata (names, shapes, dtypes, byte offsets) followed by raw tensor data in a flat binary layout, enabling parsers to validate file structure and extract tensors without executing untrusted code. The format achieves zero-copy loading through memory mapping where tensor data is accessed directly from disk without intermediate copying into RAM, reducing model loading time from seconds to milliseconds for large language models (LLaMA 70B, Falcon 180B with hundreds of gigabytes of weights) and enabling memory-efficient inference where only accessed tensor regions are paged into physical memory, particularly valuable for edge deployment on devices with limited RAM where traditional loading would exceed memory capacity. Safetensors supports all standard numeric data types (float32, float16, bfloat16, int8, uint8) used in modern neural networks including quantized models, with explicit endianness specification ensuring cross-platform compatibility between x86, ARM, and GPU architectures, and tensor metadata encoding shapes up to 8 dimensions with clear ordering (row-major/column-major) preventing misinterpretation that causes silent model corruption. The format provides built-in integrity verification through file structure validation and optional checksums, detecting corrupted downloads or storage errors before model loading, while supporting streaming and partial loading where specific layers or tensors can be extracted without parsing the entire file, enabling selective model component updates (e.g., replacing adapter modules, swapping attention heads) or distributed model serving where different nodes host different model shards. Integration with major deep learning frameworks through official libraries (safetensors Python package, Rust crate) enables seamless conversion from existing checkpoint formats (torch.save to safetensors.torch.save_file) with equivalent API ergonomics, while Hugging Face Hub defaults to Safetensors for all uploaded models since 2023, with automatic conversion tools for legacy pickle checkpoints and transparent fallback mechanisms ensuring backward compatibility. The format specification is framework-agnostic, supporting not only PyTorch and TensorFlow but also JAX (Flax), ONNX Runtime, and custom inference engines (llama.cpp, vLLM, TensorRT-LLM), enabling model portability across deployment stacks without format conversions that risk numerical precision degradation or metadata loss. Security benefits extend beyond code execution prevention to defense against adversarial model poisoning where malicious actors distribute trojan models disguised as benign checkpointsSafetensors' inability to execute code during loading eliminates entire classes of supply chain attacks targeting the model distribution phase, complementing runtime defenses against adversarial inputs. For model repositories and registries (Hugging Face Hub, TensorFlow Hub, PyTorch Hub), Safetensors reduces infrastructure risk by eliminating server-side vulnerabilities where automated model scanning or preview generation could trigger malicious pickle payloads, enabling safer community model sharing at scale. Performance benchmarks demonstrate loading speedups of 10-100 versus pickle for large models due to zero-copy memory mapping and elimination of Python object reconstruction overhead, with particularly dramatic improvements for quantized models (GPTQ, AWQ, GGUF-compatible formats) where 4-bit or 8-bit weights pack efficiently into Safetensors' raw binary layout without requiring custom pickle classes. The format's simplicitya JSON header plus raw tensor bytesenables easy implementation in languages beyond Python (Rust, C++, Go, JavaScript) supporting diverse deployment environments from mobile (CoreML, TensorFlow Lite conversion pipelines) to web (WASM-based inference) to embedded systems (microcontroller ML frameworks), democratizing safe model deployment across the full spectrum of AI/ML applications without platform-specific serialization lock-in.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/huggingface/safetensors",
      "formal_specification": "https://github.com/huggingface/safetensors",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:42"
      ]
    },
    {
      "id": "B2AI_STANDARD:375",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "SVG",
      "description": "Scalable Vector Graphics Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "audiovisual",
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:99"
      ],
      "purpose_detail": "Scalable Vector Graphics (SVG) Version 1.1, a modularized language for describing two-dimensional vector and mixed vector/raster graphics in XML.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.w3.org/Graphics/SVG/About.html",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:19"
      ]
    },
    {
      "id": "B2AI_STANDARD:376",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "SHACL",
      "description": "Shapes Constraint Language",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:99"
      ],
      "purpose_detail": "A language for validating RDF graphs against a set of conditions. These conditions are provided as shapes and other constructs expressed in the form of an RDF graph. RDF graphs that are used in this manner are called shapes graphs in SHACL and the RDF graphs that are validated against a shapes graph are called data graphs. As SHACL shape graphs are used to validate that data graphs satisfy a set of conditions they can also be viewed as a description of the data graphs that do satisfy these conditions. Such descriptions may be used for a variety of purposes beside validation, including user interface building, code generation and data integration.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.w3.org/TR/shacl/"
    },
    {
      "id": "B2AI_STANDARD:377",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "SGI",
      "description": "Silicon Graphics image format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "audiovisual",
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "purpose_detail": "Silicon Graphics Image (SGI or RGB) is a raster graphics file format developed by Silicon Graphics, Inc. for storing and displaying digital images on SGI workstations and UNIX systems. The format supports 8-bit, 16-bit, and 24-bit color depths with optional alpha channel for transparency, allowing representation of grayscale, RGB, and RGBA images with various bit depths per channel. SGI images can be stored uncompressed or with run-length encoding (RLE) compression for reduced file sizes while maintaining lossless quality. The format was widely used in professional computer graphics, visual effects, scientific visualization, and medical imaging during the 1980s-1990s when SGI workstations dominated high-end graphics computing. SGI files use .sgi, .rgb, .rgba, .bw, or .int file extensions depending on color configuration and bit depth. The format specifies image dimensions, number of channels (1 for grayscale, 3 for RGB, 4 for RGBA), compression method, and pixel data in a header-based structure readable by graphics software on big-endian systems. While largely superseded by more modern formats like PNG and TIFF for general use, SGI format remains relevant in legacy scientific visualization applications, particularly in medical imaging archives, computational fluid dynamics visualization, and legacy 3D rendering pipelines. ImageMagick, GIMP, and specialized scientific visualization software maintain SGI format support for backward compatibility with historical image datasets. The format's simplicity and direct pixel representation made it suitable for high-performance graphics rendering on SGI's proprietary hardware and OpenGL-based visualization systems.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikipedia.org/wiki/Silicon_Graphics_Image",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:19"
      ]
    },
    {
      "id": "B2AI_STANDARD:378",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "SSSOM",
      "description": "Simple Standard for Sharing Ontological Mappings",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "SSSOM is a Simple Standard for Sharing Ontological Mappings, providing a TSV-based representation for ontology term mappings, a comprehensive set of standard metadata elements to describe mappings, and a standard translation between the TSV and the Web Ontology Language (OWL).",
      "is_open": true,
      "requires_registration": false,
      "url": "https://mapping-commons.github.io/sssom/",
      "publication": "doi:10.1093/database/baac035",
      "formal_specification": "https://github.com/mapping-commons/sssom",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:41",
        "B2AI_SUBSTRATE:6"
      ]
    },
    {
      "id": "B2AI_STANDARD:379",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "statismo",
      "description": "Statismo format",
      "related_to": [
        "B2AI_STANDARD:339"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Statismo defines a storage format (Statistical Image And Shape Models) based on HDF5, which includes all the information necessary to use the model, as well as meta-data about the model creation, which helps to make model building reproducible.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/statismo/statismo",
      "formal_specification": "https://github.com/statismo/statismo",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:16"
      ]
    },
    {
      "id": "B2AI_STANDARD:380",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "SDMX",
      "description": "Statistical Data and Metadata eXchange standard",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:239",
          "category": "B2AI:Application",
          "name": "Natural Language Question Answering over SDMX Statistical Databases",
          "description": "A natural language question answering (QA) system built to interact with SDMX-structured statistical databases enables users to query official statistics in plain English by parsing natural language questions, mapping linguistic elements to SDMX dimensions and codelists via Data Structure Definitions, identifying comparison and aggregation intents, detecting temporal and geographic constraints, ranking candidate SDMX datasets by query relevance, and retrieving numeric values from SDMX web services to generate natural language responses. Implemented over OECD's SDMX web service endpoints covering 12 statistical tables across 3 categories (labor, education, health), the QA system performs module-level AI/NLP tasks including question-type detection classifying queries as value requests or list requests (evaluated on 100 queries), time and location extraction parsing FROM/TO/THAN temporal expressions and geographic mentions (30 queries), comparison operator identification detecting greater-than/less-than/equal relationships (35 queries), aggregation intent recognition identifying count/min/max operations and item enumeration requests (30 queries), topic and table identification selecting relevant SDMX tables from query keywords with Top-1 accuracy 58% and Top-category accuracy 94% (48 queries distinguishing known vs unknown SDMX topics), and dimension-value extraction mapping query terms to specific SDMX dimension values such as SEX=FEMALE or COUNTRY=FRANCE with 78% accuracy (40 queries across 10 tables). The system leverages SDMX metadata structures where Data Structure Definitions list dimensions and codelists (allowed values) enabling semantic matching between natural language terms and standardized codes, and SDMX web services (SOAP/WSDL endpoints) provide programmatic access to multidimensional statistical arrays with cells containing numeric observations. Dataset ranking computes proximity scores based on keyword overlap between user queries and SDMX table metadata (titles, descriptions, dimension labels), presenting top-ranked candidates for user selection when ambiguity persists. Limitations include linguistic pattern coverage restricted to English templates, domain knowledge gaps requiring manual codelist mappings, and difficulty handling multi-hop reasoning across related indicators, but the system demonstrates concrete operational AI/NLP capabilities directly over SDMX infrastructure, enabling non-technical users to access complex official statistics through conversational interfaces without learning SDMX query syntax or navigating multidimensional data catalogs.",
          "used_in_bridge2ai": false,
          "references": [
            "https://inria.hal.science/hal-03021075/document"
          ]
        },
        {
          "id": "B2AI_APP:240",
          "category": "B2AI:Application",
          "name": "Gingado ML Dataset Augmentation via SDMX Protocol",
          "description": "Gingado is an open-source Python library for machine learning in economics and finance that leverages the SDMX (Statistical Data and Metadata eXchange) protocol to programmatically augment user-provided datasets with official statistical series from authoritative sources (central banks, statistical offices, international organizations), enabling empirical testing of whether SDMX-sourced macroeconomic indicators improve model performance in prediction and classification tasks. The library integrates with standard ML tooling including scikit-learn workflows, providing utilities for data augmentation where users specify target economic indicators (GDP growth, inflation rates, unemployment percentages, trade balances, interest rates, exchange rates) and gingado automatically retrieves corresponding time series from SDMX web service endpoints (IMF, OECD, Eurostat, World Bank, BIS), aligns temporal frequencies (quarterly, monthly, annual) with user data, handles missing values through forward-filling or interpolation, and merges SDMX series into feature matrices maintaining temporal ordering for supervised learning. SDMX protocol advantages for ML augmentation include standardized dimension definitions ensuring consistent interpretation of time periods, geographic areas, and indicator classifications across data providers; automated metadata retrieval specifying units of measure, seasonal adjustment status, and revision policies enabling appropriate preprocessing; and RESTful API access with filtering by date ranges and country codes reducing manual data collection overhead. Gingado supports benchmarking workflows where models trained on original features are compared against models augmented with SDMX series using cross-validation, with performance metrics (RMSE, MAE, R-squared for regression; accuracy, F1-score for classification) quantifying the marginal contribution of official statistics to predictive accuracy, addressing research questions such as \"Do central bank policy rate series improve corporate default prediction?\" or \"Does trade balance data enhance exchange rate forecasting?\" The library provides simulation capabilities for generating synthetic economic scenarios and documentation features for reproducible ML experiments, with SDMX-sourced provenance metadata (data provider, publication date, methodology links) embedded in experiment logs supporting transparent reporting of data sources in research publications and regulatory submissions. Discussions with SDMX technical experts informed library design choices regarding API endpoint selection, codelist mapping strategies, and handling of SDMX versioning where structural definitions evolve over time, ensuring robust data ingestion despite heterogeneity in SDMX implementations across international organizations.",
          "used_in_bridge2ai": false,
          "references": [
            "https://www.bis.org/publ/work1122.pdf"
          ]
        },
        {
          "id": "B2AI_APP:241",
          "category": "B2AI:Application",
          "name": "SDMX Vocabularies for Scientometric Knowledge Graph Construction",
          "description": "A scientometric indicator modeling system employs SDMX vocabularies (sdmx-attribute, sdmx-dimension, sdmx-measure) combined with W3C Data Cube ontology to generate machine-readable RDF observations representing research output metrics, enabling automated knowledge graph construction and graph-based analytics suitable for AI and explainable ML workflows. The system extends sdmx-measure:obsValue to represent numeric measures including publication counts, citation totals, researcher headcounts, and postdoctoral researcher numbers, and extends sdmx-dimension:refPeriod to represent temporal intervals (academic years, funding periods) by reusing time interval instances from UK reference data servers, ensuring semantic interoperability with other statistical datasets adhering to SDMX and Data Cube standards. Two interactive Python notebooks (one for multidimensional indicators with multiple grouping dimensions, one for unidimensional indicators with single time dimension) programmatically unpivot CSV input files containing raw scientometric data, generate SDMX-annotated RDF triples in Turtle serialization format, and produce 10 output RDF files representing complete observation sets with explicit dimension values, measures, and attributes. Data Structure Definitions declare dimensions (research institution, academic department, funding source, time period), attributes (collection methodology, data quality flags, confidentiality levels), and measures (publication output, citation impact, personnel counts), following SDMX modeling patterns for official statistics but applied to research evaluation context. The generated RDF observations are imported into a Neo4j graph database using the n10s (Neosemantics) plugin via import.fetch commands, creating a property graph representation of scientometric indicators where nodes represent institutions, departments, and time periods connected by edges labeled with SDMX dimension relationships and measure values, enabling CYPHER queries for cross-institutional comparisons, temporal trend analysis, and correlation discovery among research metrics. SKOS (Simple Knowledge Organization System) concept schemes organize indicator taxonomies (publication types, citation impact classes, researcher career stages) with hierarchical broader/narrower relationships, supporting aggregation queries such as \"total publications across all natural science departments\" by traversing SKOS hierarchies. While the system does not explicitly demonstrate downstream AI/ML models in the documented work, the resulting SDMX-structured knowledge graph provides foundational infrastructure commonly used for graph neural networks, knowledge graph embeddings, link prediction, and explainable AI systems that require semantically consistent, machine-readable statistical observations with explicit provenance and quality metadata, illustrating how SDMX vocabularies extend beyond economic statistics to support AI-ready knowledge graphs in scientometrics and research evaluation domains.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1186/s40537-022-00562-x"
          ]
        },
        {
          "id": "B2AI_APP:242",
          "category": "B2AI:Application",
          "name": "Graph Neural Networks on SDMX-Inspired Open Statistics Knowledge Graphs",
          "description": "Graph Neural Network (GNN) models for predictive analytics over open government statistics leverage SDMX-inspired concepts (sdmx:timePeriod for temporal dimensions, sdmx:refArea for geographic regions, sdmx:sex for demographic breakdowns) within linked data cubes constructed using W3C Data Cube vocabulary, demonstrating how SDMX semantic standardization facilitates explainable AI on spatiotemporal official statistics. A house price estimation system for Scotland constructs an open statistics knowledge graph by integrating multiple linked data sources (Scottish Statistics census indicators, geographic boundary datasets, temporal reference series) where observations are structured as RDF triples following Data Cube patterns with dimensions borrowed from UK Government Linked Data Working Group definitions that draw inspiration from SDMX guidelines, ensuring consistent semantic annotation of statistical dimensions across datasets. The knowledge graph is queried via SPARQL to extract feature vectors for machine learning, with each house sale record linked to spatiotemporal context through SDMX-style dimensions (sdmx:timePeriod connecting sales to quarterly or monthly time slices, sdmx:refArea linking properties to census output areas and administrative regions, demographic dimensions capturing population characteristics, age distributions, employment statistics, and deprivation indices at multiple geographic granularities). Three GNN architectures are evaluated Chebyshev Spectral Graph Convolution (ChebNet) using polynomial filters for localized spatial feature aggregation, Graph Convolutional Network (GCN) applying first-order approximations for efficient neighborhood aggregation, and GraphSAGE sampling fixed-size neighborhoods to enable scalable inductive learning on large graphseach trained to predict house prices by learning representations that encode both property-specific features and graph-structured relationships among geographic areas, temporal periods, and demographic indicators. Explainability analysis via GNNExplainer identifies which SDMX-aligned dimensions (time periods with specific economic conditions, geographic areas with characteristic demographics, interactions between temporal trends and spatial patterns) contribute most to price predictions for individual properties, with SHAP (SHapley Additive exPlanations) values quantifying marginal contributions of each dimension to model outputs, enabling interpretable insights such as \"price increase driven primarily by temporal dimension indicating post-recession recovery period combined with refArea indicating proximity to urban employment centers.\" The SDMX-inspired semantic structure ensures that explanations reference standardized dimension concepts (time, geography, demographics) with clear definitions and hierarchical relationships (neighborhoods within districts, months within years), supporting transparent communication of model reasoning to non-technical stakeholders including policymakers, real estate professionals, and regulatory auditors. While the system reuses SDMX-inspired concepts rather than directly consuming SDMX APIs or official SDMX-ML/SDMX-JSON data streams, it demonstrates operational benefits of SDMX semantic modeling for AI workflows shared dimension vocabularies enable seamless integration of heterogeneous statistical sources, hierarchical codelists support multi-resolution spatial and temporal aggregation, and standardized metadata facilitate automated feature engineering where GNN input graphs are constructed programmatically from SPARQL queries over Data Cube observations without manual schema mapping, collectively illustrating how SDMX-aligned linked data infrastructure enhances reproducibility, interoperability, and explainability in statistical machine learning applications.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.3390/technologies12080128"
          ]
        }
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:88"
      ],
      "purpose_detail": "The Statistical Data and Metadata eXchange (SDMX) standard is an international initiative sponsored by major statistical organizations (IMF, OECD, Eurostat, World Bank, ECB, BIS, UN Statistics Division) providing ISO-standardized technical specifications (ISO 17369) for exchanging and sharing statistical data and metadata, enabling interoperable data flows among national statistical offices, international organizations, central banks, and research institutions through common information models, data structure definitions, and multiple serialization formats (SDMX-ML XML, SDMX-JSON, SDMX-CSV, SDMX-RDF). SDMX models statistical data as multidimensional cubes where observations are indexed by dimensions (time period, geographic area, indicator, sex, age group) and associated with measures (observed values, units, observation status) and attributes (confidentiality flags, estimation methods, data quality annotations), with Data Structure Definitions (DSDs) serving as schemas that specify allowed dimensions, their codelists (enumerated valid values), dimension cardinalities, and constraints, enabling automated validation and semantic interoperability where systems can interpret statistical series without manual codebook consultation. The standard encompasses an information model (conceptual entities: concepts, codelists, category schemes, data flows, organizational schemes defining data providers and recipients), structural metadata (DSDs defining cube structures, concept schemes providing semantic definitions, code lists enumerating dimension values with hierarchies), and reference metadata (methodological notes, collection procedures, quality indicators, revision policies) that together enable comprehensive statistical data documentation supporting reproducible research and informed reuse. SDMX Registry services provide centralized metadata repositories where organizations publish DSDs, codelists, and dataflow definitions with persistent identifiers, enabling consuming applications to discover available datasets, retrieve structural metadata to interpret data, and access reference metadata documenting quality and methodology, with SDMX web services (RESTful APIs and SOAP endpoints) supporting both metadata queries (retrieve DSDs for dataset X) and data queries (retrieve GDP series for OECD countries 2010-2020) with filtering by dimension values, time ranges, and aggregation levels. Common serialization formats include SDMX-ML (XML-based verbose format with full structural annotations), SDMX-JSON (lightweight format for web applications and JavaScript clients), SDMX-CSV (tabular format with embedded metadata headers for spreadsheet compatibility), and SDMX-RDF (linked data representation using W3C Data Cube vocabulary with sdmx-dimension, sdmx-measure, sdmx-attribute properties enabling SPARQL queries and integration into knowledge graphs), supporting diverse consumption patterns from web dashboards to statistical analysis tools to machine learning pipelines. Cross-domain exchange relies on SDMX Global Registry (maintained by SDMX consortia) hosting authoritative codelists (ISO 3166 country codes, ISO 4217 currency codes, ISO 639 language codes, SNA 2008 economic classifications) and cross-domain concepts (reference areas, time periods, frequencies, units of measure) enabling semantic alignment where GDP data from Eurostat, World Bank, and IMF use shared dimension definitions and codes, facilitating federated queries and comparative analysis across sources. The standard supports hierarchical and temporal codelists where dimension values nest (countries within regions, industries within sectors, months within quarters) enabling drill-down analysis and aggregation, with validity periods documenting when codes are active (e.g., currency codes before/after Euro adoption, country codes before/after political changes) ensuring correct temporal interpretation. SDMX adoption spans global statistical production with 80+ implementing organizations including all G20 statistical offices, over 150 published DSDs covering national accounts, labor statistics, prices, trade, finance, health, education, and environment domains, and thousands of registered dataflows with billions of observations exchanged annually via SDMX infrastructure, making it the de facto standard for official statistics dissemination. Machine learning and artificial intelligence applications leverage SDMX through natural language question answering systems that parse user queries and map linguistic elements to SDMX dimensions and codelists using Data Structure Definitions to identify relevant tables and dimensions, retrieve numeric values from SDMX web services, and generate responses explaining temporal trends or cross-country comparisons, with evaluation of dimension-value extraction accuracy, table ranking performance, and comparison/aggregation intent detection; programmatic dataset augmentation via SDMX APIs where ML pipelines in economics and finance (gingado library for Python/scikit-learn workflows) automatically retrieve official statistical series through SDMX protocol endpoints to enrich feature matrices with macroeconomic indicators (GDP growth, inflation, unemployment, trade balances) sourced from authoritative providers, supporting empirical testing of performance improvements when SDMX-derived features supplement domain datasets; semantic knowledge graph construction using SDMX vocabularies (sdmx-dimension, sdmx-measure, sdmx-attribute) combined with W3C Data Cube to generate RDF observations representing scientometric indicators, importing structured statistical metadata into graph databases (Neo4j) for graph analytics and explainable AI workflows that reason over semantically coherent indicator relationships; and graph neural network modeling over open statistics knowledge graphs that reuse SDMX-inspired concepts (sdmx:timePeriod for temporal dimensions, sdmx:refArea for geographic areas, sdmx:sex for demographic breakdowns) within linked data cubes, enabling GNN architectures (ChebNet, GCN, GraphSAGE) to predict outcomes (house prices) from spatiotemporal statistical patterns with explainability analysis (GNNExplainer, SHAP) identifying which SDMX-structured dimensions contribute most to predictions, demonstrating how SDMX semantic standardization facilitates AI interpretability on official statistics.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://sdmx.org/"
    },
    {
      "id": "B2AI_STANDARD:381",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "STL",
      "description": "STereoLithography File Format Family",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "STL files describe only the surface geometry of a three-dimensional object without any representation of color, texture or other common CAD model attributes. The STL format specifies both ASCII and binary representations.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.loc.gov/preservation/digital/formats/fdd/fdd000504.shtml"
    },
    {
      "id": "B2AI_STANDARD:382",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "SDD",
      "description": "Structured Descriptive Data",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:93"
      ],
      "purpose_detail": "The goal of the Structured Descriptive Data (SDD) standard is to allow capture, transport, caching and archiving of descriptive data in all the forms shown above, using a platform- and application-independent, international standard. Such a standard is crucial to enabling lossless porting of data between existing and future software platforms including identification, data-mining and analysis tools, and federated databases.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.tdwg.org/standards/sdd/",
      "formal_specification": "https://github.com/tdwg/sdd"
    },
    {
      "id": "B2AI_STANDARD:383",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "TIFF",
      "description": "Tagged Image File Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "audiovisual",
        "fileformat",
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:116"
      ],
      "purpose_detail": "An image file format for storing raster graphics images.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikipedia.org/wiki/TIFF",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:19"
      ]
    },
    {
      "id": "B2AI_STANDARD:384",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "TAR",
      "description": "TAR archive file format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A tar (tape archive) file format is an archive created by tar, a UNIX-based utility used to package files together for backup or distribution purposes. It contains multiple files (also known as a tarball) stored in an uncompressed format along with metadata about the archive. Tar files are not compressed archive files. They are often compressed with file compression utilities such as gzip or bzip2.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.loc.gov/preservation/digital/formats/fdd/fdd000531.shtml",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:52"
      ]
    },
    {
      "id": "B2AI_STANDARD:385",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "TAPIR",
      "description": "TDWG Access Protocol for Information Retrieval",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:93"
      ],
      "purpose_detail": "The TDWG Access Protocol for Information Retrieval (TAPIR) is a Web Service protocol and XML schema to perform queries across distributed databases of varied physical and logical structure. It was originally designed to be used by federated networks. TAPIR is intended for communication between applications, using HTTP as the transport mechanism. TAPIR's flexibility makes it suitable to both very simple service implementations where the provider only responds to a set of pre-defined queries, or more advanced implementations where the provider software can dynamically parse complex queries referencing output models supplied by the client.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.tdwg.org/standards/tapir/"
    },
    {
      "id": "B2AI_STANDARD:386",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "UCUM",
      "description": "The Unified Code for Units of Measure",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "codesystem",
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:84"
      ],
      "purpose_detail": "A common syntax for communication of quantities and their units.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://unitsofmeasure.org/ucum"
    },
    {
      "id": "B2AI_STANDARD:387",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "WAV",
      "description": "Waveform Audio File Format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "audiovisual",
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:37"
      ],
      "purpose_detail": "Waveform Audio File Format (WAVE or WAV due to its filename extension is an audio file format standard.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikipedia.org/wiki/WAV",
      "formal_specification": "https://sites.google.com/site/musicgapi/technical-documents/wav-file-format",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:48",
        "B2AI_SUBSTRATE:49"
      ]
    },
    {
      "id": "B2AI_STANDARD:388",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "OWL",
      "description": "Web Ontology Language",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:99"
      ],
      "purpose_detail": "The Web Ontology Language (OWL) is a family of knowledge representation languages or ontology languages for authoring ontologies or knowledge bases. The languages are characterized by formal semantics and RDF/XML-based serializations for the Semantic Web. OWL is endorsed by the World Wide Web Consortium (W3C) and has attracted academic, medical and commercial interest. The OWL 2 Web Ontology Language, informally OWL 2, is an ontology language for the Semantic Web with formally defined meaning. OWL 2 ontologies provide classes, properties, individuals, and data values and are stored as Semantic Web documents. OWL 2 ontologies can be used along with information written in RDF, and OWL 2 ontologies themselves are primarily exchanged as RDF documents.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.w3.org/TR/owl-overview/"
    },
    {
      "id": "B2AI_STANDARD:389",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "WDL",
      "description": "Workflow Description Language",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "workflowlanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The Workflow Description Language (WDL) is a way to specify data processing workflows with a human-readable and -writeable syntax. WDL makes it straightforward to define analysis tasks, chain them together in workflows, and parallelize their execution. The language makes common patterns simple to express, while also admitting uncommon or complicated behavior; and strives to achieve portability not only across execution platforms, but also different types of users. Whether one is an analyst, a programmer, an operator of a production system, or any other sort of user, WDL should be accessible and understandable.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://openwdl.org/",
      "formal_specification": "https://github.com/openwdl/wdl"
    },
    {
      "id": "B2AI_STANDARD:390",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "XBM",
      "description": "X PixMap bitmap image format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "audiovisual",
        "deprecated",
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "purpose_detail": "X PixMap (XBM) is an image file format used by the X Window System. Replaced by XPM.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikipedia.org/wiki/X_PixMap",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:19"
      ]
    },
    {
      "id": "B2AI_STANDARD:391",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "XPM",
      "description": "X PixMap image format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "audiovisual",
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "purpose_detail": "X PixMap (XPM) is an image file format used by the X Window System.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikipedia.org/wiki/X_PixMap"
    },
    {
      "id": "B2AI_STANDARD:392",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "xarray",
      "description": "xarray",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "An open source project and Python package that introduces labels in the form of dimensions, coordinates, and attributes on top of raw NumPy-like arrays, which allows for more intuitive, more concise, and less error-prone user experience.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://xarray.dev/",
      "formal_specification": "https://github.com/pydata/xarray",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:1",
        "B2AI_SUBSTRATE:50"
      ]
    },
    {
      "id": "B2AI_STANDARD:393",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "YAML",
      "description": "YAML Ain't Markup Language",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A human-readable data-serialization language. It is commonly used for configuration files and in applications where data is being stored or transmitted.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikipedia.org/wiki/YAML"
    },
    {
      "id": "B2AI_STANDARD:394",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "Zarr",
      "description": "Zarr",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A format for storage of large N-dimensional typed arrays. Has implementations in multiple programming languages.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://zarr.dev/",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:1",
        "B2AI_SUBSTRATE:24",
        "B2AI_SUBSTRATE:51"
      ]
    },
    {
      "id": "B2AI_STANDARD:395",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "ZIP",
      "description": "ZIP compressed file format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "An archive file format that supports lossless data compression.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://en.wikipedia.org/wiki/ZIP_(file_format)",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:52"
      ]
    },
    {
      "id": "B2AI_STANDARD:396",
      "category": "B2AI_STANDARD:ModelRepository",
      "name": "AdapterHub",
      "description": "AdapterHub",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "AdapterHub is a centralized repository and framework for parameter-efficient transfer learning, enabling researchers to share, discover, and compose lightweight adapter modules that modify pre-trained transformer models for specific tasks without full fine-tuning, reducing computational costs by up to 99% while maintaining 95-98% of full fine-tuning performance across diverse NLP tasks. Adapters are compact neural network modules (typically 0.5-8% of base model parameters) inserted between transformer layers that learn task-specific transformations through bottleneck architectures (down-projection, non-linearity, up-projection) while keeping the original pre-trained weights frozen, enabling a single base model (BERT, RoBERTa, XLM-R, T5) to support hundreds of tasks through modular adapter stacking rather than maintaining separate full model copies for each application. Built as an extension to Hugging Face's transformers library, AdapterHub integrates seamlessly with existing training workflows requiring only two additional lines of code to train and share adapters (model.add_adapter(\"task_name\") and model.train_adapter(\"task_name\")), with the adapter-transformers library providing unified interfaces for adapter loading, composition, and configuration management across 20+ adapter architectures including Pfeiffer adapters (original bottleneck design), Houlsby adapters (parallel insertion), LoRA (Low-Rank Adaptation using rank decomposition), prefix tuning (learning soft prompts), and compacter adapters (using hypercomplex multiplication for further parameter reduction). The repository hosts over 2,000 pre-trained adapters spanning classification (sentiment analysis, topic categorization, toxicity detection), sequence labeling (named entity recognition, part-of-speech tagging, chunking), question answering (SQuAD, Natural Questions, BoolQ), semantic parsing (dependency parsing, constituency parsing), and generation tasks (summarization, translation, data-to-text), trained on standard benchmarks (GLUE, SuperGLUE, XTREME for multilingual evaluation) with documented performance metrics, training hyperparameters, and dataset characteristics enabling reproducible adapter reuse. AdapterHub supports multilingual transfer where adapters trained on high-resource languages (English, Chinese) compose with language-specific adapters for low-resource languages (Swahili, Quechua) through cross-lingual transfer, enabling zero-shot or few-shot adaptation to languages with limited labeled data by leveraging shared multilingual representations in base models like XLM-RoBERTa while learning language-specific patterns in lightweight adapter modules. Adapter composition enables modular multi-task learning where task-specific adapters (e.g., \"sentiment\" + \"sarcasm-detection\") stack or fuse to create specialized models combining complementary capabilities, with fusion mechanisms (weighted averaging, attention-based selection) determining how adapter outputs combine, supporting applications like aspect-based sentiment analysis that requires both entity recognition and opinion classification capabilities. The platform provides standardized adapter configurations (reduction factors controlling bottleneck dimensions, placement strategies specifying which layers receive adapters, activation functions) ensuring reproducibility and fair comparisons across research groups, while adapter cards document training procedures, base model requirements, evaluation results on held-out data, and intended use cases with known limitations. For enterprise and research applications, AdapterHub enables efficient model deployment where a single shared base model loads task-specific adapters on-demand based on inference requests, reducing GPU memory footprint from gigabytes to megabytes per task and enabling multi-tenant serving where different users access different task adapters simultaneously without model duplication, particularly valuable for edge deployment (mobile devices, embedded systems) where memory constraints prohibit storing multiple full models. The framework supports continual learning scenarios where new task adapters are trained incrementally without forgetting previous tasks (avoiding catastrophic forgetting) since base model weights remain frozen and each task's knowledge is encapsulated in its dedicated adapter module, enabling long-lived systems that accumulate capabilities over time. AdapterHub accelerates research by providing baseline adapters for benchmark datasets, enabling rapid prototyping where researchers can compare novel adapter architectures against established designs using consistent evaluation protocols, and supporting meta-analyses examining how adapter capacity (parameter count), placement strategies (serial vs. parallel insertion), and fusion techniques affect transfer learning efficiency across task types, model sizes, and linguistic phenomena, collectively advancing understanding of parameter-efficient transfer learning and democratizing access to task-specific model customization without requiring extensive computational resources for full fine-tuning.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://adapterhub.ml/",
      "publication": "doi:10.48550/arXiv.2007.07779",
      "formal_specification": "https://github.com/adapter-hub/Hub"
    },
    {
      "id": "B2AI_STANDARD:397",
      "category": "B2AI_STANDARD:ModelRepository",
      "name": "Bioimage",
      "description": "Bioimage Model Zoo",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "purpose_detail": "A community-driven, fully open resource where standardized pre-trained models can be shared, explored, tested, and downloaded for further adaptation or direct deployment in multiple end user-facing tools (e.g., ilastik, deepImageJ, QuPath, StarDist, ImJoy, ZeroCostDL4Mic, CSBDeep).",
      "is_open": true,
      "requires_registration": false,
      "url": "https://bioimage.io/#/",
      "publication": "doi:10.1101/2022.06.07.495102",
      "formal_specification": "https://github.com/bioimage-io/bioimage.io"
    },
    {
      "id": "B2AI_STANDARD:398",
      "category": "B2AI_STANDARD:ModelRepository",
      "name": "HF Models",
      "description": "Hugging Face  Models",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "dataregistry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Hugging Face Models (huggingface.co/models) is the world's largest open-source repository and collaboration platform for machine learning models, hosting over 500,000 pre-trained models spanning natural language processing, computer vision, audio processing, multimodal learning, reinforcement learning, and tabular data applications, with standardized model cards, inference APIs, and version control enabling reproducible AI research and democratized access to state-of-the-art architectures. Founded in 2016 and rapidly becoming the de facto standard for sharing transformer-based models, the platform provides Git-based versioning (Git-LFS for large files) allowing researchers to track model iterations, fork and modify existing models, and collaborate on model development with pull requests and issue tracking, while the huggingface_hub Python library enables programmatic model download, upload, and inference integration with popular frameworks (PyTorch, TensorFlow, JAX, ONNX). The repository encompasses language models ranging from compact distilled variants (DistilBERT, TinyBERT for edge deployment) to massive foundation models (LLaMA, Mistral, GPT-NeoX, BLOOM with billions of parameters), with specialized architectures for named entity recognition (RoBERTa-NER, SpaCy transformers), sentiment analysis (BERT-base-cased fine-tuned on SST-2), question answering (ELECTRA-QA, DeBERTa-SQuAD), translation (MarianMT covering 1,200+ language pairs, NLLB-200 for low-resource languages), summarization (BART, T5, Pegasus), and code generation (CodeGen, StarCoder, Code Llama trained on GitHub repositories). Beyond NLP, the platform hosts vision transformers (ViT, DeiT, Swin) pre-trained on ImageNet-21k enabling transfer learning for medical imaging, satellite analysis, and manufacturing quality control; object detection models (DETR, YOLOv8, Mask R-CNN) with COCO weights; image segmentation architectures (SegFormer, Mask2Former); audio models for automatic speech recognition (Whisper supporting 100+ languages, Wav2Vec2 for self-supervised learning) and audio classification (Audio Spectrogram Transformer); and multimodal models (CLIP, BLIP, Flamingo) enabling vision-language tasks including image captioning, visual question answering, and text-to-image generation (Stable Diffusion, ControlNet). Each model includes a standardized model card documenting intended use cases, training data composition with dataset sources and sizes, evaluation metrics on benchmark datasets, known limitations and biases, ethical considerations (potential misuse scenarios, fairness assessments), carbon emissions from training (kg CO2 equivalent), computational requirements (GPU memory, inference latency), and licensing terms (Apache 2.0, MIT, CreativeML for generative models), supporting informed model selection and responsible AI deployment. The Inference API provides instant model testing via web interface or curl requests without local setup, enabling rapid prototyping and model comparison, while Hugging Face Spaces hosts interactive demos (Gradio, Streamlit apps) showcasing model capabilities and allowing non-technical stakeholders to evaluate outputs before integration. Integration with the transformers library (100M+ downloads) provides unified interfaces (AutoModel, pipeline API) abstracting framework differences and enabling single-line model loading with automatic tokenizer/feature extractor selection, while optimizations including 8-bit quantization (bitsandbytes), Flash Attention 2 for memory efficiency, and ONNX Runtime conversion reduce inference costs for production deployment. The platform supports private repositories for enterprise use cases, organizations for team collaboration, gated models requiring license acceptance (LLaMA 2, BLOOM), and community-driven model evaluation through the Open LLM Leaderboard benchmarking language models on standardized tasks (MMLU, HellaSwag, TruthfulQA) with automated evaluation pipelines ensuring fair comparisons. For AI/ML practitioners, Hugging Face Models accelerates development by providing production-ready checkpoints eliminating weeks of pretraining compute (training GPT-3-scale models costs millions of dollars), enables domain adaptation through fine-tuning on specialized datasets (legal documents, biomedical literature, scientific papers) leveraging pretrained representations, and supports model serving via Inference Endpoints (managed GPU instances) or export to TensorFlow Lite/CoreML for mobile deployment, collectively reducing the barrier to deploying sophisticated AI systems from research prototypes to production applications serving millions of users.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://huggingface.co/models",
      "formal_specification": "https://github.com/huggingface/huggingface_hub"
    },
    {
      "id": "B2AI_STANDARD:399",
      "category": "B2AI_STANDARD:ModelRepository",
      "name": "modelzoo.co",
      "description": "Model Zoo",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Model Zoo (modelzoo.co) is a curated discovery platform that aggregates pre-trained machine learning models from diverse frameworks (TensorFlow, PyTorch, Caffe, MXNet, ONNX), providing a centralized index for finding models across computer vision, natural language processing, speech recognition, and reinforcement learning domains. Originally launched as a community-driven resource, Model Zoo enables practitioners to search for state-of-the-art architectures by task (object detection, semantic segmentation, translation), dataset (ImageNet, COCO, WMT), or framework compatibility. The platform serves as a model marketplace connecting researchers publishing novel architectures with practitioners seeking production-ready implementations, reducing the barrier to adopting cutting-edge techniques. Model Zoo listings typically include architecture descriptions, training configurations, performance benchmarks (accuracy, inference speed), framework-specific code repositories, and pre-trained weight files for transfer learning. Unlike framework-specific repositories (PyTorch Hub, TensorFlow Hub), Model Zoo provides cross-framework search capabilities, enabling users to discover equivalent architectures implemented in multiple ecosystems and compare performance characteristics across frameworks. The platform supports both academic research models (recent conference publications) and industry-proven architectures (ResNet, BERT variants), with community ratings and download metrics indicating model popularity and reliability. Model Zoo facilitates reproducibility by linking to original papers, training datasets, and hyperparameter configurations, while model cards provide metadata on intended use cases, known limitations, and ethical considerations. For AI/ML practitioners, Model Zoo accelerates prototyping by providing a starting point for transfer learning, enabling rapid experimentation with pre-trained models fine-tuned on domain-specific data rather than training from scratch, particularly valuable when compute resources or labeled data are limited.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://modelzoo.co/"
    },
    {
      "id": "B2AI_STANDARD:400",
      "category": "B2AI_STANDARD:ModelRepository",
      "name": "OpenML",
      "description": "OpenML",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "dataregistry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "OpenML is an open-source platform and collaborative ecosystem for sharing, organizing, and automating machine learning experiments, providing standardized access to over 21,000 datasets, 7,000 tasks, 100,000+ algorithm implementations (flows), and millions of experimental results (runs) with rich metadata enabling reproducible research, automated machine learning (AutoML), and meta-learning across the global ML research community. Founded in 2013 by Joaquin Vanschoren and collaborators, OpenML addresses the reproducibility crisis in machine learning by establishing a centralized repository where researchers deposit not only datasets and code but complete experimental pipelines including data preprocessing steps, algorithm hyperparameters, performance metrics, runtime statistics, and computational environment details, all indexed by unique persistent identifiers and searchable via a RESTful API and Python/R/Java client libraries. The platform defines four core entities that structure ML experiments datasets (tabular data, time series, images) with comprehensive metadata including feature types, class distributions, missing value patterns, and provenance; tasks that specify learning problems (classification, regression, clustering) with defined train/test splits, evaluation metrics, and target variables, ensuring fair comparisons across studies; flows representing algorithm implementations with specific library versions, hyperparameter schemas, and software dependencies captured via containerization or environment specifications; and runs documenting execution results linking a specific flow applied to a specific task, recording predictions, evaluation scores, confusion matrices, learning curves, computational cost, and reproducibility artifacts. OpenML's standardized task definitions enable apples-to-apples benchmarking where multiple research groups apply different algorithms to identical problems with fixed data splits and metrics, eliminating the variability introduced when each paper uses custom evaluation protocols and reporting only favorable splits, thereby providing more reliable performance estimates and accelerating identification of genuinely superior methods versus tuning artifacts. The platform's API supports programmatic experiment automation where AutoML systems (Auto-sklearn, TPOT, H2O AutoML) query OpenML for benchmark tasks, execute hyperparameter optimization across multiple algorithms, and automatically upload results with full reproducibility metadata, creating a continuously growing knowledge base of algorithm performance across diverse problem characteristics (dataset size, dimensionality, class imbalance, feature types) that informs meta-learning models predicting which algorithms will perform best on new unseen datasets based on dataset meta-features (number of instances, attributes, classes, skewness, entropy). Integration with popular ML frameworks through openml-python, openml-r, and mlr3 packages enables one-line dataset loading (openml.datasets.get_dataset(id)), task retrieval with pre-defined splits preserving reproducibility, and automatic run uploading that captures scikit-learn pipelines, XGBoost models, or deep learning architectures with complete hyperparameter configurations and performance metrics, reducing friction in contributing to community benchmarks. OpenML's metadata richness supports advanced queries identifying datasets with specific characteristics (e.g., \"binary classification tasks with 1,000-10,000 instances, 50-200 features, <10% missing values\") for systematic empirical studies, meta-analyses aggregating results across hundreds of datasets to assess algorithm performance in different regimes, and educational applications where students explore the relationship between dataset properties and model behavior through interactive visualizations of performance landscapes. The platform promotes FAIR principles (Findable, Accessible, Interoperable, Reusable) by assigning DOI identifiers to datasets, versioning all entities to track changes, requiring open licenses (CC BY, CC0, public domain), and providing machine-readable metadata (ARFF format, JSON schemas) that integrates with scholarly infrastructure (DataCite, Zenodo). For AI/ML research, OpenML serves as a benchmark suite for evaluating novel algorithms against established baselines across diverse tasks, a data source for meta-learning studies training models to recommend algorithms or initialize hyperparameters based on dataset characteristics, an infrastructure for collaborative competitions where participants upload solutions to shared tasks and leaderboards update automatically, and a teaching resource providing curated datasets with ground-truth labels and documented preprocessing steps suitable for ML coursework and tutorials, collectively advancing reproducible, transparent, and cumulative progress in machine learning research.",
      "is_open": true,
      "requires_registration": true,
      "url": "https://www.openml.org/",
      "formal_specification": "https://github.com/openml/OpenML"
    },
    {
      "id": "B2AI_STANDARD:401",
      "category": "B2AI_STANDARD:ModelRepository",
      "name": "PyTorch Hub",
      "description": "PyTorch Hub",
      "related_to": [
        "B2AI_STANDARD:816"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "PyTorch Hub is the official model repository and distribution system for the PyTorch deep learning framework, providing programmatic access to pre-trained models through a standardized torch.hub API that enables single-line model loading with automatic dependency resolution and weight downloading. Launched by Facebook AI Research (now Meta AI) and the PyTorch Foundation, Hub hosts curated models from leading research institutions including FAIR, NVIDIA, Hugging Face, Intel, and academic labs, covering computer vision (YOLOv5, ResNet, EfficientNet), NLP (Transformers, RoBERTa), speech (Silero models, Tacotron2), and video understanding (SlowFast, X3D) domains. Each Hub model is defined by a hubconf.py file in a GitHub repository that specifies entry points, dependencies, and preprocessing pipelines, ensuring reproducible model instantiation across environments. PyTorch Hub supports both feature extraction (frozen backbone models) and fine-tuning workflows (unfrozen weights), with models returning standard torch.nn.Module objects compatible with PyTorch training loops, data loaders, and distributed training frameworks (DDP, FSDP). The platform integrates with PyTorch's TorchScript compilation for deployment optimization, ONNX export for cross-framework compatibility, and TorchServe for production serving. Hub models include metadata specifying input/output tensor shapes, preprocessing requirements (normalization statistics, resize dimensions), and performance benchmarks (latency, throughput) across hardware configurations (CPU, GPU, mobile). The repository supports version pinning via Git commit hashes or tags, enabling deterministic model loading and reproducible research results. For researchers, PyTorch Hub accelerates experimentation by providing battle-tested implementations of recent architectures (often released alongside conference publications) with pre-trained ImageNet, COCO, or Kinetics weights, reducing training time from weeks to hours through transfer learning. In AI/ML pipelines, Hub models serve as feature extractors for downstream tasks (medical imaging, satellite analysis), few-shot learning backbones, and initialization points for domain adaptation, with the torch.hub.load() API supporting custom repositories for internal enterprise model sharing.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://pytorch.org/hub/",
      "formal_specification": "https://github.com/pytorch/hub",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:33"
      ]
    },
    {
      "id": "B2AI_STANDARD:402",
      "category": "B2AI_STANDARD:ModelRepository",
      "name": "TFHub",
      "description": "Tensorflow Hub",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:37"
      ],
      "purpose_detail": "TensorFlow Hub (tfhub.dev) is Google's official repository for reusable machine learning modules, providing pre-trained model components as SavedModel or TF2 format assets that integrate seamlessly into TensorFlow and Keras workflows via the tensorflow-hub library. Launched in 2018, TF Hub pioneered the concept of \"transfer learning building blocks\" by packaging not just model weights but complete computational graphs including preprocessing layers, embedding extractors, and prediction heads that can be composed into larger architectures. Hub modules span text embeddings (Universal Sentence Encoder, BERT, LaBSE), image feature vectors (MobileNet, EfficientNet, ResNet), object detection (SSD, Faster R-CNN), image generation (BigGAN, StyleGAN), video understanding (I3D, MoViNet), and audio processing (YAMNet, TRILL). Each module provides a consistent interface via hub.KerasLayer or hub.load(), supporting both feature extraction (trainable=False) and fine-tuning (trainable=True) modes with automatic gradient flow through module internals. TF Hub emphasizes model cards with detailed documentation on training data, performance metrics, intended use cases, and ethical considerations (bias, fairness), promoting responsible AI deployment. The platform supports multiple serving formats including TensorFlow Lite for mobile/edge deployment, TensorFlow.js for in-browser inference, and TensorFlow Serving for production APIs, with modules optimized for quantization and pruning. Hub's standardized interface enables model composition where text embeddings feed into classification heads, or image encoders combine with text encoders for multimodal learning. For researchers, TF Hub reduces training time and computational costs by providing pre-trained representations on large-scale datasets (Wikipedia, ImageNet, YouTube-8M) that transfer effectively to specialized domains with limited data. In AI/ML production systems, Hub modules accelerate deployment by providing battle-tested, versioned components with defined input/output signatures, enabling A/B testing of different encoders and rapid iteration on model architectures without retraining entire pipelines from scratch.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://tfhub.dev/"
    },
    {
      "id": "B2AI_STANDARD:403",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "ASA/ANSI S3.20",
      "description": "Acoustical Society of America / American National Standards Institute S3.20",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:7",
        "B2AI_ORG:4"
      ],
      "purpose_detail": "Definitions for terms used in human bioacoustics.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://webstore.ansi.org/Standards/ASA/ANSIASAS3202015R2020"
    },
    {
      "id": "B2AI_STANDARD:404",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "ADO",
      "description": "Alzheimer's Disease Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:7"
      ],
      "purpose_detail": "concepts related to Alzheimer's Disease",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/Fraunhofer-SCAI-Applied-Semantics/ADO",
      "formal_specification": "https://github.com/Fraunhofer-SCAI-Applied-Semantics/ADO"
    },
    {
      "id": "B2AI_STANDARD:405",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "ARO",
      "description": "Antibiotic Resistance Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Antibiotic resistance genes and mutations",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/arpcard/aro",
      "publication": "doi:10.1093/nar/gkz935",
      "formal_specification": "https://github.com/arpcard/aro"
    },
    {
      "id": "B2AI_STANDARD:406",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "APOLLO_SV",
      "description": "Apollo Structured Vocabulary",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Terms and relations for interoperation between epidemic models and public health application software.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/ApolloDev/apollo-sv",
      "formal_specification": "https://github.com/ApolloDev/apollo-sv"
    },
    {
      "id": "B2AI_STANDARD:407",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "AIO",
      "description": "Artificial Intelligence Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:59",
          "category": "B2AI:Application",
          "name": "AI/ML Ontology for Model Metadata and Pipeline Documentation",
          "description": "AIO (Artificial Intelligence Ontology) is used in biomedical AI for standardizing the description of machine learning models, algorithms, and workflows, enabling semantic search for AI methods, automated model selection, and reproducible research documentation. Researchers leverage AIO to annotate AI models with formal descriptions of their architecture, training data requirements, and applicable use cases, facilitating discovery of appropriate models for specific biomedical tasks. The ontology supports automated reasoning about AI pipeline compatibility, enables knowledge graphs that link models to publications and datasets, and provides structured vocabulary for documenting AI experiments in compliance with reproducibility standards. AIO enables large language models to better understand and recommend AI approaches for biomedical problems.",
          "used_in_bridge2ai": false
        }
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The Artificial Intelligence Ontology (AIO) is a comprehensive formal ontology that provides standardized terminology and semantic relationships for describing artificial intelligence systems, methods, and concepts. Developed to address the need for consistent AI terminology across research and applications, AIO models classes and relationships describing deep learning networks, their component layers and activation functions, machine learning algorithms, data processing techniques, and potential algorithmic biases. The ontology contains 443 classes organized in a hierarchical structure with a maximum depth of 5 levels, covering fundamental AI concepts from basic computational methods to complex neural architectures. AIO serves as a critical resource for AI researchers, practitioners, and systems developers who need standardized vocabularies for annotating AI models, describing experimental procedures, ensuring reproducibility, and enabling semantic interoperability between AI systems and databases. The ontology facilitates automated reasoning about AI systems, supports metadata annotation for AI workflows, and contributes to the broader goal of making artificial intelligence research more findable, accessible, interoperable, and reusable.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://bioportal.bioontology.org/ontologies/AIO",
      "formal_specification": "https://github.com/berkeleybop/artificial-intelligence-ontology"
    },
    {
      "id": "B2AI_STANDARD:408",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "BFO",
      "description": "Basic Formal Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Basic Formal Ontology (BFO) is a small, upper-level ontology designed for supporting information retrieval, analysis, and integration in scientific and other domains. As a genuine upper ontology, BFO provides foundational categories that are domain-neutral and applicable across all areas of scientific investigation, rather than containing domain-specific terms. BFO distinguishes between continuants (entities that endure through time, such as objects, qualities, and spatial regions) and occurrents (entities that unfold over time, such as processes and temporal regions), providing a rigorous framework for representing the fundamental structure of reality. The ontology employs formal logic (first-order logic, OWL2) to define its 39 core classes and relations, ensuring precise semantics and enabling automated reasoning. BFO is used by over 550 ontology-driven projects worldwide as the top-level framework for domain ontologies in biomedicine (OBO Foundry ontologies like GO, CHEBI, Uberon), healthcare (OGMS for disease), environmental science, manufacturing, and military intelligence. The ontology promotes interoperability by providing consistent upper-level structure, enabling ontology integration and cross-domain data federation. BFO has been developed through extensive international collaboration, with contributions from philosophers, logicians, and domain scientists, and is continuously refined based on practical applications. In AI/ML contexts, BFO provides foundational structure for knowledge graphs, enabling ontology-guided reasoning, semantic data integration across heterogeneous sources, knowledge representation for explainable AI, and principled ontology alignment, supporting knowledge-driven machine learning where symbolic foundations enhance statistical learning with formal semantics.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://basic-formal-ontology.org/",
      "publication": "doi:10.3233/AO-160164",
      "formal_specification": "https://github.com/BFO-ontology/BFO"
    },
    {
      "id": "B2AI_STANDARD:409",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "BCO",
      "description": "Biological Collections Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Biodiversity data, including data on museum collections and environmental/metagenomic samples.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/BiodiversityOntologies/bco",
      "formal_specification": "https://github.com/BiodiversityOntologies/bco"
    },
    {
      "id": "B2AI_STANDARD:410",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "FBBI",
      "description": "Biological Imaging Methods Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "purpose_detail": "Sample preparation, visualization and imaging methods used in biomedical research.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://cellimagelibrary.org/",
      "formal_specification": "https://github.com/CRBS/Biological_Imaging_Methods_Ontology/"
    },
    {
      "id": "B2AI_STANDARD:411",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "BSPO",
      "description": "Biological Spatial Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Spatial concepts, anatomical axes, gradients, regions, planes, sides, and surfaces.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/obophenotype/biological-spatial-ontology",
      "publication": "doi:10.1186/2041-1480-5-34",
      "formal_specification": "https://github.com/obophenotype/biological-spatial-ontology"
    },
    {
      "id": "B2AI_STANDARD:412",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "BTO",
      "description": "BRENDA tissue / enzyme source",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A structured controlled vocabulary for the source of an enzyme comprising tissues, cell lines, cell types and cell cultures.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://www.brenda-enzymes.org/",
      "publication": "doi:10.1093/nar/gkq968",
      "formal_specification": "https://github.com/BRENDA-Enzymes/BTO"
    },
    {
      "id": "B2AI_STANDARD:413",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "WBPHENOTYPE",
      "description": "C. elegans phenotype ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A structured controlled vocabulary of Caenorhabditis elegans phenotypes.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/obophenotype/c-elegans-phenotype-ontology",
      "publication": "doi:10.1186/1471-2105-12-32",
      "formal_specification": "https://github.com/obophenotype/c-elegans-phenotype-ontology"
    },
    {
      "id": "B2AI_STANDARD:414",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "CVDO",
      "description": "Cardiovascular Disease Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:7"
      ],
      "purpose_detail": "Entities related to cardiovascular diseases.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/OpenLHS/CVDO",
      "formal_specification": "https://github.com/OpenLHS/CVDO"
    },
    {
      "id": "B2AI_STANDARD:415",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "CLO",
      "description": "Cell Line Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Standardize and integrate cell line information and to support computer-assisted reasoning.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/CLO-Ontology/CLO",
      "publication": "doi:10.1186/2041-1480-5-3",
      "formal_specification": "https://github.com/CLO-Ontology/CLO"
    },
    {
      "id": "B2AI_STANDARD:416",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "CL",
      "description": "Cell Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The Cell Ontology (CL) is an OBO Foundry ontology covering biological cell types with curation focused on animal cell types and interoperability with specialized cell type ontologies in other biological domains. CL is tightly integrated with the Uberon multi-species anatomy ontology (for recording cell location) and the Gene Ontology (GO, which uses CL as its main cell type reference and provides cell function annotations). Built on FAIR principles, CL enables community-driven curation with active embedded editors from multiple projects responsive on the issue tracker. The ontology is released in multiple standard formats (OWL/RDF/XML, OBO, JSON obographs) with multiple variants: full (all imports merged, reasoner-classified), base (not pre-reasoned, only CL axioms including non-CL class references), and simple (pre-reasoned, CL classes only), all with resolvable version IRIs for persistent access. CL is integrated into standard tools including Ubergraph (for logical queries like finding cells by location), Ontology Access Kit (OAK), and major browsers (OLS, Ontobee). The ontology supports major initiatives including BICCN cell type knowledge explorer, HubMap Human Reference Atlas, ENCODE, FANTOM5, Single Cell Expression Atlas, Human Cell Atlas, and CellKB. CL enables AI/ML applications including OnClass for automatic cell type classification, Brain Data Standards Ontology (BDSO) for cell type navigation and search, and provides standardized cell type annotations essential for single-cell omics machine learning, cross-dataset integration, and cell type discovery algorithms.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://cell-ontology.github.io/",
      "publication": "doi:10.1186/s13326-016-0088-7",
      "formal_specification": "https://github.com/obophenotype/cell-ontology"
    },
    {
      "id": "B2AI_STANDARD:417",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "CHIRO",
      "description": "CHEBI Integrated Role Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:3"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:29"
      ],
      "purpose_detail": "A distinct role hierarchy for chemicals.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/obophenotype/chiro",
      "publication": "doi:10.26434/chemrxiv.12591221.v1",
      "formal_specification": "https://github.com/obophenotype/chiro"
    },
    {
      "id": "B2AI_STANDARD:418",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "CHEBI",
      "description": "Chemical Entities of Biological Interest",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:3"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:29"
      ],
      "purpose_detail": "ChEBI is an open-access OBO Foundry database and ontology of chemical entities covering constitutionally or isotopically distinct atoms, molecules, ions, ion pairs, radicals, complexes, conformers, groups, chemical substances, and classes of molecular entities. The database contains over 195,000 entries of naturally occurring molecules or synthetic compounds used to intervene in biological processes, with macromolecules directly encoded by genomes (nucleic acids, proteins, peptides) excluded. ChEBI incorporates ontological classification defining relationships between chemical entities and their parent/child classes, enabling queries based on chemical class and role. The database provides comprehensive information including nomenclature following IUPAC and NC-IUBMB standards, molecular formulas, InChI and SMILES identifiers, literature citations, cross-references to other databases, and species data. ChEBI supports text and structure searches and is released in multiple formats (SDF, OBO, OWL, flat file, SQL dumps). As an ELIXIR Core Data Resource and Global Core Biodata Resource, ChEBI enables AI/ML applications in cheminformatics, drug discovery, metabolomics, systems biology, and chemical-phenotype association studies.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.ebi.ac.uk/chebi/",
      "publication": "doi:10.1093/nar/gkv1031",
      "formal_specification": "https://github.com/ebi-chebi/ChEBI"
    },
    {
      "id": "B2AI_STANDARD:419",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "CHEMINF",
      "description": "Chemical Information Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:3"
      ],
      "purpose_detail": "Descriptors commonly used in cheminformatics software applications and the algorithms which generate them.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/semanticchemistry/semanticchemistry",
      "publication": "doi:10.1371/journal.pone.0025513",
      "formal_specification": "https://github.com/semanticchemistry/semanticchemistry"
    },
    {
      "id": "B2AI_STANDARD:420",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "CHMO",
      "description": "Chemical Methods Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:3"
      ],
      "purpose_detail": "The Chemical Methods Ontology (CHMO) provides a comprehensive, standardized vocabulary for describing experimental methods, techniques, and instruments used in chemical analysis, material synthesis, and sample preparation, maintained by the Royal Society of Chemistry (RSC) and aligned with OBO Foundry principles. CHMO encompasses three primary domains: analytical methods for data collection (mass spectrometry including ESI-MS, MALDI-TOF, GC-MS, LC-MS; spectroscopic techniques including NMR, IR, UV-Vis, Raman, X-ray spectroscopy; electron microscopy including SEM, TEM, STEM; and diffraction methods), separation and sample preparation techniques (chromatography including HPLC, GC, TLC, size-exclusion, affinity, ion-exchange; electrophoresis including SDS-PAGE, capillary electrophoresis, isoelectric focusing; sample ionization methods including electrospray, MALDI, electron impact; and extraction procedures), and material synthesis methods (chemical vapor deposition, epitaxy, sol-gel processes, crystallization, polymerization, and nanoparticle synthesis). The ontology also describes instruments and equipment (mass spectrometers, chromatography columns, detectors, vacuum systems, heating/cooling apparatus) and their components, operational parameters (temperature, pressure, flow rate, voltage, resolution), and measurement outputs (spectra, chromatograms, diffraction patterns, images). CHMO integrates with OBI (Ontology for Biomedical Investigations) for process and measurement concepts, CHEBI (Chemical Entities of Biological Interest) for chemical substances, and other OBO ontologies for cross-domain applications in metabolomics, proteomics, and materials science. The ontology provides both textual and formal OWL definitions enabling automated reasoning, classification of methods by input material types, conditions of application, and output data formats. CHMO supports reproducibility in chemical research by standardizing method descriptions in electronic laboratory notebooks, method sections of publications, protocols repositories, and analytical chemistry databases. Applications include semantic search for analytical protocols, automated method selection based on sample properties, FAIRification of chemical data workflows, text mining of chemical literature for method extraction, and quality control in analytical laboratories. The ontology is developed using the Ontology Development Kit (ODK) and distributed under CC-BY-4.0 license, with releases available in OBO, OWL, and JSON formats through http://purl.obolibrary.org/obo/chmo.owl.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/rsc-ontologies/rsc-cmo",
      "formal_specification": "https://github.com/rsc-ontologies/rsc-cmo"
    },
    {
      "id": "B2AI_STANDARD:421",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "LABO",
      "description": "clinical LABoratory Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "purpose_detail": "An ontology of informational entities formalizing clinical laboratory tests prescriptions and reporting documents.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/OpenLHS/LABO",
      "publication": "doi:10.5281/zenodo.6522019",
      "formal_specification": "https://github.com/OpenLHS/LABO"
    },
    {
      "id": "B2AI_STANDARD:422",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "CMO",
      "description": "Clinical measurement ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "purpose_detail": "Morphological and physiological measurement records generated from clinical and model organism research and health programs.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/rat-genome-database/CMO-Clinical-Measurement-Ontology",
      "publication": "doi:10.1186/2041-1480-4-26",
      "formal_specification": "https://github.com/rat-genome-database/CMO-Clinical-Measurement-Ontology"
    },
    {
      "id": "B2AI_STANDARD:423",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "CVX",
      "description": "Clinical Vaccines Administered",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:13"
      ],
      "purpose_detail": "The Clinical Vaccines Administered (CVX) code set is a comprehensive standardized vocabulary developed and maintained by the CDC's National Center for Immunization and Respiratory Diseases (NCIRD) that provides unique numeric identifiers for all vaccines available in the United States healthcare system. This essential code set includes both active vaccines currently available for patient administration and inactive vaccines that may appear in historical immunization records, enabling complete tracking of vaccination history across a patient's lifetime. CVX codes are specifically designed for use in HL7 Version 2.3.1 and 2.5.1 immunization messages and electronic health record systems, facilitating standardized communication between healthcare providers, immunization information systems (IIS), and public health agencies. Each CVX code entry includes detailed information about vaccine status (active, inactive, pending, non-US, or never active), last update timestamp, and clinical notes providing context about usage and availability. When paired with MVX (manufacturer) codes, CVX codes can precisely identify specific trade-named vaccine products, supporting accurate inventory management, adverse event reporting, vaccine safety monitoring, and public health surveillance activities essential for maintaining population immunity and preventing vaccine-preventable diseases.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www2a.cdc.gov/vaccines/iis/iisstandards/vaccines.asp?rpt=cvx"
    },
    {
      "id": "B2AI_STANDARD:424",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "CARO",
      "description": "Common Anatomy Reference Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "An upper level ontology to facilitate interoperability between existing anatomy ontologies for different species.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/obophenotype/caro/",
      "formal_specification": "https://github.com/obophenotype/caro/"
    },
    {
      "id": "B2AI_STANDARD:425",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "CTCAE",
      "description": "Common Terminology Criteria for Adverse Events",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:4",
        "B2AI_TOPIC:7"
      ],
      "purpose_detail": "Common Terminology Criteria for Adverse Events (CTCAE) is widely accepted throughout the oncology community as the standard classification and severity grading scale for adverse events in cancer therapy clinical trials and other oncology settings.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://bioportal.bioontology.org/ontologies/CTCAE",
      "formal_specification": "https://ctep.cancer.gov/protocoldevelopment/electronic_applications/ctc.htm"
    },
    {
      "id": "B2AI_STANDARD:426",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "CDAO",
      "description": "Comparative Data Analysis Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A formalization of concepts and relations relevant to evolutionary comparative analysis.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/evoinfo/cdao",
      "publication": "doi:10.4137/EBO.S2320",
      "formal_specification": "https://github.com/evoinfo/cdao"
    },
    {
      "id": "B2AI_STANDARD:427",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "CDNO",
      "description": "Compositional Dietary Nutrition Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Nutritional attributes of material entities that contribute to human diet.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://cdno.info/",
      "publication": "doi:10.3389/fnut.2022.928837",
      "formal_specification": "https://github.com/Southern-Cross-Plant-Science/cdno"
    },
    {
      "id": "B2AI_STANDARD:428",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "CIO",
      "description": "Confidence Information Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Capture confidence information about annotations.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/BgeeDB/confidence-information-ontology",
      "publication": "doi:10.1093/database/bav043",
      "formal_specification": "https://github.com/BgeeDB/confidence-information-ontology"
    },
    {
      "id": "B2AI_STANDARD:429",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "CRO",
      "description": "Contributor Role Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:16"
      ],
      "purpose_detail": "A classification of the diverse roles performed in the work leading to a published research output in the sciences.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/data2health/contributor-role-ontology",
      "formal_specification": "https://github.com/data2health/contributor-role-ontology"
    },
    {
      "id": "B2AI_STANDARD:430",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "COB",
      "description": "Core Ontology for Biology and Biomedicine",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:75"
      ],
      "purpose_detail": "Terms from a wide range of OBO projects to improve interoperability.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/OBOFoundry/COB",
      "formal_specification": "https://github.com/OBOFoundry/COB"
    },
    {
      "id": "B2AI_STANDARD:431",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "CIDO",
      "description": "Coronavirus Infectious Disease Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:7"
      ],
      "purpose_detail": "Ontologically represent and standardize various aspects of coronavirus infectious.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/cido-ontology/cido",
      "publication": "doi:10.1038/s41597-020-0523-6",
      "formal_specification": "https://github.com/cido-ontology/cido"
    },
    {
      "id": "B2AI_STANDARD:432",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "CTO",
      "description": "CTO Core Ontology of Clinical Trials",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:33"
      ],
      "purpose_detail": "A structured resource integrating basic terms and concepts in the context of clinical trials.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/ClinicalTrialOntology/CTO/",
      "formal_specification": "https://github.com/ClinicalTrialOntology/CTO/"
    },
    {
      "id": "B2AI_STANDARD:433",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "DUO",
      "description": "Data Use Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:34"
      ],
      "purpose_detail": "The Data Use Ontology (DUO) provides standardized vocabulary for describing data use conditions, restrictions, and requirements in biomedical and genomics research. Developed by the Global Alliance for Genomics and Health (GA4GH), DUO enables semantic tagging of datasets with consent-derived restrictions (e.g., health/medical/biomedical research only, disease-specific research, non-commercial use, research ethics approval). The ontology supports automated data access matching, where algorithms determine whether a researcher's purpose is compatible with dataset restrictions, enabling services like DUOS (Data Use Oversight System) and EGA (European Genome-phenome Archive) to streamline data sharing while respecting participant consent. DUO extends NIH dbGaP data use categories with hierarchical structure for logical inference, includes consent codes for international data sharing, and implements ADA-M (Automated Data Access Matrix) for granular permissions. Used by 60+ million weekly downloads, DUO facilitates GDPR-aware data governance, phenotype-driven differential diagnostics, and translational research. The ontology ensures that informed consent language translates into machine-readable terms, accelerating responsible data reuse for AI/ML training datasets, clinical phenotyping, and genomic diagnostics while maintaining participant privacy and ethical oversight.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/EBISPOT/DUO",
      "formal_specification": "https://github.com/EBISPOT/DUO"
    },
    {
      "id": "B2AI_STANDARD:434",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "DEB",
      "description": "Devices, Experimental scaffolds and Biomaterials Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "An ontology developed to facilitate information curation in the area of medical devices, experimental scaffolds and biomaterials.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://bioportal.bioontology.org/ontologies/DEB",
      "publication": "doi:10.1002/adfm.201909910",
      "formal_specification": "https://github.com/ProjectDebbie/Ontology_DEB"
    },
    {
      "id": "B2AI_STANDARD:435",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "devops-infra",
      "description": "Devops Infrastructure Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "This ontology network aims at representing the main sets of entities and relationships used in the context of DevOps infrastructure. It is the result of a collaboration between Huawei Research Ireland and the Ontology Engineering Group at Universidad Politcnica de Madrid. It originally started from an analysis of the Configuration Management Databases used by Huawei Research Ireland for the management of a large part of its DevOps infrastructure, and has evolved into an ontology that may be used as a starting point for the standardisation of the representation of CMDB-related data across vendors.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://oeg-upm.github.io/devops-infra/index.html",
      "formal_specification": "https://github.com/oeg-upm/devops-infra"
    },
    {
      "id": "B2AI_STANDARD:436",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "DISDRIV",
      "description": "Disease Drivers Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Ontology for drivers and triggers of human diseases, built to classify ExO ontology exposure stressors. An application ontology.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/DiseaseOntology/DiseaseDriversOntology",
      "formal_specification": "https://github.com/DiseaseOntology/DiseaseDriversOntology"
    },
    {
      "id": "B2AI_STANDARD:437",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "DPO",
      "description": "Drosophila Phenotype Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:25"
      ],
      "purpose_detail": "Commonly encountered and/or high level Drosophila phenotypes.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/FlyBase/flybase-controlled-vocabulary/wiki",
      "publication": "doi:10.1186/2041-1480-4-30",
      "formal_specification": "https://github.com/FlyBase/drosophila-phenotype-ontology"
    },
    {
      "id": "B2AI_STANDARD:438",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "DTO",
      "description": "Drug Target Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:8",
        "B2AI_TOPIC:26"
      ],
      "purpose_detail": "Drug Target Ontology (DTO) is being developed at the University of Miami in the research group of Stephan Schrer. DTO was developed as part of the Illuminating the Druggable Genome (IDG) project (https://commonfund.nih.gov/idg/overview), is supported by grant (IDG Knowledge Management Center, (U54CA189205). DTO is a novel semantic framework to formalize knowledge about drug targets and is developed as a reference for drug targets with the longer-term goal to create a community standard that will facilitate the integration of diverse drug discovery information from numerous heterogeneous resources.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://bioportal.bioontology.org/ontologies/DTO",
      "publication": "doi:10.1186/s13326-017-0161-x",
      "formal_specification": "http://drugtargetontology.org/"
    },
    {
      "id": "B2AI_STANDARD:439",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "DIDEO",
      "description": "Drug-drug Interaction and Drug-drug Interaction Evidence Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:8"
      ],
      "purpose_detail": "The Potential Drug-drug Interaction and Potential Drug-drug Interaction Evidence Ontology",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/DIDEO/DIDEO",
      "formal_specification": "https://github.com/DIDEO/DIDEO"
    },
    {
      "id": "B2AI_STANDARD:440",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "EDAM",
      "description": "EMBRACE Data And Methods Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:29"
      ],
      "purpose_detail": "Data types, identifiers, and formats",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/edamontology/edamontology",
      "publication": "doi:10.1093/bioinformatics/btt113",
      "formal_specification": "https://github.com/edamontology/edamontology"
    },
    {
      "id": "B2AI_STANDARD:441",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "MFOEM",
      "description": "Emotion Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Affective phenomena such as emotions, moods, appraisals and subjective feelings.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/jannahastings/emotion-ontology",
      "formal_specification": "https://github.com/jannahastings/emotion-ontology"
    },
    {
      "id": "B2AI_STANDARD:442",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "ENVO",
      "description": "Environment Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:11"
      ],
      "purpose_detail": "Environmental systems, components, and processes.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://environmentontology.org/",
      "formal_specification": "https://github.com/EnvironmentOntology/envo"
    },
    {
      "id": "B2AI_STANDARD:443",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "ECTO",
      "description": "Environmental conditions, treatments and exposures ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:11"
      ],
      "purpose_detail": "Exposures to experimental treatments of plants and model organisms.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/EnvironmentOntology/environmental-exposure-ontology",
      "formal_specification": "https://github.com/EnvironmentOntology/environmental-exposure-ontology"
    },
    {
      "id": "B2AI_STANDARD:444",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "EVI",
      "description": "Evidence Graph Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:232",
          "category": "B2AI:Application",
          "name": "FAIRSCAPE Computation-Level Provenance and Evidence Graphs",
          "description": "FAIRSCAPE (Framework for FAIR and Scalable Computation and Analytics Processing Engine) implements the Evidence Graph Ontology (EVI) to create complete, machine-interpretable provenance graphs for every computational result in biomedical analytics workflows, embedding root evidence graph URIs in result metadata and assigning persistent identifiers (PIDs) to all software, datasets, and computation objects for long-term auditability and reproducibility. The framework generates EVI-compliant evidence graphs that link computations to the specific software versions, input datasets, runtime parameters, execution environments (operating system, library versions, container images), and personnel (researchers, data analysts, PIs) involved in producing each result, with all objects assigned globally unique persistent identifiers resolvable to detailed metadata including checksums, timestamps, licenses, and provenance relationships. FAIRSCAPE can execute Apache Spark jobs, Python/R scripts, workflow orchestration systems (Nextflow, Snakemake, CWL), or user-supplied Docker/Singularity containers, preserving complete provenance across heterogeneous execution modes and recording evidence graphs that capture computation-uses-software, computation-uses-input-dataset, computation-generates-output-dataset relationships following EVI's formal semantics. The evidence graph model supports inferential reasoning over provenance, enabling automated queries such as \"retrieve all datasets derived from raw instrument file X,\" \"identify which software versions contributed to result Y,\" or \"verify that analysis Z used validated, quality-controlled input data,\" which are essential for AI/ML workflows requiring training-data lineage, reproducible feature engineering, and audit trails for regulatory submissions. All results are annotated with FAIR metadata using the EVI evidence graph model, ensuring that archived data and software remain accessible, validatable, reproducible, and reusable across projects and institutions, directly supporting explainable AI requirements where model predictions must be traceable to documented data sources and computational methods with verifiable provenance chains.",
          "used_in_bridge2ai": true,
          "references": [
            "https://doi.org/10.1007/s12021-021-09529-4"
          ]
        },
        {
          "id": "B2AI_APP:233",
          "category": "B2AI:Application",
          "name": "FAIRSCAPE AI-Readiness Extensions with Pre-Model Explainability",
          "description": "FAIRSCAPE's AI-readiness extensions operationalize the Evidence Graph Ontology (EVI) to embed deep provenance graphs, data dictionaries, schema definitions, and feature-validation results within RO-Crate JSON-LD packages, supporting pre-model explainability (XAI), training-data readiness assessments, and automated dataset documentation for machine learning workflows. EVI-based provenance captures detailed relationships among computations, software, inputs, and outputs (modeled as \"Computation uses Software and Input, usedBy Computation generates Output\") and is serialized as JSON-LD for inclusion in RO-Crate metadata files that accompany datasets through packaging, storage, sharing, and reuse across repositories and compute environments. The FAIRSCAPE server assigns Archival Resource Key (ARK) persistent identifiers to every RO-Crate package, software component, dataset, and computation object, creating resolvable evidence chains where each node in the provenance graph can be dereferenced to access detailed metadata, software repositories (Zenodo archives with DOIs), data dictionaries defining feature semantics and measurement units, and validation reports documenting data quality checks (via Frictionless Data JSON Schema generation and Great Expectations rule evaluation). Human-readable datasheets are automatically generated from machine-readable JSON-LD provenance graphs, providing AI/ML practitioners with comprehensive dataset documentation including data sources, transformation steps, responsible parties, quality metrics, known limitations, and intended use casesinformation required for informed model development, bias identification, and regulatory compliance. Integration with validation tooling ensures that EVI graphs incorporate feature-level validation results as evidence nodes, linking each feature to pass/fail status on distributional checks (normality tests, outlier detection, range constraints) and enabling automated gatekeeping where ML pipelines halt training if provenance indicates critical validation failures or incomplete data lineage. The combined RO-Crate-plus-EVI approach supports pre-model explainability by documenting dataset characteristics, sample demographics, temporal coverage, missing-data patterns, and known biases before models are trained, enabling practitioners to assess fitness-for-purpose, identify potential fairness issues, and make informed decisions about data preprocessing, feature engineering, and model architecture choices based on documented evidence rather than assumptions.",
          "used_in_bridge2ai": true,
          "references": [
            "https://doi.org/10.1101/2024.12.23.629818"
          ]
        },
        {
          "id": "B2AI_APP:234",
          "category": "B2AI:Application",
          "name": "Bridge2AI Training-Data Lineage and Transformation Tracking",
          "description": "Bridge2AI AI-readiness recommendations explicitly specify using the Evidence Graph Ontology (EVI) or W3C PROV-O for machine-readable provenance to trace datasets back to ground-truth sources, document data transformation steps with links to versioned software, identify responsible people and organizations, and attach verifiability measures (checksums, validation reports) that support explainable ML, model auditability, and dataset integrity validation across collaborative biomedical research projects. The guidelines recommend encoding complete data lineage showing how analysis-ready training datasets derive from raw clinical observations, instrument readings, imaging studies, or omics assays, with each transformation step (quality filtering, normalization, feature extraction, imputation, harmonization across sites) linked to specific software releases archived in sustainable repositories (Zenodo, Software Heritage) with persistent identifiers (DOIs, SWHIDs) and documented parameter settings, enabling reproducibility and error diagnosis when model performance deviates from expectations. Machine-readable provenance formats like EVI enable automated queries over provenance graphs to answer critical questions for AI/ML workflows such as \"Which preprocessing operations were applied to this feature?\", \"What software version performed the data harmonization?\", \"Which institution contributed samples with characteristic X?\", and \"Are all transformation steps validated and checksummed?\", supporting training-data readiness assessments, model explainability analyses, and regulatory audits. Identification of responsible actors (data collectors, curators, analysts, PIs, institutional data stewards) within provenance graphs supports accountability, credit assignment via scholarly citation of datasets and software, and compliance with data governance policies requiring documentation of who accessed or modified data at each processing stage. Verifiability measures embedded in EVI graphsincluding cryptographic checksums (SHA-256) for raw and processed data files, validation reports from schema checkers (Frictionless Data) and quality frameworks (Great Expectations), and data quality metrics (completeness percentages, outlier counts, distributional statistics)enable automated integrity checks where ML pipelines verify that input data matches expected characteristics before commencing training, and audit systems detect unauthorized modifications or data corruption by comparing current checksums to provenance-recorded values. The integration of EVI-based provenance with dataset documentation standards (Datasheets for Datasets, Healthsheets, Data Cards) ensures that training data for AI models arrives with comprehensive metadata covering data sources, collection methods, preprocessing steps, known limitations, potential biases, privacy protections, and intended use casesinformation essential for responsible AI development, fairness assessments, and transparent reporting of model development to regulators, ethics boards, and end-users.",
          "used_in_bridge2ai": true,
          "references": [
            "https://doi.org/10.1101/2024.10.23.619844"
          ]
        },
        {
          "id": "B2AI_APP:235",
          "category": "B2AI:Application",
          "name": "CM4AI Dataset Packaging with Sample-Level Provenance and Archived Software",
          "description": "The Center for Multi-modal AI (CM4AI) functional genomics generating consortium integrates the Evidence Graph Ontology (EVI) into RO-Crate JSON-LD metadata packages alongside Schema.org and DataCite vocabularies to provide sample-level provenance linking data files to specific experiment identifiers, biological sample IDs, and archived software versions with DOIs, enabling reproducible dataset assembly, traceable sample-to-data relationships, and cross-tool interoperability within ML pipelines for genomic analysis. FAIRSCAPE clients employed by CM4AI researchers generate RO-Crate packages that embed dataset schemas (defining features, data types, measurement units, controlled vocabularies), data dictionaries (mapping column names to biological concepts and ontology terms), and EVI-modeled provenance graphs capturing the complete computational lineage of processed datasets from raw sequencing reads or imaging data through quality control, alignment, feature extraction, and normalization steps to analysis-ready matrices suitable for machine learning. Provenance graphs explicitly connect data files to specific biological contextfor example, linking chromatin accessibility peak files to the cell line sample IDs (ENCODE accessions, Cellosaurus IDs), experimental protocols (ATAC-seq, ChIP-seq with specific antibody targets), and sequencing platforms used to generate the dataenabling queries such as \"retrieve all datasets from HepG2 cells treated with compound X\" or \"identify replicates with consistent quality metrics across batch Y.\" Software components used in data processing workflows are archived to Zenodo to obtain Digital Object Identifiers and DataCite metadata records, with ARK persistent identifiers assigned by the FAIRSCAPE server linking provenance graph nodes to specific Zenodo-archived software releases (tagged versions with release notes, documentation, and test datasets), ensuring that every computational step in a dataset's lineage is traceable to a citable, versioned, archived software artifact accessible for inspection, reuse, or validation. The combined EVI-RO-Crate-DataCite approach supports reproducible dataset assembly for ML model development by documenting exactly which samples contributed to training, validation, and test partitions; which preprocessing pipelines and parameter settings were applied to each partition; and which software versions generated derived features or predicted labels, enabling transparent reporting of dataset provenance in publications, regulatory submissions, and data-sharing agreements while facilitating cross-tool interoperability where downstream ML frameworks (scikit-learn, PyTorch, TensorFlow) can parse RO-Crate metadata to automatically configure data loaders, validate feature schemas, and log dataset provenance in experiment tracking systems (MLflow, Weights & Biases).",
          "used_in_bridge2ai": true,
          "references": [
            "https://doi.org/10.48550/arxiv.2509.10432"
          ]
        },
        {
          "id": "B2AI_APP:236",
          "category": "B2AI:Application",
          "name": "ML-Ops Integration with Automated Metadata and Defeasible Reasoning",
          "description": "Bridge2AI standards and ML-Ops frameworks integrate the Evidence Graph Ontology (EVI) into machine learning operations pipelines to support defeasible reasoning (reasoning with contestable or revisable evidence about data quality, method appropriateness, and model performance), automated metadata generation from computational logs, and scalable audit trails linking model development, training, evaluation, and deployment phases to documented datasets, software, and responsible parties. EVI's support for defeasible reasoning enables encoding of provisional or contested claims about data and methodsfor example, marking a feature as \"likely biased based on preliminary fairness analysis, pending expert review\" or documenting that \"model performance on subgroup X is acceptable under assumption A but requires validation under alternative assumption B\"with evidence graphs maintaining complete revision histories showing when assessments were updated, which evidence prompted revisions, and who authorized changes, supporting transparent documentation of model development decisions and enabling retrospective audits of how data quality concerns or bias findings influenced final model specifications. Integration with ML-Ops templates (MLflow Projects, TensorFlow Extended pipelines, Kubeflow workflows) allows EVI evidence graphs to be automatically generated from execution logs, with nodes representing data ingestion steps, preprocessing operations, hyperparameter tuning runs, training epochs, evaluation metrics, and deployment configurations, and edges capturing dependencies and data flow among these pipeline stages, providing a complete audit trail of the ML development lifecycle without requiring manual documentation by data scientists. Automated metadata generation leverages EVI's structured provenance model to parse workflow logs and extract key informationsoftware versions, dataset identifiers with checksums, hyperparameter settings, compute resources used, training duration, convergence metrics, validation set performanceand populate human-readable datasheets, model cards, and XAI documentation templates that can be automatically updated whenever pipelines are rerun with new data or modified configurations, reducing documentation burden and ensuring that provenance records remain synchronized with actual computational artifacts. The combination of EVI provenance with ML-Ops experiment tracking systems enables queries across development iterations such as \"compare validation accuracy across all runs using dataset version 2.1 versus 2.2,\" \"identify which preprocessing variant introduced the feature distribution shift detected in model M,\" or \"trace model N back to the specific training samples that contributed most to its decision boundary,\" supporting systematic optimization of ML pipelines, root-cause analysis of performance regressions, and evidence-based justification of modeling choices in regulatory submissions or peer-reviewed publications.",
          "used_in_bridge2ai": true,
          "references": [
            "https://doi.org/10.48550/arxiv.2509.10432"
          ]
        },
        {
          "id": "B2AI_APP:237",
          "category": "B2AI:Application",
          "name": "Clinical Metadata Provenance for AI-Ready Health Data",
          "description": "Clinical and health data applications of the Evidence Graph Ontology (EVI) within Bridge2AI projects focus on capturing protocol-level provenance, linking clinical measurements to controlled terminologies and common data models, and establishing governance-compliant audit trails that support AI-readiness requirements for explainability, computability, and bias identification in multi-site clinical datasets. EVI-based provenance records document measurement protocols (sample collection procedures, assay platforms, instrument calibration records, software versions for clinical analyzers), diagnostic criteria (ICD-10/SNOMED-CT codes with version timestamps), and medication coding (RxNorm identifiers with dosage, frequency, and administration route), linking each recorded observation to the specific protocol, code system version, and responsible clinician or laboratory that generated the data, enabling downstream AI applications to account for measurement heterogeneity across sites, temporal changes in diagnostic criteria (e.g., DSM-5 versus DSM-5-TR), and differences in medication formulations when training models on aggregated clinical data. Integration with common data models (OMOP CDM, FHIR) and controlled terminologies (LOINC for lab tests, SNOMED-CT for diagnoses, RxNorm for medications) is documented in EVI provenance graphs showing which terminology mapping pipelines and concept-set definitions were applied to harmonize heterogeneous source data into standardized representations, supporting reproducible queries across institutions and transparent reporting of cohort definitions, inclusion/exclusion criteria, and feature engineering steps in multi-site ML studies. Governance features enabled by EVI provenanceincluding audit trails documenting who accessed or modified records (with timestamps and justifications), anomaly monitoring detecting unexpected changes in data distributions or missingness patterns that might indicate quality issues or unauthorized modifications, and version-controlled documentation of data use agreements, institutional review board approvals, and informed consent proceduressupport compliance with HIPAA, GDPR, and institutional data governance policies while enabling retrospective analysis of how dataset composition or processing decisions affected model performance or fairness across demographic subgroups. Privacy-preserving practices documented in EVI provenance graphssuch as tiered access controls limiting sensitive fields to authorized researchers, geographic generalization replacing precise locations with census tracts or zip code prefixes, and date shifting applying consistent random offsets to preserve temporal relationships while preventing re-identificationare explicitly recorded as transformation steps with links to responsible data custodians and institutional policies, enabling AI model developers to understand which privacy protections are in effect, assess their impact on model performance (e.g., does date shifting affect longitudinal disease progression models?), and document privacy-preserving data handling in ethics applications and regulatory submissions. Common data quality risks in clinical datasetsincluding incorrect timestamps from timezone inconsistencies or daylight saving transitions, inconsistent units mixing metric and imperial measurements, and duplicate records from multiple data feedsare detected through automated quality checks (Great Expectations rules, Frictionless Data schema validation) with results embedded as evidence nodes in EVI graphs, enabling gating mechanisms where ML pipelines refuse to train on data failing critical quality thresholds and alerting data curators to issues requiring manual review or systematic correction across the data warehouse.",
          "used_in_bridge2ai": true,
          "references": [
            "https://doi.org/10.48550/arxiv.2509.10432"
          ]
        },
        {
          "id": "B2AI_APP:238",
          "category": "B2AI:Application",
          "name": "Evidence Graph Structures for Scientific Claim Verification and LLM Augmentation",
          "description": "Graph-based evidence representations inspired by the Evidence Graph paradigm are applied to scientific claim verification tasks, where sentence-level evidence graphs structure rationales, capture stance relationships, and support retrieval-augmented generation (RAG) pipelines that improve large language model (LLM) reasoning by providing explicit, pruned evidence chains linking claims to supporting or refuting sentences from scientific literature. In claim verification workflows, evidence graphs are constructed with nodes representing sentences extracted from scientific abstracts or full-text articles and edges capturing contextual relationships (same paragraph, same section, citation links), hierarchical document structure (sentence within section within article), or learned semantic similarity (embedding-based nearest neighbors in evidence space), enabling graph-based retrieval (GraphRAG) that selectively retrieves multi-hop evidence paths more effectively than flat vector similarity search by leveraging document structure and inter-sentence dependencies. The verification pipeline operates in three stages enhanced by evidence graph representations first, evidence-paragraph retrieval uses BM25 or dense retrieval to identify candidate documents containing relevant information; second, rationale-sentence selection employs graph-updated sentence embeddings (where node representations are refined via graph neural networks incorporating edge information from the evidence graph) to identify the most salient sentences supporting or refuting the claim; third, stance classification predicts whether each rationale supports, refutes, or provides neutral evidence for the claim, with graph structure enabling multi-rationale aggregation where multiple supporting sentences collectively determine overall stance. Pruning strategies address the risk that fully connected evidence graphs introduce noise and fail to reflect meaningful document structure by retaining only high-confidence edges (sentence pairs with cosine similarity above threshold, adjacent sentences in key sections, citation-linked sentences across papers) and removing spurious connections, resulting in sparser graphs that improve downstream model accuracy by focusing attention on relevant evidence while filtering distractors. Evidence graphs serve as external structured knowledge to augment LLMs through two mechanisms in retrieval-augmented generation (RAG), relevant subgraphs (paths from claim node to evidence nodes with annotated stances) are serialized and inserted into LLM prompts, providing explicit rationale chains that improve reasoning accuracy and enable citation of specific supporting sentences; in fine-tuning or in-context learning, evidence graphs provide training supervision where models learn to follow graph-structured reasoning paths rather than relying solely on semantic encoding in pretrained weights, improving performance on domain-specific scientific verification tasks (SciFact dataset) where specialized knowledge and explicit reasoning chains outperform pure retrieval or generation approaches. The graph-structured rationale approach enhances explainability and auditability by providing interpretable traces for model decisionseach predicted stance can be traced to specific sentence nodes in the evidence graph, which in turn link to source documents with section and paragraph provenanceenabling human reviewers to verify whether model predictions align with cited evidence, identify when models misinterpret context or miss key counterevidence, and assess whether training data contained biased or incomplete evidence graphs that might propagate systematic errors into model predictions.",
          "used_in_bridge2ai": false,
          "references": [
            "https://openreview.net/pdf?id=cYAFwjY2bY"
          ]
        }
      ],
      "collection": [
        "standards_process_maturity_draft",
        "implementation_maturity_pilot"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:116"
      ],
      "purpose_detail": "The Evidence Graph Ontology (EVI) extends core concepts from the W3C Provenance Ontology (PROV-O) to provide a formal semantic framework for representing evidence, provenance, and computational reproducibility in biomedical research, specifically designed to support AI-readiness requirements including explainability, auditability, training-data lineage, and transparent model development workflows. Developed within the FAIR (Findable, Accessible, Interoperable, Reusable) data ecosystem and extensively deployed in Bridge2AI projects, EVI enables machine-readable representation of complete evidence chains connecting computational results to their originating datasets, software versions, runtime parameters, execution environments, and responsible personnel, addressing the critical gap between textual descriptions of methods and formal, verifiable records of computational provenance essential for reproducible science and regulatory compliance. The ontology defines classes and properties for modeling Computations (executable processes with specific runtime configurations), Software (versioned code with persistent identifiers), Datasets (input and output data objects with checksums and format specifications), Evidence Graphs (structured provenance networks capturing relationships among these entities via predicates like uses, usedBy, generates, and wasGeneratedBy), and Evidence Objects (nodes in provenance graphs representing specific computational artifacts or data transformations). EVI supports inferential reasoning over evidence graphs through OWL2 semantics, enabling automated queries such as 'trace all datasets contributing to this machine learning model's training set,' 'identify which software versions were used to generate features for classifier X,' or 'verify that all transformation steps from raw clinical data to analysis-ready datasets are documented with checksums and responsible parties.' The ontology is serialized as JSON-LD for web compatibility and embedded within RO-Crate (Research Object Crate) metadata packages, where EVI graphs coexist with Schema.org and DataCite vocabularies to provide comprehensive dataset documentation combining human-readable descriptions with machine-interpretable provenance. EVI's integration with persistent identifier systems (Archival Resource Keys/ARKs, Digital Object Identifiers/DOIs) ensures that every node in an evidence graphwhether representing a dataset, software release, computation run, or derived resultcan be assigned a globally unique, resolvable identifier, enabling long-term auditability and cross-repository citation of computational artifacts. Applications span FAIRSCAPE (Framework for FAIR and Scalable Computation and Analytics Processing Engine), a production biomedical analytics platform that creates complete EVI evidence graphs for every computational result (Spark jobs, scripts, workflows, containers) with root graph URIs embedded in result metadata and all objects (datasets, software, computations) assigned persistent IDs resolvable to execution details, supporting machine-interpretable provenance and reproducibility; FAIRSCAPE's AI-readiness extensions that embed EVI provenance graphs within RO-Crate JSON-LD packages alongside data dictionaries, schema definitions, and feature-validation results, with ARK PIDs automatically assigned to RO-Crates, software components, datasets, and computation objects, enabling automated generation of human-readable datasheets from machine-readable provenance to support pre-model explainability (XAI) and training-data readiness assessments; Bridge2AI project guidelines that explicitly recommend EVI (or PROV-O) for machine-readable provenance to trace datasets back to ground-truth sources, document data transformation steps with links to versioned software, identify people and organizations responsible for data collection and processing, and attach checksums for verifiabilitypractices fundamental to explainable ML, model auditability, and dataset integrity validation; CM4AI (Center for Multi-modal AI) and other Bridge2AI generating consortia that integrate EVI into RO-Crate metadata to connect data files to specific sample identifiers, experiment protocols, and archived software versions (with Zenodo DOIs), enabling reproducible dataset assembly, sample-level provenance tracking, and cross-tool interoperability within ML pipelines; and ML-Ops integration frameworks where EVI evidence graphs are incorporated into ML workflow templates to support defeasible reasoning (reasoning with contestable or revisable claims about data quality and method appropriateness), automated metadata generation from computational logs to populate XAI documentation, and scalable audit trails for model development, training, evaluation, and deployment phases. EVI's defeasible reasoning capabilitiesdistinguishing between conclusive evidence and evidence subject to revision or reinterpretationare particularly valuable for AI/ML contexts where data quality assessments, feature engineering decisions, and model performance evaluations may need to be updated as new validation results or bias analyses become available, with the evidence graph maintaining a complete historical record of all revisions and their justifications. The ontology addresses practical AI/ML workflow requirements: training-data lineage tracing ensures that every feature in a training matrix can be traced through transformation pipelines back to raw instrument readings or clinical observations with documented provenance at each step; transformation tracking links each preprocessing operation (normalization, imputation, feature extraction, data augmentation) to specific software versions, parameter settings, and validation results, enabling reproducibility and error diagnosis; integrity verification embeds checksums, file formats, and validation reports in evidence graphs, supporting automated checks that training data matches expected schemas and quality thresholds before model training commences; and actor attribution records which individuals, teams, or automated processes performed each operation, supporting accountability, credit assignment, and regulatory audits. EVI enables pre-model explainability by documenting dataset characteristics, feature distributions, bias assessments, and data-quality metrics in structured provenance graphs that can be queried to answer questions like 'What geographic regions are represented in the training data?' or 'Which data transformation introduced missing values in feature X?'information essential for interpreting model predictions and identifying potential biases before models are trained. Integration with validation frameworks (Frictionless Data for schema validation, Great Expectations for data quality assertions) allows EVI graphs to incorporate validation results as evidence nodes, linking features to pass/fail status on distributional checks, outlier detection, or fairness metrics, and enabling automated gatekeeping where ML pipelines halt if provenance graphs indicate critical validation failures. The ontology's RO-Crate packaging ensures that EVI provenance travels with datasets across repositories, compute environments, and organizational boundaries, supporting federated learning scenarios where multiple institutions contribute training data and the combined evidence graph documents data sources, harmonization procedures, privacy-preserving transformations, and institutional data use agreements, all resolvable via persistent identifiers without exposing underlying sensitive records. EVI's adoption within Bridge2AI reflects its alignment with FAIR principles and AI-readiness criteria: Findable through persistent identifiers and searchable metadata repositories, Accessible via resolvable URIs and standard APIs, Interoperable through OWL2/RDF semantics and JSON-LD serialization compatible with web frameworks, and Reusable through explicit licensing, detailed provenance, and validation records that enable confident reuse of datasets and computational workflows across projects and institutions.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://evidencegraph.github.io/EVI/index.html",
      "formal_specification": "https://github.com/EvidenceGraph/EVI"
    },
    {
      "id": "B2AI_STANDARD:445",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "ECO",
      "description": "Evidence ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "An ontology for experimental and other evidence statements.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.evidenceontology.org/",
      "publication": "doi:10.1093/nar/gkab1025",
      "formal_specification": "https://github.com/evidenceontology/evidenceontology"
    },
    {
      "id": "B2AI_STANDARD:446",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "XCO",
      "description": "Experimental condition ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Conditions under which physiological and morphological measurements are made both in the clinic and in studies involving humans or model organisms.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://rgd.mcw.edu/rgdweb/ontology/view.html?acc_id=XCO:0000000",
      "publication": "doi:10.1186/2041-1480-4-26",
      "formal_specification": "https://github.com/rat-genome-database/XCO-experimental-condition-ontology"
    },
    {
      "id": "B2AI_STANDARD:447",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "EXO",
      "description": "Exposure ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:11"
      ],
      "purpose_detail": "Vocabularies for describing exposure data to inform understanding of environmental health.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/CTDbase/exposure-ontology",
      "formal_specification": "https://github.com/CTDbase/exposure-ontology"
    },
    {
      "id": "B2AI_STANDARD:448",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "FYPO",
      "description": "Fission Yeast Phenotype Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:25"
      ],
      "purpose_detail": "Phenotypes observed in fission yeast.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/pombase/fypo",
      "publication": "doi:10.1093/bioinformatics/btt266",
      "formal_specification": "https://github.com/pombase/fypo"
    },
    {
      "id": "B2AI_STANDARD:449",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "FIDEO",
      "description": "Food Interactions with Drugs Evidence Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:8"
      ],
      "purpose_detail": "Food-Drug interactions automatically extracted from scientific literature.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://gitub.u-bordeaux.fr/erias/fideo",
      "formal_specification": "https://github.com/getbordea/fideo/"
    },
    {
      "id": "B2AI_STANDARD:450",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "FOODON",
      "description": "Food Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A broadly scoped ontology representing entities which bear a food role. It encompasses materials in natural ecosystems and agriculture tha...",
      "is_open": true,
      "requires_registration": false,
      "url": "https://foodon.org/",
      "publication": "doi:10.1038/s41538-018-0032-6",
      "formal_specification": "https://github.com/FoodOntology/foodon/"
    },
    {
      "id": "B2AI_STANDARD:451",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "FOBI",
      "description": "Food-Biomarker Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Represent food intake data and associate it with metabolomic data",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/pcastellanoescuder/FoodBiomarkerOntology",
      "publication": "doi:10.1093/bioinformatics/btab626",
      "formal_specification": "https://github.com/pcastellanoescuder/FoodBiomarkerOntology"
    },
    {
      "id": "B2AI_STANDARD:452",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "FOVT",
      "description": "FuTRES Ontology of Vertebrate Traits",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:25"
      ],
      "purpose_detail": "Application ontology used to convert vertebrate trait data in spreadsheets to triples.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://futres.org/",
      "formal_specification": "https://github.com/futres/fovt"
    },
    {
      "id": "B2AI_STANDARD:453",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "GSSO",
      "description": "Gender, Sex, and Sexual Orientation Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:6"
      ],
      "purpose_detail": "Terms for annotating interdisciplinary information concerning gender, sex, and sexual orientation.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://gsso.research.cchmc.org/",
      "formal_specification": "https://github.com/Superraptor/GSSO"
    },
    {
      "id": "B2AI_STANDARD:454",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "GO",
      "description": "Gene Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:36"
      ],
      "purpose_detail": "Function of genes and gene products.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://geneontology.org/",
      "publication": "doi:10.1093/nar/gkaa1113",
      "formal_specification": "https://github.com/geneontology/go-ontology"
    },
    {
      "id": "B2AI_STANDARD:455",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "GENEPIO",
      "description": "Genomic Epidemiology Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Vocabulary necessary to identify, document and research foodborne pathogens.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://genepio.org/",
      "formal_specification": "https://github.com/GenEpiO/genepio"
    },
    {
      "id": "B2AI_STANDARD:456",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "GECKO",
      "description": "Genomics Cohorts Knowledge Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:4",
        "B2AI_TOPIC:6"
      ],
      "purpose_detail": "The Genomics Cohorts Knowledge Ontology (GECKO) provides a standardized vocabulary for describing attributes of genomics cohorts and individual-level data in population-based research studies. Developed by the CINECA (Common Infrastructure for National Cohorts in Europe, Canada, and Africa) project and maintained for the International HundredK+ Cohorts Consortium (IHCC), GECKO enables harmonized representation of cohort metadata across diverse genomics studies, biobanks, and research consortia. The ontology encompasses five major categories: cohort design characteristics (prospective/retrospective, longitudinal/cross-sectional, case-control, family-based), participant demographics and socioeconomic attributes, phenotypic data collection methods (questionnaires, clinical assessments, biospecimen types), genomics data types (whole genome sequencing, exome sequencing, genome-wide association studies, RNA-seq, methylation arrays), and data access policies with consent frameworks. GECKO standardizes terminology for cohort recruitment strategies, inclusion/exclusion criteria, sample sizes, age ranges, ancestry populations, geographic locations, and follow-up durations critical for cohort discovery and meta-analysis planning. The ontology integrates with other OBO Foundry ontologies including BFO (Basic Formal Ontology) as top-level and OBI (Ontology for Biomedical Investigations) as mid-level, ensuring semantic interoperability. GECKO provides two products: the OBO-compliant gecko.owl for formal ontology applications, and ihcc-gecko.owl tailored for IHCC cohort cataloging with specialized browser labels and categorization. Automated tools based on JSON schema mapping files enable generation of harmonized data dictionaries and FAIRified metadata for cohort studies. Applications include cohort discovery portals enabling researchers to identify suitable cohorts for collaborative studies, standardized phenotype harmonization across studies for meta-GWAS, ethical data sharing frameworks through consent ontology terms, and FAIR data principles implementation in population genomics repositories. GECKO facilitates interoperability between cohort catalogs like BBMRI-ERIC, Maelstrom Research, dbGaP, and EGA by providing common terminology for cohort characteristics, enabling federated queries across international biobanks. The ontology is distributed under Creative Commons Attribution 4.0 International License, ensuring open access for academic and commercial research.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/IHCC-cohorts/GECKO",
      "formal_specification": "https://github.com/IHCC-cohorts/GECKO"
    },
    {
      "id": "B2AI_STANDARD:457",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "GENO",
      "description": "Genotype Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:58"
      ],
      "purpose_detail": "GENO is an OWL2 OBO Foundry ontology representing levels of genetic variation specified in genotypes to support genotype-to-phenotype (G2P) data aggregation and analysis across diverse research communities and sources. The core model is a graph decomposing genotypes into smaller components of variation, from complete genotypes specifying sequence variation across entire genomes down to specific allelic variants and sequence alterations. This partonomy structure enables integrated analysis of G2P data where phenotype annotations are made at different granularity levels. GENO describes genotype attributes including zygosity, genomic position, expression, dominance, and functional dependencies or consequences of variants. Beyond heritable genomic sequence variation, GENO represents transient variation in gene expression from knockdown reagents or overexpression constructs, representing this variation in terms of targeted genes to parallel sequence variation representation. GENO models G2P associations focusing on genotype-phenotype-environment interplay and uses the Scientific Evidence and Provenance Information Ontology (SEPIO) for provenance and experimental evidence. The ontology is orthogonal to but integrates with the Sequence Ontology (SO), Human Phenotype Ontology (HPO), Feature Annotation Location Description Ontology (FALDO), and Variation Ontology (VariO), supporting AI/ML applications in variant effect prediction, phenotype association analysis, and precision medicine.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/monarch-initiative/GENO-ontology",
      "formal_specification": "https://github.com/monarch-initiative/GENO-ontology"
    },
    {
      "id": "B2AI_STANDARD:458",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "GEO",
      "description": "Geographical Entity Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:14"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:96"
      ],
      "purpose_detail": "An ontology of geographical entities",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/ufbmi/geographical-entity-ontology/",
      "formal_specification": "https://github.com/ufbmi/geographical-entity-ontology/"
    },
    {
      "id": "B2AI_STANDARD:459",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "GMDN",
      "description": "Global Medical Device Nomenclature",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:35"
      ],
      "purpose_detail": "The Global Medical Device Nomenclature (GMDN) is a comprehensive set of terms, within a structured category hierarchy, which name and group ALL medical device products including implantables, medical equipment, consumables, and diagnostic devices.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://www.gmdnagency.org/"
    },
    {
      "id": "B2AI_STANDARD:460",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "GNO",
      "description": "Glycan Naming and Subsumption Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "GlyTouCan provides stable accessions for glycans described at varyious degrees of characterization, including compositions (no linkage).",
      "is_open": true,
      "requires_registration": false,
      "url": "https://gnome.glyomics.org/",
      "publication": "doi:10.5281/zenodo.6678279",
      "formal_specification": "https://github.com/glygen-glycan-data/GNOme"
    },
    {
      "id": "B2AI_STANDARD:461",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "HSO",
      "description": "Health Surveillance Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:92"
      ],
      "purpose_detail": "The Health Surveillance Ontology (HSO) provides a comprehensive framework for standardizing terminology and concepts related to health surveillance systems, disease monitoring programs, and public health data collection across human and veterinary medicine. Developed by the Swedish Veterinary Agency (SVA) and aligned with OBO Foundry principles, HSO enables interoperability between surveillance databases, epidemiological studies, and public health information systems by providing consistent vocabulary for surveillance activities, case definitions, reporting requirements, and data quality metrics. The ontology encompasses surveillance system architectures (passive surveillance, active surveillance, sentinel surveillance, syndromic surveillance), data collection methodologies (laboratory-based surveillance, clinical reporting, population surveys, environmental monitoring), case classification criteria (confirmed cases, probable cases, suspect cases based on laboratory/clinical/epidemiological evidence), temporal and spatial granularity specifications (reporting periods, geographic resolution, population denominators), and data quality indicators (completeness, timeliness, representativeness, sensitivity, specificity). HSO supports One Health approaches by bridging human, animal, and environmental health surveillance domains, facilitating detection of zoonotic disease emergence, antimicrobial resistance tracking, and foodborne outbreak investigations. Applications include standardization of surveillance system metadata for interoperability between national and international health agencies (WHO, ECDC, OIE), automated validation of surveillance data submissions, harmonization of case definitions across jurisdictions, and machine-readable representation of surveillance protocols for reproducibility. HSO integrates with disease ontologies (DO, DOID), pathogen ontologies (IDO, NCBITaxon), and geographic ontologies (GAZ) to provide comprehensive semantic framework for epidemiological data. The ontology enables FAIR principles implementation in public health surveillance by making surveillance system documentation findable, accessible, interoperable, and reusable.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://w3id.org/hso",
      "formal_specification": "https://github.com/SVA-SE/HSO"
    },
    {
      "id": "B2AI_STANDARD:462",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "HL7 Vocabulary",
      "description": "HL7 Vocabulary",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:40"
      ],
      "purpose_detail": "An index to the HL7-supported Code Systems.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.hl7.org/documentcenter/public/standards/vocabulary/vocabulary_tables/infrastructure/vocabulary/vocabulary.html#voc-systems"
    },
    {
      "id": "B2AI_STANDARD:463",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "HOM",
      "description": "Homology Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Concepts related to homology, as well as other concepts used to describe similarity and non-homology.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/BgeeDB/homology-ontology",
      "publication": "doi:10.1016/j.tig.2009.12.012",
      "formal_specification": "https://github.com/BgeeDB/homology-ontology"
    },
    {
      "id": "B2AI_STANDARD:464",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "HOOM",
      "description": "HPO - ORDO Ontological Module",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:25"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:80"
      ],
      "purpose_detail": "Orphanet provides phenotypic annotations of the rare diseases in the Orphanet nomenclature using the Human Phenotype Ontology (HPO). HOOM is a module that qualifies the annotation between a clinical entity and phenotypic abnormalities according to a frequency and by integrating the notion of diagnostic criterion.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://bioportal.bioontology.org/ontologies/HOOM",
      "formal_specification": "https://www.orphadata.com/ontologies/"
    },
    {
      "id": "B2AI_STANDARD:465",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "HANCESTRO",
      "description": "Human Ancestry Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A systematic description of the ancestry concepts used in the NHGRI-EBI Catalog",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/EBISPOT/ancestro",
      "publication": "doi:10.1186/s13059-018-1396-2",
      "formal_specification": "https://github.com/EBISPOT/ancestro"
    },
    {
      "id": "B2AI_STANDARD:466",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "HSAPDV",
      "description": "Human Developmental Stages",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The Human Developmental Stages ontology (HsapDv) provides standardized terminology for describing human lifecycle stages from conception through senescence, developed by the Bgee group in collaboration with Uberon and EHDAA2 (Human Developmental Anatomy Ontology) developers to enable precise temporal annotation of biological data across developmental biology, clinical research, and population studies. HsapDv encompasses both embryonic and postnatal stages, utilizing Carnegie staging system for prenatal development (Carnegie stages 1-23 covering days 1-56 post-fertilization, approximately embryonic weeks 1-8) which provides morphology-based developmental milestones independent of gestational age variations. Embryonic stages capture critical developmental events including fertilization, cleavage, blastocyst formation, gastrulation, neurulation, organogenesis, and fetal development through birth. Postnatal stages include neonatal period (birth to 28 days), infancy (1 month to 2 years), early childhood (2-6 years), middle childhood (6-12 years), adolescence (12-18 years encompassing puberty), young adulthood (18-40 years), middle adulthood (40-65 years), and late adulthood/senescence (65+ years) with subdivisions for geriatric populations. Each stage is formally defined with temporal boundaries, morphological characteristics, physiological milestones (motor skills, cognitive development, hormonal changes), and relationships to other developmental stages through \"immediately_preceded_by\" and \"part_of\" relations. HsapDv integrates with Uberon for anatomical structure development timing, enabling queries like \"when does the cerebral cortex develop\" or \"which genes are expressed in neural tube during neurulation.\" Applications span developmental biology research (temporal annotation of gene expression atlases, single-cell RNA-seq developmental trajectories, epigenetic modification timelines), clinical medicine (prenatal diagnosis, developmental delay assessment, age-appropriate clinical reference ranges), teratology studies (critical periods for teratogen exposure), pharmacology (age-specific drug metabolism and dosing), and epidemiology (age-stratified disease incidence). HsapDv enables cross-species developmental comparisons through alignment with other species-specific developmental ontologies (mouse MmusDv, zebrafish ZFS), facilitating translational research and comparative embryology. The ontology is distributed in OBO and OWL formats through http://purl.obolibrary.org/obo/hsapdv.owl and browsable via OBO Foundry portals, supporting reproducible temporal annotation in biomedical databases, developmental atlases, and clinical decision support systems.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/obophenotype/developmental-stage-ontologies/wiki/HsapDv",
      "formal_specification": "https://github.com/obophenotype/developmental-stage-ontologies"
    },
    {
      "id": "B2AI_STANDARD:467",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "DOID",
      "description": "Human Disease Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:7"
      ],
      "purpose_detail": "The Human Disease Ontology (DOID) is a comprehensive, standardized ontology that provides a hierarchical classification system for human diseases organized primarily by etiology (underlying cause). Developed as part of the Open Biomedical Ontologies (OBO) Foundry, DOID serves as a cornerstone reference for disease terminology in biomedical research, clinical informatics, and healthcare applications. The ontology integrates multiple disease classification systems including ICD, SNOMED CT, UMLS, and MeSH, providing extensive cross-references that enable interoperability between different medical coding systems. DOID structures diseases into logical hierarchies based on disease mechanisms, affected anatomical systems, and causal agents, enabling both broad categorical searches and precise disease identification. Each disease concept includes standardized names, definitions, synonyms, and relationships to parent and child terms, creating a rich semantic network that supports computational analysis of disease data. The ontology is extensively used in genomics databases, electronic health records, biomedical literature annotation, drug discovery pipelines, and epidemiological studies where consistent disease terminology is essential for data integration and comparative analysis.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://www.disease-ontology.org",
      "formal_specification": "https://github.com/DiseaseOntology/HumanDiseaseOntology"
    },
    {
      "id": "B2AI_STANDARD:468",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "HPO",
      "description": "Human Phenotype Ontology",
      "related_to": [
        "B2AI_STANDARD:784"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:25"
      ],
      "purpose_detail": "The Human Phenotype Ontology (HPO) provides standardized vocabulary for phenotypic abnormalities encountered in human disease, containing over 18,000 terms and 156,000+ annotations to hereditary diseases. Each HPO term describes a specific phenotypic feature (e.g., \"Atrial septal defect\" HP:0001631) organized in a hierarchical structure from general to specific findings. Developed using medical literature, Orphanet, DECIPHER, and OMIM, HPO enables precise phenotype-driven differential diagnostics, genomic variant interpretation, and translational research. The ontology integrates with major biomedical resources and powers phenotype matching algorithms that rank diseases by clinical feature similarity. HPO is foundational for rare disease diagnosis tools (Exomiser, Phenomizer, PhenoTips), electronic health record phenotyping, and clinical decision support systems. As a Monarch Initiative flagship product and GA4GH driver project, HPO enables semantic integration across species, connecting human phenotypes to model organism phenotypes for translational research. The ontology supports deep phenotyping in genomics studies, electronic health record phenotype extraction, natural language processing for clinical notes, and phenotype-driven gene prioritization. HPO annotations link phenotypes to genes, diseases, and publications, facilitating genotype-phenotype correlation studies. In AI/ML applications, HPO powers phenotype-based similarity learning for rare disease diagnosis, automated phenotype extraction from clinical narratives using NLP, ontology-guided feature engineering for predictive models, knowledge graph embeddings for disease gene discovery, and multi-modal patient representation learning combining genomics, phenotypes, and clinical data to support precision medicine and clinical genomics.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://hpo.jax.org/",
      "formal_specification": "https://github.com/obophenotype/human-phenotype-ontology"
    },
    {
      "id": "B2AI_STANDARD:469",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "XLMOD",
      "description": "HUPO-PSI cross-linking and derivatization reagents controlled vocabulary",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:28"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:41"
      ],
      "purpose_detail": "A structured controlled vocabulary for cross-linking reagents used with proteomics mass spectrometry.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.psidev.info/groups/controlled-vocabularies"
    },
    {
      "id": "B2AI_STANDARD:470",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "HTN",
      "description": "Hypertension Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:7"
      ],
      "purpose_detail": "An ontology for representing clinical data about hypertension.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/aellenhicks/htn_owl",
      "formal_specification": "https://github.com/aellenhicks/htn_owl"
    },
    {
      "id": "B2AI_STANDARD:471",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "IDO",
      "description": "Infectious Disease Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:7"
      ],
      "purpose_detail": "A set of interoperable ontologies that will together provide coverage of the infectious disease domain. IDO core is the upper-level ontology...",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/infectious-disease-ontology/infectious-disease-ontology",
      "formal_specification": "https://github.com/infectious-disease-ontology/infectious-disease-ontology"
    },
    {
      "id": "B2AI_STANDARD:472",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "IAO",
      "description": "Information Artifact Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "An ontology of information entities.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/information-artifact-ontology/IAO",
      "formal_specification": "https://github.com/information-artifact-ontology/IAO"
    },
    {
      "id": "B2AI_STANDARD:473",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "ICO",
      "description": "Informed Consent Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "purpose_detail": "An ontology of clinical informed consents",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/ICO-ontology/ICO",
      "formal_specification": "https://github.com/ICO-ontology/ICO"
    },
    {
      "id": "B2AI_STANDARD:474",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "ICEO",
      "description": "Integrative and Conjugative Element Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "An integrated biological ontology for the description of bacterial integrative and conjugative elements (ICEs).",
      "is_open": true,
      "requires_registration": false,
      "url": "http://db-mml.sjtu.edu.cn/ICEberg/",
      "publication": "doi:10.1038/s41597-021-01112-5",
      "formal_specification": "https://github.com/ontoice/ICEO"
    },
    {
      "id": "B2AI_STANDARD:475",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "ITO",
      "description": "Intelligence Task Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:87"
      ],
      "purpose_detail": "Comprehensive, curated and interlinked data of artificial intelligence tasks, benchmarks, AI performance metrics, benchmark results and research papers.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://openbiolink.github.io/ITOExplorer/",
      "formal_specification": "https://github.com/OpenBioLink/ITO"
    },
    {
      "id": "B2AI_STANDARD:476",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "INO",
      "description": "Interaction Network Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "An ontology of interactions and interaction networks.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/INO-ontology/ino",
      "publication": "doi:10.1186/2041-1480-6-2",
      "formal_specification": "https://github.com/INO-ontology/ino"
    },
    {
      "id": "B2AI_STANDARD:477",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "IOBC",
      "description": "Interlinking Ontology for Biological Concepts",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:1"
      ],
      "purpose_detail": "biological, biomedical, and related concepts",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/kushidat/IOBC",
      "publication": "doi:10.1007/s00354-019-00074-y",
      "formal_specification": "https://github.com/kushidat/IOBC"
    },
    {
      "id": "B2AI_STANDARD:478",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "KISAO",
      "description": "Kinetic Simulation Algorithm Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Algorithms for simulating biology, their parameters, and their outputs.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://co.mbine.org/standards/kisao",
      "formal_specification": "https://github.com/SED-ML/KiSAO"
    },
    {
      "id": "B2AI_STANDARD:479",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "MP",
      "description": "Mammalian Phenotype Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:25"
      ],
      "purpose_detail": "Standard terms for annotating mammalian phenotypic data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/mgijax/mammalian-phenotype-ontology",
      "publication": "doi:10.1007/s00335-012-9421-3",
      "formal_specification": "https://github.com/mgijax/mammalian-phenotype-ontology"
    },
    {
      "id": "B2AI_STANDARD:480",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "MVX",
      "description": "Manufacturers of Vaccines",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:13"
      ],
      "purpose_detail": "Code set for active and inactive manufacturers of vaccines in the US.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www2a.cdc.gov/vaccines/iis/iisstandards/vaccines.asp?rpt=mvx"
    },
    {
      "id": "B2AI_STANDARD:481",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "MS",
      "description": "Mass spectrometry ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:28"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:41"
      ],
      "purpose_detail": "A structured controlled vocabulary for the annotation of experiments concerned with proteomics mass spectrometry.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://www.psidev.info/groups/controlled-vocabularies",
      "publication": "doi:10.1093/database/bat009",
      "formal_specification": "https://github.com/HUPO-PSI/psi-ms-CV"
    },
    {
      "id": "B2AI_STANDARD:482",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "MMO",
      "description": "Measurement method ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A representation of the variety of methods used to make clinical and phenotype measurements.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://rgd.mcw.edu/rgdweb/ontology/view.html?acc_id=MMO:0000000",
      "publication": "doi:10.1186/2041-1480-4-26",
      "formal_specification": "https://github.com/rat-genome-database/MMO-Measurement-Method-Ontology/"
    },
    {
      "id": "B2AI_STANDARD:483",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "OLATDV",
      "description": "Medaka Developmental Stages",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The Medaka Developmental Stages ontology (OlatDv) provides standardized terminology for describing developmental stages of medaka (Oryzias latipes, Japanese rice fish), a key teleost model organism in developmental biology, genetics, toxicology, and comparative vertebrate research, based on Iwamatsu staging system and developed from the original Medaka Fish Ontology (MFO) by Thorsten Henrich. Medaka serves as a powerful model organism complementing zebrafish and mouse due to its transparent embryos enabling live imaging, short generation time (2-3 months), small size suitable for laboratory culture, fully sequenced genome with conserved vertebrate gene organization, and established genetic tools including transgenesis, CRISPR/Cas9 editing, and mutant libraries. OlatDv encompasses embryonic and larval stages from fertilization through sexual maturity, currently focusing on pre-adult development. Embryonic stages follow Iwamatsu's morphological staging system (stages 1-40) covering fertilization (stage 1), cleavage (stages 2-7), blastula (stages 8-10), gastrulation (stages 11-17), neurulation (stages 18-20), organogenesis (stages 21-30), and pre-hatching development (stages 31-40 leading to hatching at approximately day 7-10 post-fertilization at 26C). Each stage is defined by specific morphological landmarks including somite numbers, heart development, pigmentation patterns, fin bud appearance, eye development, and gill filament formation. Post-hatching larval stages capture metamorphosis, scale formation, sex differentiation, and juvenile maturation through first reproduction. OlatDv provides temporal annotations crucial for comparative developmental biology studies examining vertebrate evolution, particularly teleost-specific genome duplication events and developmental innovations. Applications include temporal annotation of gene expression databases (Medaka Expression Database), developmental toxicology studies (OECD fish embryo toxicity tests using medaka as alternative to zebrafish), endocrine disruption research (sex determination mechanisms), carcinogenesis studies (medaka exhibits spontaneous tumor formation), and aging research (short lifespan enables longitudinal studies). Integration with Uberon anatomical ontology enables queries linking developmental stage to organ system maturation, essential for understanding tissue-specific gene expression changes during development. OlatDv facilitates cross-species developmental comparisons through alignment with zebrafish ZFS, frog XAO, and mammalian developmental ontologies, supporting evolutionary developmental biology (evo-devo) research and identification of conserved versus lineage-specific developmental programs across vertebrates. The ontology is distributed through OBO Foundry as olatdv.obo and olatdv.owl formats.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/obophenotype/developmental-stage-ontologies/wiki/OlatDv",
      "formal_specification": "https://github.com/obophenotype/developmental-stage-ontologies"
    },
    {
      "id": "B2AI_STANDARD:484",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "MAXO",
      "description": "Medical Action Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:58"
      ],
      "purpose_detail": "Terms for medical procedures, interventions, therapies, treatments, and recommendations.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/monarch-initiative/MAxO",
      "formal_specification": "https://github.com/monarch-initiative/MAxO"
    },
    {
      "id": "B2AI_STANDARD:485",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "MED-RT",
      "description": "Medication Reference Terminology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "codesystem"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:8"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:74"
      ],
      "purpose_detail": "Formal ontological representations of medication terminology, pharmacologic classifications, and asserted authoritative relationships between them. Replaces NDF-RT. Provided through UMLS.",
      "is_open": true,
      "requires_registration": true,
      "url": "https://evs.nci.nih.gov/ftp1/NDF-RT/Introduction%20to%20MED-RT.pdf"
    },
    {
      "id": "B2AI_STANDARD:486",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "MFOMD",
      "description": "Mental Disease Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:7"
      ],
      "purpose_detail": "Mental diseases such as schizophrenia, annotated with DSM-IV and ICD codes where applicable.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/jannahastings/mental-functioning-ontology",
      "formal_specification": "https://github.com/jannahastings/mental-functioning-ontology"
    },
    {
      "id": "B2AI_STANDARD:487",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "MF",
      "description": "Mental Functioning Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The Mental Functioning Ontology (MF) is an OBO Foundry ontology providing structured terminology for mental processes, cognitive functions, emotional states, and behavioral phenomena to support standardized annotation of neuroscience data, psychiatric research, psychological assessments, and mental health informatics. Developed by Janna Hastings and collaborators, MF extends the Basic Formal Ontology (BFO) upper-level framework and integrates with domain ontologies including the Mental Disease Ontology (MD) for psychiatric disorders, the Cognitive Atlas for cognitive processes, the Emotion Ontology for affective states, and the Gene Ontology (GO) for molecular underpinnings of neural function. The ontology comprehensively represents cognitive domains including perception (visual, auditory, somatosensory, chemosensory), attention (selective, divided, sustained), memory (working, episodic, semantic, procedural), executive functions (planning, inhibition, cognitive flexibility, decision-making), language processing, and reasoning, alongside emotional and motivational constructs such as valence, arousal, mood states, personality traits, and social cognition (theory of mind, empathy, social perception). MF captures temporal dynamics of mental processes (onset, duration, termination), intensity dimensions, and contextual dependencies, enabling nuanced representation of psychological phenomena. Integration with clinical terminologies like DSM-5, ICD-11, and RDoC (Research Domain Criteria) facilitates translational psychiatry linking basic neuroscience to clinical phenotypes. Applications include standardized annotation of neuroimaging studies identifying brain regions associated with specific mental functions, computational psychiatry modeling mental disorders as disruptions in cognitive and emotional processes, natural language processing extracting mental state descriptions from clinical notes, AI-based mental health assessment systems, meta-analysis of psychological experiments through unified terminology, and personalized medicine approaches tailoring psychiatric treatments to individual cognitive profiles. MF follows OBO Foundry principles with open-source development, logical consistency checking, and community-driven refinement, essential for computational neuroscience, digital mental health platforms, cognitive science research, and integrative brain databases.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/jannahastings/mental-functioning-ontology",
      "formal_specification": "https://github.com/jannahastings/mental-functioning-ontology"
    },
    {
      "id": "B2AI_STANDARD:488",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "MOD",
      "description": "Metadata vocabulary for Ontology Description and Publication",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "An OWL ontology and application profile to capture metadata information for ontologies, vocabularies or semantic resources/artefacts in general.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/sifrproject/MOD-Ontology",
      "publication": "doi:10.1007/978-3-319-70863-8_17",
      "formal_specification": "https://github.com/sifrproject/MOD-Ontology"
    },
    {
      "id": "B2AI_STANDARD:489",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "MRO",
      "description": "MHC Restriction Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The Major Histocompatibility Complex (MHC) Restriction Ontology (MRO) is a specialized ontology developed by the Immune Epitope Database (IEDB) to provide standardized terminology for describing MHC restriction phenomena in immunological experiments and data. MHC restriction refers to the biological process by which T-cell recognition of antigens is limited to peptides presented by specific MHC molecules that are compatible with the T-cell's own MHC background. MRO systematically organizes the complex relationships between MHC alleles, T-cell responses, and antigen presentation contexts that are crucial for understanding adaptive immune responses, vaccine development, and transplantation immunology. The ontology enables precise annotation of immunological experiments by providing controlled vocabulary terms for MHC class I and class II molecules, their allelic variants, restriction patterns, and associated experimental conditions. This standardization is essential for comparative immunology studies, epitope mapping projects, and the development of personalized immunotherapies where accurate description of MHC-restricted immune responses is critical for data interpretation and clinical translation.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/IEDB/MRO",
      "formal_specification": "https://github.com/IEDB/MRO"
    },
    {
      "id": "B2AI_STANDARD:490",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "MIAPA",
      "description": "MIAPA Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "An application ontology to formalize annotation of phylogenetic data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.evoio.org/wiki/MIAPA",
      "publication": "doi:10.1089/omi.2006.10.231",
      "formal_specification": "https://github.com/evoinfo/miapa/"
    },
    {
      "id": "B2AI_STANDARD:491",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "MPIO",
      "description": "Minimum PDDI Information Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Minimum information regarding potential drug-drug interaction information.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/MPIO-Developers/MPIO",
      "formal_specification": "https://github.com/MPIO-Developers/MPIO"
    },
    {
      "id": "B2AI_STANDARD:492",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "MCRO",
      "description": "Model Card Report Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "modelcards"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "An OWL2-based artifact that represents and formalizes model card report information. The current release of this ontology utilizes standard concepts and properties from OBO Foundry ontologies.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/UTHealth-Ontology/MCRO",
      "publication": "doi:10.1186/s12859-022-04797-6",
      "formal_specification": "https://github.com/UTHealth-Ontology/MCRO"
    },
    {
      "id": "B2AI_STANDARD:493",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "MI",
      "description": "Molecular Interactions Controlled Vocabulary",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:20",
        "B2AI_TOPIC:26"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:41"
      ],
      "purpose_detail": "Vocabulary for the annotation of experiments concerned with protein-protein interactions.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/HUPO-PSI/psi-mi-CV",
      "formal_specification": "https://github.com/HUPO-PSI/psi-mi-CV"
    },
    {
      "id": "B2AI_STANDARD:494",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "MOP",
      "description": "Molecular Process Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:29"
      ],
      "purpose_detail": "The Molecular Process Ontology (MOP) provides standardized vocabulary for describing molecular-level processes, transformations, and mechanisms that occur in chemical and biochemical systems, maintained by the Royal Society of Chemistry as part of the RSC ontology ecosystem. MOP encompasses reaction mechanisms (nucleophilic substitution, electrophilic addition, radical reactions, pericyclic reactions), molecular interactions (hydrogen bonding, van der Waals forces, - stacking, hydrophobic interactions, electrostatic interactions), conformational changes (protein folding, ligand-induced conformational shifts, allosteric transitions), energy transfer processes (fluorescence resonance energy transfer FRET, photoinduced electron transfer, vibrational energy relaxation), and transport phenomena (diffusion, membrane permeation, active transport, facilitated diffusion). The ontology provides detailed mechanistic descriptions including activation energies, transition states, reaction intermediates, rate-determining steps, and catalytic cycles essential for understanding chemical reactivity and biological function at the molecular scale. MOP integrates with the Chemical Methods Ontology (CHMO) for experimental techniques, CHEBI for chemical entities, and the Gene Ontology (GO) for biological processes, enabling comprehensive annotation of molecular transformations from pure chemistry through biochemistry to systems biology. Applications include annotation of reaction databases for synthetic chemistry planning, mechanistic modeling of enzymatic catalysis, drug-target interaction mechanisms for rational drug design, metabolic pathway analysis with detailed reaction mechanisms, and computational chemistry workflow documentation. MOP supports reproducibility in mechanistic studies by standardizing descriptions of reaction conditions, stereochemical outcomes, regioselectivity, and stereoselectivity patterns. The ontology enables semantic searches for reactions by mechanism type, facilitating discovery of analogous transformations across different chemical contexts and supporting retrosynthetic analysis in computer-aided synthesis planning tools.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.ebi.ac.uk/ols/ontologies/mop",
      "formal_specification": "https://github.com/rsc-ontologies/rxno"
    },
    {
      "id": "B2AI_STANDARD:495",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "MONDO",
      "description": "Mondo Disease Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:7"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:58"
      ],
      "purpose_detail": "MONDO is an OBO Foundry ontology providing a unified disease terminology that harmonizes disease definitions across multiple resources including HPO, OMIM, SNOMED CT, ICD, ORDO, DO, MedGen, GARD, and others. It addresses the proliferation of inconsistent disease mappings by providing logic-based structure with precise 1:1 equivalence axioms connecting to other resources, validated by OWL reasoning. MONDO contains over 25,880 diseases including 22,919 human diseases (4,727 cancers, 1,074 infectious diseases, 11,601 Mendelian diseases, 15,857 rare diseases) and 2,960 non-human diseases, with 129,785 database cross-references and 108,076 synonyms (exact, narrow, broad, and related). The ontology provides hierarchical classification for disease grouping and rolling up and uses precise semantic annotations for each mapping rather than loose cross-references. MONDO is released in three formats: mondo-with-equivalents.owl (with OWL equivalence axioms and inter-ontology axiomatization using CL, Uberon, GO, HP, RO, NCBITaxon), mondo.obo (simplified with xrefs), and mondo-with-equivalents.json. Coordinated with the Human Phenotype Ontology (HPO) which describes phenotypic features, MONDO supports AI/ML applications in disease classification, phenotype-disease association, rare disease diagnosis, and cross-resource knowledge integration.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://mondo.monarchinitiative.org/",
      "publication": "doi:10.1093/nar/gkw1128",
      "formal_specification": "https://github.com/monarch-initiative/mondo"
    },
    {
      "id": "B2AI_STANDARD:496",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "EMAPA",
      "description": "Mouse Developmental Anatomy Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Mouse anatomy covering embryonic development and postnatal stages.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://www.informatics.jax.org/expression.shtml",
      "formal_specification": "https://github.com/obophenotype/mouse-anatomy-ontology"
    },
    {
      "id": "B2AI_STANDARD:497",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "MPATH",
      "description": "Mouse pathology ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A structured controlled vocabulary of mutant and transgenic mouse pathology phenotypes.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://www.pathbase.net/",
      "formal_specification": "https://github.com/PaulNSchofield/mpath"
    },
    {
      "id": "B2AI_STANDARD:498",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "RXNO",
      "description": "Name Reaction Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:3"
      ],
      "purpose_detail": "Connects organic name reactions to their roles in an organic synthesis and to processes in MOP",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/rsc-ontologies/rxno",
      "formal_specification": "https://github.com/rsc-ontologies/rxno"
    },
    {
      "id": "B2AI_STANDARD:499",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "NDC",
      "description": "National Drug Code",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:8"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:31"
      ],
      "purpose_detail": "Information about finished drug products, unfinished drugs and compounded drug products",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.accessdata.fda.gov/scripts/cder/ndc/index.cfm"
    },
    {
      "id": "B2AI_STANDARD:500",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "NCBITAXON",
      "description": "NCBI organismal classification",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:224",
          "category": "B2AI:Application",
          "name": "Neural Species Normalization with NCBI Taxonomy as Target Ontology",
          "description": "A bi-encoder species normalization pipeline uses NCBI Taxonomy as the canonical target ontology for linking species mentions in biomedical text to standardized taxon identifiers. The system constructs a comprehensive dictionary from NCBI Taxonomy names (scientific names, synonyms, common names) and employs BM25 lexical retrieval to generate top-10 candidate NCBI Taxonomy IDs for each detected species mention, addressing the challenge of matching ambiguous or variant textual mentions to over 2 million unique species and 16+ million species names in the NCBI database. A BERT-based neural re-ranker (using bert-base-uncased and BioBERT variants) scores and ranks the candidate taxonomy IDs, with the highest-scoring candidate selected as the normalized entity link. Evaluation on standard species NER corpora (LINNAEUS and S800) demonstrates that BM25+BioBERT achieves superior accuracy compared to BM25 alone and baseline tools (OrganismTagger, ORGANISMS), with the re-ranker correcting semantic and lexical mismatches (e.g., disambiguating \"children\" to Homo sapiens taxid:9606 rather than bacterial genus Childrena taxid:525814 based on context). Performance is constrained by candidate generation coverage, as only mentions with at least one correct candidate in the BM25 top-10 can be correctly linked, highlighting NCBI Taxonomy's role as both the source corpus for candidate generation and the structured label space for supervised training and evaluation of neural normalization models.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.48550/arxiv.2310.14366"
          ]
        },
        {
          "id": "B2AI_APP:225",
          "category": "B2AI:Application",
          "name": "Species-to-Gene Assignment via Sequence Labeling with NCBI Taxon IDs",
          "description": "A hybrid dictionary and machine learning system assigns species information to gene mentions in biomedical literature by treating NCBI Taxonomy IDs as explicit structured labels for sequence-labeling models. The pipeline first performs species named entity recognition using a dictionary-based tagger that maps textual mentions to NCBI Taxonomy IDs (achieving 94.3% F-measure), producing concept identifiers that serve as training and evaluation targets. The species-to-gene assignment task is then framed as sequence labeling rather than pairwise binary classification, reducing computational complexity (avoiding quadratic candidate pairs) while improving accuracy from 65.8% to 81.3% by applying biomedical pre-trained language models (PubMedBERT, Bioformer) that predict which NCBI taxon ID should be assigned to each gene mention based on surrounding context. The approach addresses the scale challenge of NCBI Taxonomy (more than 2 million unique species with over 16 million species names) by combining high-coverage dictionary-based recognition with contextual ML models that resolve ambiguity when multiple species are mentioned in the same sentence or paragraph. The system integrates with gene normalization pipelines (GNormPlus) by embedding species-assignment rules (SR4GN), using NCBI taxids to link genes to their organism of origin and enable organism-specific database queries, demonstrating NCBI Taxonomy as a critical semantic layer connecting biomedical entities across literature mining workflows.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.48550/arxiv.2205.03853"
          ]
        },
        {
          "id": "B2AI_APP:226",
          "category": "B2AI:Application",
          "name": "Dictionary-Based Taxonomic NER with NCBI Taxonomy Gazetteers",
          "description": "Taxonomic named entity recognition (NER) systems for ecological and evolutionary literature commonly employ dictionary-based approaches built from NCBI Taxonomy and other taxonomic databases to detect and normalize organism mentions to canonical identifiers. NCBI Taxonomy serves as a primary source for constructing gazetteers that map textual mentions (scientific binomials, vernacular names, synonyms) to standardized taxon IDs and hierarchical classifications, enabling entity normalization that links recognized mentions to formal nomenclature, phylogenetic context, and cross-referenced molecular data. Dictionary-based systems leveraging NCBI resources (such as those described by Gerner et al. 2010 and Pafilis et al. 2013) provide high precision and straightforward mapping to canonical names, IDs, and taxonomic ranks, but face recall limitations when encountering newly described taxa not yet in NCBI Taxonomy, vernacular or regional common names absent from NCBI synonym lists, and historical nomenclature predating current taxonomic conventions. These coverage gaps motivate hybrid approaches augmenting dictionary matching with machine learning classifiers (such as TaxoNERD's deep neural models) that can generalize beyond exact dictionary entries while still grounding predictions in NCBI Taxonomy structure. The integration of NCBI Taxonomy dictionaries with ML-based sequence labeling or contextual embedding models enables robust taxonomic entity extraction across diverse text sources, balancing the precision and standardization of curated taxonomic resources with the flexibility and recall of learned pattern recognition.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1111/2041-210x.13778"
          ]
        },
        {
          "id": "B2AI_APP:227",
          "category": "B2AI:Application",
          "name": "Deep Neural Networks for 16S rRNA Taxonomic Classification",
          "description": "A deep learning approach to pattern recognition for short DNA sequences applies convolutional and recurrent neural networks to predict species and genus labels directly from 16S ribosomal RNA reads, using taxonomic assignments derived from NCBI-aligned reference databases as supervised training labels. The models are trained on 16S sequences spanning over 13,000 distinct species, with each read labeled at species and genus ranks according to reference database taxonomy that follows NCBI Taxonomy conventions for organismal classification. The deep neural network achieves near-perfect read-level species classification accuracy on held-in species and produces more accurate genus-level assignments for reads from held-out species compared to traditional k-mer, alignment-based, and taxonomic binning baselines (including tools like Kraken). Taxonomic labels serve multiple roles in this workflow as per-read supervision for end-to-end training, as hierarchical evaluation targets enabling separate assessment at species versus genus levels, as ground-truth derived from curated sequence-taxonomy mappings in reference databases, and as the basis for robustness testing via held-out species evaluation that mimics real-world scenarios where query organisms are not represented in training data. The work demonstrates that supervised deep learning on taxonomically labeled amplicon sequences can match or exceed the performance of established reference-based taxonomic assignment methods, with NCBI-style taxonomy providing the structured label space and hierarchical evaluation framework essential for training and validating neural taxonomic classifiers.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1101/353474"
          ]
        },
        {
          "id": "B2AI_APP:228",
          "category": "B2AI:Application",
          "name": "MT-MAG Machine Learning for Hierarchical Genome Taxonomic Assignment",
          "description": "MT-MAG (Machine learning-based Taxonomic assignment of Metagenome-Assembled Genomes) employs alignment-free k-mer frequency features (k=7) and machine learning classifiers to provide complete or partial hierarchical taxonomic classifications with rank-specific confidence scores for MAGs and isolate genomes. The tool leverages NCBI Taxonomy as a primary source of hierarchical labels attached to public reference genomes, using these taxonomic paths (from species through genus, family, order, class, phylum to domain) as ground-truth for training multi-rank classifiers and as evaluation targets for assessing classification accuracy at each taxonomic level. MT-MAG is positioned within the broader landscape of genome taxonomic assignment tools that rely on NCBI Taxonomy or alternative systems like GTDB (Genome Taxonomy Database), with the paper noting that many public genomes carry \"attached National Center for Biotechnology Information (NCBI) taxonomy\" and contrasting alignment-based tools (GTDB-Tk) with marker-based (IDTAXA) and ML-based approaches (Kraken2, BERTax). NCBI taxonomic ranks structure the model's output space, enabling interpretable partial classifications that report \"confident down to family level\" when species or genus assignment is uncertain, and confidence scores at all ranks guide users in determining classification reliability. The explicit use of hierarchical NCBI taxonomy as training labels and evaluation criteria demonstrates how standardized taxonomic resources enable supervised learning for genome classification tasks, providing the semantic framework for multi-level predictions and facilitating comparison of ML-based assignments against curated reference taxonomies.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1371/journal.pone.0283536"
          ]
        },
        {
          "id": "B2AI_APP:229",
          "category": "B2AI:Application",
          "name": "Deep Learning Read Classifiers with NCBI Taxonomy Label Backbone",
          "description": "Deep learning-based taxonomic assignment classifiers for metagenomic reads and contigs (such as DL-TODA and related architectures) are trained to classify sequencing data against reference databases organized by NCBI Taxonomy, using taxon identifiers and hierarchical relationships as the structured label space for end-to-end neural network models. These classifiers learn to map DNA sequence k-mer patterns, nucleotide composition features, and sequence embeddings to NCBI taxonomic categories at multiple ranks (species, genus, family), with NCBI Taxonomy providing the canonical reference framework that defines output classes, training targets, and evaluation metrics. Models report high accuracy at species and genus levels on benchmark datasets where ground-truth labels are derived from known NCBI taxonomic assignments of reference genomes, and taxonomic hierarchy embedded in NCBI classifications enables error analysis distinguishing between closely related taxa (e.g., misclassifications within the same genus or family) versus distant taxa. The use of NCBI Taxonomy as the label backbone ensures compatibility with standard bioinformatics pipelines (QIIME2, mothur, Kraken2) that report community composition as NCBI taxon ID frequency tables, facilitates integration with genomic databases indexed by taxids, and enables direct comparison of deep learning classifier predictions against established alignment-based and k-mer-based taxonomic profiling methods that also use NCBI Taxonomy as the reference classification system, demonstrating how standardized taxonomic resources serve as essential infrastructure for supervised learning in metagenomics.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1101/2023.01.27.525929"
          ]
        },
        {
          "id": "B2AI_APP:230",
          "category": "B2AI:Application",
          "name": "Phylogenetic Hierarchical Softmax for Scalable Taxonomic Classification",
          "description": "Phylo-HS (Phylogenetic Hierarchical Softmax) addresses the computational bottleneck of large-output softmax layers in neural network taxonomic classifiers by structuring the output layer according to the phylogenetic and taxonomic tree derived from NCBI Taxonomy, decomposing single-step classification over thousands of taxa into a series of hierarchical binary decisions that follow the taxonomic hierarchy. Traditional hierarchical softmax methods group classes by frequency or learned similarity, but Phylo-HS explicitly uses the NCBI taxonomic tree (reflecting evolutionary relationships and Linnaean ranks) to define hierarchical clusters, predicting first at higher taxonomic ranks (e.g., phylum, class) before refining predictions to lower ranks (genus, species). On metagenomic read classification tasks with approximately 5,000 taxonomic classes, Phylo-HS achieves roughly 10-fold training speedup and improved accuracy compared to frequency-based hierarchical softmax baselines, demonstrating that taxonomy-aware output structure enhances both computational efficiency and predictive performance. The approach leverages NCBI Taxonomy's hierarchical organization (superkingdom, phylum, class, order, family, genus, species) as an inductive bias that aligns with the true structure of biological diversity, enabling gradient flow and parameter sharing across related taxa during training and providing interpretable intermediate predictions at multiple taxonomic ranks. This application exemplifies how NCBI Taxonomy serves not only as a label space but as an architectural prior that can be embedded directly into neural network training objectives and loss functions to improve scalability and accuracy in large-class taxonomic classification problems.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1101/2025.01.27.634943"
          ]
        },
        {
          "id": "B2AI_APP:231",
          "category": "B2AI:Application",
          "name": "Taxonomy-Informed Embeddings and Multiple Instance Learning for Microbiomes",
          "description": "Metagenomic deep learning workflows leverage NCBI Taxonomy to structure sample-level representations through taxonomy-aligned embeddings and multiple instance learning (MIL) frameworks that treat microbiome samples as bags of taxonomic units. Methods such as GMEmbeddings align amplicon sequence variants (ASVs) or operational taxonomic units (OTUs) to NCBI taxonomic references and construct pretrained embedding matrices (using techniques like GloVe) where each row corresponds to an NCBI taxon and learned vectors capture co-occurrence patterns and ecological relationships across samples, enabling transfer learning and dimensionality reduction for downstream classification tasks. Metagenome2Vec bins metagenomic reads to species-level taxonomic assignments using NCBI Taxonomy (via tools like fastDNA) and represents each sample as a bag of taxon embeddings, applying multiple instance learning models (DeepSets, Set Transformer, MIL-VAE) that aggregate taxon-level features into sample-level predictions for phenotype classification (e.g., disease status, environmental conditions). IDMIL and related approaches extend this paradigm by treating taxonomic composition vectors (where each dimension corresponds to an NCBI taxid and values represent read counts or relative abundances) as input to set-based neural architectures that are permutation-invariant with respect to taxon ordering. Taxonomy-aware learning methods inject NCBI hierarchical structure into neural network architectures by incorporating taxonomy trees as graph neural network scaffolds, hierarchical attention mechanisms that weight closely related taxa similarly, and taxonomy-guided regularization losses that penalize misclassifications of phylogenetically distant taxa more heavily than closely related taxa. These approaches demonstrate how NCBI Taxonomy serves as a semantic framework for constructing biologically meaningful feature representations, enabling models to leverage evolutionary relationships and taxonomic structure to improve generalization, interpretability, and data efficiency in machine learning applied to microbiome and metagenomic datasets.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1099/mgen.0.001231"
          ]
        }
      ],
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:74"
      ],
      "purpose_detail": "The NCBI Taxonomy Database (NCBITAXON) provides a comprehensive hierarchical classification of over 2 million organisms spanning all domains of life (Bacteria, Archaea, Eukarya, Viruses), maintained by the National Center for Biotechnology Information as the authoritative taxonomic backbone for GenBank, RefSeq, and all NCBI molecular sequence databases, with each taxon assigned a unique stable numerical identifier (NCBI Taxonomy ID) enabling consistent cross-referencing across genomic, proteomic, and literature resources worldwide. Originally developed in the 1990s to organize nucleotide sequence submissions, NCBI Taxonomy has evolved into the de facto standard for organismal classification in bioinformatics, with taxonomic IDs embedded in FASTA headers (e.g., 'gi|123456|ref|NP_001234.1| protein [Homo sapiens]' implicitly links to taxid:9606), sequence records (organism field), GenBank features (db_xref='taxon:9606'), UniProt entries (OX line with NCBI taxid), and publication metadata in PubMed Central, creating a universal identifier system that links molecular data to organismal context across millions of database records. The taxonomy encompasses named ranks following Linnaean hierarchy (superkingdom, kingdom, phylum, class, order, family, genus, species) plus unranked intermediate nodes ('clade', 'no rank') to capture phylogenetic relationships not fitting classical ranks, with each taxon record including scientific name (binomial nomenclature for species following ICZN/ICBN rules), authority (author and year), common names in multiple languages, synonyms capturing nomenclatural changes, genetic code specifications (nuclear and mitochondrial translation tables essential for correct ORF prediction), lineage strings showing complete paths from root to taxon, and cross-references to external databases (ITIS, GBIF, WoRMS, Index Fungorum). Updates occur continuously as new species are described in literature, taxonomic revisions are published (e.g., bacterial taxonomy shifts from Cavalier-Smith to GTDB-informed classifications), and sequences are deposited for previously unsequenced organisms, with monthly public releases ensuring synchronization across the bioinformatics ecosystem. The OBO Foundry NCBITAXON ontology represents this database in OWL/OBO formats with taxon IDs as URIs (NCBITaxon:9606 for Homo sapiens), taxonomic ranks as relationship types (rdfs:subClassOf chains encoding 'is_a' relationships from species through genus/family/order up to superkingdom), and semantic annotations enabling reasoning, SPARQL queries, and integration with other OBO ontologies (Gene Ontology, Uberon anatomy, Environment Ontology). NCBI Taxonomy's scale and coverage present unique challenges: the database includes organisms with minimal molecular data (single barcode sequences), extinct species when historical sequences exist, environmental samples and metagenome-assembled genomes (MAGs) with placeholder taxonomic assignments (e.g., 'uncultured bacterium', 'environmental samples' subtree with taxid:12908), hybrid organisms, and viral/phage taxonomy reflecting Baltimore classification alongside host relationships, requiring disambiguation strategies in downstream applications. Critical applications span genomics databases (Ensembl, UCSC Genome Browser, NCBI Genome using taxids to organize genome assemblies and enable taxon-specific BLAST), biodiversity informatics platforms (GBIF, iNaturalist, Encyclopedia of Life, Catalogue of Life using NCBI Taxonomy as reconciliation backbone or primary classification source), metagenomics and microbiome studies (QIIME2, mothur, Kraken2, MetaPhlAn assigning amplicon sequence variants and shotgun reads to NCBI taxa and reporting community composition as taxid frequency tables), phylogenetic databases (TimeTree, Open Tree of Life, TreeBASE mapping phylogenetic trees to NCBI taxa for comparative analyses), and literature mining systems (Europe PMC, PubTator Central extracting and normalizing species mentions to NCBI taxids enabling queries like 'find all papers mentioning Mycobacterium tuberculosis taxid:1773 and gene BCG_0001'). The taxonomy enables comparative genomics workflows identifying orthologous gene families across taxa, synteny analyses within defined clades, horizontal gene transfer detection requiring taxonomic context to identify donor-recipient relationships, and evolutionary rate calculations dependent on accurate phylogenetic distances encoded in taxonomic hierarchy. Machine learning and artificial intelligence applications leverage NCBI Taxonomy's identifiers, names, and hierarchical structure across natural language processing for species entity recognition and normalization (biomedical NER systems using NCBI taxids as canonical concept identifiers with neural re-ranking models selecting correct taxids from candidate lists generated via NCBI name dictionaries, improving linking accuracy on standard corpora like LINNAEUS and S800), species-to-gene assignment in literature mining (sequence-labeling models using NCBI taxids as structured labels to assign correct species to each gene mention, raising assignment accuracy from 65.8% to 81.3% compared to rule-based baselines, and addressing challenges of 16+ million species names in NCBI Taxonomy scale), metagenomics and microbiome analysis with deep learning classifiers for taxonomic profiling (convolutional and recurrent neural networks trained on 16S rRNA reads using species and genus labels derived from NCBI-aligned reference databases, achieving near-perfect read-level species classification and robust genus assignments for held-out species compared to k-mer alignment and binning baselines, trained on over 13,000 species with taxonomic labels as core supervision), machine learning classification of metagenome-assembled genomes (alignment-free k-mer classifiers outputting complete or partial taxonomic paths with rank-specific confidences, using NCBI taxonomy attached to public genomes as hierarchical labels for training and evaluation, enabling interpretable partial classifications when full species assignment is uncertain), deep learning read and contig classifiers (end-to-end neural networks trained to assign sequencing reads and assembled contigs to NCBI taxonomic references, reporting high species and genus performance with taxonomy providing label backbone and evaluation framework), taxonomy-aware neural network architectures and training objectives (hierarchical softmax models structured by phylogenetic and taxonomic trees using NCBI hierarchy to define output layer clusters, yielding order-of-magnitude training speedups and accuracy improvements on large-class datasets with approximately 5,000 taxonomic classes, contrasting with frequency-based hierarchical softmax approaches), taxonomic entity linking and normalization in ecological and evolutionary literature (dictionary-based NER systems built from NCBI Taxonomy databases enabling entity normalization to canonical names and identifiers, providing high precision but limited recall for vernacular names and newly described taxa, motivating hybrid machine learning augmentation), and taxonomy-informed embeddings and multiple instance learning for microbiome analysis (binning reads by species using NCBI taxonomy to create taxon-level embeddings via methods like GloVe, treating metagenome samples as bags of taxon embeddings for multiple instance learning classifiers including DeepSets and Set Transformers, using amplicon sequence variants aligned to NCBI taxa as embedding matrix rows, and injecting taxonomy trees into neural architectures and loss functions for hierarchical classification with evolutionary context).",
      "is_open": true,
      "requires_registration": false,
      "url": "http://www.ncbi.nlm.nih.gov/taxonomy",
      "formal_specification": "https://github.com/obophenotype/ncbitaxon"
    },
    {
      "id": "B2AI_STANDARD:501",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "NCIT",
      "description": "NCI Thesaurus OBO Edition",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:74"
      ],
      "purpose_detail": "A reference terminology that includes broad coverage of the cancer domain, including cancer related diseases.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/NCI-Thesaurus/thesaurus-obo-edition",
      "formal_specification": "https://github.com/NCI-Thesaurus/thesaurus-obo-edition"
    },
    {
      "id": "B2AI_STANDARD:502",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "NBO",
      "description": "Neuro Behavior Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Human and animal behaviours and behavioural phenotypes",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/obo-behavior/behavior-ontology/",
      "formal_specification": "https://github.com/obo-behavior/behavior-ontology/"
    },
    {
      "id": "B2AI_STANDARD:503",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "NeuroNames",
      "description": "Neuronames Brain Hierarchy",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:22"
      ],
      "purpose_detail": "A Comprehensive Hierarchical Nomenclature for Structures of the Primate Brain (human and macaque)",
      "is_open": true,
      "requires_registration": false,
      "url": "http://braininfo.rprc.washington.edu/aboutBrainInfo.aspx#NeuroNames",
      "publication": "doi:10.1007/s12021-011-9128-8"
    },
    {
      "id": "B2AI_STANDARD:504",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "NOMEN",
      "description": "Nomenclatural ontology for biological names",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A nomenclatural ontology for biological names (not concepts). It encodes the goverened rules of nomenclature.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/SpeciesFileGroup/nomen",
      "formal_specification": "https://github.com/SpeciesFileGroup/nomen"
    },
    {
      "id": "B2AI_STANDARD:505",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "NCRO",
      "description": "Non-Coding RNA Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:33"
      ],
      "purpose_detail": "An ontology for non-coding RNA, both of biological origin, and engineered.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/OmniSearch/ncro",
      "formal_specification": "https://github.com/OmniSearch/ncro"
    },
    {
      "id": "B2AI_STANDARD:506",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "NCPT",
      "description": "Nutrition Care Process Terminology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Terms for nutrition assessment, diagnosis, intervention, and monitoring/evaluation.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://www.ncpro.org/"
    },
    {
      "id": "B2AI_STANDARD:507",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "OMO",
      "description": "OBO Metadata Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Terms that are used to annotate ontology terms for all OBO ontologies.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/information-artifact-ontology/ontology-metadata",
      "formal_specification": "https://github.com/information-artifact-ontology/ontology-metadata"
    },
    {
      "id": "B2AI_STANDARD:508",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "ONTONEO",
      "description": "Obstetric and Neonatal Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A structured controlled vocabulary to provide a representation of the data from electronic health records involved in the care of pregnancy.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/ontoneo-project/Ontoneo",
      "formal_specification": "https://github.com/ontoneo-project/Ontoneo"
    },
    {
      "id": "B2AI_STANDARD:509",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "ONTOAVIDA",
      "description": "OntoAvida ontology for Avida digital evolution platform",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Vocabulary for the description of the most widely-used computational approach for studying digital evolution.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://gitlab.com/fortunalab/ontoavida",
      "formal_specification": "https://gitlab.com/fortunalab/ontoavida"
    },
    {
      "id": "B2AI_STANDARD:510",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "OBIB",
      "description": "Ontology for Biobanking",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Annotation and modeling of biobank repository and biobanking administration.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/biobanking/biobanking",
      "formal_specification": "https://github.com/biobanking/biobanking"
    },
    {
      "id": "B2AI_STANDARD:511",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "OBI",
      "description": "Ontology for Biomedical Investigations",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Description of life-science and clinical investigations.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://obi-ontology.org",
      "publication": "doi:10.1371/journal.pone.0154556",
      "formal_specification": "https://github.com/obi-ontology/obi"
    },
    {
      "id": "B2AI_STANDARD:512",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "OGMS",
      "description": "Ontology for General Medical Science",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:4",
        "B2AI_TOPIC:7"
      ],
      "purpose_detail": "Treatment of disease and diagnosis and on carcinomas and other pathological entities.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/OGMS/ogms",
      "formal_specification": "https://github.com/OGMS/ogms"
    },
    {
      "id": "B2AI_STANDARD:513",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "OMIT",
      "description": "Ontology for MIRNA Target",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Data exchange standards and common data elements in the microRNA (miR) domain.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/OmniSearch/omit",
      "formal_specification": "https://github.com/OmniSearch/omit"
    },
    {
      "id": "B2AI_STANDARD:514",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "ONE",
      "description": "Ontology for Nutritional Epidemiology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Research output of nutritional epidemiologic studies.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/cyang0128/Nutritional-epidemiologic-ontologies",
      "publication": "doi:10.3390/nu11061300",
      "formal_specification": "https://github.com/cyang0128/Nutritional-epidemiologic-ontologies"
    },
    {
      "id": "B2AI_STANDARD:515",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "ONS",
      "description": "Ontology for Nutritional Studies",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Description of concepts in the nutritional studies domain.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/enpadasi/Ontology-for-Nutritional-Studies",
      "publication": "doi:10.1186/s12263-018-0601-y",
      "formal_specification": "https://github.com/enpadasi/Ontology-for-Nutritional-Studies"
    },
    {
      "id": "B2AI_STANDARD:516",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "OAE",
      "description": "Ontology of Adverse Events",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:4",
        "B2AI_TOPIC:7"
      ],
      "purpose_detail": "Ontology of Adverse Events (OAE) is a community-developed biomedical ontology following OBO Foundry principles that provides standardized representation and classification of adverse events resulting from medical interventions including drug administration, vaccination, medical devices, procedures, and dietary supplements, enabling systematic analysis of safety data across clinical trials, pharmacovigilance systems, and electronic health records. OAE structures adverse events hierarchically under upper-level classes from the Basic Formal Ontology (BFO), integrating with domain ontologies including OGMS (disease processes), VO (vaccine components), DRON (drug products), and UBERON (anatomical structures) to capture mechanistic relationships between interventions, biological processes, and observed adverse outcomes. The ontology distinguishes between adverse events (any untoward medical occurrence temporally associated with intervention use) and adverse drug reactions (events with causal relationship to intervention), incorporating causality assessment frameworks (Naranjo scale, WHO-UMC criteria) as logical axioms that infer ADR status based on evidence patterns. OAE represents clinical manifestations (symptoms, signs, laboratory abnormalities), severity grades (CTCAE scales from mild to life-threatening), temporal patterns (immediate hypersensitivity, delayed reactions, cumulative toxicity), and anatomical localizations, supporting detailed phenotyping of safety profiles. The ontology enables cross-study aggregation of adverse event data by providing standardized terms that harmonize heterogeneous reporting formats from FDA FAERS, EMA EudraVigilance, WHO VigiBase, and clinical trial databases, facilitating meta-analyses of intervention safety and identification of rare adverse events invisible in individual studies. OAE supports pharmacovigilance signal detection by structuring adverse event hierarchies that enable mining of parent-child term relationships, discovering drug-event associations through disproportionality analysis (ROR, IC025), and prioritizing signals for regulatory review. In vaccine safety surveillance, OAE terms annotate Brighton Collaboration case definitions for standardized adverse event reporting post-vaccination (AEFI), supporting global safety monitoring networks coordinated by WHO. For AI/ML applications, OAE provides structured labels for training natural language processing models to extract adverse event mentions from clinical notes, social media posts, and regulatory documents, enables knowledge graph construction linking drugs, vaccines, adverse events, and patient characteristics for predictive safety modeling, and supports explainable AI systems that generate human-interpretable safety signals by reasoning over ontology-encoded mechanistic pathways connecting interventions to adverse outcomes, ultimately enhancing patient safety through earlier detection and characterization of intervention-related harms.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/OAE-ontology/OAE/",
      "formal_specification": "https://github.com/OAE-ontology/OAE/"
    },
    {
      "id": "B2AI_STANDARD:517",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "OBCS",
      "description": "Ontology of Biological and Clinical Statistics",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Biological and clinical statistics.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/obcs/obcs",
      "formal_specification": "https://github.com/obcs/obcs"
    },
    {
      "id": "B2AI_STANDARD:518",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "OBA",
      "description": "Ontology of Biological Attributes",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A collection of biological attributes (traits) covering all kingdoms of life.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://wiki.geneontology.org/index.php/Extensions/x-attribute",
      "formal_specification": "https://github.com/obophenotype/bio-attribute-ontology"
    },
    {
      "id": "B2AI_STANDARD:519",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "OGSF",
      "description": "Ontology of Genetic Susceptibility Factor",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "An application ontology to represent genetic susceptibility to a specific disease, adverse event, or a pathological process.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/linikujp/OGSF",
      "formal_specification": "https://github.com/linikujp/OGSF"
    },
    {
      "id": "B2AI_STANDARD:520",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "OHPI",
      "description": "Ontology of Host Pathogen Interactions",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Host-pathogen interactions and virulence factors.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/OHPI/ohpi",
      "publication": "doi:10.1093/nar/gky999",
      "formal_specification": "https://github.com/OHPI/ohpi"
    },
    {
      "id": "B2AI_STANDARD:521",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "OHMI",
      "description": "Ontology of Host-Microbiome Interactions",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Entities and relations related to microbiomes, microbiome host organisms (e.g., human and mouse), and the interactions between the hosts and microbiomes at different conditions.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/ohmi-ontology/ohmi",
      "formal_specification": "https://github.com/ohmi-ontology/ohmi"
    },
    {
      "id": "B2AI_STANDARD:522",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "OMRSE",
      "description": "Ontology of Medically Related Social Entities",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "This ontology covers the domain of social entities that are related to health care, such as demographic information and the roles of various...",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/ufbmi/OMRSE/wiki/OMRSE-Overview",
      "publication": "doi:10.1186/s13326-016-0087-8",
      "formal_specification": "https://github.com/ufbmi/OMRSE"
    },
    {
      "id": "B2AI_STANDARD:523",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "OOSTT",
      "description": "Ontology of Organizational Structures of Trauma centers and Trauma systems",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:96"
      ],
      "purpose_detail": "Organizational components of trauma centers and trauma systems.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/OOSTT/OOSTT",
      "formal_specification": "https://github.com/OOSTT/OOSTT"
    },
    {
      "id": "B2AI_STANDARD:524",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "OPMI",
      "description": "Ontology of Precision Medicine and Investigation",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Entities and relations associated with precision medicine and related investigations at different conditions.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/OPMI/opmi",
      "formal_specification": "https://github.com/OPMI/opmi"
    },
    {
      "id": "B2AI_STANDARD:525",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "ORNASEQ",
      "description": "Ontology of RNA Sequencing",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:33"
      ],
      "purpose_detail": "An application ontology designed to annotate next-generation sequencing experiments performed on RNA.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/safisher/ornaseq",
      "formal_specification": "https://github.com/safisher/ornaseq"
    },
    {
      "id": "B2AI_STANDARD:526",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "OVAE",
      "description": "Ontology of Vaccine Adverse Events",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:4",
        "B2AI_TOPIC:7"
      ],
      "purpose_detail": "A biomedical ontology in the domain of vaccine adverse events.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/OVAE-Ontology/ovae",
      "formal_specification": "https://github.com/OVAE-Ontology/ovae"
    },
    {
      "id": "B2AI_STANDARD:527",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "OHD",
      "description": "Oral Health and Disease Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:7"
      ],
      "purpose_detail": "Content of dental practice health records.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/oral-health-and-disease-ontologies/ohd-ontology",
      "publication": "doi:10.1186/s13326-020-00222-0",
      "formal_specification": "https://github.com/oral-health-and-disease-ontologies/ohd-ontology"
    },
    {
      "id": "B2AI_STANDARD:528",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "ORDO",
      "description": "Orphanet Rare Disease Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:7"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:80"
      ],
      "purpose_detail": "The Orphanet Rare Disease ontology (ORDO) is jointly developed by Orphanet and the EBI to provide a structured vocabulary for rare diseases capturing relationships between diseases, genes and other relevant features which will form a useful resource for the computational analysis of rare diseases.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://bioportal.bioontology.org/ontologies/ORDO",
      "formal_specification": "https://www.orphadata.com/ontologies/"
    },
    {
      "id": "B2AI_STANDARD:529",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "PHIPO",
      "description": "Pathogen Host Interaction Phenotype Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Species-neutral phenotypes observed in pathogen-host interactions.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/PHI-base/phipo",
      "publication": "doi:10.1093/nar/gkab1037",
      "formal_specification": "https://github.com/PHI-base/phipo"
    },
    {
      "id": "B2AI_STANDARD:530",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "TRANS",
      "description": "Pathogen Transmission Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:7"
      ],
      "purpose_detail": "An ontology representing the disease transmission process during which the pathogen is transmitted directly or indirectly.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/DiseaseOntology/PathogenTransmissionOntology",
      "publication": "doi:10.1093/nar/gkp832",
      "formal_specification": "https://github.com/DiseaseOntology/PathogenTransmissionOntology"
    },
    {
      "id": "B2AI_STANDARD:531",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "PW",
      "description": "Pathway ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:21"
      ],
      "purpose_detail": "A controlled vocabulary for annotating gene products to pathways.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://rgd.mcw.edu/rgdweb/ontology/search.html",
      "publication": "doi:10.1186/2041-1480-5-7",
      "formal_specification": "https://github.com/rat-genome-database/PW-Pathway-Ontology"
    },
    {
      "id": "B2AI_STANDARD:532",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "PSDO",
      "description": "Performance Summary Display Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "purpose_detail": "Ontology to reproducibly study visualizations of clinical performance",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/Display-Lab/psdo",
      "formal_specification": "https://github.com/Display-Lab/psdo"
    },
    {
      "id": "B2AI_STANDARD:533",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "PHENIO",
      "description": "Phenomics Integrated Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:3",
        "B2AI_TOPIC:7",
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:21",
        "B2AI_TOPIC:25"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:58"
      ],
      "purpose_detail": "An application ontology for accessing and comparing knowledge concerning phenotypes across species and genetic backgrounds.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/monarch-initiative/phenio",
      "formal_specification": "https://github.com/monarch-initiative/phenio"
    },
    {
      "id": "B2AI_STANDARD:534",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "PATO",
      "description": "Phenotype And Trait Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:25"
      ],
      "purpose_detail": "Phenotypic qualities (properties, attributes or characteristics).",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/pato-ontology/pato",
      "formal_specification": "https://github.com/pato-ontology/pato"
    },
    {
      "id": "B2AI_STANDARD:535",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "PCO",
      "description": "Population and Community Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Groups of interacting organisms such as populations and communities.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/PopulationAndCommunityOntology/pco",
      "formal_specification": "https://github.com/PopulationAndCommunityOntology/pco"
    },
    {
      "id": "B2AI_STANDARD:536",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "PROCO",
      "description": "Process Chemistry Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:3"
      ],
      "purpose_detail": "Process chemistry, the chemical field concerned with scaling up laboratory syntheses to commercially viable processes.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/proco-ontology/PROCO",
      "formal_specification": "https://github.com/proco-ontology/PROCO"
    },
    {
      "id": "B2AI_STANDARD:537",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "PSI-MOD",
      "description": "Protein modification",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:26"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:41"
      ],
      "purpose_detail": "PSI-MOD is an ontology consisting of terms that describe protein chemical modifications",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.psidev.info/groups/protein-modifications",
      "publication": "doi:10.1038/nbt0808-864",
      "formal_specification": "https://github.com/HUPO-PSI/psi-mod-CV"
    },
    {
      "id": "B2AI_STANDARD:538",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "PR",
      "description": "PRotein Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:26"
      ],
      "purpose_detail": "The Protein Ontology (PR) is a comprehensive formal ontology that provides standardized terminology and semantic relationships for describing protein-related entities across the complete spectrum of protein science. Developed by the PRotein Ontology consortium, PR serves as a unifying framework that integrates protein sequence, structure, and functional information into a coherent knowledge representation system. The ontology encompasses multiple levels of protein organization including protein families and complexes, individual protein molecules, protein domains and regions, post-translational modifications, and protein isoforms generated through alternative splicing or processing. PR maintains extensive cross-references to major protein databases including UniProt, NCBI, and Ensembl, enabling seamless integration with existing protein annotation resources. The ontology supports advanced protein function annotation by providing precise vocabulary for describing enzymatic activities, binding sites, regulatory mechanisms, and cellular localization patterns. PR is essential for proteomics data standardization, comparative protein analysis, functional genomics research, and systems biology applications where consistent protein terminology facilitates data integration, automated reasoning, and knowledge discovery across diverse experimental platforms.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/PROconsortium/PRoteinOntology/",
      "formal_specification": "https://github.com/PROconsortium/PRoteinOntology/"
    },
    {
      "id": "B2AI_STANDARD:539",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "PCL",
      "description": "Provisional Cell Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:2"
      ],
      "purpose_detail": "Cell types that are provisionally defined by experimental techniques such as single cell or single nucleus transcriptomics.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/obophenotype/provisional_cell_ontology",
      "formal_specification": "https://github.com/obophenotype/provisional_cell_ontology"
    },
    {
      "id": "B2AI_STANDARD:540",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "RBO",
      "description": "Radiation Biology Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:1",
        "B2AI_TOPIC:11"
      ],
      "purpose_detail": "Effects of radiation on biota in terrestrial and space environments.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/Radiobiology-Informatics-Consortium/RBO",
      "formal_specification": "https://github.com/Radiobiology-Informatics-Consortium/RBO"
    },
    {
      "id": "B2AI_STANDARD:541",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "RadLex",
      "description": "RadLex radiology lexicon",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:85"
      ],
      "purpose_detail": "A comprehensive set of radiology terms for use in radiology reporting, decision support, data mining, data registries, education and research.",
      "is_open": true,
      "requires_registration": true,
      "url": "https://www.rsna.org/practice-tools/data-tools-and-standards/radlex-radiology-lexicon"
    },
    {
      "id": "B2AI_STANDARD:542",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "RS",
      "description": "Rat Strain Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Rat Strain Ontology (RS) is a structured vocabulary maintained by the Rat Genome Database (RGD) at the Medical College of Wisconsin, providing standardized nomenclature and hierarchical classification for laboratory rat strains, wild-derived strains, mutant lines, consomic/congenic strains, and transgenic/knockout rat models used in biomedical research, with integration into the OBO Foundry ecosystem supporting cross-species comparative genomics and phenotype studies. RS catalogs 5,000+ rat strains spanning inbred strains with defined genetic backgrounds (Wistar, Sprague-Dawley, Fischer 344, Brown Norway), outbred stocks with genetic heterogeneity, recombinant inbred lines for QTL mapping, consomic strains where complete chromosomes are substituted between strains, and gene-edited models (CRISPR knockouts, transgenic insertions) targeting specific disease mechanisms. The ontology structures strain relationships through \"derived from\" properties linking parent-progeny strains, \"has genetic background\" properties specifying founding strains for congenic lines, and \"model of\" properties connecting strains to disease phenotypes (hypertension, diabetes, cancer susceptibility), enabling navigation of complex breeding schemes and genetic derivations. RS terms include strain-specific metadata on phenotypic characteristics (coat color, obesity, behavioral traits), genetic markers (microsatellites, SNPs), tissue/cell sources, husbandry requirements, and availability from repositories (Rat Resource and Research Center, Charles River), facilitating experimental planning and reproducibility. The ontology integrates with RGD's comprehensive annotations linking strains to quantitative trait loci (QTL), genes, pathways, diseases, and phenotypes (Mammalian Phenotype Ontology terms), supporting genome-wide association studies (GWAS), eQTL mapping, and systems genetics analyses that connect genetic variation to physiological endpoints. RS enables translational research by mapping rat models to human disease orthologs, facilitating target validation for drug discovery where rat pharmacology and toxicology data inform human clinical trial design, particularly for cardiovascular disease, neurodegeneration, and metabolic disorders where rat models recapitulate human pathophysiology better than mouse models. In AI/ML applications for precision medicine, RS provides structured metadata for training genomic prediction models that associate strain genotypes with phenotypic outcomes, supports knowledge graph construction linking rat genetic data to human disease mechanisms for cross-species inference, and enables meta-analyses aggregating phenotypic data across studies by standardizing strain identifiers, ultimately accelerating translational discoveries by leveraging the rat's role as a premier mammalian model for human disease research.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://rgd.mcw.edu/rgdweb/search/strains.html",
      "publication": "doi:10.1186/2041-1480-4-36",
      "formal_specification": "https://github.com/rat-genome-database/RS-Rat-Strain-Ontology"
    },
    {
      "id": "B2AI_STANDARD:543",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "RO",
      "description": "Relation Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Relationship types shared across multiple ontologies.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://oborel.github.io/",
      "formal_specification": "https://github.com/oborel/obo-relations"
    },
    {
      "id": "B2AI_STANDARD:544",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "RxNorm",
      "description": "RxNorm",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "codesystem",
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:8"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:74"
      ],
      "purpose_detail": "Medication terminology. Provided through UMLS.",
      "is_open": true,
      "requires_registration": true,
      "url": "https://www.nlm.nih.gov/research/umls/rxnorm/index.html"
    },
    {
      "id": "B2AI_STANDARD:545",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "SEPIO",
      "description": "Scientific Evidence and Provenance Information Ontology",
      "related_to": [
        "B2AI_STANDARD:256"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The Scientific Evidence and Provenance Information Ontology (SEPIO) provides a structured framework for representing scientific evidence and provenance information supporting knowledge claims. It supports rich, computable representations of the evidence and provenance behind scientific assertions, particularly for genetic variants and their clinical interpretations.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/monarch-initiative/SEPIO-ontology",
      "publication": "doi:10.5281/zenodo.5214269",
      "formal_specification": "https://sepio-framework.github.io/sepio-linkml/",
      "responsible_organization": [
        "B2AI_ORG:58"
      ]
    },
    {
      "id": "B2AI_STANDARD:546",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "SSN",
      "description": "Semantic Sensor Network Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:99"
      ],
      "purpose_detail": "An ontology for describing sensors and their observations, the involved procedures, the studied features of interest, the samples used to do so, and the observed properties, as well as actuators.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.w3.org/TR/vocab-ssn/",
      "formal_specification": "https://github.com/w3c/sdw"
    },
    {
      "id": "B2AI_STANDARD:547",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "SO",
      "description": "Sequence types and features ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A structured controlled vocabulary for sequence annotation, for the exchange of annotation data and for the description of sequence objects.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://www.sequenceontology.org/",
      "publication": "doi:10.1016/j.jbi.2010.03.002",
      "formal_specification": "https://github.com/The-Sequence-Ontology/SO-Ontologies"
    },
    {
      "id": "B2AI_STANDARD:548",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "SCDO",
      "description": "Sickle Cell Disease Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The Sickle Cell Disease Ontology (SCDO) provides comprehensive standardized terminology for describing all aspects of sickle cell disease (SCD), a group of inherited hemoglobin disorders affecting millions globally, particularly populations of African, Mediterranean, Middle Eastern, and Indian ancestry. Developed by the H3ABioNet consortium and aligned with OBO Foundry principles, SCDO encompasses genetic variants (HbS, HbC, HbE, beta-thalassemia mutations), disease phenotypes (sickle cell anemia HbSS, HbSC disease, HbS-beta thalassemia, sickle cell trait), clinical manifestations (vaso-occlusive crises, acute chest syndrome, stroke, priapism, splenic sequestration, chronic organ damage), laboratory findings (hemoglobin electrophoresis patterns, reticulocyte counts, bilirubin levels, fetal hemoglobin percentages), complications (pulmonary hypertension, renal dysfunction, avascular necrosis, leg ulcers, retinopathy), treatment modalities (hydroxyurea, blood transfusions, hematopoietic stem cell transplantation, gene therapy, pain management), and patient outcomes (quality of life measures, hospitalization rates, mortality). The ontology integrates genetic, clinical, laboratory, and treatment concepts to support comprehensive SCD patient data management and research. SCDO enables standardized phenotyping for genotype-phenotype correlation studies identifying disease modifiers (fetal hemoglobin levels, alpha-thalassemia co-inheritance, genetic polymorphisms affecting disease severity), facilitates clinical trial recruitment through precise inclusion/exclusion criteria specification, and supports electronic health record integration for automated SCD surveillance and quality improvement initiatives. Applications include natural history studies characterizing disease progression patterns, pharmacogenomics research examining hydroxyurea response variability, health disparities research documenting access to disease-modifying therapies, and global SCD registries enabling cross-population comparisons. SCDO facilitates data harmonization across international SCD cohorts, enabling meta-analyses and collaborative research essential for rare disease studies where no single center has sufficient patient numbers.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://scdontology.h3abionet.org/",
      "formal_specification": "https://github.com/scdodev/scdo-ontology"
    },
    {
      "id": "B2AI_STANDARD:549",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "SWO",
      "description": "Software ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Software tools, their types, tasks, versions, provenance and associated data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/allysonlister/swo",
      "publication": "doi:10.1186/2041-1480-5-25",
      "formal_specification": "https://github.com/allysonlister/swo"
    },
    {
      "id": "B2AI_STANDARD:550",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "CFDs",
      "description": "Standard Current Procedural Terminology Consumer Friendly Descriptors",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Translate each code descriptor from the official CPT code set into language that is easily understood by the average patient and/or his or her caregiver. The objective is to simplify the highly technical CPT code descriptors into something more patient-focused and patient-friendly.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://commerce.ama-assn.org/catalog/media/Consumer-and-Clinician-Descriptors-in-CPT-Data-Files.pdf",
      "responsible_organization": [
        "B2AI_ORG:3"
      ]
    },
    {
      "id": "B2AI_STANDARD:551",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "STATO",
      "description": "Statistics Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:47"
      ],
      "purpose_detail": "The Statistics Ontology (STATO) provides comprehensive standardized vocabulary for statistical methods, experimental design concepts, hypothesis testing procedures, and statistical measures used across life sciences, biomedical research, and data-intensive scientific domains, developed by the ISA-tools community to support reproducible research and transparent reporting of statistical analyses. STATO encompasses five major categories: statistical tests (parametric tests including t-tests, ANOVA, ANCOVA, linear regression, mixed models; non-parametric tests including Mann-Whitney U, Kruskal-Wallis, Wilcoxon signed-rank, Spearman correlation; and specialized methods like survival analysis, multivariate analysis, time series analysis), probability distributions (normal, binomial, Poisson, exponential, chi-square, t-distribution, F-distribution) essential for understanding test assumptions, descriptive statistics (measures of central tendency including mean, median, mode; measures of dispersion including standard deviation, variance, interquartile range; and measures of shape including skewness, kurtosis), data types and variables (categorical/nominal, ordinal, continuous, discrete, dependent/independent variables, covariates, confounding variables), and experimental design concepts (randomization, blocking, replication, control groups, factorial designs, crossover designs, longitudinal studies). STATO provides formal OWL definitions enabling automated reasoning about test conditions of application, linking test selection to data characteristics (e.g., use Mann-Whitney U when comparing two independent groups with non-normal distributions), and capturing assumptions (normality, homoscedasticity, independence) that must be verified before test application. Each statistical method includes textual definitions for human understanding, formal logical definitions for machine reasoning, associated R code snippets via 'R-command' annotations enabling direct implementation, and documentation of appropriate use cases and interpretation guidelines. STATO integrates with BFO (Basic Formal Ontology) as upper-level ontology and OBI (Ontology for Biomedical Investigations) for process definitions, ensuring interoperability across biomedical ontologies. Applications include annotation of analysis methods in ISA-Tab metadata files for omics studies, standardized reporting of statistical procedures in publications to meet journal guidelines (CONSORT, STROBE, ARRIVE), automated validation of statistical analysis workflows in computational notebooks, education and training through formal definitions of statistical concepts with R implementation examples, and text mining of scientific literature to extract statistical method usage patterns. STATO supports reproducibility by precisely specifying analysis procedures, capturing multiple testing correction methods (Bonferroni, Benjamini-Hochberg FDR, permutation tests), effect size measures (Cohen's d, odds ratios, hazard ratios), and confidence interval calculations. The ontology facilitates peer review by enabling reviewers to verify appropriate test selection and assists researchers in choosing correct statistical methods based on study design and data characteristics.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://stato-ontology.org/",
      "formal_specification": "https://github.com/ISA-tools/stato"
    },
    {
      "id": "B2AI_STANDARD:552",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "SYMP",
      "description": "Symptom Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Disease symptoms, with symptoms encompasing perceived changes in function, sensations or appearance reported by a patient.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://symptomontologywiki.igs.umaryland.edu/mediawiki/index.php/Main_Page",
      "publication": "doi:10.1093/nar/gkab1063",
      "formal_specification": "https://github.com/DiseaseOntology/SymptomOntology"
    },
    {
      "id": "B2AI_STANDARD:553",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "SNOMED CT",
      "description": "Systematized Nomenclature of Medicine - Clinical Terms",
      "related_to": [
        "B2AI_STANDARD:769"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "codesystem",
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:74"
      ],
      "purpose_detail": "Standard for electronic exchange of clinical health information. Provided through UMLS.",
      "is_open": true,
      "requires_registration": true,
      "url": "https://www.nlm.nih.gov/healthit/snomedct/index.html"
    },
    {
      "id": "B2AI_STANDARD:554",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "SNMI",
      "description": "Systematized Nomenclature of Medicine, International Version",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:74"
      ],
      "purpose_detail": "Systematized Nomenclature of Medicine International (SNMI) is a historical medical terminology and classification system that preceded SNOMED CT. Developed in the 1960s-1970s by the College of American Pathologists, SNMI represented one of the earliest attempts to create a comprehensive, multi-axial medical nomenclature covering topography (anatomy), morphology (structural changes), etiology (causes), and function (physiological processes). SNMI employed a systematic coding structure where concepts were represented by alphanumeric codes and could be combined using post-coordination to express complex clinical findings (e.g., combining topography codes with morphology codes to describe disease locations and characteristics). The terminology was designed primarily for pathology and clinical documentation, providing structured vocabulary for diagnoses, procedures, and laboratory findings. SNMI evolved through several versions including SNOMED (Systematized Nomenclature of Medicine), SNOMED II, and SNOMED III before being integrated into SNOMED CT (Clinical Terms) in 2002 through merger with UK's Clinical Terms Version 3 (Read Codes). While SNMI itself is now deprecated and replaced by SNOMED CT for current clinical use, it is historically significant as a foundational medical terminology that pioneered multi-axial compositional approaches to medical concept representation. Legacy SNMI data may still exist in historical medical records and research databases, requiring mapping to modern terminologies for interoperability. Understanding SNMI's structure provides context for SNOMED CT's architecture and the evolution of standardized medical terminologies used in electronic health records, clinical research, and healthcare data analytics.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://bioportal.bioontology.org/ontologies/SNMI"
    },
    {
      "id": "B2AI_STANDARD:555",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "SBO",
      "description": "Systems Biology Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:1"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:29"
      ],
      "purpose_detail": "Terms commonly used in Systems Biology and computational modeling.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/EBI-BioModels/SBO",
      "formal_specification": "https://github.com/EBI-BioModels/SBO"
    },
    {
      "id": "B2AI_STANDARD:556",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "TAXRANK",
      "description": "Taxonomic rank vocabulary",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:1"
      ],
      "purpose_detail": "A vocabulary of taxonomic ranks (species, family, phylum, etc).",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/phenoscape/taxrank",
      "publication": "doi:10.1186/2041-1480-4-34",
      "formal_specification": "https://github.com/phenoscape/taxrank"
    },
    {
      "id": "B2AI_STANDARD:557",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "T4FS",
      "description": "terms4FAIRskills",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A terminology for the skills necessary to make data FAIR and to keep it FAIR.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://obofoundry.org/ontology/t4fs.html",
      "publication": "doi:10.5281/zenodo.4772741",
      "formal_specification": "https://github.com/terms4fairskills/FAIRterminology"
    },
    {
      "id": "B2AI_STANDARD:558",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "DRON",
      "description": "The Drug Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:8"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:96"
      ],
      "purpose_detail": "An ontology to support comparative effectiveness researchers studying claims data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/ufbmi/dron",
      "publication": "doi:10.1186/s13326-017-0121-5",
      "formal_specification": "https://github.com/ufbmi/dron"
    },
    {
      "id": "B2AI_STANDARD:559",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "OBOE",
      "description": "The Extensible Observation Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:62"
      ],
      "purpose_detail": "The Extensible Observation Ontology (OBOE) is a formal ontology for capturing the semantics of scientific observation and measurement.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://bioportal.bioontology.org/ontologies/OBOE",
      "formal_specification": "https://github.com/NCEAS/oboe/"
    },
    {
      "id": "B2AI_STANDARD:560",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "OGG",
      "description": "The Ontology of Genes and Genomes",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "A formal ontology of genes and genomes of biological organisms.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://bitbucket.org/hegroup/ogg/src/master/",
      "formal_specification": "https://bitbucket.org/hegroup/ogg/src/master/"
    },
    {
      "id": "B2AI_STANDARD:561",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "PDRO",
      "description": "The Prescription of Drugs Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:8"
      ],
      "purpose_detail": "An ontology to describe entities related to prescription of drugs",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/OpenLHS/PDRO",
      "publication": "doi:10.3390/ijerph182212025",
      "formal_specification": "https://github.com/OpenLHS/PDRO"
    },
    {
      "id": "B2AI_STANDARD:562",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "TXPO",
      "description": "Toxic Process Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Terms involving toxicity courses and processes.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://toxpilot.nibiohn.go.jp/",
      "formal_specification": "https://github.com/txpo-ontology/TXPO/"
    },
    {
      "id": "B2AI_STANDARD:563",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "UBERON",
      "description": "Uberon",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "UBERON is an OBO Foundry ontology providing an integrated cross-species anatomy ontology covering anatomical structures in animals, with a focus on multi-species interoperability. The ontology enables semantic annotation of anatomical entities across diverse species, supporting comparative anatomy research, phenotype studies, and integration with specialized anatomies. UBERON is tightly integrated with the Gene Ontology (GO) and the Cell Ontology (CL), using formal ontology design patterns to represent anatomical locations and relationships. It follows FAIR principles and is released in standard formats (OWL, OBO, JSON obographs) with resolvable version IRIs. UBERON is integrated into standard tools including Ubergraph for logical queries (e.g., finding cell types by location), the Ontology Access Kit (OAK), and major browsers (OLS, Ontobee, BioPortal). The ontology supports AI/ML applications by providing standardized anatomical annotations for training data across species and enabling cross-species knowledge transfer in biomedical machine learning models.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://obophenotype.github.io/uberon/",
      "publication": "doi:10.1186/gb-2012-13-1-r5",
      "formal_specification": "https://github.com/obophenotype/uberon"
    },
    {
      "id": "B2AI_STANDARD:564",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "Metathesaurus",
      "description": "UMLS Metathesaurus",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:74",
        "B2AI_ORG:115"
      ],
      "purpose_detail": "Biomedical terminology and hierarchical relationships between concepts.",
      "is_open": true,
      "requires_registration": true,
      "url": "https://www.nlm.nih.gov/research/umls/knowledge_sources/metathesaurus/index.html"
    },
    {
      "id": "B2AI_STANDARD:565",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "UPHENO",
      "description": "Unified phenotype ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:25"
      ],
      "purpose_detail": "Integrates multiple phenotype ontologies into a unified cross-species phenotype ontology.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/obophenotype/upheno",
      "formal_specification": "https://github.com/obophenotype/upheno"
    },
    {
      "id": "B2AI_STANDARD:566",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "UO",
      "description": "Units of measure ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The Units Ontology - a tool for integrating units of measurement in science",
      "is_open": true,
      "requires_registration": false,
      "url": "https://obofoundry.org/ontology/uo.html",
      "publication": "doi:10.1093/database/bas033"
    },
    {
      "id": "B2AI_STANDARD:567",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "UO",
      "description": "Units of measurement ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Metrical units for use in conjunction with PATO.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/bio-ontology-research-group/unit-ontology",
      "formal_specification": "https://github.com/bio-ontology-research-group/unit-ontology"
    },
    {
      "id": "B2AI_STANDARD:568",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "UMDNS",
      "description": "Universal Medical Device Nomenclature System",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:27"
      ],
      "purpose_detail": "Universal Medical Device Nomenclature System (UMDNS) is a nomenclature that has been officially adopted by many nations. UMDNS facilitates identifying, processing, filing, storing, retrieving, transferring, and communicating data about medical devices. The nomenclature is used in applications ranging from hospital inventory and work-order controls to national agency medical device regulatory systems.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://www.ecri.org/solutions/umdns"
    },
    {
      "id": "B2AI_STANDARD:569",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "VO",
      "description": "Vaccine Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The Vaccine Ontology (VO) is an OBO Foundry community-based biomedical ontology that systematically represents vaccine components, vaccine types, vaccination procedures, vaccine-induced immune responses, and vaccine-preventable infectious diseases in a standardized machine-readable format to support vaccine research, immunization program management, and vaccinomics data integration. Developed through international collaboration coordinated by the University of Michigan Medical School, VO encompasses comprehensive coverage of licensed human and veterinary vaccines including live-attenuated vaccines, inactivated vaccines, subunit vaccines, toxoid vaccines, mRNA vaccines, viral vector vaccines, and conjugate vaccines, representing their antigenic components, adjuvants, preservatives, manufacturing processes, and delivery routes. The ontology integrates with multiple biomedical ontologies including the Infectious Disease Ontology (IDO) for pathogen-host interactions, the Ontology for Biomedical Investigations (OBI) for vaccination protocols, the Protein Ontology (PRO) for vaccine antigens, and SNOMED CT and ICD codes for clinical documentation, enabling semantic interoperability across vaccine databases and immunization information systems. VO formally represents immunization schedules (primary series, booster doses, catch-up schedules), adverse events following immunization (AEFI) including local reactions and systemic effects, contraindications and precautions for specific populations, vaccine efficacy and effectiveness measures, and herd immunity thresholds. Applications include standardized annotation of vaccine clinical trials data, integration of vaccine safety surveillance systems (VAERS, Vaccine Safety Datalink), comparative vaccine effectiveness studies across populations and time periods, semantic queries of vaccine literature through PubMed and clinical databases, machine learning models predicting vaccine immunogenicity or reactogenicity from vaccine composition, and global immunization data harmonization supporting WHO vaccination coverage monitoring and pandemic preparedness. VO follows OBO Foundry principles with open-source development, versioned releases, and persistent URIs, making it essential infrastructure for computational vaccinology and evidence-based immunization policy.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/vaccineontology/VO",
      "publication": "doi:10.1186/2041-1480-3-17",
      "formal_specification": "https://github.com/vaccineontology/VO"
    },
    {
      "id": "B2AI_STANDARD:570",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "VBO",
      "description": "Vertebrate Breed Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:58"
      ],
      "purpose_detail": "Vertebrate Breed Ontology (VBO) is a comprehensive ontology providing standardized nomenclature and hierarchical classification for vertebrate animal breeds across 38 species including cattle, sheep, goats, pigs, horses, chickens, dogs, and cats, developed collaboratively by the Monarch Initiative, Online Mendelian Inheritance in Animals (OMIA), and FAO's Domestic Animal Diversity Information System (DAD-IS). VBO structures breed information using OBO Foundry principles, capturing breed names, synonyms (alternate spellings, historical names, local language variants), geographical origins, breed characteristics (size, coat color, production traits), and relationships to parent breeds or breed groups. The ontology integrates with DAD-IS's 15,000+ national breed populations representing 8,800+ breeds maintained by 182 countries' National Coordinators, ensuring global coverage and continuous updates reflecting new breed registrations and naming conventions. VBO supports cross-species breed queries by organizing breeds under species-specific classes (bovine breeds, equine breeds) while maintaining inter-breed relationships such as \"derived from\" for composite breeds and \"related to\" for breeds sharing genetic heritage. The ontology enables precise phenotype-genotype associations in animal genetics research by providing stable breed identifiers (VBO IDs) that link to genomic datasets, genetic variant databases, and phenotype repositories, facilitating genome-wide association studies (GWAS) and quantitative trait locus (QTL) mapping in livestock. VBO integrates with other biomedical ontologies including the Livestock Breed Ontology (LBO), Mammalian Phenotype Ontology (MP), and UBERON anatomy ontology to support complex queries across breed characteristics, anatomical features, and disease susceptibilities. For agricultural genomics, VBO enables tracking of breed-specific genetic diversity, conservation status of rare breeds (endangered, critical, vulnerable), and lineage documentation for breeding programs aimed at preserving genetic resources. In AI/ML applications, VBO provides structured metadata for training computer vision models on breed classification, natural language processing systems for extracting breed information from veterinary records, and knowledge graphs connecting breed genetics to production traits (milk yield, growth rate, disease resistance) and animal welfare indicators, supporting precision livestock farming and genomic selection programs.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/monarch-initiative/vertebrate-breed-ontology",
      "formal_specification": "https://github.com/monarch-initiative/vertebrate-breed-ontology"
    },
    {
      "id": "B2AI_STANDARD:571",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "VTO",
      "description": "Vertebrate Taxonomy Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "\"The Vertebrate Taxonomy Ontology (VTO) provides a comprehensive unified taxonomic hierarchy encompassing both extinct and extant vertebrate taxa, integrating authoritative sources to serve comparative biology, evolutionary research, paleontology, biodiversity informatics, and model organism databases requiring consistent taxonomic classification across deep time. VTO addresses the fundamental challenge that different biological disciplines maintain separate taxonomic frameworksNCBI Taxonomy focuses on species with genetic sequence data deposited in GenBank/RefSeq (inherently biased toward extant, well-studied organisms), while paleontological databases like the Paleobiology Database (PaleoDB) capture fossil taxa spanning 540 million years of vertebrate evolution but lack integration with molecular resources. VTO bridges this divide by maintaining a backbone hierarchy based on NCBI Taxonomy for extant taxa (ensuring compatibility with genomics/proteomics databases indexing sequences by taxon identifiers), while incorporating PaleoDB taxonomic information to represent extinct lineages including stem-group vertebrates, armored fishes (placoderms, acanthodians), early tetrapods, and extinct clades within extant groups (e.g., non-avian dinosaurs, pterosaurs, ichthyosaurs, plesiosaurs, mammalian stem groups). The ontology incorporates more authoritative hierarchies for specific vertebrate groups through integration with the Teleost Taxonomy Ontology (TTO) maintained by the Zebrafish Model Organism Database, providing detailed classification for the 30,000+ teleost fish species (comprising more than half of extant vertebrate diversity), and AmphibiaWeb taxonomy for the 8,500+ amphibian species with emphasis on recently described taxa and refined phylogenetic relationships. Each taxon term includes standardized nomenclature following ICZN (International Code of Zoological Nomenclature) and ICNP (International Code of Nomenclature for Prokaryotes) rules, synonyms capturing taxonomic revisions and historical names, temporal ranges for extinct taxa (e.g., 'Late Cretaceous, 100.5-66.0 Ma'), and cross-references to NCBI Taxonomy IDs, PaleoDB taxon numbers, and Encyclopedia of Life pages. VTO supports the Phenoscape Knowledgebase (KB), a semantic database linking evolutionary phenotypes annotated from 200+ phylogenetic comparative studies with developmental biology and genetics data from model organisms, enabling queries like 'find genes associated with fin-to-limb morphological transitions' by traversing ontology relationships connecting fossil taxa exhibiting transitional phenotypes (e.g., Tiktaalik, Acanthostega) to homologous structures in zebrafish, Xenopus, and mouse annotated with genetic perturbation data. Applications span biodiversity informatics platforms (GBIF, iNaturalist, EOL) requiring standardized taxonomic backbones for aggregating species occurrence data and range maps, museum specimen databases linking physical collections to phylogenetic context, conservation genomics identifying evolutionarily significant units (ESUs) and management units within endangered species complexes based on phylogenetic position, and ecological modeling incorporating phylogenetic diversity metrics (Faith's PD, phylogenetic endemism) for conservation prioritization. VTO enables cross-species phenotype comparisons essential for understanding trait evolution and developmental constraints, supporting evo-devo research connecting genetic regulatory networks across vertebrate phylogeny. Machine learning applications leverage VTO's hierarchical structure for taxonomic prediction from morphological imaging data, where classifiers trained on extant species can incorporate extinct taxa's phenotypic annotations to inform predictions about fossil specimens, phylogenetic imputation methods predicting missing trait values based on taxonomic proximity weighted by VTO relationships, comparative genomics studies identifying taxon-specific gene family expansions and losses by mapping genomic features to VTO taxa and inferring evolutionary events at internal nodes representing common ancestors, and meta-analyses of phenotype-genotype associations across model organisms where VTO provides the semantic framework for translating findings between zebrafish cardiovascular mutants, Xenopus neural crest development, chicken limb patterning, and mouse craniofacial genetics by reasoning over homologous anatomical structures and shared ancestry. Integration with other OBO Foundry ontologies including Uberon (cross-species anatomy), Gene Ontology (biological processes and molecular functions), and PATO (phenotypic qualities) enables complex semantic queries spanning taxonomy, anatomy, and genetics, such as 'retrieve all vertebrate taxa with documented neural crest-derived pharyngeal arch skeletal elements exhibiting heterochronic developmental timing shifts relative to ancestral condition' which requires coordinating VTO taxon relationships, Uberon anatomical part-of hierarchies, and GO developmental process annotations. VTO's CC0 public domain license and OBO Foundry compliance ensure interoperability with global biodiversity and biomedical data ecosystems, with monthly releases synchronized to NCBI Taxonomy updates, PaleoDB taxonomic revisions, and community-contributed nomenclatural corrections.\"",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/phenoscape/vertebrate-taxonomy-ontology",
      "formal_specification": "https://github.com/phenoscape/vertebrate-taxonomy-ontology"
    },
    {
      "id": "B2AI_STANDARD:572",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "VT",
      "description": "Vertebrate trait ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:25"
      ],
      "purpose_detail": "The Vertebrate Trait Ontology (VT) provides comprehensive standardized terminology for describing measurable or observable phenotypic characteristics across vertebrate species, maintained by the AnimalGenome organization to support agricultural genomics, comparative biology, quantitative genetics, and translational research linking animal models to human health. VT encompasses broad trait categories including morphological traits (body size, organ dimensions, skeletal measurements, integument characteristics), physiological traits (metabolic parameters, cardiovascular function, respiratory capacity, immune response, reproductive performance), behavioral traits (temperament, learning ability, social interactions, feeding behavior), production traits critical for livestock industries (growth rate, feed efficiency, milk yield, egg production, meat quality, wool production), disease resistance traits (pathogen susceptibility, vaccine response, parasite load), and life history traits (longevity, fertility, developmental timing). Each trait is formally defined with measurement methodologies, units, biological context, and relationships to anatomical structures (via Uberon) and biological processes (via Gene Ontology). VT serves as the primary phenotype ontology for agricultural animal genomics databases including AnimalQTLdb (quantitative trait loci database for cattle, pig, chicken, sheep, horse, rainbow trout), supporting annotation of QTL mapping studies that identify genomic regions influencing economically important traits. The ontology enables cross-species phenotype comparisons essential for translational research, allowing researchers to leverage livestock as biomedical models (e.g., pig cardiovascular traits for human heart disease research, sheep bone traits for osteoporosis studies). Applications span livestock breeding programs through genomic selection where VT standardizes trait definitions for estimated breeding values (EBVs), wildlife conservation genetics for monitoring population health indicators, aquaculture for trait improvement in fish and shellfish, and comparative physiology studies examining adaptive evolution of traits across vertebrate lineages. VT facilitates GWAS (genome-wide association studies) meta-analyses by harmonizing trait definitions across studies, enables phenotype-driven queries in genomics databases (\"find all QTL affecting milk fat percentage in dairy cattle\"), and supports machine learning models predicting complex traits from genotype data by providing structured phenotype representations. Integration with other ontologies including Mammalian Phenotype Ontology (MP), Human Phenotype Ontology (HPO), and Clinical Measurement Ontology (CMO) enables bidirectional translation between agricultural animal research and human biomedicine. VT is distributed under CC-BY-NC 4.0 license through http://purl.obolibrary.org/obo/vt.owl with releases synchronized to OBO Foundry and AgroPortal.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/AnimalGenome/vertebrate-trait-ontology",
      "formal_specification": "https://github.com/AnimalGenome/vertebrate-trait-ontology"
    },
    {
      "id": "B2AI_STANDARD:573",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "EUPATH",
      "description": "VEuPathDB ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Support ontology for the Eukaryotic Pathogen, Host & Vector Genomics Resource (VEuPathDB; https://veupathdb.org).",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/VEuPathDB-ontology/VEuPathDB-ontology",
      "publication": "doi:10.5281/zenodo.6685957",
      "formal_specification": "https://github.com/VEuPathDB-ontology/VEuPathDB-ontology"
    },
    {
      "id": "B2AI_STANDARD:574",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "XPO",
      "description": "Xenopus Phenotype Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:25"
      ],
      "purpose_detail": "The Xenopus Phenotype Ontology (XPO) is a formal ontology for representing anatomical, cellular, and gene function phenotypes observed throughout the development of the African frogs Xenopus laevis and Xenopus tropicalis. XPO enables standardized annotation of phenotypes in Xenopus research, supporting integration with genotype, phenotype, and disease data across species. The ontology is used by Xenbase and other resources to facilitate cross-species comparisons, data sharing, and computational analysis of developmental and functional phenotypes in model organism research.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/obophenotype/xenopus-phenotype-ontology",
      "publication": "doi:10.1186/s12859-022-04636-8",
      "formal_specification": "https://github.com/obophenotype/xenopus-phenotype-ontology"
    },
    {
      "id": "B2AI_STANDARD:575",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "ZP",
      "description": "Zebrafish Phenotype Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "obofoundry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:25"
      ],
      "purpose_detail": "All phenotypes of the Zebrafish model organism.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/obophenotype/zebrafish-phenotype-ontology",
      "formal_specification": "https://github.com/obophenotype/zebrafish-phenotype-ontology"
    },
    {
      "id": "B2AI_STANDARD:576",
      "category": "B2AI_STANDARD:DataStandardOrTool",
      "name": "PHVS_Race_HL7_2x",
      "description": "CDC Race and Ethnicity Code Set",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:6"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:12",
        "B2AI_ORG:40"
      ],
      "purpose_detail": "A code set for use in coding race and ethnicity data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.cdc.gov/phin/resources/vocabulary/documents/cdc-race--ethnicity-background-and-purpose.pdf",
      "formal_specification": "https://phinvads.cdc.gov/vads/ViewValueSet.action?id=B246B692-6DF8-E111-B875-001A4BE7FA90"
    },
    {
      "id": "B2AI_STANDARD:577",
      "category": "B2AI_STANDARD:DataStandardOrTool",
      "name": "RFC 5646",
      "description": "IETF Request for Comment 5646 Tags for Identifying Languages",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5",
        "B2AI_TOPIC:6"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:45"
      ],
      "purpose_detail": "Best practices for the structure, content, construction, and semantics of language tags for use in cases where it is desirable to indicate the language used in an information object.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.rfc-editor.org/rfc/rfc5646"
    },
    {
      "id": "B2AI_STANDARD:578",
      "category": "B2AI_STANDARD:DataStandardOrTool",
      "name": "ITUT E.123",
      "description": "International Telecommunication Union E.123 Notation for national and international telephone numbers, e-mail addresses and web addresses",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:31"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:50"
      ],
      "purpose_detail": "Standard notation for printing telephone numbers, E-mail addresses and Web addresses.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.itu.int/rec/T-REC-E.123-200102-I/en"
    },
    {
      "id": "B2AI_STANDARD:579",
      "category": "B2AI_STANDARD:DataStandardOrTool",
      "name": "ITUT E.164",
      "description": "International Telecommunication Union E.164 The international public telecommunication numbering plan",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:31"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:50"
      ],
      "purpose_detail": "Number structure and functionality for the five categories of numbers used for international public telecommunication - geographic areas, global services, Networks, groups of countries (GoC) and resources for trials.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.itu.int/rec/T-REC-E.164-201011-I/en"
    },
    {
      "id": "B2AI_STANDARD:704",
      "category": "B2AI_STANDARD:Registry",
      "name": "bio.tools",
      "description": "bio.tools",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "dataregistry",
        "softwareregistry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:1"
      ],
      "purpose_detail": "bio.tools is a comprehensive, community-driven registry and portal for bioinformatics software tools, databases, and services developed under the ELIXIR Tools Platform, providing standardized descriptions and metadata for over 27,000 resources spanning genomics, proteomics, structural biology, systems biology, clinical informatics, and computational biology workflows. Each registered tool entry follows the biotoolsSchema specification defining structured metadata fields including tool name, description, homepage URL, version information, programming language, operating system compatibility, license type, publication citations, and crucially, semantic annotations using EDAM ontology (EDAM - ontology of bioinformatics operations, data types, formats, and topics) terms that classify tools by their function (e.g., \"Sequence alignment\", \"Variant calling\", \"Phylogenetic tree generation\"), input/output data types (e.g., \"Protein sequence\", \"Gene expression matrix\", \"Pathway diagram\"), supported file formats (e.g., \"FASTA\", \"VCF\", \"BAM/SAM\"), and scientific application topics (e.g., \"Transcriptomics\", \"Metagenomics\", \"Drug discovery\"), enabling precise semantic search and automated tool discovery. The registry provides RESTful API access allowing programmatic queries to retrieve tool metadata, filter by EDAM annotations, search by keywords or identifiers (biotools IDs, DOIs, PMIDs), and integrate bio.tools data into external systems including workflow management platforms (Galaxy, Nextflow, Snakemake), package managers (Bioconda, BioContainers), and institutional tool catalogs. bio.tools implements quality metrics for entries including information completeness scores, citation counts, version currency indicators, and community ratings, with curation workflows supported by community moderators who review submissions, validate metadata accuracy, resolve duplicate entries, and maintain EDAM annotation consistency. The platform serves as the authoritative tool registry for ELIXIR infrastructure connecting 23 European research organizations, integrating with ELIXIR Core Data Resources (UniProt, ENA, PDB, EMBL-EBI services), supporting findability through schema.org markup for search engine indexing, and providing training materials and documentation for tool developers to register and maintain their resources. bio.tools facilitates reproducibility in computational biology by providing persistent identifiers (biotools IDs like biotools:blast) for citing tools in publications and methods sections, versioning information to document exact tool versions used in analyses, and links to containerized deployments (Docker, Singularity) ensuring long-term availability. For AI and machine learning applications in bioinformatics, bio.tools enables systematic tool landscape analysis where researchers can use API queries and EDAM annotations to identify all available tools for specific tasks (e.g., retrieving the 200+ tools annotated with \"Machine learning\" operation, filtering by input data types to find ML tools accepting gene expression matrices), supports automated workflow composition where workflow engines can discover compatible tools by matching output data types of one tool to input requirements of downstream tools using EDAM semantic relationships, facilitates meta-research on bioinformatics software ecosystems through network analysis of tool relationships (citation networks, input/output compatibility graphs, co-usage patterns from workflow repositories), enables recommendation systems that suggest relevant tools to users based on their data types and analysis objectives by learning from historical tool usage patterns and EDAM ontology structure, supports benchmarking studies comparing multiple tools for the same task where bio.tools metadata provides the tool universe and EDAM annotations ensure fair comparisons across functionally equivalent implementations, and allows training of natural language processing models to automatically extract tool mentions from scientific literature and map them to canonical bio.tools entries using the rich metadata and publication linkage, creating comprehensive maps of which computational methods are actually used across different research domains and facilitating evidence-based tool selection in automated analysis pipelines.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://bio.tools/",
      "formal_specification": "https://github.com/bio-tools/biotoolsregistry/"
    },
    {
      "id": "B2AI_STANDARD:705",
      "category": "B2AI_STANDARD:Registry",
      "name": "Bioconductor",
      "description": "Bioconductor",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "softwareregistry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The mission of the Bioconductor project is to develop, support, and disseminate free open source software that facilitates rigorous and reproducible analysis of data from current and emerging biological assays. We are dedicated to building a diverse, collaborative, and welcoming community of developers and data scientists.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.bioconductor.org/",
      "formal_specification": "https://github.com/Bioconductor/BiocManager",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:40"
      ]
    },
    {
      "id": "B2AI_STANDARD:706",
      "category": "B2AI_STANDARD:Registry",
      "name": "BioPortal",
      "description": "BioPortal",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "ontologyregistry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:1"
      ],
      "purpose_detail": "BioPortal is a comprehensive, open-access ontology repository and collaborative platform developed by the National Center for Biomedical Ontology (NCBO) at Stanford, hosting over 1,000 biomedical ontologies, terminologies, and vocabularies. The platform provides centralized access to diverse biomedical knowledge organization systems including OBO Foundry ontologies (GO, Uberon, CHEBI, HPO), medical terminologies (SNOMED CT, ICD, RxNorm, LOINC), domain-specific vocabularies, and community-developed ontologies. BioPortal offers rich functionality including: ontology browsing with hierarchical visualization and concept lookup; powerful search across all ontologies with autocomplete and recommendations; mapping services for finding correspondences between ontologies; annotator services for identifying ontology concepts in free text; versioning and change tracking for ontology evolution; widgets and web services (REST APIs) for programmatic access; and community features for commenting, discussions, and ontology reviews. The platform serves as infrastructure for ontology developers (submission, hosting, versioning), data curators (annotation, mapping, validation), and application developers (APIs, widgets, SPARQL endpoints). BioPortal enables semantic annotation of biomedical data, cross-resource data integration through ontology mappings, and standardized vocabularies for clinical research, genomics, drug discovery, and translational medicine. In AI/ML applications, BioPortal supports ontology-based feature engineering for predictive models, semantic search for literature and dataset discovery, text mining and NLP by providing concept recognizers, knowledge graph construction by linking data to standardized ontologies, and explainable AI through ontological reasoning and structured domain knowledge integration.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://bioportal.bioontology.org/",
      "formal_specification": "https://github.com/ncbo"
    },
    {
      "id": "B2AI_STANDARD:707",
      "category": "B2AI_STANDARD:Registry",
      "name": "Bioregistry",
      "description": "Bioregistry",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "standardsregistry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The Bioregistry is an open-source, community-curated registry and meta-registry that catalogs prefixes, identifier formats, and metadata for biomedical ontologies, databases, and controlled vocabularies. It integrates and harmonizes information from multiple registries (e.g., OBO Foundry, Identifiers.org, OLS), providing a unified resource for resolving compact URIs (CURIEs) and supporting semantic interoperability. The Bioregistry also functions as a resolver, mapping CURIEs to web resources, and is governed by transparent contribution and review processes. It is widely used for data integration, annotation, and knowledge graph construction in the life sciences.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://bioregistry.io/",
      "publication": "doi:10.1101/2022.07.08.499378",
      "formal_specification": "https://github.com/biopragmatics/bioregistry"
    },
    {
      "id": "B2AI_STANDARD:708",
      "category": "B2AI_STANDARD:Registry",
      "name": "Bridge2AI registry",
      "description": "Bridge to Artificial Intelligence Registry",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "standardsregistry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Standards, tools, reference implementations, and related resources.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/bridge2ai/b2ai-standards-registry",
      "formal_specification": "https://github.com/bridge2ai/b2ai-standards-registry"
    },
    {
      "id": "B2AI_STANDARD:709",
      "category": "B2AI_STANDARD:Registry",
      "name": "CDISC SHARE",
      "description": "CDISC Shared Health And Research Electronic library",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:15"
      ],
      "purpose_detail": "CDISC launched the CDISC Shared Health And Research Electronic library (SHARE) to provide the standards metadata in machine-readable formats to facilitate the automated management and implementation of the standards.",
      "is_open": true,
      "requires_registration": true,
      "url": "https://www.cdisc.org/faq/share/what-cdisc-share",
      "publication": "PUBMED:29888049"
    },
    {
      "id": "B2AI_STANDARD:710",
      "category": "B2AI_STANDARD:Registry",
      "name": "Database Commons",
      "description": "Database Commons",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A catalog of worldwide biological databases maintained by the China National Center for Bioinformation,",
      "is_open": true,
      "requires_registration": false,
      "url": "https://ngdc.cncb.ac.cn/databasecommons/",
      "publication": "doi:10.1016/j.gpb.2022.12.004"
    },
    {
      "id": "B2AI_STANDARD:711",
      "category": "B2AI_STANDARD:Registry",
      "name": "Dockstore",
      "description": "Dockstore",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "softwareregistry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:34"
      ],
      "purpose_detail": "Dockstore is a comprehensive, open-source platform developed by the Ontario Institute for Cancer Research (OICR) and other collaborators that serves as a central registry for sharing, discovering, and executing containerized bioinformatics tools and computational workflows. Built on modern container technologies including Docker and Singularity, Dockstore enables researchers to package their analytical pipelines with all dependencies and configurations, ensuring reproducibility across different computing environments. The platform supports multiple workflow languages including Common Workflow Language (CWL), Workflow Description Language (WDL), and Nextflow, providing flexibility for diverse computational approaches in genomics, proteomics, and systems biology. Dockstore integrates with popular code repositories like GitHub, GitLab, and Bitbucket, enabling version-controlled development and automated testing of computational tools. The platform facilitates scientific collaboration by allowing researchers to discover validated, ready-to-use analytical workflows, reducing duplication of effort and accelerating research discovery. With built-in execution engines and cloud integration capabilities, Dockstore supports scalable workflow execution on local clusters, cloud platforms, and high-performance computing systems, making advanced bioinformatics accessible to researchers regardless of their computational expertise.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://dockstore.org/",
      "formal_specification": "https://github.com/dockstore/dockstore"
    },
    {
      "id": "B2AI_STANDARD:712",
      "category": "B2AI_STANDARD:Registry",
      "name": "Fairsharing",
      "description": "Fairsharing",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "standardsregistry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "FAIRsharing (fairsharing.org) is a comprehensive, curated registry of research data standards, databases, repositories, and policies that promotes Findable, Accessible, Interoperable, and Reusable (FAIR) data practices across life sciences, environmental sciences, social sciences, and humanities domains. Established by the UK BBSRC, ELIXIR, and Oxford e-Research Centre, FAIRsharing catalogs 2,000+ data standards (ontologies, terminologies, formats, models), 1,800+ databases and repositories (discipline-specific archives, institutional repositories, generalist repositories), and 800+ data policies from funders, journals, and institutions, providing a one-stop resource for discovering community-endorsed resources that facilitate data reuse and reproducibility. Each FAIRsharing record includes structured metadata describing the resource's scope, governance (community-driven, institutional, commercial), licenses, interoperability capabilities, and relationships to other standards/databases, with persistent identifiers (FAIRsharing DOIs) enabling stable citations in data management plans and publications. The registry employs a domain-tagging system covering biomedical sciences (genomics, proteomics, metabolomics), environmental sciences (climate, biodiversity, geosciences), social sciences, and interdisciplinary fields, with cross-links to related resources illustrating standard-database-policy connections within research ecosystems. FAIRsharing supports researchers in selecting appropriate standards for data annotation (MIAME for microarrays, STROBE for epidemiology) and repositories for data deposition (GEO for gene expression, PDB for protein structures), facilitating compliance with journal and funder mandates for open data sharing. The platform integrates with research infrastructure projects (ELIXIR, RDA, CODATA) and provides APIs for embedding FAIRsharing recommendations into data management planning tools, electronic lab notebooks, and repository submission interfaces. For data stewards, FAIRsharing enables discovery of domain-specific controlled vocabularies (ontologies, taxonomies) that enhance dataset interoperability, semantic search capabilities, and cross-study integration in meta-analyses. In AI/ML research, FAIRsharing guides selection of training data repositories with rich metadata, standardized formats that minimize preprocessing overhead, and permissive licenses enabling model training, while the registry's coverage of ML-specific resources (model registries, benchmark datasets, evaluation metrics) supports reproducible AI research aligned with FAIR principles for algorithms and models (FAIR4ML).",
      "is_open": true,
      "requires_registration": false,
      "url": "https://fairsharing.org/",
      "formal_specification": "https://github.com/FAIRsharing",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:9"
      ]
    },
    {
      "id": "B2AI_STANDARD:713",
      "category": "B2AI_STANDARD:DataStandardOrTool",
      "name": "LinkML registry",
      "description": "LinkML schema registry",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "standardsregistry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "schemas and machine-actionable standards",
      "is_open": true,
      "requires_registration": false,
      "url": "https://linkml.io/linkml-registry/registry/",
      "formal_specification": "https://github.com/linkml/linkml-registry/"
    },
    {
      "id": "B2AI_STANDARD:714",
      "category": "B2AI_STANDARD:Registry",
      "name": "NCI caDSR",
      "description": "National Cancer Institute Cancer Data Standards Repository",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "standardsregistry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:4",
        "B2AI_TOPIC:7"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:71"
      ],
      "purpose_detail": "Registry and repository for oncology research common data elements and forms.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://datascience.cancer.gov/resources/metadata",
      "formal_specification": "https://cdebrowser.nci.nih.gov/cdebrowserClient/cdeBrowser.html#/search"
    },
    {
      "id": "B2AI_STANDARD:715",
      "category": "B2AI_STANDARD:Registry",
      "name": "NRDR",
      "description": "National Radiology Data Registry",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "dataregistry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The primary purpose of NRDR is to aid facilities with their quality improvement programs and efforts to improve patient care by comparing facility data to that of their region and the nation. A practice or facility may choose to participate in any or all registries as appropriate for their practice. When a facility joins more than one registry, the warehouse allows information to be shared across registries within the facility.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://nrdr.acr.org/Portal/Nrdr/Main/page.aspx",
      "publication": "doi:10.1016/j.jacr.2011.05.014"
    },
    {
      "id": "B2AI_STANDARD:716",
      "category": "B2AI_STANDARD:Registry",
      "name": "OBO Foundry",
      "description": "Open Biological and Biomedical Ontology Foundry",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "ontologyregistry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:75"
      ],
      "purpose_detail": "The OBO Foundry is a collaborative community initiative for developing interoperable ontologies for the biological and biomedical sciences. It establishes and maintains a set of principles for ontology development to ensure quality, consistency, and interoperability across its library of domain ontologies. The OBO Foundry principles address key requirements including open availability, common syntax and semantics, clearly defined scope and content, use of well-documented collaborative procedures, orthogonality with other ontologies, provision of unique identifiers, and adherence to established naming conventions. The Foundry provides comprehensive community resources including the OBO tutorial, ontology browsers and tools, operations committees and working groups, and communication channels (mailing list, Slack workspace). The OBO Library registry provides standardized access to member ontologies in multiple formats (YAML, JSON-LD, RDF/Turtle) and includes domain-specific ontologies covering anatomy (UBERON), cell types (CL), diseases (MONDO), chemicals (ChEBI), genotypes (GENO), phenotypes (HPO), and many others. The OBO Foundry infrastructure includes consistent use of permanent URLs (PURLs), version control via GitHub, automated quality checks, and standardized release workflows. By ensuring semantic interoperability through shared upper-level ontologies and design patterns, OBO Foundry enables cross-domain data integration essential for AI/ML applications in biomedical knowledge graphs, automated reasoning, data harmonization, and multi-modal machine learning across diverse biological datasets.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://obofoundry.org/",
      "publication": "doi:10.1093/database/baab069"
    },
    {
      "id": "B2AI_STANDARD:717",
      "category": "B2AI_STANDARD:Registry",
      "name": "PwC",
      "description": "Papers With Code",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A free and open resource with Machine Learning papers, code, datasets, methods and evaluation tables.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://paperswithcode.com/",
      "formal_specification": "https://github.com/paperswithcode"
    },
    {
      "id": "B2AI_STANDARD:718",
      "category": "B2AI_STANDARD:DataStandardOrTool",
      "name": "r3data",
      "description": "Registry of Research Data Repositories",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "dataregistry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "re3data is a global registry of research data repositories. The registry covers research data repositories from different academic disciplines. re3data presents repositories for the permanent storage and access of data sets to researchers, funding bodies, publishers and scholarly institutions. re3data aims to promote a culture of sharing, increased access and better visibility of research data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.re3data.org/",
      "publication": "doi:10.5281/zenodo.6697943"
    },
    {
      "id": "B2AI_STANDARD:719",
      "category": "B2AI_STANDARD:Registry",
      "name": "3DP Registry",
      "description": "RSNA-ACR 3D Printing Registry",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "dataregistry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:85"
      ],
      "purpose_detail": "The joint RSNA and American College of Radiology (ACR) 3D printing clinical data registry collects 3D printing data at the point of clinical care. With the goal of improving both patient care and characterizing resource utilization, the brand-new registry collects anonymized 3D printing case information, clinical indications and intended uses for printed models, source imaging, model construction techniques and effort, 3D printing techniques and effort, and the clinical impact of the models.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://www.rsna.org/practice-tools/RSNA-ACR-3D-printing-registry"
    },
    {
      "id": "B2AI_STANDARD:720",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "clinical-problem-standardization",
      "description": "A corpus-driven standardization framework for encoding clinical problems with HL7 FHIR",
      "related_to": [
        "B2AI_STANDARD:109"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:40"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:845",
        "B2AI_STANDARD:846",
        "B2AI_STANDARD:847",
        "B2AI_STANDARD:850"
      ],
      "purpose_detail": "A framework for transforming free-text problem descriptions into standardized Health Level 7 (HL7) Fast Healthcare Interoperability Resources (FHIR) models.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/OHNLP/clinical-problem-standardization",
      "publication": "doi:10.1016/j.jbi.2020.103541",
      "formal_specification": "https://github.com/OHNLP/clinical-problem-standardization"
    },
    {
      "id": "B2AI_STANDARD:721",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "ARES",
      "description": "A Research Exploration System",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:76"
      ],
      "purpose_detail": "A Research Exploration System designed to improved the transparency of observational data research. ARES is an opinionated framework that delineates three levels of observational data assessment.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/OHDSI/Ares",
      "formal_specification": "https://github.com/OHDSI/Ares"
    },
    {
      "id": "B2AI_STANDARD:722",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Aesara",
      "description": "Aesara",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:63",
          "category": "B2AI:Application",
          "name": "Probabilistic Modeling and Bayesian Inference for Biomedicine",
          "description": "Aesara (successor to Theano) is used in biomedical AI for implementing probabilistic graphical models, Bayesian neural networks, and statistical inference algorithms that quantify uncertainty in clinical predictions. Researchers leverage Aesara's symbolic math capabilities and automatic differentiation to build sophisticated probabilistic models for tasks like uncertainty-aware disease risk prediction, Bayesian optimization of drug dosing regimens, and hierarchical models for multi-center clinical studies. The library enables AI applications that provide calibrated confidence intervals for predictions, perform approximate Bayesian inference through variational methods, and integrate domain knowledge through informative priors. Aesara is particularly valuable when prediction uncertainty quantification is critical for clinical decision-making.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "machinelearningframework"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Aesara is a Python library that allows one to define, optimize, and efficiently evaluate mathematical expressions involving multi-dimensional arrays.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/aesara-devs/aesara",
      "formal_specification": "https://github.com/aesara-devs/aesara",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:1"
      ]
    },
    {
      "id": "B2AI_STANDARD:723",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "AWS",
      "description": "Amazon Web Services",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "cloudservice"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Amazon Web Services (AWS) is the world's most comprehensive and broadly adopted cloud computing platform, providing 200+ fully managed services spanning compute (EC2, Lambda serverless), storage (S3 object storage, EBS block storage, EFS file systems), databases (RDS, DynamoDB, Redshift), networking (VPC, CloudFront CDN), machine learning (SageMaker, Bedrock, Rekognition), analytics (EMR, Athena, Kinesis), security (IAM, KMS, CloudHSM), and application integration (SQS, SNS, EventBridge) across 38 geographic regions with 120 availability zones globally. Launched in 2006, AWS pioneered the Infrastructure-as-a-Service (IaaS) model, transforming IT economics by offering pay-as-you-go pricing, elastic scalability, and eliminating upfront capital expenditures for hardware, enabling organizations from startups to enterprises to access enterprise-grade infrastructure previously available only to large corporations. AWS's compute offerings range from general-purpose EC2 instances for traditional workloads to specialized instance types optimized for memory-intensive applications (R-series), compute-intensive HPC (C-series, Hpc7a), GPU-accelerated deep learning (P5, G5, Trn1 with AWS Trainium/Inferentia chips), and serverless Lambda functions billed per millisecond execution time. The platform's storage hierarchy includes S3 for durable object storage with 99.999999999% durability, S3 Glacier for archival with retrieval SLAs from minutes to hours, EBS for low-latency block storage attached to EC2 instances, and EFS for scalable shared file systems supporting concurrent access across thousands of compute nodes. AWS's managed database services eliminate operational overhead of patching, backups, and replication, offering relational databases (Amazon RDS for MySQL, PostgreSQL, Oracle, SQL Server; Amazon Aurora with MySQL/PostgreSQL compatibility), NoSQL databases (DynamoDB for key-value, DocumentDB for MongoDB workloads), graph databases (Neptune), time-series databases (Timestream), and data warehouses (Redshift) with columnar storage for OLAP queries on petabyte-scale datasets. For AI/ML workloads, AWS SageMaker provides end-to-end ML lifecycle management with built-in algorithms, Jupyter notebooks, distributed training across GPU clusters, hyperparameter tuning, model deployment to real-time/batch inference endpoints, and MLOps capabilities (model registry, monitoring, CI/CD pipelines). AWS Bedrock offers access to foundation models from AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon Titan via unified API with retrieval-augmented generation (RAG) capabilities connecting models to enterprise data in S3 or knowledge bases. The platform's security model includes IAM for fine-grained access control with role-based policies, KMS for encryption key management, CloudHSM for FIPS 140-2 Level 3 hardware security modules, AWS Shield for DDoS protection, GuardDuty for threat detection, and comprehensive compliance certifications (SOC 1/2/3, PCI-DSS, HIPAA, FedRAMP, ISO 27001) enabling deployment of regulated workloads. AWS supports hybrid cloud architectures through AWS Outposts (on-premises hardware running AWS services), AWS Direct Connect (dedicated network connections bypassing public internet), and Storage Gateway (seamless cloud-extension of on-premises storage), enabling gradual cloud migrations and latency-sensitive edge computing scenarios. For AI/ML infrastructure in Bridge2AI and biomedical research, AWS provides scalable compute for genome sequencing analysis (AWS Batch, EC2 Spot instances), secure storage for HIPAA-compliant patient data (S3 with server-side encryption, VPC endpoints preventing internet exposure), managed databases for clinical trial data (RDS with automated backups, multi-AZ high availability), federated learning frameworks (AWS HealthLake for FHIR-based interoperability), and specialized AI services for medical imaging analysis (AWS HealthImaging, Amazon Rekognition Custom Labels), accelerating research workflows while maintaining data privacy, auditability, and compliance with healthcare regulations.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://aws.amazon.com/"
    },
    {
      "id": "B2AI_STANDARD:724",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Amundsen",
      "description": "Amundsen",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Amundsen is a data discovery and metadata engine for improving the productivity of data analysts, data scientists and engineers when interacting with data. It does that today by indexing data resources (tables, dashboards, streams, etc.) and powering a page-rank style search based on usage patterns (e.g. highly queried tables show up earlier than less queried tables).",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.amundsen.io/",
      "formal_specification": "https://github.com/amundsen-io/amundsen"
    },
    {
      "id": "B2AI_STANDARD:725",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Anduril",
      "description": "Anduril",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "toolkit"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:20"
      ],
      "purpose_detail": "Anduril is a workflow platform for analyzing large data sets. Anduril provides facilities for analyzing high-thoughput data in biomedical research, and the platform is fully extensible by third parties.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://anduril.org/site/",
      "publication": "doi:10.1093/bioinformatics/btz133",
      "formal_specification": "https://bitbucket.org/anduril-dev/anduril/src/stable/"
    },
    {
      "id": "B2AI_STANDARD:726",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Apache Atlas",
      "description": "Apache Atlas",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:5"
      ],
      "purpose_detail": "Apache Atlas provides open metadata management and governance capabilities for organizations to build a catalog of their data assets, classify and govern these assets and provide collaboration capabilities around these data assets for data scientists, analysts and the data governance team.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://atlas.apache.org/",
      "formal_specification": "https://github.com/apache/atlas"
    },
    {
      "id": "B2AI_STANDARD:727",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Spark",
      "description": "Apache Spark",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:5"
      ],
      "purpose_detail": "Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://spark.apache.org/docs/latest/index.html",
      "formal_specification": "https://github.com/apache/spark"
    },
    {
      "id": "B2AI_STANDARD:728",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Taverna",
      "description": "Apache Taverna",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "deprecated",
        "workflowlanguage"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Taverna is a domain-independent suite of tools used to design and execute data-driven workflows.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://incubator.apache.org/projects/taverna.html"
    },
    {
      "id": "B2AI_STANDARD:729",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Appyters",
      "description": "Appyters",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "datavisualization",
        "notebookplatform"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Appyters extend the Jupyter Notebook language to support external, end-user configurable variables. Appyters can be considered a meta Jupyter Notebook language that is compatible with standard Jupyter Notebook execution.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://appyters.maayanlab.cloud/",
      "publication": "doi:10.1016/j.patter.2021.100213",
      "formal_specification": "https://github.com/MaayanLab/appyter-catalog"
    },
    {
      "id": "B2AI_STANDARD:730",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "ATHENA",
      "description": "ATHENA",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:76"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:844"
      ],
      "purpose_detail": "A resource of searchable and loadable standardized vocabularies.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://athena.ohdsi.org/search-terms/terms",
      "formal_specification": "https://github.com/OHDSI/Athena"
    },
    {
      "id": "B2AI_STANDARD:731",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "ATLAS",
      "description": "ATLAS",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:76"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:844"
      ],
      "purpose_detail": "An open source software tool for researchers to conduct scientific analyses on standardized observational data converted to the OMOP Common Data Model V5. Researchers can create cohorts by defining groups of people based on an exposure to a drug or diagnosis of a particular condition using healthcare claims data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://atlas-demo.ohdsi.org/#/home",
      "formal_specification": "https://github.com/OHDSI/Atlas"
    },
    {
      "id": "B2AI_STANDARD:732",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "AtriumDB",
      "description": "AtriumDB",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:64",
          "category": "B2AI:Application",
          "name": "High-Frequency Physiological Waveform Analysis",
          "description": "AtriumDB is used in AI applications for managing and analyzing high-frequency physiological waveform data at scale, enabling deep learning models for intensive care monitoring, perioperative risk prediction, and continuous vital sign analysis. The platform's efficient storage and query capabilities support training of neural networks on bedside monitor data including ECG, arterial blood pressure, and other high-resolution physiological signals collected over extended periods. AI systems leverage AtriumDB to access synchronized multi-parameter waveforms for developing early warning systems, detecting subtle physiological deterioration, and predicting adverse events in critically ill patients. The database's time-series optimization enables real-time AI inference on streaming waveform data.",
          "used_in_bridge2ai": false
        }
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:4",
        "B2AI_TOPIC:37"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:117"
      ],
      "purpose_detail": "A database of continuously-recorded physiological waveform data and other associated clinical and medical device data. Also the platform for storage and retrieval of clinical waveform data.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://laussenlabs.ca/atriumdb/",
      "publication": "doi:10.1088/1361-6579/ab7cb5"
    },
    {
      "id": "B2AI_STANDARD:733",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "ACHILLES",
      "description": "Automated Characterization of Health Information at Large-scale Longitudinal Evidence System",
      "related_to": [
        "B2AI_STANDARD:243"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:76"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:844"
      ],
      "purpose_detail": "Provides descriptive statistics on an OMOP CDM database.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://ohdsi.github.io/TheBookOfOhdsi/DataQuality.html#data-quality-checks",
      "formal_specification": "https://github.com/OHDSI/Achilles"
    },
    {
      "id": "B2AI_STANDARD:734",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "balance",
      "description": "balance package",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:55"
      ],
      "purpose_detail": "A Python package for balancing biased data samples.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://import-balance.org/",
      "formal_specification": "https://github.com/facebookresearch/balance"
    },
    {
      "id": "B2AI_STANDARD:735",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "BigQuery",
      "description": "BigQuery",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:37"
      ],
      "purpose_detail": "A fully managed, serverless data warehouse that enables scalable analysis over petabytes of data. It is a Platform as a Service (PaaS) that supports querying using ANSI SQL.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://cloud.google.com/bigquery",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:4"
      ]
    },
    {
      "id": "B2AI_STANDARD:736",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Biopython",
      "description": "Biopython",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "toolkit"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:1"
      ],
      "purpose_detail": "Biopython is a set of freely available tools for biological computation written in Python by an international team of developers. It is a distributed collaborative effort to develop Python libraries and applications which address the needs of current and future work in bioinformatics. The source code is made available under the Biopython License, which is extremely liberal and compatible with almost every license in the world.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://biopython.org/",
      "formal_specification": "https://github.com/biopython/biopython"
    },
    {
      "id": "B2AI_STANDARD:737",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Biotite",
      "description": "Biotite",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "toolkit"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:20"
      ],
      "purpose_detail": "This package bundles popular tasks in computational molecular biology into a uniform Python library.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.biotite-python.org/",
      "formal_specification": "https://github.com/biotite-dev/biotite"
    },
    {
      "id": "B2AI_STANDARD:738",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "BrainStat",
      "description": "BrainStat",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:22"
      ],
      "purpose_detail": "A toolbox for the statistical analysis and context decoding of neuroimaging data. It implements both univariate and multivariate linear models and interfaces with the BigBrain Atlas, Allen Human Brain Atlas and Nimare databases.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://brainstat.readthedocs.io/",
      "publication": "doi:10.1016/j.neuroimage.2022.119807",
      "formal_specification": "https://github.com/MICA-MNI/BrainStat"
    },
    {
      "id": "B2AI_STANDARD:739",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Cavatica",
      "description": "Cavatica data analysis platform",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:1"
      ],
      "purpose_detail": "A data analysis and sharing platform designed to accelerate discovery in a scalable, cloud-based compute environment where data, results, and workflows are shared among the world's research community. Developed by Seven Bridges and funded in-part by a grant from the National Institutes of Health (NIH) Common Fund, CAVATICA is continuously updated with new tools and datasets.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://www.cavatica.org/"
    },
    {
      "id": "B2AI_STANDARD:740",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Celery",
      "description": "Celery - Distributed Task Queue",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Celery is a simple, flexible, and reliable distributed system to process vast amounts of messages, while providing operations with the tools required to maintain such a system. Its a task queue with focus on real-time processing, while also supporting task scheduling.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://docs.celeryq.dev/en/latest/",
      "formal_specification": "https://github.com/celery/celery"
    },
    {
      "id": "B2AI_STANDARD:741",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "CLiXO",
      "description": "Clique eXtracted Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:21"
      ],
      "purpose_detail": "An updated version of the CliXO (Clique eXtracted Ontology) algorithm for inferring gene ontology terms from pairwise gene similarity data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/fanzheng10/CliXO-1.0",
      "publication": "doi:10.1126/science.abf3067",
      "formal_specification": "https://github.com/fanzheng10/CliXO-1.0"
    },
    {
      "id": "B2AI_STANDARD:742",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Comet",
      "description": "Comet",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Platform for managing machine learning models.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://www.comet.com/",
      "formal_specification": "https://github.com/comet-ml"
    },
    {
      "id": "B2AI_STANDARD:743",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "CDAPS",
      "description": "Community Detection APplication and Service",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:21"
      ],
      "purpose_detail": "Performs multiscale community detection and functional enrichment for network analysis through a service-oriented architecture. These features are provided by integrating popular community detection algorithms and enrichment tools. All the algorithms and tools run remotely on a dedicated server.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://cdaps.readthedocs.io/",
      "publication": "doi:10.1371/journal.pcbi.1008239",
      "formal_specification": "https://github.com/cytoscape/cy-community-detection"
    },
    {
      "id": "B2AI_STANDARD:744",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "CD-CLiXO",
      "description": "Community Detection CliXO",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:12",
        "B2AI_TOPIC:21"
      ],
      "purpose_detail": "Builds a CDAPS compatible community detection Docker image using CliXO.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/idekerlab/cdclixo",
      "formal_specification": "https://github.com/idekerlab/cdclixo"
    },
    {
      "id": "B2AI_STANDARD:745",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "CORAL",
      "description": "Contextual Ontology-based Repository Analysis Library",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "datamodel"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A framework for rigorous self-validated data modeling and integrative, reproducible data analysis",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/jmchandonia/CORAL",
      "publication": "doi:10.1093/gigascience/giac089",
      "formal_specification": "https://github.com/jmchandonia/CORAL"
    },
    {
      "id": "B2AI_STANDARD:746",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "CML library",
      "description": "Continuous Machine Learning",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Continuous Machine Learning (CML) is an open-source library for implementing continuous integration & delivery (CI/CD) in machine learning projects. Use it to automate parts of your development workflow, including model training and evaluation, comparing ML experiments across your project history, and monitoring changing datasets.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://cml.dev/",
      "formal_specification": "https://github.com/iterative/cml"
    },
    {
      "id": "B2AI_STANDARD:747",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Cromwell",
      "description": "Cromwell Workflow Management System",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Cromwell is an open-source Workflow Management System for bioinformatics.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://cromwell.readthedocs.io/en/stable/",
      "publication": "doi:10.7490/f1000research.1114634.1",
      "formal_specification": "https://github.com/broadinstitute/cromwell"
    },
    {
      "id": "B2AI_STANDARD:748",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Cytoscape",
      "description": "Cytoscape software",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:21"
      ],
      "purpose_detail": "An open source software platform for visualizing complex networks and integrating these with any type of attribute data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://cytoscape.org/",
      "formal_specification": "https://github.com/cytoscape/cytoscape"
    },
    {
      "id": "B2AI_STANDARD:749",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Dagster",
      "description": "Dagster",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Open source orchestration platform for the development, production, and observation of data assets.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://dagster.io/",
      "formal_specification": "https://github.com/dagster-io/dagster"
    },
    {
      "id": "B2AI_STANDARD:750",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "DANCE",
      "description": "DANCE platform",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "scrnaseqanalysis"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:34"
      ],
      "purpose_detail": "A Python toolkit to support deep learning models for analyzing single-cell gene expression at scale.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://omicsml.ai/",
      "formal_specification": "https://github.com/OmicsML/dance"
    },
    {
      "id": "B2AI_STANDARD:751",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "DRS",
      "description": "Data Repository Service",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:34"
      ],
      "purpose_detail": "The Data Repository Service (DRS) is a standard for describing and accessing data objects in a data repository. The DRS provides a RESTful API for accessing data objects, as well as a set of metadata fields for describing the data objects. The DRS is designed to be used in conjunction with the GA4GH Data Repository Service API, which provides a standard way to access data objects in a data repository.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://ga4gh.github.io/data-repository-service-schemas/preview/release/drs-1.2.0/docs/"
    },
    {
      "id": "B2AI_STANDARD:752",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "DVC",
      "description": "Data Version Control",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Data Version Control (DVC) is an open-source version control system for data science and machine learning projects. It is designed to help data scientists and machine learning engineers manage their data, models, and experiments in a reproducible and collaborative way.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://dvc.org/",
      "formal_specification": "https://github.com/iterative/dvc"
    },
    {
      "id": "B2AI_STANDARD:753",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "DataHub",
      "description": "DataHub",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "DataHub is a metadata platform. It provides a central place to manage and discover data assets, including datasets, dashboards, and machine learning models. DataHub is designed to be extensible and customizable.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://datahubproject.io/",
      "formal_specification": "https://github.com/linkedin/datahub"
    },
    {
      "id": "B2AI_STANDARD:754",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Datasette",
      "description": "Datasette",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Datasette is a comprehensive open-source tool ecosystem designed for exploring, analyzing, and publishing data through an intuitive web interface and accompanying JSON API. Created by Simon Willison, Datasette transforms data of any shape into interactive websites that enable users to perform exploratory data analysis, create visualizations, and share findings with colleagues without requiring extensive technical expertise. The platform excels in three primary use cases: exploratory data analysis (automatically detecting patterns in CSV, JSON, and database data), instant data publishing (using the `datasette publish` command to deploy data to cloud hosting providers like Google Cloud Run, Heroku, or Vercel), and rapid prototyping (spinning up JSON APIs in minutes for proof-of-concept development). Datasette is part of a broader ecosystem of 44 related tools and 154 plugins that enhance productivity when working with structured data. The tool supports multiple data import formats, provides built-in security features for data access control, and offers extensive customization options through its plugin architecture. Datasette serves data journalists, museum curators, archivists, local governments, scientists, researchers, and anyone seeking to make their data more accessible and discoverable through web-based interfaces.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://datasette.io/",
      "formal_specification": "https://github.com/simonw/datasette"
    },
    {
      "id": "B2AI_STANDARD:755",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "DENDRO",
      "description": "DENDRO",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "scrnaseqanalysis"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:34"
      ],
      "purpose_detail": "An analysis method for scRNA-seq data that clusters single cells into genetically distinct subclones and reconstructs the phylogenetic tree relating the subclones.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/zhouzilu/DENDRO",
      "publication": "doi:10.1186/s13059-019-1922-x",
      "formal_specification": "https://github.com/zhouzilu/DENDRO"
    },
    {
      "id": "B2AI_STANDARD:756",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Determined",
      "description": "Determined",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:66",
          "category": "B2AI:Application",
          "name": "Scalable ML Training Platform for Healthcare Research",
          "description": "Determined is used in biomedical AI for managing large-scale deep learning experiments, hyperparameter tuning, and distributed training across GPU clusters in research and healthcare organizations. AI teams leverage Determined's automated experiment tracking, resource scheduling, and hyperparameter optimization to efficiently develop medical imaging models, genomic prediction algorithms, and clinical NLP systems. The platform provides reproducible experiment management with automatic checkpointing, fault tolerance for long-running medical imaging training jobs, and collaborative features for multi-institutional research teams. Determined's integration with healthcare security requirements and support for diverse model frameworks makes it valuable for organizations scaling from research to production clinical AI.",
          "used_in_bridge2ai": false,
          "references": [
            "https://docs.determined.ai/"
          ]
        }
      ],
      "collection": [
        "machinelearningframework"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Determined is an open-source deep learning training platform.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.determined.ai/",
      "formal_specification": "https://github.com/determined-ai/determined"
    },
    {
      "id": "B2AI_STANDARD:757",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Dicoogle",
      "description": "Dicoogle Picture Archiving and Communications System",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "purpose_detail": "Dicoogle is an open source Picture Archiving and Communications System (PACS) archive. Its modular architecture allows the quick development of new functionalities, due the availability of a Software Development Kit (SDK).",
      "is_open": true,
      "requires_registration": false,
      "url": "https://dicoogle.com/",
      "publication": "doi:10.1109/ISCC50000.2020.9219545",
      "formal_specification": "https://github.com/bioinformatics-ua/dicoogle"
    },
    {
      "id": "B2AI_STANDARD:758",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "DigitalOcean",
      "description": "DigitalOcean",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "cloudservice"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "DigitalOcean is a comprehensive cloud infrastructure platform that provides scalable computing resources designed to simplify cloud deployment for developers, startups, and enterprises. The platform offers a complete suite of cloud services including Droplets (virtual machines), Kubernetes clusters, App Platform for application deployment, managed databases, object storage (Spaces), load balancers, and specialized AI/ML infrastructure including GPU-powered Droplets with NVIDIA H100, H200, and AMD MI325X accelerators. DigitalOcean distinguishes itself through developer-friendly interfaces, predictable pricing without bandwidth overage charges, and extensive community-contributed tutorials and documentation. The platform serves over 600,000 customers ranging from individual developers to large enterprises, offering both self-service infrastructure management and managed services with premium support options. DigitalOcean's global data center network provides 99.99% uptime SLAs and supports diverse use cases including web hosting, application development, AI inference workloads, container orchestration, and data processing pipelines, making it particularly attractive for cost-conscious organizations seeking reliable cloud infrastructure without the complexity of larger cloud providers.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://www.digitalocean.com/"
    },
    {
      "id": "B2AI_STANDARD:759",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "DoubletDecon",
      "description": "DoubletDecon",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "scrnaseqanalysis"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:34"
      ],
      "purpose_detail": "An approach that detects doublet cell capture artifacts in scRNA-seq data with a combination of deconvolution analyses and the identification of unique cell-state gene expression.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/EDePasquale/DoubletDecon",
      "publication": "doi:10.1016/j.celrep.2019.09.082",
      "formal_specification": "https://github.com/EDePasquale/DoubletDecon"
    },
    {
      "id": "B2AI_STANDARD:760",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Egeria",
      "description": "Egeria",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Open metadata and governance for enterprises - automatically capturing, managing and exchanging metadata between tools and platforms, no matter the vendor.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://egeria-project.org/",
      "formal_specification": "https://github.com/odpi/egeria"
    },
    {
      "id": "B2AI_STANDARD:761",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Eido",
      "description": "Eido",
      "related_to": [
        "B2AI_STANDARD:260",
        "B2AI_STANDARD:345"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Eido is used to 1) validate or 2) convert format of sample metadata. Sample metadata is stored according to the standard PEP specification. For validation, eido is based on JSON Schema and extends it with new features, like required input files.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://eido.databio.org/",
      "publication": "doi:10.1093/gigascience/giab077",
      "formal_specification": "https://github.com/pepkit/eido"
    },
    {
      "id": "B2AI_STANDARD:762",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "ENHANCE",
      "description": "Expression denoising heuristic using aggregation of neighbors and principal component extraction",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "scrnaseqanalysis"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:34"
      ],
      "purpose_detail": "ENHANCE is a computational method for accurate denoising of single-cell RNA-sequencing data that addresses technical noise while preserving biological signal. The algorithm uses k-nearest neighbor aggregation to identify similar cells, followed by principal component analysis (PCA) to remove noise components while retaining significant biological variation. The method automatically determines the optimal number of neighbors (k) based on target transcript counts and identifies significant principal components using variance fold-thresholding against simulated noise datasets. ENHANCE processes UMI-count matrices in tab-separated format, supports gzip compression, and provides configurable parameters for neighbor selection, PC threshold determination, and precision control. The tool enhances downstream analyses such as clustering, trajectory inference, and differential expression by reducing technical artifacts while maintaining cell type distinctions and biological relationships in single-cell transcriptomic data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/yanailab/enhance",
      "publication": "doi:10.1101/655365",
      "formal_specification": "https://github.com/yanailab/enhance"
    },
    {
      "id": "B2AI_STANDARD:763",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "FDP",
      "description": "FAIR Data Point",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "FAIR Data Point (FDP) is a REST API for creating, storing, and serving FAIR metadata. This FDP implementation also presents a Web-based graphical user interface (GUI). The metadata contents are generated semi-automatically according to the FAIR Data Point software specification document.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/FAIRDataTeam/FAIRDataPoint",
      "publication": "doi:10.1162/dint_a_00160",
      "formal_specification": "https://github.com/FAIRDataTeam/FAIRDataPoint"
    },
    {
      "id": "B2AI_STANDARD:764",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "FAIRSCAPE",
      "description": "FAIRSCAPE digital commons framework",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:67",
          "category": "B2AI:Application",
          "name": "FAIR ML Workflows and Computational Reproducibility",
          "description": "FAIRSCAPE is used in AI applications to create Findable, Accessible, Interoperable, and Reusable machine learning workflows with comprehensive provenance tracking and metadata management. AI researchers leverage FAIRSCAPE to package complete ML pipelines including data preprocessing, model training, validation, and deployment with detailed computational provenance, enabling reproducible AI research and regulatory compliance. The framework supports automated metadata capture during model development, versioning of datasets and models, and generation of computational notebooks that document the entire ML lifecycle. FAIRSCAPE is particularly valuable for multi-institutional AI studies where workflow transparency, data lineage, and reproducibility are essential for scientific validity and clinical translation.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "implementation_maturity_pilot"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:116"
      ],
      "purpose_detail": "A reusable computational framework, enabling simplified access to modern scalable cloud-based components. FAIRSCAPE fully implements the FAIR data principles and extends them to provide fully FAIR Evidence, including machine-interpretable provenance of datasets, software and computations, as metadata for all computed results.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://fairscape.github.io/",
      "publication": "doi:10.1007/s12021-021-09529-4",
      "formal_specification": "https://github.com/fairscape/fairscape"
    },
    {
      "id": "B2AI_STANDARD:765",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "FastAI",
      "description": "FastAI",
      "related_to": [
        "B2AI_STANDARD:816"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:68",
          "category": "B2AI:Application",
          "name": "Rapid Medical AI Prototyping and Transfer Learning",
          "description": "FastAI is used in biomedical research for rapid prototyping and training of deep learning models with minimal code, particularly valuable for researchers without extensive ML engineering backgrounds. Healthcare researchers leverage FastAI's high-level APIs, best-practice defaults, and powerful transfer learning capabilities to quickly develop models for medical image classification, clinical text analysis, and tabular clinical data prediction. The library's sophisticated learning rate scheduling, progressive resizing, and mixed precision training enable efficient use of computational resources, while its extensive documentation and educational materials democratize AI development in medicine. FastAI is particularly popular for exploratory research, kaggle-style medical AI competitions, and educational settings teaching clinical AI.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "machinelearningframework"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/fastai/fastai",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:33"
      ]
    },
    {
      "id": "B2AI_STANDARD:766",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Galaxy",
      "description": "Galaxy",
      "related_to": [
        "B2AI_STANDARD:334"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "toolkit"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:20"
      ],
      "purpose_detail": "Galaxy is an open source, web-based platform for data intensive biomedical research.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://usegalaxy.org/",
      "publication": "doi:10.1093/nar/gky379",
      "formal_specification": "https://github.com/galaxyproject/galaxy"
    },
    {
      "id": "B2AI_STANDARD:767",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "GCP",
      "description": "Google Cloud Platform",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:69",
          "category": "B2AI:Application",
          "name": "Cloud-Based Biomedical AI Infrastructure and Model Deployment",
          "description": "Google Cloud Platform is extensively used in AI applications for scalable biomedical data processing, large-scale model training, and deployment of clinical AI systems with healthcare-compliant infrastructure. AI researchers leverage GCP's specialized services including Vertex AI for managed ML pipelines, BigQuery for analyzing massive clinical datasets, and Cloud Healthcare API for FHIR-based data integration. The platform enables distributed training of deep learning models on genomic sequences, medical images, and electronic health records at population scale, while providing HIPAA-compliant infrastructure for clinical AI deployment. GCP's TPU infrastructure accelerates training of large biomedical language models and computer vision systems, and its AutoML capabilities democratize AI development for healthcare institutions.",
          "used_in_bridge2ai": false,
          "references": [
            "https://cloud.google.com/healthcare-api",
            "https://cloud.google.com/vertex-ai"
          ]
        }
      ],
      "collection": [
        "cloudservice"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:37",
        "B2AI_ORG:115"
      ],
      "purpose_detail": "Google Cloud Platform (GCP) is a comprehensive cloud computing platform offering over 200+ services spanning compute, storage, databases, networking, AI/machine learning, data analytics, and developer tools across Google's global infrastructure. Core compute offerings include Compute Engine VMs, App Engine for platform-as-a-service deployment, Google Kubernetes Engine for container orchestration, and Cloud Functions for serverless computing. For AI and machine learning, GCP provides Vertex AI as a unified platform for building, deploying, and scaling ML models, including access to Google's Gemini large language models, AutoML capabilities, and specialized hardware like Tensor Processing Units (TPUs) for accelerated training. Storage solutions include Cloud Storage for object storage, persistent disks for block storage, and Filestore for managed file storage. Database services encompass Cloud SQL for managed relational databases (MySQL, PostgreSQL, SQL Server), Cloud Spanner for globally distributed relational databases, BigQuery for serverless data warehousing and analytics, Firestore for NoSQL document databases, and Bigtable for wide-column NoSQL. Healthcare and life sciences researchers benefit from specialized services like Healthcare API for FHIR-based health data exchange, Life Sciences API for genomics pipeline execution, and Cloud Healthcare Console for managing sensitive patient data with HIPAA and HITRUST compliance. BigQuery ML enables researchers to create and execute machine learning models using SQL queries directly on large biomedical datasets. GCP is explicitly used in Bridge2AI projects for scalable biomedical data processing, AI model training and deployment, and collaborative research infrastructure. The platform emphasizes security with encryption at rest and in transit, IAM for access control, VPC for network isolation, and compliance certifications including HIPAA, FedRAMP, ISO 27001, and SOC 2. GCP's global network spans 40+ regions and 120+ edge locations, providing low-latency access and data residency options for international research collaborations. Integration with Google Workspace, Looker for business intelligence, and open-source frameworks like TensorFlow and PyTorch facilitates comprehensive workflows from data ingestion through analysis, visualization, and publication.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://cloud.google.com/"
    },
    {
      "id": "B2AI_STANDARD:768",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "GrAPE",
      "description": "Graph Representation leArning, Predictions and Evaluation library",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:70",
          "category": "B2AI:Application",
          "name": "Graph Embedding and Network Medicine AI",
          "description": "GrAPE (Graph Automatic Programming Environment) is used in biomedical AI for creating graph embeddings and training graph neural networks on biological networks, enabling applications in drug discovery, protein function prediction, and disease gene prioritization. Researchers leverage GrAPE's efficient graph processing capabilities to learn low-dimensional representations of large-scale biological networks including protein-protein interactions, gene regulatory networks, and knowledge graphs. The tool supports AI applications that predict novel drug-target interactions, identify disease modules, and perform link prediction in biomedical knowledge graphs. GrAPE's optimization for large graphs enables training on genome-scale networks and integration of multi-omics data through graph-based representations.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1093/gigascience/giab044"
          ]
        }
      ],
      "collection": [
        "graphdataplatform",
        "machinelearningframework"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:21"
      ],
      "purpose_detail": "A fast graph processing and embedding library, designed to scale with big graphs and to run on both off-the-shelf laptop and desktop computers and High Performance Computing clusters of workstations.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/AnacletoLAB/grape",
      "publication": "doi:10.48550/arXiv.2110.06196",
      "formal_specification": "https://github.com/AnacletoLAB/grape",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:14",
        "B2AI_SUBSTRATE:15"
      ]
    },
    {
      "id": "B2AI_STANDARD:769",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "HESML",
      "description": "Half-Edge Semantic Measures Library",
      "related_to": [
        "B2AI_STANDARD:553"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:32",
        "B2AI_TOPIC:16"
      ],
      "purpose_detail": "HESML is an efficient, scalable and large Java software library of ontology-based semantic similarity measures and Information Content (IC) models based on WordNet, SNOMED-CT, MeSH or any other OBO-based ontology.",
      "is_open": true,
      "requires_registration": false,
      "url": "http://hesml.lsi.uned.es/",
      "publication": "doi:10.1186/S12859-021-04539-0",
      "formal_specification": "https://github.com/jjlastra/HESML"
    },
    {
      "id": "B2AI_STANDARD:770",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Hangar",
      "description": "Hangar",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Hangar is a version control system specifically designed for managing tensor data and numerical arrays in machine learning and scientific computing workflows, providing Git-like versioning capabilities for datasets that are too large or complex for traditional version control systems. Built with Python and optimized for performance, Hangar enables data scientists and ML engineers to track changes in training datasets, model weights, embeddings, and experimental results with full history, branching, merging, and collaboration features. The system uses content-addressable storage with automatic data deduplication, storing only changed array blocks rather than entire files, making it efficient for large multidimensional datasets common in deep learning (image datasets, genomic sequences, time series, medical imaging volumes). Hangar provides atomic commits ensuring data consistency, supports concurrent read access for distributed training, and enables reproducible machine learning by linking dataset versions to specific model training runs. Key features include automatic detection of array schema changes, efficient storage of sparse arrays, lazy loading for memory-efficient data access, and integration with NumPy, PyTorch, and TensorFlow workflows. Applications span ML experiment tracking where different dataset versions are tested with model architectures, collaborative dataset curation where multiple researchers contribute annotations or corrections, model debugging through inspection of training data that caused specific failures, and regulatory compliance in medical AI where dataset provenance and versioning history must be maintained. Hangar addresses critical gaps in ML operations by providing reproducibility (exact dataset version used for published results), collaboration (merge dataset contributions from multiple sources), and auditability (full history of dataset modifications with timestamps and contributors), essential for rigorous scientific research and production ML systems.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://hangar-py.readthedocs.io/en/stable/",
      "formal_specification": "https://github.com/tensorwerk/hangar-py",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:42"
      ]
    },
    {
      "id": "B2AI_STANDARD:771",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "HADES",
      "description": "Health Analytics Data-to-Evidence Suite",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:76"
      ],
      "purpose_detail": "HADES (formally known as the OHDSI Methods Library) is a set of open source R packages for large scale analytics, including population characterization, population-level causal effect estimation, and patient-level prediction.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://ohdsi.github.io/Hades/",
      "formal_specification": "https://github.com/OHDSI/Hades"
    },
    {
      "id": "B2AI_STANDARD:772",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "HAIM",
      "description": "Holistic AI in Medicine framework",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:71",
          "category": "B2AI:Application",
          "name": "Holistic Multi-Modal Healthcare AI Integration",
          "description": "HAIM (Holistic AI in Medicine) framework is used for developing and evaluating multi-modal AI systems that integrate diverse healthcare data types including imaging, time-series, tabular clinical data, and text for comprehensive patient assessment. The framework enables training of models that leverage complementary information across modalities to improve diagnostic accuracy, risk prediction, and clinical decision support beyond single-modality approaches. HAIM provides standardized methods for multi-modal data fusion, handles missing modalities gracefully, and enables systematic evaluation of how different data types contribute to model performance. Applications include integrated ICU monitoring systems, comprehensive cancer diagnosis combining radiology and pathology, and patient deterioration prediction using all available clinical data streams.",
          "used_in_bridge2ai": false,
          "references": [
            "https://doi.org/10.1038/s41746-022-00689-4"
          ]
        }
      ],
      "collection": [
        "multimodal"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A unified Holistic AI in Medicine (HAIM) framework to facilitate the generation and testing of AI systems that leverage multimodal inputs.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/lrsoenksen/HAIM",
      "publication": "doi:10.1038/s41746-022-00689-4",
      "formal_specification": "https://github.com/lrsoenksen/HAIM"
    },
    {
      "id": "B2AI_STANDARD:773",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "HTSeq",
      "description": "HTSeq",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "A Python library to facilitate the rapid development of high throughput sequencing data analysis scripts. HTSeq offers parsers for many common data formats in HTS projects, as well as classes to represent data, such as genomic coordinates, sequences, sequencing reads, alignments, gene model information and variant calls, and provides data structures that allow for querying via genomic coordinates.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/htseq/htseq",
      "publication": "doi:10.1093/bioinformatics/btu638",
      "formal_specification": "https://github.com/htseq/htseq"
    },
    {
      "id": "B2AI_STANDARD:774",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "ImJoy",
      "description": "ImJoy",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "purpose_detail": "A plugin powered hybrid computing platform for deploying deep learning applications such as advanced image analysis tools. ImJoy runs on mobile and desktop environment cross different operating systems, plugins can run in the browser, localhost, remote and cloud servers.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://imjoy.io/",
      "publication": "doi:10.1038/s41592-019-0627-0",
      "formal_specification": "https://github.com/imjoy-team/ImJoy"
    },
    {
      "id": "B2AI_STANDARD:775",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "i2b2",
      "description": "Informatics for Integrating Biology and the Bedside platform",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:42"
      ],
      "purpose_detail": "A system for searching and exchanging clinical data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.i2b2.org/software/index.html",
      "publication": "doi:10.1093/jamia/ocv188",
      "formal_specification": "https://github.com/i2b2"
    },
    {
      "id": "B2AI_STANDARD:776",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "ITK",
      "description": "Insight Segmentation and Registration Toolkit",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "purpose_detail": "The Insight Toolkit (ITK) is an open-source, cross-platform toolkit for N-dimensional scientific image processing, segmentation, and registration.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://itk.org/",
      "formal_specification": "https://github.com/InsightSoftwareConsortium/ITK"
    },
    {
      "id": "B2AI_STANDARD:777",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "IMPatienT",
      "description": "Integrated digital Multimodal PATIENt daTa framework",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:9",
        "B2AI_TOPIC:15"
      ],
      "purpose_detail": "A free and open-source web application to digitize, process and explore multimodal patient data. IMPatienT has a modular architecture, including four components to (i) create a standard vocabulary for a domain, (ii) digitize and process free-text data by mapping it to a set of standard terms, (iii) annotate images and perform image segmentation, and (iv) generate an automatic visualization dashboard to provide insight on the data and perform automatic diagnosis suggestions.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://impatient.lbgi.fr",
      "formal_specification": "https://github.com/lambda-science/IMPatienT"
    },
    {
      "id": "B2AI_STANDARD:778",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "IMP",
      "description": "Integrative Modeling Platform",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:27"
      ],
      "purpose_detail": "An open source C++ and Python toolbox for solving complex modeling problems, and a number of applications for tackling some common problems in a user-friendly way. IMP can also be used from the Chimera molecular modeling system, or via one of several web applications.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://integrativemodeling.org/",
      "formal_specification": "https://github.com/salilab/imp"
    },
    {
      "id": "B2AI_STANDARD:779",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Jupyter",
      "description": "Jupyter Notebook",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "notebookplatform"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A web-based notebook environment for interactive computing.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://jupyter.org/",
      "formal_specification": "https://github.com/jupyter/notebook"
    },
    {
      "id": "B2AI_STANDARD:780",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Kaldi",
      "description": "Kaldi Speech Recognition Toolkit",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:37"
      ],
      "purpose_detail": "Kaldi is an open-source toolkit for speech recognition research and development, written primarily in C++ with scripting interfaces, providing comprehensive implementations of state-of-the-art automatic speech recognition (ASR) algorithms including deep neural networks (DNNs), hidden Markov models (HMMs), Gaussian mixture models (GMMs), and modern end-to-end architectures. Developed by researchers at Johns Hopkins University and maintained by a global community, Kaldi emphasizes flexibility, efficiency, and reproducibility in speech recognition pipelines. The toolkit provides modular components for acoustic feature extraction (MFCCs, PLPs, filterbank energies, pitch features), acoustic modeling (DNN-HMM hybrid systems, time-delay neural networks TDNNs, chain models with lattice-free MMI training), language modeling (n-gram models, recurrent neural network language models), and decoding (weighted finite-state transducers WFSTs for efficient search). Kaldi supports both speaker-independent and speaker-adaptive recognition, with tools for vocal tract length normalization, feature-space maximum likelihood linear regression (fMLLR), and i-vector/x-vector speaker embeddings for robust recognition across diverse acoustic conditions. The toolkit includes recipes for training ASR systems on standard benchmarks (LibriSpeech, WSJ, Switchboard, Fisher, TED-LIUM, AMI, CHiME) with documented pipelines enabling researchers to replicate published results and adapt models to new datasets and languages. Applications span transcription of clinical conversations for medical documentation, analysis of patient-physician interactions, voice biomarker extraction for neurological disease monitoring (Parkinson's, ALS, dementia), mental health assessment through speech characteristics, and accessibility tools for hearing-impaired patients. Kaldi's extensive documentation, active mailing list, and well-tested recipes make it accessible for both research and production deployment despite its complexity, bridging traditional statistical ASR approaches with modern deep learning methods.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://kaldi-asr.org/",
      "formal_specification": "https://github.com/kaldi-asr/kaldi"
    },
    {
      "id": "B2AI_STANDARD:781",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Keepsake",
      "description": "Keepsake",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Version control for machine learning. A Python library that uploads files and metadata (like hyperparameters) to Amazon S3 or Google Cloud Storage.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://keepsake.ai/",
      "formal_specification": "https://github.com/replicate/keepsake"
    },
    {
      "id": "B2AI_STANDARD:782",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "khmer",
      "description": "khmer",
      "related_to": [
        "B2AI_STANDARD:183"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "khmer is a free, open-source software library and suite of command-line tools for efficient analysis of high-throughput DNA sequencing data, including genomes, transcriptomes, metagenomes, and single-cell datasets. It implements advanced algorithms for probabilistic k-mer counting, digital normalization, compressible De Bruijn graph representation, and graph partitioning, enabling scalable de novo assembly, error correction, and data reduction. khmer is designed for use in UNIX environments and is supported by extensive documentation, community protocols, and integration with popular bioinformatics workflows. It is widely used for preprocessing and quality control in large-scale sequencing projects.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://khmer.readthedocs.io/",
      "publication": "doi:10.12688/f1000research.6924.1",
      "formal_specification": "https://github.com/dib-lab/khmer/"
    },
    {
      "id": "B2AI_STANDARD:783",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Koza",
      "description": "Koza data transformation framework",
      "related_to": [
        "B2AI_STANDARD:346"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:58"
      ],
      "purpose_detail": "Transform csv, json, yaml, jsonl, and xml and converting them to a target csv, json, or jsonl format based on your dataclass model. Koza also can output data in the KGX format. Write data transforms in semi-declarative Python. Configure source files, expected columns/json properties and path filters, field filters, and metadata in yaml. Create or import mapping files to be used in ingests (eg id mapping, type mappings). Create and use translation tables to map between source and target vocabularies.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/monarch-initiative/koza",
      "formal_specification": "https://github.com/monarch-initiative/koza",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:6"
      ]
    },
    {
      "id": "B2AI_STANDARD:784",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "LIRICAL",
      "description": "LIkelihood Ratio Interpretation of Clinical AbnormaLities",
      "related_to": [
        "B2AI_STANDARD:468"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:7",
        "B2AI_TOPIC:25",
        "B2AI_TOPIC:35"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:58"
      ],
      "purpose_detail": "Performs phenotype-driven prioritization of candidate diseases and genes in the setting of genomic diagnostics (exome or genome) in which the phenotypic abnormalities are described as Human Phenotype Ontology (HPO) terms.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://lirical.readthedocs.io/",
      "publication": "doi:10.1016/j.ajhg.2020.06.021",
      "formal_specification": "https://github.com/TheJacksonLaboratory/LIRICAL"
    },
    {
      "id": "B2AI_STANDARD:785",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Linode",
      "description": "Linode",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "cloudservice"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Linode (now part of Akamai Connected Cloud) is a developer-focused cloud computing platform emphasizing simplicity, predictable pricing, and high performance for compute, storage, and networking services across a globally distributed infrastructure. Core compute offerings include Essential Compute instances (Shared CPU, Dedicated CPU, High Memory, and Premium CPU), GPU instances for parallel processing workloads including machine learning and scientific computing, and Accelerated Compute for specialized workloads. Container orchestration is provided through Linode Kubernetes Engine (LKE) for managed Kubernetes clusters, and App Platform for rapid cloud-native application deployment with automated infrastructure management. Storage solutions encompass Block Storage for scalable persistent volumes, Object Storage with S3-compatible API for unstructured data, and automated Backups for data protection. Managed Databases support MySQL and PostgreSQL with automated maintenance, backups, and high availability configurations. Networking features include NodeBalancers for load distribution, Cloud Firewall for security, DNS Manager, Private Networking (VLAN), and DDoS protection. Linode differentiates itself through transparent flat pricing bundling CPU, transfer, storage, and RAM without hidden egress fees or complex tier structures, making it particularly attractive for budget-conscious researchers and startups. The platform provides comprehensive API access, CLI tools, Terraform provider, and extensive documentation for programmatic infrastructure management. Following acquisition by Akamai in 2022, Linode benefits from integration with Akamai's global CDN network and security capabilities while maintaining its developer-friendly approach and competitive pricing. Linode's global presence includes data centers across North America, Europe, Asia-Pacific, and emerging markets, providing edge computing capabilities and reduced latency for distributed applications. The platform offers free DDoS protection, 24/7/365 support, and a Cloud Computing Foundations Certification program for developer education. While more streamlined than hyperscale competitors, Linode provides essential cloud services suitable for web applications, data pipelines, containerized workloads, and computational research without vendor lock-in through standard open-source technologies.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://www.linode.com/"
    },
    {
      "id": "B2AI_STANDARD:786",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "MAGIC",
      "description": "Markov Affinity-based Graph Imputation of Cells",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "scrnaseqanalysis"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:34"
      ],
      "purpose_detail": "A method for imputing missing values in scRNA-seq data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.krishnaswamylab.org/projects/magic",
      "publication": "doi:10.1016/j.cell.2018.05.061",
      "formal_specification": "https://github.com/KrishnaswamyLab/MAGIC"
    },
    {
      "id": "B2AI_STANDARD:787",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Marquez",
      "description": "Marquez",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Marquez is an open source metadata service for the collection, aggregation, and visualization of a data ecosystem's metadata. It maintains the provenance of how datasets are consumed and produced, provides global visibility into job runtime and frequency of dataset access, centralization of dataset lifecycle management, and much more. Marquez was released and open sourced by WeWork.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://lfaidata.foundation/projects/marquez/",
      "formal_specification": "https://github.com/MarquezProject/marquez"
    },
    {
      "id": "B2AI_STANDARD:788",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "MONAI",
      "description": "Medical Open Network for AI",
      "related_to": [
        "B2AI_STANDARD:816"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:72",
          "category": "B2AI:Application",
          "name": "Medical Imaging Deep Learning Framework and Workflows",
          "description": "MONAI (Medical Open Network for AI) is a specialized PyTorch-based framework extensively used for developing deep learning models on 3D medical imaging data across radiology, pathology, and microscopy. Researchers and clinical AI developers leverage MONAI's domain-specific transforms (intensity normalization, resampling, augmentation), pretrained models for medical imaging tasks, and optimized data loaders for large 3D volumes to accelerate development of segmentation, classification, and detection models. The framework provides standardized workflows for common medical imaging AI tasks, federated learning capabilities for multi-institutional collaboration, and Auto3DSeg for automated model development. MONAI's integration with clinical imaging standards and deployment tools makes it a bridge between research and clinical implementation.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "machinelearningframework"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "MONAI is a PyTorch-based, open-source framework for deep learning in healthcare imaging, part of PyTorch Ecosystem.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://monai.io/",
      "formal_specification": "https://github.com/Project-MONAI/MONAI",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:33"
      ]
    },
    {
      "id": "B2AI_STANDARD:789",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Metacat",
      "description": "Metacat",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:65"
      ],
      "purpose_detail": "Metacat is a unified metadata exploration API service. You can explore Hive, RDS, Teradata, Redshift, S3 and Cassandra. Metacat provides you information about what data you have, where it resides and how to process it.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/Netflix/metacat",
      "formal_specification": "https://github.com/Netflix/metacat"
    },
    {
      "id": "B2AI_STANDARD:790",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Azure",
      "description": "Microsoft Azure",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "cloudservice"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:56"
      ],
      "purpose_detail": "Microsoft Azure is a comprehensive enterprise-grade cloud computing platform offering 200+ services spanning compute, storage, databases, networking, AI/machine learning, analytics, IoT, and developer tools with deep integration into Microsoft's enterprise ecosystem. Core compute services include Azure Virtual Machines for Windows and Linux workloads, Azure App Service for web application hosting, Azure Kubernetes Service (AKS) for container orchestration, Azure Container Instances for serverless containers, and Azure Functions for event-driven serverless computing. For AI and machine learning, Azure provides Azure AI Studio (including Microsoft Foundry for AI agent development), Azure Machine Learning for end-to-end ML workflows, Azure Cognitive Services for pre-trained AI models (vision, speech, language), and Azure OpenAI Service for accessing GPT, Codex, and DALL-E models. Data and analytics capabilities include Azure Synapse Analytics for unified data warehousing and big data analytics, Azure Databricks for Apache Spark-based analytics, Azure Data Lake for scalable data storage, and Microsoft Fabric for unified data platform integration. Database services encompass Azure SQL Database for managed relational databases, Azure Cosmos DB for globally distributed NoSQL, Azure Database for PostgreSQL/MySQL/MariaDB, and Azure DocumentDB for MongoDB-compatible document storage. Healthcare and life sciences benefit from Azure Health Data Services for FHIR-based health data management, Azure Genomics for scalable genomics pipeline execution, Azure Healthcare APIs, and compliance with HIPAA, HITRUST, GxP, and FDA 21 CFR Part 11 regulations. Researchers leverage Azure Machine Learning for model training with GPU/FPGA acceleration, automated ML (AutoML) capabilities, MLOps for model lifecycle management, and integration with popular frameworks like TensorFlow, PyTorch, and scikit-learn. Azure's hybrid cloud capabilities through Azure Arc enable consistent management across on-premises, multi-cloud, and edge environments, crucial for institutions with existing infrastructure investments. Enterprise integration with Active Directory, Microsoft 365, Windows Server, SQL Server, and Visual Studio provides seamless authentication, collaboration, and development workflows. Azure's global infrastructure spans 60+ regions worldwide with data residency and compliance options for international regulations. Security features include Azure Security Center, Azure Sentinel for SIEM, Azure Active Directory for identity management, encryption at rest and in transit, and extensive compliance certifications across healthcare, government, and financial sectors.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://azure.microsoft.com/"
    },
    {
      "id": "B2AI_STANDARD:791",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "MinIO",
      "description": "MinIO",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "cloudservice"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "MinIO is a high-performance object storage system that implements the Amazon S3 API for cloud-native environments. The system is designed as software-defined storage and provides S3-compatible operations with reported latency under 10ms and high throughput capabilities. MinIO is designed to run on Kubernetes and can be deployed across public cloud platforms, private cloud infrastructure, and edge environments. The architecture uses a distributed design that supports scaling beyond traditional storage system limits, with deployment capabilities from single servers to exabyte-scale installations using a single namespace. The platform includes automated data management features such as self-healing, load balancing, and data protection with erasure coding. Security features include encryption at rest and in transit, identity and access management integration, and policy-based access controls. MinIO is used in AI and machine learning workflows, providing storage integration with frameworks like PyTorch and data lakehouse technologies including Apache Iceberg. The system supports data lakehouse analytics engines such as Apache Spark and Trino for structured and unstructured data processing. Client libraries are available for multiple programming languages including Go, Python, Java, .NET, Rust, and JavaScript. Associated tools include mc (command-line client for object operations), DirectPV (a Kubernetes CSI driver), and a Kubernetes operator for cluster deployment and management. The source code is released under the GNU AGPLv3 license. The project reports over 2 billion downloads and maintains an active development community with more than 50,000 stars on GitHub.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://min.io/",
      "formal_specification": "https://github.com/minio/"
    },
    {
      "id": "B2AI_STANDARD:792",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "MLMD",
      "description": "ML Metadata",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:37"
      ],
      "purpose_detail": "ML Metadata (MLMD) is a library for recording and retrieving metadata associated with ML developer and data scientist workflows.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.tensorflow.org/tfx/guide/mlmd",
      "formal_specification": "https://github.com/google/ml-metadata"
    },
    {
      "id": "B2AI_STANDARD:793",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "MLflow",
      "description": "MLflow platform",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "MLflow is an open source platform to manage the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry",
      "is_open": true,
      "requires_registration": false,
      "url": "https://mlflow.org/",
      "formal_specification": "https://github.com/mlflow/mlflow/"
    },
    {
      "id": "B2AI_STANDARD:794",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "MLPro",
      "description": "MLPro framework",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:73",
          "category": "B2AI:Application",
          "name": "Reinforcement Learning Framework for Adaptive Healthcare Systems",
          "description": "MLPro is used in biomedical AI for developing reinforcement learning agents that learn optimal treatment strategies, adaptive monitoring protocols, and personalized intervention policies through interaction with healthcare environments. Researchers leverage MLPro's modular framework to implement and compare different RL algorithms for applications like dynamic treatment regimen optimization, adaptive clinical trial design, and automated ICU management. The platform supports simulation-based learning where agents train on synthetic patients before clinical deployment, multi-agent systems for coordinated care, and safe exploration strategies essential for healthcare applications. MLPro enables systematic evaluation of RL approaches for sequential decision-making problems in medicine where actions have long-term consequences.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "machinelearningframework"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "MLPro (Machine Learning Professional) is a middleware framework for standardized machine learning development in Python. The framework provides an object-oriented architecture that enables the integration and standardization of multiple machine learning paradigms through reusable process models and templates. MLPro consists of several sub-frameworks organized as modules: MLPro-BF (Basic Functions) provides cross-sectional infrastructure including mathematics, data management, plotting capabilities, and logging; MLPro-SL supports supervised learning workflows; MLPro-RL implements reinforcement learning processes; MLPro-GT covers game theory applications; and MLPro-OA focuses on online adaptive machine learning. The architecture uses standardized interfaces that allow ML models to be embedded into training pipelines and operational workflows in a modular fashion. The framework integrates with external machine learning libraries including River (for online learning), scikit-learn (for supervised learning), and OpenML (for dataset and experiment management), providing wrappers that standardize their usage within MLPro processes. Development follows object-oriented design principles with test-driven development for quality assurance and continuous integration/deployment (CI/CD) practices. The framework includes an extension hub for third-party integrations, comprehensive documentation with API references, and an example pool demonstrating various use cases. MLPro supports hybrid ML applications that combine multiple learning paradigms and real-time adaptive systems. The project is developed and maintained by the Group for Automation Technology and Learning Systems at South Westphalia University of Applied Sciences in Germany. The source code is released under the Apache 2.0 license and is available through PyPI for installation.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://mlpro.readthedocs.io/",
      "publication": "doi:10.1016/j.simpa.2022.100421",
      "formal_specification": "https://github.com/fhswf/MLPro"
    },
    {
      "id": "B2AI_STANDARD:795",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "MoClust",
      "description": "MoClust",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "scrnaseqanalysis"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:23"
      ],
      "purpose_detail": "A novel joint clustering framework that can be applied to several types of single-cell multi-omics data. A selective automatic doublet detection module that can identify and filter out doublets is introduced in the pretraining stage to improve data quality. Omics-specific autoencoders are introduced to characterize the multi-omics data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://zenodo.org/record/7306504",
      "publication": "doi:10.1093/bioinformatics/btac736"
    },
    {
      "id": "B2AI_STANDARD:796",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "MCT",
      "description": "Model Card Toolkit",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "modelcards"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:37"
      ],
      "purpose_detail": "The Model Card Toolkit (MCT) streamlines and automates generation of Model Cards, machine learning documents that provide context and transparency into a model's development and performance.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/tensorflow/model-card-toolkit",
      "publication": "doi:10.48550/arXiv.1810.03993",
      "formal_specification": "https://github.com/tensorflow/model-card-toolkit"
    },
    {
      "id": "B2AI_STANDARD:797",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "MongoDB",
      "description": "MongoDB",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A non-relational document database that provides support for JSON-like storage.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.mongodb.com/",
      "formal_specification": "https://github.com/mongodb/mongo",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:13",
        "B2AI_SUBSTRATE:22",
        "B2AI_SUBSTRATE:9"
      ]
    },
    {
      "id": "B2AI_STANDARD:798",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Monocle2",
      "description": "Monocle 2",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:21"
      ],
      "purpose_detail": "An algorithm that uses reversed graph embedding to describe multiple fate decisions in a fully unsupervised manner.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/cole-trapnell-lab/monocle-release",
      "publication": "doi:10.1038/nmeth.4402",
      "formal_specification": "https://github.com/cole-trapnell-lab/monocle-release"
    },
    {
      "id": "B2AI_STANDARD:799",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "MIMaL",
      "description": "Multi-Omic Integration by Machine Learning",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "multimodal"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:23"
      ],
      "purpose_detail": "MIMaL is a new method for integrating multiomic data using SHAP model explanations.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://mimal.app/",
      "publication": "doi:10.1093/bioinformatics/btac631",
      "formal_specification": "https://github.com/jessegmeyerlab/MIMaL"
    },
    {
      "id": "B2AI_STANDARD:800",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "MuSIC",
      "description": "Multi-Scale Integrated Cell pipeline",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:74",
          "category": "B2AI:Application",
          "name": "Multi-Scale Cell Type Annotation and Classification",
          "description": "MuSIC (Multi-Scale Integrated Cell classification) is used in AI applications for automated cell type annotation in single-cell RNA-seq data by leveraging reference datasets and hierarchical classification strategies. The tool employs machine learning algorithms that integrate multiple sources of evidence including marker genes, reference atlases, and cross-dataset mapping to assign cell type labels with confidence scores. AI systems build upon MuSIC's probabilistic framework to develop more sophisticated deep learning models for cell state identification, rare cell type discovery, and cross-species cell type mapping. The multi-scale approach enables AI models to make predictions at varying levels of granularity, from broad cell classes to fine-grained subtypes, which is essential for biological interpretation and downstream analysis.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "multimodal"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:26",
        "B2AI_TOPIC:28"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:116"
      ],
      "purpose_detail": "MuSIC is a hierarchical map of human cell architecture created from integrating immunofluorescence images in the Human Protein Atlas with affinity purification experiments from the BioPlex resource. Integration involves configuring each approach to produce a general measure of protein distance, then calibrating the two measures using machine learning.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://nrnb.org/music/",
      "publication": "doi:10.1038/s41586-021-04115-9",
      "formal_specification": "https://github.com/idekerlab/MuSIC"
    },
    {
      "id": "B2AI_STANDARD:801",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "MySQL",
      "description": "MySQL",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A relational database management system developed by Oracle that is based on structured query language (SQL).",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.mysql.com/",
      "formal_specification": "https://github.com/mysql/mysql-server",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:23",
        "B2AI_SUBSTRATE:37",
        "B2AI_SUBSTRATE:9"
      ]
    },
    {
      "id": "B2AI_STANDARD:802",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Neo4j",
      "description": "Neo4j Graph Data Platform",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "graphdataplatform"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:21"
      ],
      "purpose_detail": "Neo4j is a native graph database platform implementing property graph model with ACID-compliant transactions, designed for storing and querying highly connected data through nodes, relationships, and properties, enabling efficient traversal of complex relationship patterns at scale. Developed by Neo4j, Inc. and available in both open-source Community Edition and commercial Enterprise Edition, Neo4j uses the declarative Cypher query language allowing pattern-matching queries that naturally express graph relationships (e.g., `MATCH (person:Person)-[:FRIENDS_WITH]->(friend) RETURN person, friend`) without complex join operations required in relational databases. The architecture employs index-free adjacency where each node directly references its adjacent nodes enabling constant-time traversals regardless of graph size, native graph storage optimized for relationship-heavy queries, and clustered deployment supporting high availability, horizontal scaling, and causal consistency across distributed systems. Key features include rich graph algorithms library (Graph Data Science) implementing PageRank, community detection, shortest path, centrality measures, and similarity algorithms; APOC (Awesome Procedures on Cypher) extending functionality with graph refactoring, data integration, and advanced algorithms; support for temporal queries and multiple graph projections; and integration with analytics tools (Apache Spark, Python data science stack) and machine learning frameworks. Biomedical and AI applications span knowledge graphs integrating biomedical ontologies, drug databases, genomic data, and literature for drug repurposing and target discovery; clinical decision support modeling patient-symptom-disease-treatment relationships for diagnosis recommendation; biological network analysis representing protein-protein interactions, metabolic pathways, gene regulatory networks for systems biology research; healthcare interoperability mapping relationships between FHIR resources, HL7 messages, and clinical terminologies; and ML feature engineering extracting graph embeddings, network features, and relationship patterns as input for predictive models. Neo4j enables real-time recommendation systems, fraud detection networks, identity and access management, supply chain optimization, and social network analysis, essential for data scientists working with connected data, bioinformaticians building biological knowledge bases, clinical informaticians developing patient care pathways, and ML engineers requiring graph-structured features.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://neo4j.com/",
      "formal_specification": "https://github.com/neo4j/neo4j",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:14",
        "B2AI_SUBSTRATE:15",
        "B2AI_SUBSTRATE:25",
        "B2AI_SUBSTRATE:9"
      ]
    },
    {
      "id": "B2AI_STANDARD:803",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "NETME",
      "description": "NETME",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:16"
      ],
      "purpose_detail": "Starting from a set of fulltext obtained from PubMed, through an easy-to-use web interface, interactively extracts a group of biological elements stored into a selected list of ontological databases and then synthesizes a network with inferred relations among such elements.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://netme.click/#/",
      "publication": "doi:10.1007/S41109-021-00435-X"
    },
    {
      "id": "B2AI_STANDARD:804",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "NetSeekR",
      "description": "NetSeekR network analysis R package",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "scrnaseqanalysis"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:34"
      ],
      "purpose_detail": "A network analysis pipeline for RNA-Seq time series data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/igbb-popescu-lab/NetSeekR",
      "publication": "doi:10.1186/S12859-021-04554-1",
      "formal_specification": "https://github.com/igbb-popescu-lab/NetSeekR"
    },
    {
      "id": "B2AI_STANDARD:805",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "NDEx",
      "description": "Network Data Exchange",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "graphdataplatform"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:21"
      ],
      "purpose_detail": "The NDEx Project provides an open-source framework where scientists and organizations can store, share, manipulate, and publish biological network knowledge.",
      "is_open": true,
      "requires_registration": true,
      "url": "https://www.ndexbio.org/"
    },
    {
      "id": "B2AI_STANDARD:806",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "NeuCA",
      "description": "NeuCA - Neural-network based Cell Annotation tool",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:75",
          "category": "B2AI:Application",
          "name": "Cell Type Annotation in Single-Cell Genomics",
          "description": "NeuCA (Neural network-based Cell Annotation) is used in AI applications for automated cell type identification in single-cell RNA sequencing data, leveraging deep learning to achieve accurate, scalable annotation across diverse tissues and species. The tool employs neural network architectures optimized for single-cell expression profiles to classify cells based on marker gene expression patterns, enabling rapid analysis of large-scale atlas projects and clinical samples. NeuCA's transfer learning capabilities allow models trained on reference atlases to annotate new datasets with limited manual curation, supporting applications in cancer cell identification, immune profiling, and developmental biology. The method provides confidence scores for cell type assignments and can identify novel or transitional cell states.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "scrnaseqanalysis"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:34"
      ],
      "purpose_detail": "A R/Bioconductor tool for cell type annotation using single-cell RNA-seq data. It is a supervised cell label assignment method that uses existing scRNA-seq data with known labels to train a neural network-based classifier, and then predict cell labels in single-cell RNA-seq data of interest.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/haoharryfeng/NeuCA",
      "publication": "doi:10.1038/s41598-021-04473-4",
      "formal_specification": "https://github.com/haoharryfeng/NeuCA"
    },
    {
      "id": "B2AI_STANDARD:807",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Nextflow",
      "description": "Nextflow",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:20"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:90"
      ],
      "purpose_detail": "Enables scalable and reproducible scientific workflows using software containers. It allows the adaptation of pipelines written in the most common scripting languages.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.nextflow.io/",
      "formal_specification": "https://github.com/nextflow-io/nextflow"
    },
    {
      "id": "B2AI_STANDARD:808",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "AnVIL",
      "description": "NHGRI Analysis Visualization and Informatics Lab-space",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "cloudplatform"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:13"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:73"
      ],
      "purpose_detail": "AnVIL is NHGRI's Genomic Data Science Analysis, Visualization, and Informatics Lab-Space.",
      "is_open": true,
      "requires_registration": true,
      "url": "https://anvilproject.org/",
      "formal_specification": "https://github.com/anvilproject"
    },
    {
      "id": "B2AI_STANDARD:809",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "OmicsEV",
      "description": "OmicsEV",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:23"
      ],
      "purpose_detail": "An R package for quality evaluation of omics data tables. For each data table, OmicsEV uses a series of methods to evaluate data depth, data normalization, batch effect, biological signal, platform reproducibility, and multi-omics concordance, producing comprehensive visual and quantitative evaluation results that help assess data quality of individual data tables and facilitate the identification of the optimal data processing method and parameters for the omics study under investigation.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/bzhanglab/OmicsEV",
      "publication": "doi:10.1093/bioinformatics/btac698",
      "formal_specification": "https://github.com/bzhanglab/OmicsEV"
    },
    {
      "id": "B2AI_STANDARD:810",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "OAK",
      "description": "Ontology Access Kit",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:58"
      ],
      "purpose_detail": "OAK provides a collection of interfaces for various ontology operations.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://incatools.github.io/ontology-access-kit/",
      "formal_specification": "https://github.com/INCATools/ontology-access-kit"
    },
    {
      "id": "B2AI_STANDARD:811",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "ODK",
      "description": "Ontology Development Kit",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:58"
      ],
      "purpose_detail": "A toolkit and workflow system for managing the ontology life-cycle.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/INCATools/ontology-development-kit",
      "publication": "doi:10.1093/database/baac087",
      "formal_specification": "https://github.com/INCATools/ontology-development-kit"
    },
    {
      "id": "B2AI_STANDARD:812",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "OpenHealth",
      "description": "OpenHealth",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:18"
      ],
      "purpose_detail": "An open-source platform for wearable health monitoring. It aims to design a standard set of hardware/software and wearable devices that can enable autonomous collection of clinically relevant data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://sites.google.com/view/openhealth-wearable-health/home",
      "publication": "doi:10.1109/MDAT.2019.2906110"
    },
    {
      "id": "B2AI_STANDARD:813",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "pandas",
      "description": "pandas",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "An open source data analysis and manipulation tool built on top of the Python programming language.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://pandas.pydata.org/",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:29",
        "B2AI_SUBSTRATE:8"
      ]
    },
    {
      "id": "B2AI_STANDARD:814",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Panel",
      "description": "Panel",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "datavisualization"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "An open-source Python library that lets you create custom interactive web apps and dashboards by connecting user-defined widgets to plots, images, tables, or text.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://panel.holoviz.org/",
      "formal_specification": "https://github.com/holoviz/panel"
    },
    {
      "id": "B2AI_STANDARD:815",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "PostgreSQL",
      "description": "PostgreSQL",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "An open source object-relational database system.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.postgresql.org/",
      "formal_specification": "https://github.com/postgres/postgres",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:31",
        "B2AI_SUBSTRATE:37",
        "B2AI_SUBSTRATE:9"
      ]
    },
    {
      "id": "B2AI_STANDARD:816",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "PyTorch",
      "description": "PyTorch",
      "related_to": [
        "B2AI_STANDARD:354"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:76",
          "category": "B2AI:Application",
          "name": "Deep Learning for Biomedical Research and Clinical AI",
          "description": "PyTorch is the dominant framework for biomedical AI research, used extensively for developing deep learning models across medical imaging, genomics, drug discovery, and clinical prediction. Researchers leverage PyTorch's dynamic computational graphs, extensive ecosystem (torchvision, torchaudio, TorchIO), and pretrained models to build custom neural architectures for tasks like cancer detection in pathology images, protein structure prediction, medical report generation, and patient outcome forecasting. PyTorch's flexibility enables rapid prototyping of novel architectures, its strong academic community supports reproducible research through shared model implementations, and its production deployment tools (TorchServe, TorchScript) facilitate clinical translation of research models.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "machinelearningframework"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A popular machine learning platform.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://pytorch.org/",
      "formal_specification": "https://github.com/pytorch/pytorch",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:33"
      ]
    },
    {
      "id": "B2AI_STANDARD:817",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Quarto",
      "description": "Quarto publishing system",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "notebookplatform"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "An open-source scientific and technical publishing system built on Pandoc.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://quarto.org/",
      "formal_specification": "https://github.com/quarto-dev/quarto-cli"
    },
    {
      "id": "B2AI_STANDARD:818",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "RDCA-DAP",
      "description": "Rare Disease Cures Accelerator-Data and Analytics Platform",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "cloudplatform"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:31"
      ],
      "purpose_detail": "The Rare Disease Cures Accelerator-Data and Analytics Platform (RDCA-DAP) is an FDA-funded initiative that provides a centralized and standardized infrastructure to support and accelerate rare disease characterization, with the goal of accelerating therapy development across rare diseases.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://c-path.org/programs/rdca-dap/"
    },
    {
      "id": "B2AI_STANDARD:819",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "refget",
      "description": "refget API",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:13"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:34"
      ],
      "purpose_detail": "Enables access to reference genomic sequences without ambiguity from different databases and servers using a checksum identifier based on the sequence content itself.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://samtools.github.io/hts-specs/refget.html",
      "formal_specification": "https://samtools.github.io/hts-specs/refget.html"
    },
    {
      "id": "B2AI_STANDARD:820",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Relexi",
      "description": "Relexi",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:77",
          "category": "B2AI:Application",
          "name": "Explainable Reinforcement Learning for Clinical Decision Support",
          "description": "Relexi is used in biomedical AI for developing interpretable reinforcement learning systems that can explain their decision-making process, crucial for clinical applications where treatment recommendations must be understandable to physicians. The framework enables training of RL agents for sequential clinical decisions (medication dosing, ventilator management, treatment timing) while maintaining explainability through attention mechanisms and policy distillation. Relexi supports safe exploration in healthcare settings by incorporating domain constraints and enabling clinicians to understand why specific actions are recommended. Applications include explainable sepsis treatment protocols, interpretable insulin dosing algorithms, and transparent clinical trial enrollment strategies where understanding the agent's reasoning is essential for clinical trust and regulatory approval.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "machinelearningframework"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Relexi is an open source reinforcement learning (RL) framework written in Python and based on TensorFlow's RL library TF-Agents. Relexi allows to employ RL for environments that require computationally intensive simulations like applications in computational fluid dynamics. For this, Relexi couples legacy simulation codes with the RL library TF-Agents at scale on modern high-performance computing (HPC) hardware using the SmartSim library. Relexi thus provides an easy way to explore the potential of RL for HPC applications.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/flexi-framework/relexi",
      "publication": "doi:10.1016/j.simpa.2022.100422",
      "formal_specification": "https://github.com/flexi-framework/relexi"
    },
    {
      "id": "B2AI_STANDARD:821",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "REDCap",
      "description": "Research Electronic Data Capture",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:78",
          "category": "B2AI:Application",
          "name": "Clinical Research Data Capture for ML Model Development",
          "description": "REDCap is widely used in AI applications as a platform for collecting high-quality, structured clinical research data that feeds machine learning model development and validation studies. AI researchers leverage REDCap's data dictionaries, validation rules, and standardized data collection instruments to create clean training datasets for predictive models in clinical trials, cohort studies, and patient registries. REDCap's API enables automated data extraction for ML pipelines, and its audit trails and data quality features ensure reproducibility in AI research. The platform is particularly valuable for multi-site AI studies where standardized data collection across institutions is essential for model generalizability.",
          "used_in_bridge2ai": false
        }
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:31"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:114",
        "B2AI_ORG:117"
      ],
      "purpose_detail": "Electronic data capture software and workflow methodology for designing clinical and translational research databases.",
      "is_open": true,
      "requires_registration": true,
      "url": "https://www.project-redcap.org/",
      "publication": "doi:10.1016/j.jbi.2008.08.010"
    },
    {
      "id": "B2AI_STANDARD:822",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Synapse",
      "description": "Sage Synapse",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:86"
      ],
      "purpose_detail": "Synapse is a collaborative research platform developed by Sage Bionetworks that provides web services and tools for aggregating, organizing, analyzing, and sharing scientific data, code, and insights across biomedical research communities. The platform implements a comprehensive data management infrastructure through its Repository Services, offering RESTful JSON APIs for entity management, file storage, versioning, and access control. Core functionality includes hierarchical project organization with entities such as Projects, Folders, Files, Tables, and Views that support structured data organization and metadata annotation. Synapse provides advanced data access control through Access Requirements and Access Approvals, enabling researchers to implement controlled access policies including ACTAccessRequirement for managed access to sensitive data. The platform supports collaborative research through Teams, subscription services for change notifications, messaging capabilities, and wiki documentation integrated with research artifacts. Data governance features include a Qualified Research Program that balances broad researcher access with participant protections, research project management for data access requests, and submission workflows for access review by the Access and Compliance Team (ACT). Technical capabilities include multi-part file uploads, version tracking with provenance through Activity records, search indexing using OpenSearch for entity discovery, DOI minting for permanent identifiers, and integration with OAuth2 authentication and OpenID Connect. Synapse implements the GA4GH DRS (Data Repository Service) API specification for standardized data access, supports tabular data through Table entities with SQL-like querying, and provides form-based data collection with review workflows. The platform serves as infrastructure for open science initiatives, enabling data sharing while encoding contractual protections for research participants, and has been deployed in numerous biomedical studies including mHealth research such as the mPower Parkinson disease observational study.",
      "is_open": true,
      "requires_registration": true,
      "url": "https://www.synapse.org/",
      "publication": "doi:10.2139/ssrn.3502410",
      "formal_specification": "https://github.com/Sage-Bionetworks/Synapse-Repository-Services"
    },
    {
      "id": "B2AI_STANDARD:823",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "SemEHR",
      "description": "SemEHR",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "An open source semantic search and analytics tool for EHRs.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/CogStack/CogStack-SemEHR",
      "publication": "doi:10.1093/jamia/ocx160",
      "formal_specification": "https://github.com/CogStack/CogStack-SemEHR"
    },
    {
      "id": "B2AI_STANDARD:824",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "SAVER",
      "description": "Single-cell Analysis Via Expression Recovery",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "scrnaseqanalysis"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:34"
      ],
      "purpose_detail": "A regularized regression prediction and empirical Bayes method to recover the true gene expression profile in noisy and sparse scRNA-seq data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://mohuangx.github.io/SAVER/",
      "publication": "doi:10.1038/s41592-018-0033-z",
      "formal_specification": "https://github.com/mohuangx/SAVER"
    },
    {
      "id": "B2AI_STANDARD:825",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Snorkel",
      "description": "Snorkel",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "deprecated"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Snorkel is a platform for automated data labeling. It has since been extended into a full machine learning platform (see https://snorkel.ai/).",
      "is_open": false,
      "requires_registration": true,
      "url": "https://www.snorkel.org/",
      "formal_specification": "https://github.com/snorkel-team/snorkel"
    },
    {
      "id": "B2AI_STANDARD:826",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "SnpEff",
      "description": "SnpEff",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:25",
        "B2AI_TOPIC:35"
      ],
      "purpose_detail": "SnpEff is a variant annotation and effect prediction tool. It annotates and predicts the effects of genetic variants (such as amino acid changes).",
      "is_open": true,
      "requires_registration": false,
      "url": "https://pcingola.github.io/SnpEff/se_introduction/",
      "publication": "doi:10.4161/fly.19695",
      "formal_specification": "https://github.com/pcingola/SnpEff"
    },
    {
      "id": "B2AI_STANDARD:827",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Souporcell",
      "description": "Souporcell",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "scrnaseqanalysis"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:34"
      ],
      "purpose_detail": "A method to cluster cells using the genetic variants detected within the scRNA-seq reads.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/wheaton5/souporcell",
      "publication": "doi:10.1038/s41592-020-0820-1",
      "formal_specification": "https://github.com/wheaton5/souporcell"
    },
    {
      "id": "B2AI_STANDARD:828",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "STAR",
      "description": "Spliced Transcripts Alignment to a Reference",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:33"
      ],
      "purpose_detail": "Software based on an RNA-seq alignment algorithm that uses sequential maximum mappable seed search in uncompressed suffix arrays followed by seed clustering and stitching procedure.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/alexdobin/STAR",
      "publication": "doi:10.1093/bioinformatics/bts635",
      "formal_specification": "https://github.com/alexdobin/STAR"
    },
    {
      "id": "B2AI_STANDARD:829",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "SyBLaRS",
      "description": "Systems Biology Layout and Rendering Service",
      "related_to": [
        "B2AI_STANDARD:337",
        "B2AI_STANDARD:289"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A web service for automatic layout of biological data in various standard formats as well as construction of customized images in both raster image and scalable vector formats of these maps. Some of the supported standards are more generic such as GraphML and JSON, whereas others are specialized to biology such as SBGNML (The Systems Biology Graphical Notation Markup Language) and SBML (The Systems Biology Markup Language).",
      "is_open": true,
      "requires_registration": false,
      "url": "http://syblars.cs.bilkent.edu.tr/",
      "publication": "doi:10.1371/journal.pcbi.1010635",
      "formal_specification": "https://github.com/iVis-at-Bilkent/syblars",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:36"
      ]
    },
    {
      "id": "B2AI_STANDARD:830",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "TES",
      "description": "Task Execution Service",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:34"
      ],
      "purpose_detail": "The Task Execution Service (TES) API is a proposed standard for describing and executing tasks in a platform-agnostic way. Includes a TES API validator and a Task Execution API specification.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://ga4gh.github.io/task-execution-schemas/docs/"
    },
    {
      "id": "B2AI_STANDARD:831",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "TF",
      "description": "Tensorflow",
      "related_to": [
        "B2AI_STANDARD:354"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:79",
          "category": "B2AI:Application",
          "name": "Production-Scale Healthcare AI Systems and Deployment",
          "description": "TensorFlow is widely used for deploying production-grade AI systems in healthcare, particularly for applications requiring high-throughput inference, mobile deployment, and integration with enterprise IT infrastructure. Healthcare organizations leverage TensorFlow's mature ecosystem (TF Serving, TF Lite, TF.js) to deploy models for real-time clinical decision support, mobile diagnostic apps, and edge computing in medical devices. The framework's strong support for model optimization, quantization, and hardware acceleration (TPUs, GPUs) enables efficient deployment of complex models like retinal disease screening systems, ECG interpretation algorithms, and clinical NLP pipelines. TensorFlow Extended (TFX) provides production ML pipelines for managing data validation, model training, and continuous monitoring in regulated healthcare environments.",
          "used_in_bridge2ai": false,
          "references": [
            "https://www.tensorflow.org/tfx"
          ]
        }
      ],
      "collection": [
        "machinelearningframework"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:37"
      ],
      "purpose_detail": "TensorFlow is an end-to-end open source platform for machine learning. It has an ecosystem of tools, libraries and community resources that lets researchers and developers easily build and deploy ML powered applications.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.tensorflow.org/",
      "formal_specification": "https://github.com/tensorflow/tensorflow",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:42"
      ]
    },
    {
      "id": "B2AI_STANDARD:832",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Terra",
      "description": "Terra Community Workbench",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "cloudplatform"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:20"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:71"
      ],
      "purpose_detail": "Terra is a cloud-native platform for biomedical researchers to access data, run analysis tools, and collaborate.",
      "is_open": true,
      "requires_registration": true,
      "url": "https://app.terra.bio/",
      "formal_specification": "https://github.com/DataBiosphere/terra-ui"
    },
    {
      "id": "B2AI_STANDARD:833",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "R",
      "description": "The R Project for Statistical Computing",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A free software environment for statistical computing and graphics.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.r-project.org/",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:34",
        "B2AI_SUBSTRATE:35",
        "B2AI_SUBSTRATE:40"
      ]
    },
    {
      "id": "B2AI_STANDARD:834",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Theano",
      "description": "Theano",
      "related_to": [
        "B2AI_STANDARD:354"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:80",
          "category": "B2AI:Application",
          "name": "Legacy Deep Learning Models and Reproducible Research",
          "description": "Theano was historically important in early biomedical deep learning research and continues to be relevant for reproducing published models and maintaining legacy clinical AI systems. Many influential early papers in medical AI used Theano, and researchers still need to run these models for comparison benchmarks, reproduce published results, and maintain systems deployed before modern frameworks emerged. The library's symbolic computation approach and automatic differentiation influenced the design of current frameworks, and understanding Theano remains valuable for historical context in AI research. While active development has ceased (succeeded by Aesara), Theano-based code remains in production in some clinical settings and research archives.",
          "used_in_bridge2ai": false,
          "references": [
            "https://theano-pymc.readthedocs.io/"
          ]
        }
      ],
      "collection": [
        "deprecated",
        "machinelearningframework"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. It is being continued as aesara.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/Theano/Theano",
      "formal_specification": "https://github.com/Theano/Theano",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:1"
      ]
    },
    {
      "id": "B2AI_STANDARD:835",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "TRS",
      "description": "Tool Registry Service",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:34"
      ],
      "purpose_detail": "A proposed standard for sharing and discovering tools and workflows in a platform-agnostic way. Includes a Tool Registry Search validator and a Tool Discovery API specification.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://ga4gh.github.io/tool-registry-service-schemas/"
    },
    {
      "id": "B2AI_STANDARD:836",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "U-BRITE",
      "description": "UAB Biomedical Research Information Technology Enhancement Commons Program",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:4",
        "B2AI_TOPIC:13"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:94"
      ],
      "purpose_detail": "U-BRITE (UAB Biomedical Research Information Technology Enhancement) assembles new and existing HIPAA-compliant, high-performance informatics tools to provide researchers with a means to better manage and analyze clinical and genomic data sets and implements a translational research commons to facilitate and enable interdisciplinary team science across geographical locations.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://ubrite.org/"
    },
    {
      "id": "B2AI_STANDARD:837",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Usagi",
      "description": "Usagi",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:76"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:844"
      ],
      "purpose_detail": "An application to help create mappings between coding systems and the Vocabulary standard concepts.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/OHDSI/Usagi",
      "formal_specification": "https://github.com/OHDSI/Usagi"
    },
    {
      "id": "B2AI_STANDARD:838",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Vireo",
      "description": "Vireo",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:34"
      ],
      "purpose_detail": "A computationally efficient Bayesian model to demultiplex single-cell data from pooled experimental designs.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/single-cell-genetics/vireo",
      "publication": "doi:10.1186/s13059-019-1865-2",
      "formal_specification": "https://github.com/single-cell-genetics/vireo"
    },
    {
      "id": "B2AI_STANDARD:839",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Wasabi",
      "description": "Wasabi Cloud Storage",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "cloudservice"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Wasabi Cloud Storage is a high-performance, S3-compatible object storage service providing hot cloud storage with predictable pricing, no egress fees, and 80% lower total cost of ownership compared to hyperscaler alternatives (AWS S3, Azure Blob, Google Cloud Storage), designed for data-intensive workloads requiring frequent access and large-scale data archival. Founded in 2017 by Carbonite co-founders, Wasabi operates 16 globally distributed storage regions across North America, Europe, Asia-Pacific, with SOC-2, ISO 27001, and PCI-DSS certified data centers providing enterprise-grade security, immutability features (WORM, Object Lock), and compliance certifications (HIPAA, GDPR, FERPA) suitable for regulated industries. Wasabi's \"always hot\" architecture eliminates tiered storage complexity by storing all data on high-performance disk arrays with consistent millisecond latency for reads/writes, avoiding cold storage retrieval delays that plague glacier-tier alternatives while maintaining cost parity with archival storage services. The platform's S3 API compatibility ensures drop-in replacement for existing AWS workflows, supporting standard S3 operations (PUT, GET, LIST, multipart uploads), bucket policies, IAM-style access controls, and seamless integration with S3-compatible tools (AWS CLI, SDKs, third-party backup software, media asset managers). Wasabi's zero-fee model for egress, API requests, and reads eliminates the unpredictable costs that typically double hyperscaler storage bills, enabling cost-effective access for AI/ML training pipelines, video surveillance archives, genomic datasets, and medical imaging repositories where frequent data retrieval is essential. The service provides native integrations with major backup platforms (Veeam, Commvault, Rubrik), media workflows (Adobe Premiere, Frame.io), surveillance systems (Milestone, Hanwha), and object storage gateways, supporting hybrid cloud architectures where on-premises applications seamlessly tier to Wasabi for capacity expansion and disaster recovery. For AI/ML workloads, Wasabi's high-throughput object storage (up to 10 Gbps per connection) supports rapid dataset ingestion for model training, low-latency access to training data during distributed training across GPU clusters, and cost-effective storage for model checkpoints, experiment artifacts, and inference result archives without egress penalties for iterative model development and hyperparameter tuning requiring repeated dataset access.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://wasabi.com/"
    },
    {
      "id": "B2AI_STANDARD:840",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "W&B",
      "description": "Weights and Balances platform",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Platform for tracking, comparing, and visualizing machine learning experiments.",
      "is_open": true,
      "requires_registration": true,
      "url": "https://wandb.ai/"
    },
    {
      "id": "B2AI_STANDARD:841",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "WES",
      "description": "Workflow Execution Service",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:34"
      ],
      "purpose_detail": "The Workflow Execution Service (WES) is a GA4GH standard API specification that provides a platform-agnostic approach for submitting and monitoring workflow execution across different computational environments. WES enables researchers to run standardized workflows, currently supporting Common Workflow Language (CWL) and Workflow Description Language (WDL) formats, on multiple platforms, clouds, and execution systems using a consistent interface. The API specification is written in OpenAPI and embodies RESTful service principles, using JSON for requests and responses with standard HTTP/HTTPS transport. Core functionality includes workflow submission with parameter passing, run status monitoring through detailed logs capturing stdout and stderr output, task-level execution tracking with timing and exit codes, and workflow cancellation capabilities. WES addresses critical use cases such as \"bring your code to the data\" scenarios where researchers submit custom analyses to run on externally-owned datasets without data transfer, and best-practices pipeline execution where researchers discover workflows from shared repositories like Dockstore and execute them over controlled data environments. The service implements OAuth2 bearer token authentication and authorization, with implementations responsible for verifying user credentials and enforcing submission policies. WES provides comprehensive service introspection through its service-info endpoint, reporting supported workflow types, versions, filesystem protocols, workflow engines, and system state information. The API supports paginated listing of workflow runs, detailed run logs with output file locations, and granular task-level monitoring. Run states include QUEUED, INITIALIZING, RUNNING, COMPLETE, EXECUTOR_ERROR, SYSTEM_ERROR, CANCELED, and PREEMPTED, enabling detailed tracking of workflow execution lifecycle. As a GA4GH standard, WES promotes interoperability across bioinformatics workflow execution platforms and supports integration with related GA4GH standards including the Data Object Service for credential management and the Task Execution Service for extended task definitions.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://ga4gh.github.io/workflow-execution-service-schemas/docs/"
    },
    {
      "id": "B2AI_STANDARD:842",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Xethub",
      "description": "Xethub",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Git-based collaboration to large scale repositories of data, code, or any combination of files.",
      "is_open": true,
      "requires_registration": true,
      "url": "https://xethub.com/assets/docs/"
    },
    {
      "id": "B2AI_STANDARD:843",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "ZenML",
      "description": "ZenML",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "has_application": [
        {
          "id": "B2AI_APP:81",
          "category": "B2AI:Application",
          "name": "MLOps Orchestration for Biomedical AI Pipelines",
          "description": "ZenML is used in biomedical AI for building reproducible, production-grade machine learning pipelines with comprehensive experiment tracking, model versioning, and deployment orchestration. Healthcare AI teams leverage ZenML to standardize workflows from data ingestion through model deployment, ensuring compliance with regulatory requirements for traceability and reproducibility. The platform integrates with diverse healthcare data sources, model registries, and deployment targets while maintaining complete lineage tracking essential for clinical AI validation. ZenML enables teams to implement best practices for ML operations including automated testing, continuous training, and model monitoring in healthcare settings where reliability and auditability are critical.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "machinelearningframework"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "ZenML is an extensible, open-source MLOps framework for creating portable, production-ready MLOps pipelines.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://zenml.io/",
      "formal_specification": "https://github.com/zenml-io/zenml"
    },
    {
      "id": "B2AI_STANDARD:844",
      "category": "B2AI_STANDARD:TrainingProgram",
      "name": "OHDSI Tutorials",
      "description": "2019 OHDSI Tutorials - OMOP Common Data Model and Standardized Vocabularies",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:4",
        "B2AI_TOPIC:52"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:76"
      ],
      "purpose_detail": "This workshop is for data holders who want to apply OHDSI's data standards to their own observational datasets and researchers who want to be aware of OHDSI's data standards, so they can leverage data in OMOP CDM format for their own research purposes.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.ohdsi.org/2019-tutorials-omop-common-data-model-and-standardized-vocabularies/",
      "formal_specification": "https://github.com/OHDSI/Tutorial-CDM"
    },
    {
      "id": "B2AI_STANDARD:845",
      "category": "B2AI_STANDARD:TrainingProgram",
      "name": "CDC Introduction to FHIR",
      "description": "CDC Introduction to FHIR - Training Recordings",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:52"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:40"
      ],
      "purpose_detail": "A series of HL7 FAIR training lecture recordings made available through YouTube.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.cdc.gov/nchs/data/nvss/modernization/Introductory-Training-FHIR.pdf"
    },
    {
      "id": "B2AI_STANDARD:846",
      "category": "B2AI_STANDARD:TrainingProgram",
      "name": "FHIR Drills",
      "description": "FHIR Drills",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:52"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:40"
      ],
      "purpose_detail": "This set of pages contains a series of FHIR tutorials for those just beginning to learn the new specification. The tutorials require no prior knowledge of FHIR or REST. At present these tutorials are in their beta stage of development and we would appreciate any feedback you may have as we plan to build upon these in time to create a full set of tutorials from the very basic to the more complex.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://fhir-drills.github.io/"
    },
    {
      "id": "B2AI_STANDARD:847",
      "category": "B2AI_STANDARD:TrainingProgram",
      "name": "FHIR Fundamentals",
      "description": "HL7 FHIR Fundamentals Course",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:52"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:40"
      ],
      "purpose_detail": "This is an asynchronous, instructor-led online course that allows you to work at your own pace. Learning takes place through discussions with the instructor, tutors and peers. Assessments are in the form of weekly assignments, quizzes, exams and projects. Plan on spending 5 to 7 hours per week. There are no live lectures to attend.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://www.hl7.org/training/fhir-fundamentals.cfm",
      "formal_specification": "https://courses.hl7fundamentals.org/campus/"
    },
    {
      "id": "B2AI_STANDARD:848",
      "category": "B2AI_STANDARD:TrainingProgram",
      "name": "Learn LOINC",
      "description": "Learn LOINC",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:52"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:53"
      ],
      "purpose_detail": "Welcome to the LOINC Library. This is our A to Z collection of resources that we've collected to help you learn about LOINC and get connected to the community.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://loinc.org/learn/"
    },
    {
      "id": "B2AI_STANDARD:849",
      "category": "B2AI_STANDARD:TrainingProgram",
      "name": "Microsoft Medical Imaging",
      "description": "Microsoft Learn - Work with medical imaging data and DICOM",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:15",
        "B2AI_TOPIC:52"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:25",
        "B2AI_ORG:56"
      ],
      "purpose_detail": "Learn why DICOM standards are important. Explore the DICOM standards and DICOM service. Review the use case for radiology data in cancer treatment with examples.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://learn.microsoft.com/en-us/training/modules/medical-imaging-data/",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:11"
      ]
    },
    {
      "id": "B2AI_STANDARD:850",
      "category": "B2AI_STANDARD:TrainingProgram",
      "name": "Udemy FHIR",
      "description": "Udemy - Introduction to FHIR",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:52"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:40"
      ],
      "purpose_detail": "This course will help you understand the basics of FHIR. It is a FREE sample of a comprehensive hands-on introductory course (details inside). The full course includes direct access to the course creator via a private members-only Slack room.",
      "is_open": true,
      "requires_registration": true,
      "url": "https://www.udemy.com/course/introduction-to-fhir/"
    },
    {
      "id": "B2AI_STANDARD:851",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "FHIR US Core",
      "description": "Fast Healthcare Interoperability Resources - US Core",
      "subclass_of": [
        "B2AI_STANDARD:109"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:103",
        "B2AI_ORG:117"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:845",
        "B2AI_STANDARD:846",
        "B2AI_STANDARD:847",
        "B2AI_STANDARD:850"
      ],
      "purpose_detail": "This is subset of all FHIR profiles for the US Realm, i.e., those supporting the minimum requirements for clinical data exchange in the United States.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://build.fhir.org/ig/HL7/US-Core/index.html",
      "responsible_organization": [
        "B2AI_ORG:40"
      ]
    },
    {
      "id": "B2AI_STANDARD:852",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "FHIR USCDI",
      "description": "Fast Healthcare Interoperability Resources - US Core Data for Interoperability",
      "subclass_of": [
        "B2AI_STANDARD:851"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:103"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:845",
        "B2AI_STANDARD:846",
        "B2AI_STANDARD:847",
        "B2AI_STANDARD:850"
      ],
      "purpose_detail": "USCDI is the set of basic healthcare data types expected to supported by other systems. This is the FHIR US Core profile with all elements required by USCDI.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://build.fhir.org/ig/HL7/US-Core/uscdi.html",
      "responsible_organization": [
        "B2AI_ORG:40"
      ]
    },
    {
      "id": "B2AI_STANDARD:853",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "FHIR USCDI v1",
      "description": "Fast Healthcare Interoperability Resources - US Core Data for Interoperability, version 1",
      "subclass_of": [
        "B2AI_STANDARD:852"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:103",
        "B2AI_ORG:117"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:845",
        "B2AI_STANDARD:846",
        "B2AI_STANDARD:847",
        "B2AI_STANDARD:850"
      ],
      "purpose_detail": "USCDI is the set of basic healthcare data types expected to supported by other systems. This is the FHIR US Core profile with all elements required by USCDI v1.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://build.fhir.org/ig/HL7/US-Core/uscdi.html",
      "responsible_organization": [
        "B2AI_ORG:40"
      ]
    },
    {
      "id": "B2AI_STANDARD:854",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "FHIR USCDI v4",
      "description": "Fast Healthcare Interoperability Resources - US Core Data for Interoperability, version 4",
      "subclass_of": [
        "B2AI_STANDARD:852"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "collection": [
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:103",
        "B2AI_ORG:117"
      ],
      "has_training_resource": [
        "B2AI_STANDARD:845",
        "B2AI_STANDARD:846",
        "B2AI_STANDARD:847",
        "B2AI_STANDARD:850"
      ],
      "purpose_detail": "USCDI is the set of basic healthcare data types expected to supported by other systems. This is the FHIR US Core profile with all elements required by USCDI v4.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://build.fhir.org/ig/HL7/US-Core/uscdi.html",
      "responsible_organization": [
        "B2AI_ORG:40"
      ]
    },
    {
      "id": "B2AI_STANDARD:855",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Dedoose",
      "description": "Dedoose app",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2023-03-10",
      "collection": [
        "multimodal"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Dedoose is a cloud-based, cross-platform application specifically designed for analyzing qualitative and mixed methods research data, developed by social scientists with expertise spanning education, psychology, anthropology, and marketing to address the practical needs of researchers across diverse disciplines. The platform supports nearly any type of qualitative data including interview transcripts, focus group recordings, open-ended survey responses, field notes, documents (PDFs, Word files), multimedia content (video, audio, images), social media posts, and website content, enabling comprehensive mixed methods analysis that integrates both qualitative coding and quantitative metrics within a unified workspace. Dedoose provides a hierarchical coding system where researchers create code trees with parent and child codes, apply codes to excerpts of text or segments of media, attach descriptors and demographic variables to documents and participants, and conduct inter-rater reliability testing with pooled Cohen's kappa and other statistics to validate coding consistency across multiple analysts. The platform's real-time collaboration capabilities allow distributed research teams to work simultaneously on the same project at no additional cost, with version control, audit trails tracking all coding decisions, user permissions managing access levels, and synchronized updates ensuring all team members see changes instantly. Dedoose generates dozens of interactive visualizations including code frequency charts showing prevalence across documents or demographic groups, code co-occurrence matrices revealing relationships between themes, word clouds emphasizing prominent terms, heat maps displaying patterns across cases, timeline visualizations for longitudinal data, and customizable charts that can be filtered dynamically by descriptor variables to explore subgroup differences. The platform operates entirely through web browsers on Mac, PC, and Linux systems without requiring software installation, stores all data on secure servers with enterprise-grade encryption, complies with data protection standards, provides automatic backups, and ensures researchers maintain full control over project data with export options in multiple formats (Excel, CSV, PDF reports, coded excerpts). Dedoose is widely used across healthcare research for analyzing patient interviews and clinical qualitative data, education research for evaluating teaching methods and student feedback, market and user experience research for synthesizing customer insights and usability testing results, program evaluation for assessing intervention effectiveness through mixed methods frameworks, social science research for thematic analysis of ethnographic data, and policy research for analyzing stakeholder perspectives and public comments. The platform's mixed methods integration is particularly valuable for AI and machine learning applications where qualitative coding can inform feature engineering, coded themes can serve as training labels for natural language processing models, researcher annotations can validate AI-generated classifications, descriptive statistics from Dedoose can be incorporated into predictive models, and iterative human-in-the-loop workflows can combine automated text analysis with expert qualitative interpretation to refine algorithms and improve classification accuracy on nuanced social science constructs.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://www.dedoose.com/"
    },
    {
      "id": "B2AI_STANDARD:856",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "FAIR Data Station",
      "description": "FAIR Data Station",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2023-03-13",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A lightweight application written in Java, that aims to support researchers in managing research metadata according to the FAIR principles.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://fairbydesign.nl/",
      "publication": "doi:10.1093/gigascience/giad014"
    },
    {
      "id": "B2AI_STANDARD:857",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Patient-Led Research Scorecards",
      "description": "Patient-Led Research Scorecards",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2023-03-14",
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "The Council of Medical Specialty Societies (CMSS) and Patient-Led Research Collaborative (PLRC) have developed a sustainable collaborative model of CER based on information from and the expertise of patient communities, researchers, funders, and clinical research organizations. This model takes the form of scorecards which serve to evaluate how effective a patient group and research partner collaboration will be at conducting truly patient-led research.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://patientresearchcovid19.com/storage/2023/02/Patient-Led-Research-Scorecards.pdf"
    },
    {
      "id": "B2AI_STANDARD:858",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Zshot",
      "description": "Zshot",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2023-03-16",
      "concerns_data_topic": [
        "B2AI_TOPIC:32"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:104"
      ],
      "purpose_detail": "A framework for performing Zero and Few shot named entity recognition.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://ibm.github.io/zshot/",
      "formal_specification": "https://github.com/IBM/zshot"
    },
    {
      "id": "B2AI_STANDARD:859",
      "category": "B2AI_STANDARD:Registry",
      "name": "NIH CDE",
      "description": "NIH Common Data Elements Repository",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2023-03-21",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:74"
      ],
      "purpose_detail": "A registry of standardized, precisely defined questions, paired with sets of allowable responses, used systematically across different sites, studies, or clinical trials to ensure consistent data collection.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://cde.nlm.nih.gov/",
      "formal_specification": "https://cde.nlm.nih.gov/api"
    },
    {
      "id": "B2AI_STANDARD:860",
      "category": "B2AI_STANDARD:Registry",
      "name": "BARTOC",
      "description": "Basic Register of Thesauri, Ontologies & Classifications",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2023-03-24",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A database of Knowledge Organization Systems and KOS related registries.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://bartoc.org/"
    },
    {
      "id": "B2AI_STANDARD:861",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "DuckDB",
      "description": "DuckDB",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2023-03-24",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A database platform designed for working with tabular data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://duckdb.org/",
      "formal_specification": "https://github.com/duckdb/duckdb",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:9"
      ]
    },
    {
      "id": "B2AI_STANDARD:862",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Polars",
      "description": "Polars library",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2023-03-27",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Polars is a high-performance DataFrame library written in Rust with bindings for Python, Node.js, and R, designed as a fast alternative to pandas. The library features a multi-threaded query engine with lazy evaluation, query optimization, and streaming capabilities for processing larger-than-RAM datasets. Polars utilizes Apache Arrow columnar format for zero-copy data sharing and SIMD vectorization for cache-coherent algorithms. It supports both eager and lazy execution modes, with the lazy API enabling automatic query optimization and parallel execution across CPU cores. The library handles diverse data formats including CSV, JSON, Parquet, Delta Lake, AVRO, Excel, and direct database connections to MySQL, PostgreSQL, SQLite, and cloud storage systems. Polars provides an intuitive expression API for data manipulation operations while maintaining minimal dependencies and fast import times (70ms vs 520ms for pandas), making it suitable for data analysis, ETL pipelines, and analytical workloads requiring high performance.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.pola.rs/",
      "formal_specification": "https://github.com/pola-rs/polars/",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:8"
      ]
    },
    {
      "id": "B2AI_STANDARD:863",
      "category": "B2AI_STANDARD:Registry",
      "name": "LOV",
      "description": "Linked Open Vocabularies",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2023-03-27",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A collection of searchable ontologies and vocabularies, spanning multiple fields.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://lov.linkeddata.es/",
      "formal_specification": "https://github.com/pyvandenbussche/lov"
    },
    {
      "id": "B2AI_STANDARD:864",
      "category": "B2AI_STANDARD:Registry",
      "name": "PhenX Toolkit",
      "description": "PhenX Toolkit",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2023-03-27",
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "purpose_detail": "A catalog of recommended measurement protocols for biomedical research.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.phenxtoolkit.org/"
    },
    {
      "id": "B2AI_STANDARD:866",
      "category": "B2AI_STANDARD:OntologyOrVocabulary",
      "name": "SALON",
      "description": "Sequence Alignment Ontology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:23"
      ],
      "purpose_detail": "An OWL2 ontology for representing and semantically annotating pairwise and multiple sequence alignments.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://benhid.com/SALON/",
      "publication": "doi:10.1186/s12859-023-05190-7"
    },
    {
      "id": "B2AI_STANDARD:867",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "GCS",
      "description": "Glasgow Coma Scale",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2023-05-23",
      "collection": [
        "diagnosticinstrument"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "purpose_detail": "A neurological assessment tool used to evaluate level of consciousness based on patient responses in three categories: eye-opening, verbal response, and motor response, with a higher score indicating a more favorable neurological status.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.glasgowcomascale.org/",
      "publication": "doi:10.1016/s0140-6736(74)91639-0"
    },
    {
      "id": "B2AI_STANDARD:868",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "Frictionless",
      "description": "Frictionless data standards",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2023-05-23",
      "collection": [
        "datamodel"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "Frictionless data standards provide a comprehensive suite of lightweight, extensible specifications for describing datasets, data files, and tabular data to enhance FAIR (Findability, Accessibility, Interoperability, Reusability) principles. The core specifications include Data Package for dataset-level metadata and resource collections, Data Resource for individual file descriptions, and Table Schema for tabular data structure definition with field types, constraints, and relationships. These specifications combine to create specialized formats like Tabular Data Packages that integrate CSV/JSON data with JSON Schema-based metadata descriptors. The standards follow a \"small pieces, loosely joined\" philosophy, enabling individual components to be used independently or combined for complex data scenarios. They support cross-technology implementation with human-readable JSON metadata that facilitates machine processing, data validation, and automated discovery. The specifications enable data portability, version control, and collaborative data workflows while maintaining compatibility with existing data formats and tools in the data science ecosystem.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://specs.frictionlessdata.io/",
      "formal_specification": "https://github.com/frictionlessdata/specs"
    },
    {
      "id": "B2AI_STANDARD:869",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Compass Rose",
      "description": "Epic Compass Rose module",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2023-06-20",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:87",
          "category": "B2AI:Application",
          "name": "Population Health Analytics and Risk Stratification",
          "description": "Compass Rose (Epic's population health tool) is used in AI applications for large-scale risk stratification, care gap identification, and population-level outcome prediction across Epic's extensive user base. Machine learning models leverage Compass Rose's aggregated clinical data, standardized quality measures, and longitudinal patient tracking to develop predictive algorithms for chronic disease management, preventive care optimization, and resource allocation. AI systems built on this platform can identify high-risk patient populations, predict hospital readmissions, and recommend targeted interventions at scale. The tool's integration with Epic's EHR ecosystem enables real-time AI inference during clinical encounters and automated outreach programs guided by ML-based risk scores.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:29"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:105",
        "B2AI_ORG:115"
      ],
      "purpose_detail": "Care coordination module focused on social determinant of health factors. Part of the Epic EHR platform.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://www.epic.com/software/population-health"
    },
    {
      "id": "B2AI_STANDARD:870",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "GX",
      "description": "Great Expectations platform",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2023-06-20",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A platform for organizing, testing, and validating data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://greatexpectations.io/",
      "formal_specification": "https://github.com/great-expectations/great_expectations"
    },
    {
      "id": "B2AI_STANDARD:871",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Pinecone",
      "description": "Pinecone vector database",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2023-06-20",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A database platform built around creating vector representations of data. The basic implementation is a managed, cloud-native product, though there is a free tier.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://www.pinecone.io/",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:54",
        "B2AI_SUBSTRATE:55",
        "B2AI_SUBSTRATE:9"
      ]
    },
    {
      "id": "B2AI_STANDARD:872",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "CLIG",
      "description": "Command Line Interface Guidelines",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2023-06-20",
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "An open-source guide to help with writing command-line programs, based on UNIX principles.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://clig.dev/"
    },
    {
      "id": "B2AI_STANDARD:873",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MINSEQE",
      "description": "Minimum Information about a high-throughput nucleotide SEQuencing Experiment",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "0000-0001-5705-7831",
      "contribution_date": "2023-09-25",
      "collection": [
        "minimuminformationschema"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:13"
      ],
      "purpose_detail": "Five elements of experimental description considered essential when making sequencing data available.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.fged.org/projects/minseqe/",
      "formal_specification": "https://zenodo.org/record/5706412"
    },
    {
      "id": "B2AI_STANDARD:874",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "GGIR",
      "description": "GGIR accelerometry package",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "0000-0001-5705-7831",
      "contribution_date": "2023-09-25",
      "concerns_data_topic": [
        "B2AI_TOPIC:18"
      ],
      "purpose_detail": "An R package to process multi-day raw accelerometer data for physical activity and sleep research.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://cran.r-project.org/web/packages/GGIR/vignettes/GGIR.html",
      "formal_specification": "https://github.com/wadpac/GGIR"
    },
    {
      "id": "B2AI_STANDARD:875",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Badawy et al. 2019",
      "description": "Metadata Concepts for Advancing the Use of Digital Health Technologies in Clinical Research",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "0000-0001-5705-7831",
      "contribution_date": "2023-09-25",
      "concerns_data_topic": [
        "B2AI_TOPIC:18"
      ],
      "purpose_detail": "A proposed metadata set for digital health studies.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://figshare.com/articles/dataset/Supplementary_Material_for_Metadata_Concepts_for_Advancing_the_Use_of_Digital_Health_Technologies_in_Clinical_Research/9944303",
      "publication": "doi:10.1159/000502951"
    },
    {
      "id": "B2AI_STANDARD:876",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "GSCID/BRC CMS v1.5",
      "description": "GSCID/BRC Clinical Metadata Standard",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "0000-0001-5705-7831",
      "contribution_date": "2023-09-25",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:118"
      ],
      "purpose_detail": "A general standard for clinical metadata.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.niaid.nih.gov/research/clinical-metadata-standard"
    },
    {
      "id": "B2AI_STANDARD:877",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MI-CLAIM",
      "description": "Minimum information about clinical artificial intelligence modeling",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "0000-0001-5705-7831",
      "contribution_date": "2023-09-25",
      "has_application": [
        {
          "id": "B2AI_APP:88",
          "category": "B2AI:Application",
          "name": "Clinical AI Reporting Standards and Model Documentation",
          "description": "MI-CLAIM (Minimum Information about Clinical Artificial Intelligence Modeling) checklist is used to standardize reporting of clinical AI studies, ensuring reproducibility, transparency, and appropriate evaluation of machine learning models in healthcare. Researchers leverage MI-CLAIM guidelines to document essential details about model development, validation approaches, and clinical context that enable others to assess reliability and reproduce findings. The standard supports automated model card generation, structured documentation for regulatory submissions, and systematic reviews of clinical AI literature by providing a consistent framework for reporting. MI-CLAIM compliance facilitates responsible AI development by ensuring key information about data provenance, model limitations, and intended use cases is explicitly documented.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "minimuminformationschema"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "MI-CLAIM (Minimum Information about CLinical AI Modeling) is a reporting standard and documentation checklist developed to address transparency and reproducibility challenges in clinical artificial intelligence research, published in Nature Medicine in 2020 by a multidisciplinary team of clinical and data scientists. MI-CLAIM serves two primary purposes - enabling direct assessment of clinical impact including fairness considerations, and allowing rapid replication of the technical design process for clinical AI studies. The standard provides a comprehensive checklist in MS Word table format covering essential reporting elements including study design and data characteristics (patient demographics, inclusion/exclusion criteria, data sources, temporal validation), model development details (feature engineering, architecture selection, hyperparameter tuning, training procedures), performance evaluation (metrics across demographic subgroups, confidence intervals, comparison to clinical standards), clinical implementation considerations (decision thresholds, interpretability mechanisms, failure modes), and ethical aspects (bias assessment, fairness metrics, regulatory status). The repository encourages community feedback through GitHub Issues to continuously improve the standard as the field evolves, promoting best practices for transparent, reproducible, and equitable clinical AI development and deployment across healthcare applications.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/beaunorgeot/MI-CLAIM",
      "publication": "doi:10.1038/s41591-020-1041-y",
      "formal_specification": "https://github.com/beaunorgeot/MI-CLAIM"
    },
    {
      "id": "B2AI_STANDARD:878",
      "category": "B2AI_STANDARD:DataStandardOrTool",
      "name": "CSVW",
      "description": "CSV on the Web",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "0000-0001-5705-7831",
      "contribution_date": "2023-09-25",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A standard for describing and clarifying the content of CSV tables.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://csvw.org/",
      "formal_specification": "https://w3c.github.io/csvw/syntax/",
      "has_relevant_data_substrate": [
        "B2AI_SUBSTRATE:6"
      ]
    },
    {
      "id": "B2AI_STANDARD:879",
      "category": "B2AI_STANDARD:DataStandardOrTool",
      "name": "BagIt",
      "description": "BagIt file packaging format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "0000-0001-5705-7831",
      "contribution_date": "2023-09-25",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A set of hierarchical file layout conventions for storage and transfer of arbitrary digital content.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://datatracker.ietf.org/doc/rfc8493/",
      "publication": "doi:10.17487/RFC8493",
      "formal_specification": "https://datatracker.ietf.org/doc/rfc8493/"
    },
    {
      "id": "B2AI_STANDARD:880",
      "category": "B2AI_STANDARD:DataStandardOrTool",
      "name": "Unity Catalog",
      "description": "Unity Catalog",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "0000-0001-5705-7831",
      "contribution_date": "2024-11-02",
      "has_application": [
        {
          "id": "B2AI_APP:89",
          "category": "B2AI:Application",
          "name": "Unified Data and AI Asset Governance",
          "description": "Unity Catalog is used in biomedical AI for centralized governance of data assets, ML models, and AI artifacts across multi-cloud and hybrid healthcare IT environments. Healthcare organizations leverage Unity Catalog to implement fine-grained access controls for sensitive patient data used in model training, track lineage from raw clinical data through processed features to trained models, and ensure HIPAA compliance across distributed AI development teams. The catalog provides a single source of truth for data discovery, enables secure data sharing across research and clinical domains, and maintains comprehensive audit logs for regulatory compliance. Unity Catalog's integration with major ML platforms facilitates governed AI development where data scientists can access approved datasets while maintaining security and privacy requirements.",
          "used_in_bridge2ai": false
        }
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A universal catalog for data and AI. It includes a core set of APIs for tables, unstructured data, and AI assets.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.unitycatalog.io/",
      "formal_specification": "https://github.com/unitycatalog/unitycatalog"
    },
    {
      "id": "B2AI_STANDARD:881",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Senselab",
      "description": "Senselab package",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2023-12-06",
      "concerns_data_topic": [
        "B2AI_TOPIC:36"
      ],
      "purpose_detail": "A Python package for streamlining the processing and analysis of behavioral data, such as voice and speech patterns, with robust and reproducible methodologies.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://sensein.group/senselab/",
      "formal_specification": "https://github.com/sensein/senselab"
    },
    {
      "id": "B2AI_STANDARD:882",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "CLAIM",
      "description": "Checklist for Artificial Intelligence in Medical Imaging (CLAIM)",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2023-12-06",
      "collection": [
        "guidelines"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "purpose_detail": "CLAIM is modeled after the STARD guideline and has been extended to address applications of AI in medical imaging that include classification, image reconstruction, text analysis, and workflow optimization. It is intended to provide a framework for the development and validation of AI algorithms in medical imaging.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://doi.org/10.1148/ryai.2020200029",
      "publication": "doi:10.1148/ryai.2020200029"
    },
    {
      "id": "B2AI_STANDARD:883",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "pydicom",
      "description": "Pydicom software package",
      "related_to": [
        "B2AI_STANDARD:98"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2023-12-06",
      "has_application": [
        {
          "id": "B2AI_APP:90",
          "category": "B2AI:Application",
          "name": "Medical Imaging Data Processing for Deep Learning",
          "description": "pydicom is the essential Python library for AI researchers working with medical imaging data, enabling reading, writing, and manipulation of DICOM files in machine learning pipelines. Virtually all medical imaging AI research using Python leverages pydicom to extract pixel data and metadata from DICOM images, convert images to NumPy arrays for neural network input, and create DICOM-compliant outputs for clinical integration. The library handles diverse DICOM transfer syntaxes and encodings, enables efficient batch processing of large imaging datasets, and provides the foundation for medical imaging data loaders in PyTorch and TensorFlow. pydicom's ability to preserve clinical metadata during AI processing ensures that model outputs maintain appropriate associations with patient context and imaging parameters.",
          "used_in_bridge2ai": false
        }
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "purpose_detail": "Pydicom is a Python package for working with DICOM images.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://pydicom.github.io/",
      "formal_specification": "https://github.com/pydicom/pydicom"
    },
    {
      "id": "B2AI_STANDARD:884",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "CSVY",
      "description": "CSVY format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2025-02-16",
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "CSVY is a file format combining CSV with a YAML header.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/leeper/csvy",
      "formal_specification": "https://github.com/leeper/csvy"
    },
    {
      "id": "B2AI_STANDARD:885",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "openSMILE",
      "description": "openSMILE software",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2025-02-16",
      "used_in_bridge2ai": true,
      "collection": [
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:37"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:117"
      ],
      "purpose_detail": "openSMILE is an open-source audio feature extraction toolkit.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.audeering.com/research/opensmile/",
      "formal_specification": "https://github.com/audeering/opensmile"
    },
    {
      "id": "B2AI_STANDARD:886",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Praat",
      "description": "Praat software",
      "related_to": [
        "B2AI_STANDARD:887"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2025-02-16",
      "used_in_bridge2ai": true,
      "collection": [
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:36"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:117"
      ],
      "purpose_detail": "Praat is software for working with voice data, including speech analysis, segmentation, and synthesis.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.fon.hum.uva.nl/praat/",
      "formal_specification": "https://github.com/praat/praat"
    },
    {
      "id": "B2AI_STANDARD:887",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "Parselmouth",
      "description": "Parselmouth software",
      "related_to": [
        "B2AI_STANDARD:886"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2025-03-05",
      "used_in_bridge2ai": true,
      "collection": [
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:36"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:117"
      ],
      "purpose_detail": "Parselmouth is a Python library for working with Praat software. Parselmouth directly accesses Praat's C/C++ code (which means the algorithms and their output are exactly the same as in Praat) and provides efficient access to the program's data, but also provides an interface that looks no different from any other Python library.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://parselmouth.readthedocs.io/en/stable/",
      "formal_specification": "https://github.com/YannickJadoul/Parselmouth"
    },
    {
      "id": "B2AI_STANDARD:888",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "BigTIFF",
      "description": "BigTIFF format",
      "related_to": [
        "B2AI_STANDARD:383"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2025-03-13",
      "collection": [
        "audiovisual",
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "BigTIFF is an image format. It is a variant of the TIFF format that uses 64-bit offsets thereby supporting files up to 18,000 petabytes in size, vastly transcending TIFF's normal 4 GB limit. Since the format also supports all of the normal features and header tags of TIFF_6 and the extended metadata offered by GeoTIFF, it provides good service in the GIS domain, medical imaging, and other applications that employ large scanners or cameras.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.loc.gov/preservation/digital/formats/fdd/fdd000328.shtml"
    },
    {
      "id": "B2AI_STANDARD:889",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "torchaudio",
      "description": "TorchAudio library",
      "related_to": [
        "B2AI_STANDARD:816"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2025-03-13",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:91",
          "category": "B2AI:Application",
          "name": "Biomedical Audio Analysis and Speech-Based Diagnostics",
          "description": "torchaudio is used in AI applications for processing and analyzing biomedical audio signals including speech patterns for neurological assessment, respiratory sounds for pulmonary diagnosis, and cardiac auscultation for automated heart disease detection. Deep learning models built with torchaudio process audio biomarkers such as cough sounds for COVID-19 screening, voice characteristics for Parkinson's disease detection, and lung sounds for pneumonia classification. The library's preprocessing capabilities, pretrained models, and integration with PyTorch enable researchers to develop audio-based AI diagnostics that can be deployed on mobile devices for remote patient monitoring, telehealth applications, and resource-limited settings where traditional diagnostic equipment is unavailable.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:37"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:117"
      ],
      "purpose_detail": "A software library for audio and signal processing with PyTorch.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://pytorch.org/audio/stable/index.html"
    },
    {
      "id": "B2AI_STANDARD:890",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "RadElement",
      "description": "RadElement Common Data Elements (CDEs) for Radiology",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2025-03-13",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A set of common data elements for radiology research.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.radelement.org/"
    },
    {
      "id": "B2AI_STANDARD:891",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "FAIRGenomes",
      "description": "FAIR Genomes Semantic Model",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2025-03-13",
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "purpose_detail": "A semantic model for describing genomic data in a FAIR manner.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://github.com/fairgenomes/fairgenomes-semantic-model",
      "formal_specification": "https://github.com/fairgenomes/fairgenomes-semantic-model/blob/main/fair-genomes.yml"
    },
    {
      "id": "B2AI_STANDARD:893",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "REMBI",
      "description": "Recommended Metadata for Biological Images (REMBI)",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2025-03-13",
      "concerns_data_topic": [
        "B2AI_TOPIC:15"
      ],
      "purpose_detail": "Draft metadata guidelines to begin addressing the needs of diverse communities within light and electron microscopy",
      "is_open": true,
      "requires_registration": false,
      "url": "https://docs.google.com/spreadsheets/d/1Ck1NeLp-ZN4eMGdNYo2nV6KLEdSfN6oQBKnnWU6Npeo/edit?gid=1023506919#gid=1023506919",
      "publication": "doi:10.1038/s41592-021-01166-8"
    },
    {
      "id": "B2AI_STANDARD:894",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "PRIDE",
      "description": "PRoteomics IDEntifications Database",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:92",
          "category": "B2AI:Application",
          "name": "Proteomics Data Mining and Peptide Identification",
          "description": "PRIDE (PRoteomics IDEntifications) database is used in AI applications for training deep learning models on mass spectrometry proteomics data, enabling improved peptide identification, protein quantification, and post-translational modification prediction. Machine learning systems leverage PRIDE's extensive repository of annotated spectra to develop neural networks for de novo peptide sequencing, spectral quality assessment, and cross-linking analysis. AI models trained on PRIDE data improve upon traditional database search algorithms by learning complex patterns in fragmentation spectra, enabling identification of novel peptides, non-canonical modifications, and proteoforms. The database's standardized mzML and mzIdentML formats facilitate reproducible AI research in clinical proteomics, biomarker discovery, and precision medicine applications.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "dataregistry"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5",
        "B2AI_TOPIC:23"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:116"
      ],
      "purpose_detail": "The PRIDE PRoteomics IDEntifications (PRIDE) Archive database is a centralized, standards compliant, public data repository for mass spectrometry proteomics data, including protein and peptide identifications and the corresponding expression values, post-translational modifications and supporting mass spectra evidence (both as raw data and peak list files). PRIDE is a core member in the ProteomeXchange (PX) consortium, which provides a standardised way for submitting mass spectrometry based proteomics data to public-domain repositories. Datasets are submitted to ProteomeXchange via PRIDE and are handled by expert bio-curators. All PRIDE public datasets can also be searched in ProteomeCentral, the portal for all ProteomeXchange datasets.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.ebi.ac.uk/pride/",
      "publication": "doi:10.1093/nar/gkae1011"
    },
    {
      "id": "B2AI_STANDARD:895",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "ASCII File Format Guidelines for Earth Science Data",
      "description": "American Standard Code for Information Interchange ASCII File Format Guidelines for Earth Science Data",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:93",
          "category": "B2AI:Application",
          "name": "Environmental and Climate Health Data Integration",
          "description": "ASCII File Format Guidelines for Earth Science Data are used in AI applications that integrate environmental and climate data with biomedical datasets to model health impacts of environmental change, predict disease patterns related to climate factors, and develop early warning systems for climate-sensitive health outcomes. Machine learning models leverage standardized ASCII representations of temperature, precipitation, air quality, and other environmental variables to train on relationships between environmental exposures and health outcomes such as vector-borne disease transmission, heat-related illness, respiratory disease exacerbation, and mental health impacts. The standardized format enables AI systems to integrate NASA Earth observations with clinical and epidemiological data for One Health applications bridging human, animal, and environmental health.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "fileformat",
        "implementation_maturity_production",
        "standards_process_maturity_final"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:114"
      ],
      "purpose_detail": "American Standard Code for Information Interchange (ASCII) file format guidelines for NASA Earth science data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.earthdata.nasa.gov/about/esdis/esco/standards-practices/ascii-file-format-guidelines-earth-science-data",
      "formal_specification": "https://www.earthdata.nasa.gov/s3fs-public/imported/ESDS-RFC-027v1.1.pdf"
    },
    {
      "id": "B2AI_STANDARD:896",
      "category": "B2AI_STANDARD:SoftwareOrTool",
      "name": "GIS Toolchain",
      "description": "Geographic Informations Systems Toolchain",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:94",
          "category": "B2AI:Application",
          "name": "Geospatial Health Analytics and Environmental Exposure Modeling",
          "description": "GIS Toolchain (from OHDSI) is used in AI applications for integrating geographic information with clinical data to train models that account for environmental exposures, social determinants of health, and spatial disease patterns. Machine learning systems leverage geospatial features derived from this toolchain to develop prediction models for infectious disease spread, environmental health impacts, cancer cluster detection, and health equity analysis. AI applications combine geocoded patient addresses with environmental data layers (air quality, greenspace, food access) to create location-aware risk models that inform population health interventions and resource allocation. The toolchain's integration with OMOP CDM enables spatiotemporal analysis at scale across observational health databases.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "datamodel",
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:14"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:76",
        "B2AI_ORG:115"
      ],
      "purpose_detail": "The GIS toolchain consists of extensions to the OMOP schema, extensions to the OMOP Vocabulary, and GIS-specific software for acquiring and working with geospatial data. Together, these enable researchers to use health-related attributes of the regions where patients live in OHDSI study cohort definitions. For example, you can use the GIS toolchain to define cohorts that include regional data on exposure to toxicants or social deprivation along with EHR data on relevant health outcomes. The toolchain also includes informatics resources that support integration with the main OHDSI tool stack (HADES) and integration with externally supported solutions for geocoding and for finding and deriving relevant data sources from catalogs of available data sources. Importantly, the toolchain allows integrated analysis of geospatial and EHR data without sharing any sensitive patient location data.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://ohdsi.github.io/GIS/"
    },
    {
      "id": "B2AI_STANDARD:897",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "MI-CDM",
      "description": "Medical Imaging Common Data Model",
      "related_to": [
        "B2AI_STANDARD:98",
        "B2AI_STANDARD:243"
      ],
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:95",
          "category": "B2AI:Application",
          "name": "Medical Imaging AI Data Standardization and Multi-Site Studies",
          "description": "MI-CDM (Medical Imaging Common Data Model) is used in AI applications to standardize heterogeneous imaging metadata across institutions, enabling training of robust deep learning models on diverse multi-site imaging datasets. The common data model facilitates AI development by providing consistent representation of imaging protocols, scanner parameters, patient demographics, and clinical annotations across different PACS systems and imaging centers. Machine learning systems leverage MI-CDM to perform federated learning on distributed imaging data without raw image transfer, enable systematic bias detection across sites, and develop models that generalize across different scanner manufacturers and acquisition protocols. The standardization is critical for regulatory-grade AI where model validation requires diverse, well-characterized datasets.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "datamodel",
        "standards_process_maturity_development",
        "implementation_maturity_pilot"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:4"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:76",
        "B2AI_ORG:115"
      ],
      "purpose_detail": "The rapid growth of artificial intelligence (AI) and deep learning techniques require access to large inter-institutional cohorts of data to enable the development of robust models, e.g., targeting the identification of disease biomarkers and quantifying disease progression and treatment efficacy. The Observational Medical Outcomes Partnership Common Data Model (OMOP CDM) has been designed to accommodate a harmonized representation of observational healthcare data. This study proposes the Medical Imaging CDM (MI-CDM) extension, adding two new tables and two vocabularies to the OMOP CDM to address the structural and semantic requirements to support imaging research. The tables provide the capabilities of linking DICOM data sources as well as tracking the provenance of imaging features derived from those images. The implementation of the extension enables phenotype definitions using imaging features and expanding standardized computable imaging biomarkers. This proposal offers a comprehensive and unified approach for conducting imaging research and outcome studies utilizing imaging features.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://doi.org/10.1007/s10278-024-00982-6",
      "publication": "doi:10.1007/s10278-024-00982-6"
    },
    {
      "id": "B2AI_STANDARD:898",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "CX",
      "description": "Cytoscape Exchange",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:96",
          "category": "B2AI:Application",
          "name": "Network Biology and Graph Neural Networks",
          "description": "CX (Cytoscape Exchange) format is used in AI applications for representing biological networks that serve as input to graph neural networks for tasks such as protein function prediction, drug-target interaction prediction, and disease gene prioritization. Machine learning models leverage CX's standardized representation of nodes, edges, and network attributes to train graph-based deep learning architectures that capture complex biological relationships. AI systems use CX-encoded networks for multi-omics data integration, pathway analysis, and systems biology modeling where network topology and node features inform predictions. The format enables sharing of network models across platforms and reproducible AI research in network medicine, synthetic biology, and computational drug discovery.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "fileformat",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:21"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:116"
      ],
      "purpose_detail": "Cytoscape Exchange (CX) format is a network exchange format, designed as a flexible structure for transmission of networks. It is designed for flexibility, modularity, and extensibility, and as a message payload in common REST protocols. It is not intended as an in-memory data model for use in applications.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://cytoscape.org/cx/"
    },
    {
      "id": "B2AI_STANDARD:899",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "h5ad",
      "description": "Anndata h5ad format",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:97",
          "category": "B2AI:Application",
          "name": "Single-Cell Genomics and Deep Learning Integration",
          "description": "h5ad format (HDF5-based AnnData) is the standard file format for AI applications in single-cell genomics, enabling efficient storage and access of large-scale single-cell RNA-seq datasets for training deep learning models. AI systems leverage h5ad's structured representation of cell-by-gene expression matrices, cell metadata, and dimensionality reduction embeddings to perform cell type classification, trajectory inference, gene regulatory network inference, and batch effect correction. Deep learning frameworks integrate seamlessly with h5ad through the AnnData API, supporting applications in cancer cell identification, developmental biology, immune profiling, and drug response prediction at single-cell resolution. The format's efficient sparse matrix storage enables training on datasets with millions of cells.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:116"
      ],
      "purpose_detail": "h5ad is a format for storing and working with annotated data matrices in memory and on disk. It is produced by the Anndata Python package.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://anndata.readthedocs.io/en/stable/",
      "formal_specification": "https://github.com/scverse/anndata/blob/main/src/anndata/_io/h5ad.py"
    },
    {
      "id": "B2AI_STANDARD:900",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Dataset-JSON",
      "description": "Clinical Data Interchange Standards Consortium (CDISC) Dataset-JavaScript Object Notation",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "collection": [
        "datamodel",
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:9"
      ],
      "purpose_detail": "CDISC Dataset-JSON is a JSON-based schema specifically designed for exchanging tabular datasets in clinical studies. It is based on CDISC Dataset-JSON version 1.0 with enhancements, including smaller file sizes, additional metadata, and simpler processing. The format supports file and Application Programming Interface based data exchange, is widely supported across technologies, and can link to Define-XML for additional metadata. Dataset-JSON has the potential to replace Statistical Analysis System (SAS) version 5 XPORT Transport Format (XPT) for submission of electronic study data to regulatory agencies.",
      "is_open": true,
      "requires_registration": true,
      "url": "https://www.cdisc.org/standards/data-exchange/dataset-json",
      "formal_specification": "https://www.cdisc.org/standards/data-exchange/dataset-json/dataset-json-v1-1",
      "responsible_organization": [
        "B2AI_ORG:15"
      ]
    },
    {
      "id": "B2AI_STANDARD:901",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "EDF+",
      "description": "European Data Format Plus",
      "related_to": [
        "B2AI_STANDARD:105"
      ],
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:262",
          "category": "B2AI:Application",
          "name": "I-CARE EDF+ coma EEG ML prognostication resource",
          "description": "Large-scale harmonized clinical continuous EEG resource for machine learning-based prognostication in comatose post-cardiac arrest patients, with original EEG explicitly converted to de-identified EDF+ files supporting annotations, events, and interrupted recordings. The I-CARE (International Cardiac Arrest Research) consortium dataset comprises 56,676 hours of continuous EEG from 1,020 patients across multiple centers, standardized to EDF+ format to enable interoperable ML analytics pipelines. EDF+ capabilities for storing annotations alongside signal data facilitate labeling of seizures, burst suppression, periodic discharges, and outcome markers; interrupted recording support accommodates clinical gaps in monitoring without file fragmentation; and event annotations document stimuli, medication administration, and clinical interventions. This EDF+-based resource enables training and validation of deep learning models (CNNs on raw EEG spectrograms, LSTMs on time-series features, transformers on multi-channel sequences) for automated outcome prediction, seizure detection, and quantitative EEG biomarker extraction in critically ill patients, supporting development of clinical decision support systems for neurological prognostication after cardiac arrest where timely, accurate predictions can guide withdrawal of life-sustaining therapy decisions.",
          "references": [
            "https://doi.org/10.1038/s41597-023-02810-8"
          ]
        }
      ],
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:37"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:115"
      ],
      "purpose_detail": "An extension of the European Data Format (EDF) that maintains EDF compatibility while adding capability to store annotations, events, and stimuli alongside the signal data. It also allows for storing interrupted (non-contiguous) recordings in a single file. EDF+ can save most EEG, PSG, ECG, EMG, and Evoked Potential data that cannot be saved into common hospital information systems. The Persyst universal EEG reader supports this format.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.edfplus.info/specs/edfplus.html",
      "publication": "doi:10.1016/S1388-2457(03)00123-8"
    },
    {
      "id": "B2AI_STANDARD:902",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "ARK PIDs",
      "description": "Archival Resource Key Persistent Identifiers",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:98",
          "category": "B2AI:Application",
          "name": "Persistent Dataset Identification for ML Reproducibility",
          "description": "ARK (Archival Resource Key) persistent identifiers are used in AI applications to create stable, long-term references to training datasets, model artifacts, and research outputs, ensuring reproducibility and provenance tracking in machine learning research. AI systems leverage ARK identifiers to maintain citations to specific versions of datasets used in model training, enabling verification of results and compliance with data governance policies. The identifier scheme's flexibility supports both fine-grained object identification (individual data files) and collections (entire datasets), which is essential for documenting complex ML pipelines. ARK PIDs facilitate data sharing in federated AI research while maintaining clear attribution and enabling auditable access logs for sensitive biomedical data.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "datamodel",
        "standards_process_maturity_draft",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:116"
      ],
      "purpose_detail": "Archival Resource Key (ARK) identifiers are persistent URLs designed to support long-term access to information objects. ARKs can identify digital objects (documents, databases, images, software), physical objects (books, artifacts), living beings, and intangible objects (concepts, services). ARKs are characterized by their internal \"ark:\" label, their NAAN (Name Assigning Authority Number) identifying the naming organization, and features like metadata access through inflections (adding ? to the URL). They are free to create and use, with no fees to assign or use ARKs, and can be hosted on any web server or through the global N2T.net resolver.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://arks.org/about/",
      "formal_specification": "https://datatracker.ietf.org/doc/draft-kunze-ark/"
    },
    {
      "id": "B2AI_STANDARD:903",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "CDS",
      "description": "Clinical Dataset Structure",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2025-05-29",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:99",
          "category": "B2AI:Application",
          "name": "Clinical Decision Support Rule Integration and ML Hybridization",
          "description": "CDS (Clinical Decision Support) specification is used in AI applications to integrate machine learning models with traditional rule-based clinical decision support systems, enabling hybrid AI approaches that combine interpretable rules with data-driven predictions. AI systems leverage CDS Hooks and standardized interfaces to deploy ML models as FHIR-based services that provide real-time recommendations during clinical workflows, such as drug-drug interaction checking augmented with personalized risk predictions, or sepsis alerts that combine guideline-based criteria with neural network early warning scores. The specification enables AI applications to surface predictions within EHR user interfaces at appropriate decision points, while maintaining auditability through standardized request/response formats and provenance metadata.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "datamodel"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:7",
        "B2AI_TOPIC:4"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:114"
      ],
      "purpose_detail": "The Clinical Dataset Structure (CDS) is a standardized way to organize and describe clinical research datasets to make them readily interoperable and easily reusable by humans and machines. It addresses the challenge of integrating multiple data modalities from clinical studies by providing a simple, intuitive file and directory structure. CDS organizes data by datatype at the root level, with each datatype directory structured according to applicable standards or a recommended hierarchy of modality/device/participant directories. The standard includes specifications for metadata files that document the dataset content, structure, and participant information, optimizing datasets for AI-readiness and secondary analysis.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://cds-specification.readthedocs.io/",
      "publication": "doi:10.5281/zenodo.10867040",
      "formal_specification": "https://github.com/AI-READI/cds-specification"
    },
    {
      "id": "B2AI_STANDARD:904",
      "category": "B2AI_STANDARD:DataStandard",
      "name": "Schema.org",
      "description": "A collaborative, community-developed schema for structured data on the Internet",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2025-05-29",
      "used_in_bridge2ai": true,
      "has_application": [
        {
          "id": "B2AI_APP:100",
          "category": "B2AI:Application",
          "name": "Biomedical Knowledge Extraction and Semantic Search",
          "description": "Schema.org vocabularies, particularly the biomedical extensions (BioSchemas), are used in AI applications for automated knowledge extraction from web resources, semantic annotation of datasets, and training large language models on structured biomedical information. AI systems leverage Schema.org markup to extract structured data about proteins, genes, diseases, clinical trials, and medical conditions from web pages, enabling automated knowledge base construction and question-answering systems. The vocabulary supports AI-driven dataset discovery, metadata standardization for machine learning pipelines, and training of biomedical language models that understand relationships between biological entities. Schema.org's widespread adoption makes it valuable for web-scale biomedical data mining and federated search applications.",
          "used_in_bridge2ai": false
        }
      ],
      "collection": [
        "datamodel",
        "standards_process_maturity_final",
        "implementation_maturity_production"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:5"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:116"
      ],
      "purpose_detail": "Schema.org is a collaborative, community activity founded by Google, Microsoft, Yahoo, and Yandex with a mission to create, maintain, and promote schemas for structured data on the Internet. It provides a common vocabulary that webmasters can use to mark up their pages in ways that can be understood by major search engines and other applications. The Schema.org vocabulary consists of a set of types (e.g., Person, Event, Organization), properties (e.g., name, location, startDate), and relationships that can be used with many different encoding formats including RDFa, Microdata, and JSON-LD. As of 2024, over 45 million web domains use Schema.org markup with over 450 billion Schema.org objects. The vocabulary is continuously evolving through an open community process managed by the W3C Schema.org Community Group.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://schema.org/",
      "formal_specification": "https://github.com/schemaorg/schemaorg"
    },
    {
      "id": "B2AI_STANDARD:905",
      "category": "B2AI_STANDARD:BiomedicalStandard",
      "name": "Thermo Fisher RAW",
      "description": "Thermo Fisher RAW mass spectrometry data format",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "contribution_date": "2025-05-29",
      "used_in_bridge2ai": true,
      "collection": [
        "fileformat"
      ],
      "concerns_data_topic": [
        "B2AI_TOPIC:28"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:116"
      ],
      "purpose_detail": "The Thermo Fisher RAW format is a proprietary file format designed for storing mass spectrometry data generated by Thermo Fisher Scientific instruments. It contains detailed information about mass spectra, chromatograms, instrument parameters, and metadata from experimental runs. The format supports multiple mass spectrometry techniques and is accessed through Thermo's software tools like MSFileReader or more recent RawFileReader. While it's proprietary, various conversion tools allow transformation to open formats like mzML or mzXML for broader compatibility with third-party analysis software. The format is widely used in proteomics, metabolomics, and other mass spectrometry-based research applications where preserving the complete experimental context is essential for data interpretation and analysis.",
      "is_open": false,
      "requires_registration": true,
      "url": "https://www.thermofisher.com/us/en/home/industrial/mass-spectrometry.html",
      "responsible_organization": [
        "B2AI_ORG:123"
      ]
    },
    {
      "id": "B2AI_STANDARD:906",
      "category": "B2AI_STANDARD:TrainingProgram",
      "name": "OHDSI 2024 Global Symposium ETL Tutorial",
      "description": "Tutorial: Developing and Evaluating Your Extract, Transform, Load (ETL) Process to the OMOP CDM",
      "contributor_name": "Harry Caufield",
      "contributor_github_name": "caufieldjh",
      "contributor_orcid": "ORCID:0000-0001-5705-7831",
      "concerns_data_topic": [
        "B2AI_TOPIC:4",
        "B2AI_TOPIC:52"
      ],
      "has_relevant_organization": [
        "B2AI_ORG:76"
      ],
      "purpose_detail": "In this video tutorial, students will learn about the tools and practices developed by the OHDSI community to support the journey to establish and maintain an ETL to standardize your data to OMOP CDM and enable standardized evidence generation across a data network.",
      "is_open": true,
      "requires_registration": false,
      "url": "https://www.youtube.com/watch?v=H69dC7f-edQ"
    }
  ],
  "@type": "DataStandardOrToolContainer"
}