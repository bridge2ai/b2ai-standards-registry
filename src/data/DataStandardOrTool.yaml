data_standardortools_collection:
- id: B2AI_STANDARD:1
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: .ACE format
  formal_specification: https://web.archive.org/web/20100609072313/http://bcr.musc.edu/manuals/CONSED.txt
  is_open: true
  name: .ACE format
  purpose_detail: The ACE file format is a specification for storing data about genomic
    contigs. The original ACE format was developed for use with Consed, a program
    for viewing, editing, and finishing DNA sequence assemblies. ACE files are generated
    by various assembly programs, including Phrap, CAP3, Newbler, Arachne, AMOS (sequence
    assembly) (more specifically Minimo) and Tigr Assembler v2.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/ACE_(genomic_file_format)
- id: B2AI_STANDARD:2
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - policy
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: 2023 NIH Data Management and Sharing Policy
  is_open: true
  name: DMS
  purpose_detail: NIH has issued the Data Management and Sharing (DMS) policy (effective
    January 25, 2023) to promote the sharing of scientific data. Sharing scientific
    data accelerates biomedical research discovery, in part, by enabling validation
    of research results, providing accessibility to high-value datasets, and promoting
    data reuse for future research studies. Under the DMS policy, NIH expects that
    investigators and institutions do the following. Plan and budget for the managing
    and sharing of data, Submit a DMS plan for review when applying for funding, Comply
    with the approved DMS plan.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:67
  url: https://sharing.nih.gov/data-management-and-sharing-policy/about-data-management-and-sharing-policy/data-management-and-sharing-policy-overview
  used_in_bridge2ai: true
- id: B2AI_STANDARD:3
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Access to Biological Collections Data Schema
  is_open: true
  name: ABCD
  purpose_detail: The Access to Biological Collections Data (ABCD) Schema is an evolving
    comprehensive standard for the access to and exchange of data about specimens
    and observations (a.k.a. primary biodiversity data). The ABCD Schema attempts
    to be comprehensive and highly structured, supporting data from a wide variety
    of databases. It is compatible with several existing data standards. Parallel
    structures exist so that either (or both) atomised data and free-text can be accommodated.
    Version 1.2 is currently in use with the GBIF (Global Biodiversity Information
    Facility) and BioCASE (Biological Collection Access Service for Europe) networks.
    Apart from the GBIF and BioCASE networks, the potential for the application of
    ABCD extends to internal networks, or in-house legacy data access (e.g. datasets
    from external sources that shall not be converted and integrated into an institution's
    own data, but be kept separately, though easily accessible). By defining relations
    between terms, ABCD is a step towards an ontology for biological collections.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:93
  url: https://abcd.tdwg.org/
  has_application:
  - id: B2AI_APP:1
    category: B2AI:Application
    name: DiSSCo Digital Specimen Architecture AI-Assisted Curation
    description: The Digital Specimen Architecture uses FAIR Digital Object type descriptions
      based on TDWG standards including ABCD. This enables registered AI services
      to automatically discover digital specimens, execute allowed actions, and attach
      machine-readable annotations such as automated extraction from images, relation
      creation for knowledge graphs, and standardization or correction. ABCD defines
      the object structure and attributes in the FDO, allowing AI services to operate
      uniformly across collections and propagate outputs to aggregators such as GBIF
      and GeoCASe.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.3897/biss.7.112678
  - id: B2AI_APP:102
    category: B2AI:Application
    name: Hespi Computer Vision and OCR Pipeline for Herbarium Sheets
    description: Hespi integrates object detection, OCR and handwriting recognition,
      and a multimodal LLM for post-processing and authority control in herbarium
      digitization. The pipeline explicitly situates digitization within community
      standards, citing ABCD alongside Darwin Core. Extracted fields from labels and
      sheet components are mapped to ABCD and DwC fields to produce interoperable
      records consumable by downstream AI and analytics systems.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.48550/arxiv.2410.08740
  - id: B2AI_APP:103
    category: B2AI:Application
    name: ML-Ready Benchmark Datasets via ABCD Standardization
    description: Work on improving transcribed digital specimen data highlights ABCD
      and Darwin Core as the target schema for interoperable outputs and schema-based
      annotation systems such as AnnoSys. This ecosystem underpins ML-ready benchmark
      datasets for herbarium images and semi-automated workflows for data cleaning
      and georeferencing, facilitating training and evaluation of computer vision
      and NLP models while keeping outputs in ABCD and DwC formats for data exchange.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1093/database/baz129
- id: B2AI_STANDARD:4
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: AGP format
  is_open: true
  name: AGP
  purpose_detail: AGP format describes the assembly of a larger sequence object from
    smaller objects. The large object can be a contig, a scaffold (supercontig), or
    a chromosome. Each line (row) of the AGP file describes a different piece of the
    object, and has the column entries defined below. Extended comments follow. It
    does not serve for either a description of how sequence reads were assembled,
    or a description of the alignments between components used to construct a larger
    object. Not all of the information in proprietary assembly files can be represented
    in the AGP format. It is also not for recording the spans of features like repeats
    or genes.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:120
  url: https://www.ncbi.nlm.nih.gov/assembly/agp/AGP_Specification/
- id: B2AI_STANDARD:5
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:3
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Analytical Information Markup Language
  is_open: true
  name: AnIML
  purpose_detail: The Analytical Information Markup Language (AnIML) is the emerging
    ASTM XML standard for analytical chemistry data. It is currently in pre-release
    form. It is a combination of a highly flexible core schema that defines XML tagging
    for any kind of analytical information; A set of technique definition documents.
    These XML files, one per analytical technique, apply tight constraints to the
    flexible core and in turn are defined by the Technique Schema; Extensions to Technique
    Definitions are possible to accommodate vendor- and institution-specific data
    fields. Mission Statement Our goal is to serve as the open-source development
    platform for a new XML standard for Analytical Chemistry Information. The project
    is a collaborative effort between many groups and individuals and is sanctioned
    by the ASTM subcommittee E13.15. http://animl.cvs.sourceforge.net/viewvc/animl/schema/animl-core.xsd
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:8
  url: https://www.animl.org/
- id: B2AI_STANDARD:6
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Animal Research Reporting In Vivo Experiments
  is_open: true
  name: ARRIVE
  publication: doi:10.1371/journal.pbio.3000411
  purpose_detail: Guidelines intended to improve the reporting of animal experiments.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:121
  url: https://arriveguidelines.org/
- id: B2AI_STANDARD:7
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Annotated ECG standard
  formal_specification: https://www.hl7.org/implement/standards/product_brief.cfm?product_id=70
  is_open: true
  name: aECG
  purpose_detail: Provides a common means of electronically storing both the ECG wave
    form and associated annotations.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: https://www.hl7.org/implement/standards/product_brief.cfm?product_id=70
- id: B2AI_STANDARD:8
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:9
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Annotation and Image Markup schema
  formal_specification: https://github.com/NCIP/annotation-and-image-markup
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: AIM
  purpose_detail: 'The Annotation and Image Markup (AIM) project is the first initiative
    to propose and create a standard means of adding information and knowledge to
    medical images in a clinical environment, enabling automatic searching of image
    content. AIM provides a comprehensive solution to imaging challenges including
    the lack of agreed-upon syntax for annotation and markup, standardized semantics
    for annotations, and common formats for annotations and markup. The AIM Model
    captures descriptive information for images with user-generated graphical symbols
    into a single common information source. The project includes multiple components:
    the AIM Template Service (a web service for uploading and downloading AIM templates),
    the AIM Template Builder (a Java application for creating templates with well-defined
    questions and answer choices), and reference implementations like AIM on ClearCanvas
    Workstation. AIM captures results in terms of image regions of interest, semantic
    descriptions, inferences, calculations, and quantitative features derived by computer
    programs. It is interoperable with DICOM-SR and HL7-CDA standards while providing
    unique advantages through an explicit semantic model of imaging results.'
  related_to:
  - B2AI_STANDARD:98
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:71
  url: https://github.com/NCIP/annotation-and-image-markup
  has_application:
  - id: B2AI_APP:2
    category: B2AI:Application
    name: PACS-to-AIM Conversion for ML-Ready Label Generation
    description: A deployed workflow converts proprietary vendor DICOM presentation
      state annotations from commercial PACS systems into AIM XML using the AIM API,
      enabling standardization of legacy radiologist annotations for machine learning.
      A Python module matches lesions across longitudinal studies via 3D coordinates,
      producing AIM files that are imported into ePAD where lesions are linked over
      time and quantitative metrics are computed. This AIM conversion pipeline unlocks
      historical radiologist annotations from PACS for supervised training, radiomics
      pipelines, and generation of high-quality labeled datasets for deep learning,
      while ensuring interoperability and enabling large-scale analysis across institutions.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1007/s10278-019-00191-6
  - id: B2AI_APP:104
    category: B2AI:Application
    name: ePAD AIM-Based Radiomics and ML Data Platform
    description: ePAD stores annotations and quantitative imaging features in AIM
      XML and exposes RESTful web services allowing plugins and external tools to
      retrieve AIM annotations and associated images for downstream analysis. Plugins
      compute biomarkers and save results back into AIM format, while integrations
      with external pipelines such as QIFP and pyRadiomics enable feature extraction
      with outputs persisted in AIM. This AIM-centric architecture facilitates standardized
      training data management, radiomic feature extraction, and programmatic access
      for machine learning workflows, cohort analyses, and interoperable analytics
      environments for quantitative imaging research.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.18383/j.tom.2018.00055
- id: B2AI_STANDARD:9
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ANSI/CTA Standard - The Use of Artificial Intelligence in Health Care
    Trustworthiness
  is_open: false
  name: ANSI/CTA-2090
  purpose_detail: This standard outlines key principles for ensuring trustworthiness
    in AI applications within healthcare, focusing on human trust, technical reliability,
    and regulatory compliance. It provides a framework for evaluating AI systems in
    clinical settings, emphasizing transparency, accountability, and ethical considerations.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:4
  - B2AI_ORG:122
  url: https://shop.cta.tech/products/cta-2090
  has_application:
  - id: B2AI_APP:3
    category: B2AI:Application
    name: Pre-Development ML Scoping and Data Provenance Documentation
    description: CTA-2090 recommends concrete developer actions for AI/ML workflows
      including listing potential use cases before development to properly scope algorithm
      functionality and limits, documenting whether datasets are raw or pre-processed
      and what preprocessing was performed, and understanding how original data were
      collected to identify potential biases. These practices operationalize the standard's
      emphasis on clear intended use, data lineage, traceability, and documentation
      requirements for trustworthy ML pipelines in healthcare, ensuring teams establish
      proper foundations during planning and data preparation stages.
    used_in_bridge2ai: false
    references:
    - https://www.fdli.org/wp-content/uploads/2023/01/9-Ross.pdf
  - id: B2AI_APP:105
    category: B2AI:Application
    name: Bias Assessment and Post-Deployment Monitoring
    description: CTA-2090 guidance is applied through explicit testing for racial
      and other biases including testing on vulnerable populations, contractual diversity
      and representation benchmarks for training data, and inventories to review,
      screen, retrain, and prevent bias during development. Post-deployment operationalization
      includes ongoing real-world testing after approval and continuous review of
      algorithm results during clinical use. These practices implement the standard's
      expectations for bias identification, mitigation, representative data, lifecycle
      monitoring, and real-world performance auditing in healthcare AI systems.
    used_in_bridge2ai: false
    references:
    - https://www.fdli.org/wp-content/uploads/2023/01/9-Ross.pdf
  - id: B2AI_APP:106
    category: B2AI:Application
    name: Z-Inspection Stakeholder Co-Design and Trust Assessment
    description: Healthcare AI systems apply CTA-2090 trustworthiness principles through
      Z-Inspection, an ethically aligned co-design methodology involving interdisciplinary
      stakeholders to examine ethical, technical, medical, and legal implications
      during development and deployment. Assessments uncover dataset bias risks, deployment
      shortcomings such as accent-related accuracy degradation, and protocol effects
      on ML output accuracy. Recommended practices include building explainability
      into models, documenting how decisions are generated, and conducting real-world
      validation that translates the standard's transparency, fairness, and reproducibility
      dimensions into concrete governance and evaluation activities.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.48550/arxiv.2206.09887
- id: B2AI_STANDARD:10
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Applied Biosystems sequence read binary format file
  is_open: false
  name: AB1
  purpose_detail: A binary version of raw DNA sequence reads from Applied Biosystems
    sequencing analysis software. Also known as ABIF.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:123
  url: https://www.thermofisher.com/us/en/home/life-science/sequencing/sanger-sequencing.html
- id: B2AI_STANDARD:11
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Applied Biosystems sequence read binary format file
  formal_specification: https://tools.thermofisher.com/content/sfs/manuals/4346366_DNA_Sequenc_Analysis_5_1_UG.pdf
  is_open: false
  name: ABI
  purpose_detail: A binary version of raw DNA sequence reads from Applied Biosystems
    sequencing analysis software.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:123
  url: https://tools.thermofisher.com/content/sfs/manuals/4346366_DNA_Sequenc_Analysis_5_1_UG.pdf
- id: B2AI_STANDARD:12
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ARB software binary alignment format
  is_open: true
  name: ARB
  publication: doi:10.1093/nar/gkh293
  purpose_detail: A binary alignment format used by the ARB package.
  requires_registration: false
  url: http://www.arb-home.de/documentation.html
- id: B2AI_STANDARD:13
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Archetype Definition Language
  formal_specification: https://specifications.openehr.org/releases/AM/latest/ADL1.4.html
  is_open: true
  name: ADL
  purpose_detail: ADL (Archetype Definition Language) provides a formal, human-readable
    syntax for expressing constraint-based models of clinical information structures.
    It enables the definition of reusable archetypes that constrain generic reference
    models (such as openEHR's) to represent specific clinical concepts like blood
    pressure measurements or problem lists. The language consists of cADL (constraint
    ADL) for structural definitions, dADL (data ADL) for metadata and terminology
    bindings, and an assertion language for business rules. ADL archetypes support
    multi-lingual terminology, specialization hierarchies, and versioning, making
    them suitable for creating maintainable, semantically interoperable health information
    systems. Tools exist for authoring, compiling, and validating ADL archetypes,
    with XML exchange formats also supported. The syntax is designed to be accessible
    to both clinical domain experts and technical implementers.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:79
  url: https://specifications.openehr.org/releases/AM/latest/ADL1.4.html
- id: B2AI_STANDARD:14
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Argonaut Data Query Implementation Guide
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: StructureDefinition-argo-careplan
  purpose_detail: Specifications for sharing single sets of patient care plans. Based
    on FHIR R2.
  related_to:
  - B2AI_STANDARD:109
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: http://www.fhir.org/guides/argonaut/r2/StructureDefinition-argo-careplan.html
- id: B2AI_STANDARD:15
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:16
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Article Minimum Information Standard
  is_open: true
  name: AMIS
  purpose_detail: The curation process is significantly slowed down by missing information
    in the articles analyzed. The identity of the clones used to generate ISH probes
    and the precise sequences tested in reporter assays constituted the most frequent
    omissions. To help authors ensure in the future that necessary information is
    present in their article, the Article Minimum Information Standard (AMIS) guidelines
    have been defined. The guideline describes the mandatory (and useful) information
    that should be mentioned in literature articles to facilitate the curation process.
    These guidelines extend the minimal information defined by the MISFISHIE format
    (Deutsch at al. 2008, Nature Biotechnology).
  requires_registration: false
  url: https://www.aniseed.fr/aniseed/default/submit_data?module=aniseed&action=default:submit_data#tab-4
- id: B2AI_STANDARD:16
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Axt Alignment Format
  is_open: true
  name: Axt
  purpose_detail: The Axt (Alignment eXTended) format is a simple text-based format
    developed by UCSC Genome Browser for storing pairwise DNA sequence alignments
    between two species or sequences. Each alignment block in an Axt file consists
    of four lines - a summary line containing alignment number, chromosome names,
    alignment start and end positions for both sequences, strand information, and
    alignment score, followed by the aligned sequence from the first genome, the aligned
    sequence from the second genome, and a blank separator line. The format uses zero-based
    coordinates for the first sequence and supports alignments on either strand, with
    sequences from the negative strand reverse-complemented. Axt files are particularly
    useful for representing whole-genome alignments between species (such as human-mouse
    or human-chimp comparisons) generated by alignment tools like BLASTZ or LASTZ.
    The format's straightforward structure makes it easy to parse programmatically
    and convert to other formats. UCSC provides utilities including axtToMaf for converting
    to MAF (Multiple Alignment Format), axtChain for chaining together alignments,
    and axtBest for selecting the best alignment for each position. While Axt efficiently
    represents pairwise alignments, it has been largely superseded by MAF and chain/net
    formats for more complex multi-way alignments and hierarchical alignment representations
    in comparative genomics workflows.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:119
  url: https://genome.ucsc.edu/goldenPath/help/axt.html
- id: B2AI_STANDARD:17
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: BAM indexing format file
  is_open: true
  name: BAI
  purpose_detail: A file containing the index for a Binary Alignment Map (BAM) file.
  requires_registration: false
  url: https://www.ncbi.nlm.nih.gov/tools/gbench/tutorial6/
- id: B2AI_STANDARD:18
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: BEDgraph format
  is_open: true
  name: BEDgraph
  purpose_detail: The bedGraph format allows display of continuous-valued data in
    track format.
  related_to:
  - B2AI_STANDARD:19
  - B2AI_STANDARD:20
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:119
  url: http://genome.ucsc.edu/goldenPath/help/bedgraph.html
- id: B2AI_STANDARD:19
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Big Browser Extensible Data Format
  is_open: true
  name: bigBed
  purpose_detail: The bigBed format is an indexed binary format for storing genome
    annotation data, developed by UCSC Genome Browser as a high-performance alternative
    to text-based BED files for large datasets. BigBed files store annotation items
    representing either simple genomic features or linked collections of exons, maintaining
    BED semantics while enabling efficient region-specific data transfer. Files are
    created using the bedToBigBed utility which converts sorted BED files into compressed
    binary format with built-in indexing, requiring chromosome sizes files and optionally
    AutoSql (.as) format definitions for describing standard and custom fields. The
    format supports BED3 through BED12 plus additional user-defined fields, with features
    including itemRgb color specification, extra searchable indices via the -extraIndex
    parameter for track hub item searches, and trackDb settings for mouseOver labels,
    field filtering, and URL transformations. Only the portions of bigBed files needed
    for the currently displayed chromosomal region are transferred to the browser,
    dramatically improving performance compared to full BED file loading. The format
    supports web-accessible hosting via HTTP, HTTPS, or FTP with sparse file caching,
    and includes utilities (bigBedToBed, bigBedInfo, bigBedSummary) for extracting
    and querying data. BigBed files are widely used in UCSC Genome Browser custom
    tracks, track hubs for consortia data sharing, and integrated with genome analysis
    workflows requiring scalable annotation storage and rapid genomic region queries.
  related_to:
  - B2AI_STANDARD:18
  - B2AI_STANDARD:20
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:119
  url: https://genome.ucsc.edu/goldenPath/help/bigBed.html
- id: B2AI_STANDARD:20
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Big Wiggle Format
  is_open: true
  name: bigWig
  purpose_detail: The bigWig format is an indexed binary file format developed by
    UCSC for efficient storage and visualization of dense, continuous genomic data
    displayed as graphs in genome browsers. Created from wiggle (wig) or bedGraph
    files using the wigToBigWig or bedGraphToBigWig utilities, bigWig files enable
    rapid data access by transferring only the portions needed to display specific
    genomic regions, rather than entire datasets. This sparse file caching mechanism
    provides dramatically faster performance than text-based formats when working
    with large-scale datasets such as ChIP-seq, RNA-seq coverage, methylation levels,
    or conservation scores. The format supports various visualization options including
    customizable graph types (bar or points), scaling parameters, smoothing windows,
    logarithmic transformations, and color schemes. BigWig files can also display
    sequence logos using the dynseq feature, which scales nucleotide characters by
    base-resolution scores. The format includes utilities for data extraction (bigWigToBedGraph,
    bigWigToWig, bigWigSummary, bigWigAverageOverBed) and file inspection (bigWigInfo),
    making it suitable for both visualization and downstream computational analysis.
  related_to:
  - B2AI_STANDARD:18
  - B2AI_STANDARD:19
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:119
  url: https://genome.ucsc.edu/goldenPath/help/bigWig.html
- id: B2AI_STANDARD:21
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Binary Alignment Map / Compressed Reference-oriented Alignment Map
  has_relevant_organization:
  - B2AI_ORG:117
  - B2AI_ORG:116
  is_open: true
  name: BAM/CRAM
  purpose_detail: BAM (Binary Alignment/Map) and CRAM (Compressed Reference-oriented
    Alignment Map) are binary formats for storing DNA sequence alignments to reference
    genomes, maintained by the GA4GH Large Scale Genomics work stream. BAM is the
    binary equivalent of the text-based SAM (Sequence Alignment/Map) format, providing
    efficient storage and retrieval of aligned sequencing reads with quality scores,
    alignment positions, CIGAR strings, and optional tags defined in the SAMtags specification.
    CRAM (currently version 3.x) achieves superior compression ratios by storing differences
    from a reference sequence rather than full sequence data, using custom compression
    codecs detailed in the CRAMcodecs specification. Both formats support indexing
    (BAI for BAM, CRAI for CRAM, and CSI as a more scalable successor) enabling rapid
    random access to genomic regions. The formats are widely supported by genomics
    toolkits including samtools, htslib, htsjdk, and GATK, with standard operations
    for sorting, merging, filtering, and format conversion. CRAM offers significant
    storage savings particularly important for large-scale projects, while maintaining
    full compatibility with SAM/BAM workflows. Both formats support the htsget protocol
    for parallel streaming access and can be wrapped with crypt4gh encryption for
    secure data sharing.
  related_to:
  - B2AI_STANDARD:22
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:34
  url: https://samtools.github.io/hts-specs/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:5
    category: B2AI:Application
    name: DeepVariant Deep Learning Variant Calling
    description: DeepVariant uses BAM/CRAM alignment files as input to train convolutional
      neural networks that classify candidate genomic variants with high accuracy
      across multiple sequencing technologies. The model transforms aligned reads
      into image-like pileup tensors that capture base qualities, mapping qualities,
      and strand information, enabling the CNN to learn complex patterns that distinguish
      true genetic variants from sequencing artifacts and alignment errors. This approach
      achieves state-of-the-art performance on benchmark datasets and generalizes
      well across Illumina, PacBio, and Oxford Nanopore sequencing platforms without
      technology-specific parameter tuning.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1038/nbt.4235
    - https://doi.org/10.1038/s41587-021-01108-x
  - id: B2AI_APP:101
    category: B2AI:Application
    name: Deep Learning for Structural Variant Detection
    description: Deep learning models trained on BAM/CRAM files enable detection of
      complex structural variants including deletions, duplications, inversions, and
      translocations that are challenging for traditional callers. Neural networks
      analyze read depth, split reads, discordant read pairs, and local assembly features
      from aligned data to identify structural rearrangements, with applications in
      cancer genomics where somatic structural variants drive tumorigenesis and treatment
      resistance. These models improve sensitivity for detecting variants in repetitive
      regions and provide better breakpoint resolution than conventional methods.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1038/s41467-022-28289-w
    - https://doi.org/10.1093/bioinformatics/btab732
- id: B2AI_STANDARD:22
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Binary Alignment Map format
  formal_specification: https://samtools.github.io/hts-specs/SAMv1.pdf
  is_open: true
  name: BAM
  purpose_detail: A BAM file (.bam) is the binary version of a SAM file.
  related_to:
  - B2AI_STANDARD:21
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:34
  url: https://en.wikipedia.org/wiki/Binary_Alignment_Map
  used_in_bridge2ai: true
- id: B2AI_STANDARD:23
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Binary sequence information Format
  is_open: true
  name: 2bit
  purpose_detail: A .2bit file stores multiple DNA sequences (up to 4 Gb total) in
    a compact randomly-accessible format. The file contains masking information as
    well as the DNA itself.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:119
  url: http://genome.ucsc.edu/FAQ/FAQformat.html#format7
- id: B2AI_STANDARD:24
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Binary variant call format
  is_open: true
  name: BCF
  purpose_detail: A binary version of the variant call format (VCF).
  requires_registration: false
  url: https://samtools.github.io/bcftools/bcftools.html
- id: B2AI_STANDARD:25
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: BioCompute Object standard
  has_relevant_organization:
  - B2AI_ORG:44
  is_open: true
  name: BioCompute
  publication: doi:10.5731/pdajpst.2016.006734
  purpose_detail: 'BioCompute Objects (BCOs) are a formal standard for representing
    bioinformatics computational workflows and analyses, designed to facilitate communication
    and reproducibility in high-throughput sequencing research. Developed through
    a collaborative effort and formally recognized as IEEE Standard 2791-2020, BCOs
    structure critical workflow information into standardized domains including provenance,
    usability, extension, description, execution, parametric, input/output, and error
    domains. BCOs are represented in JSON format adhering to JSON schema draft-07,
    making them both human and machine readable. The standard addresses the challenge
    of documenting complex bioinformatics methods by providing predictable structure
    and stability for workflow communication. Upon assignment of a digital etag, three
    domains become immutable to ensure integrity: the Parametric Domain, Execution
    Domain, and I/O Domain. BCOs support regulatory compliance including FDA Title
    21 CFR Part 11 considerations for electronic records and digital signatures, with
    hash values and encryption keys generated from execution and parametric domains
    for validation purposes.'
  requires_registration: false
  url: https://docs.biocomputeobject.org/user_guide/
- id: B2AI_STANDARD:26
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:20
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Biolink Model
  has_relevant_organization:
  - B2AI_ORG:70
  is_open: true
  name: Biolink
  publication: doi:10.1111/cts.13302
  purpose_detail: Biolink Model is a comprehensive, high-level data model designed
    to standardize types and relationships in biological knowledge graphs. It provides
    a consistent framework for representing biological knowledge across various databases
    and formats, covering entities such as genes, diseases, chemical substances, organisms,
    genomics, phenotypes, and pathways. The model incorporates object-oriented classification
    and graph-oriented features, with a core set of hierarchical, interconnected classes
    (categories) and relationships (predicates). Biolink Model includes over 400 classes
    ranging from molecular entities like genes and proteins to clinical concepts like
    diseases and treatments, plus comprehensive predicates for describing relationships
    such as "treats," "causes," "associated_with," and "regulates." It supports advanced
    features like qualifiers for contextualizing relationships, evidence attribution,
    and knowledge provenance tracking. The model is particularly valuable for translational
    science applications, enabling integration of data from diverse sources including
    clinical databases, molecular biology repositories, and literature mining systems.
    It serves as the foundational schema for knowledge graphs in the Biomedical Data
    Translator project and other large-scale biomedical data integration efforts.
  requires_registration: false
  url: https://biolink.github.io/biolink-model/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:6
    category: B2AI:Application
    name: Translator Knowledge Graph for Drug Repurposing
    description: NCATS Biomedical Data Translator uses Biolink Model to integrate
      diverse biomedical knowledge sources into a unified knowledge graph supporting
      AI-driven drug repurposing and mechanism discovery. The standardized Biolink
      schema enables graph neural networks to perform multi-hop reasoning across chemical-protein-disease
      relationships, identifying candidate therapeutics by connecting drugs to diseases
      through intermediate biological entities. The Translator system's reasoning
      agents leverage Biolink predicates to score and rank hypothesized drug-disease
      associations based on mechanistic evidence paths learned from integrated data.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1111/cts.13302
- id: B2AI_STANDARD:27
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Biological Expression Language
  formal_specification: https://language.bel.bio/
  is_open: true
  name: BEL
  purpose_detail: The Biological Expression Language (BEL) is a domain-specific language
    for representing scientific findings from life sciences literature in a computable,
    structured format. BEL captures causal and correlative relationships between biological
    entities (genes, proteins, complexes, biological processes, pathways) along with
    their experimental and publication context, enabling systematic knowledge representation
    and integration. BEL statements are expressed as triples (subject-relationship-object)
    that can be combined into biological networks and knowledge graphs. Each BEL assertion
    is packaged as a "nanopub" that includes provenance information, experimental
    conditions, and citations, allowing relationships to be properly evaluated in
    context. The language supports standard biological nomenclatures (Gene Ontology,
    HGNC, ChEBI, etc.) through namespace integration and provides a simplified syntax
    that is more accessible than traditional chemical notation. BEL's computable format
    enables applications in reverse causal reasoning, heat diffusion algorithms, prior
    knowledge for machine learning models, and detection of contradictory findings
    across literature. The standard is maintained by the BEL Language Committee with
    updates managed through BEL Enhancement Proposals (BEPs).
  requires_registration: false
  url: https://bel.bio/
- id: B2AI_STANDARD:28
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Biomedical Research Integrated Domain Group Model
  formal_specification: https://github.com/CBIIT/bridg-model/
  has_relevant_organization:
  - B2AI_ORG:15
  - B2AI_ORG:31
  - B2AI_ORG:40
  - B2AI_ORG:71
  is_open: true
  name: BRIDG Model
  publication: doi:10.1093/jamia/ocx004
  purpose_detail: The Biomedical Research Integrated Domain Group (BRIDG) Model is
    a collaborative effort engaging stakeholders from the Clinical Data Interchange
    Standards Consortium (CDISC), the HL7 BRIDG Work Group, the US National Cancer
    Institute (NCI), and the US Food and Drug Administration (FDA). The goal of the
    BRIDG Model is to produce a shared view of the dynamic and static semantics for
    the domain of basic, pre-clinical, clinical, and translational research and its
    associated regulatory artifacts.
  requires_registration: false
  url: https://bridgmodel.nci.nih.gov/
- id: B2AI_STANDARD:29
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: BioPAX standard
  is_open: true
  name: BioPAX
  publication: doi:10.1038/nbt.1666
  purpose_detail: BioPAX is a standard language that aims to enable integration, exchange,
    visualization and analysis of biological pathway data. Specifically, BioPAX supports
    data exchange between pathway data groups and thus reduces the complexity of interchange
    between data formats by providing an accepted standard format for pathway data.
    By offering a standard, with well-defined semantics for pathway representation,
    BioPAX allows pathway databases and software to interact more efficiently. In
    addition, BioPAX enables the development of pathway visualization from databases
    and facilitates analysis of experimentally generated data through combination
    with prior knowledge. The BioPAX effort is coordinated closely with that of other
    pathway related standards initiatives namely; PSI-MI, SBML, CellML, and SBGN in
    order to deliver a compatible standard in the areas where they overlap.
  requires_registration: false
  url: http://www.biopax.org/
- id: B2AI_STANDARD:30
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Bioschemas
  formal_specification: https://github.com/BioSchemas/specifications
  has_relevant_organization:
  - B2AI_ORG:28
  is_open: true
  name: Bioschemas
  purpose_detail: Bioschemas is a community-driven initiative that extends Schema.org
    to improve the findability and discoverability of life sciences resources on the
    web through structured markup. The project makes two main contributions to the
    life sciences community - proposing new types and properties to Schema.org specifically
    for life science resources, and defining usage profiles over existing Schema.org
    types that specify essential, recommended, and optional properties for consistent
    markup. Bioschemas profiles significantly simplify the markup process by reducing
    complex Schema.org types to manageable subsets while ensuring compatibility with
    search engines like Google Dataset Search. The initiative has achieved major recognition
    with six Bioschemas types (BioChemEntity, ChemicalSubstance, Gene, MolecularEntity,
    Protein, Taxon) officially included in Schema.org version 13.0. Endorsed by the
    European Research Council and serving as a flagship policy of ELIXIR, Bioschemas
    supports FAIR data principles by enabling automated discovery, collation, and
    analysis of distributed life sciences resources including datasets, software applications,
    training materials, and biological entities across the web ecosystem.
  requires_registration: false
  url: https://bioschemas.org/
- id: B2AI_STANDARD:31
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Biospecimen Reporting for Improved Study Quality
  has_relevant_organization:
  - B2AI_ORG:72
  is_open: true
  name: BRISQ
  publication: doi:10.1002/cncy.20147
  purpose_detail: Human biospecimens are subject to a number of different collection,
    processing, and storage factors that can significantly alter their molecular composition
    and consistency. These biospecimen preanalytical factors, in turn, influence experimental
    outcomes and the ability to reproduce scientific results. Currently, the extent
    and type of information specific to the biospecimen preanalytical conditions reported
    in scientific publications and regulatory submissions varies widely. To improve
    the quality of research utilizing human tissues, it is critical that information
    regarding the handling of biospecimens be reported in a thorough, accurate, and
    standardized manner. The Biospecimen Reporting for Improved Study Quality (BRISQ)
    recommendations outlined herein are intended to apply to any study in which human
    biospecimens are used. The purpose of reporting these details is to supply others,
    from researchers to regulators, with more consistent and standardized information
    to better evaluate, interpret, compare, and reproduce the experimental results.
    The BRISQ guidelines are proposed as an important and timely resource tool to
    strengthen communication and publications around biospecimen-related research
    and help reassure patient contributors and the advocacy community that the contributions
    are valued and respected
  requires_registration: false
  url: https://doi.org/10.1002/cncy.20147
- id: B2AI_STANDARD:32
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:20
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: BioXSD
  formal_specification: https://github.com/bioxsd/bioxsd
  has_relevant_organization:
  - B2AI_ORG:28
  is_open: true
  name: BioXSD
  publication: doi:10.1093/bioinformatics/btq391
  purpose_detail: Data model and exchange formats for basic bioinformatics types of
    data - sequences, alignments, feature records with associated data and metadata.
  requires_registration: false
  url: http://bioxsd.org/
  has_application:
  - id: B2AI_APP:7
    category: B2AI:Application
    name: FreeContact Protein Contact Prediction ML Output Format
    description: FreeContact implements statistical learning-based approaches for protein
      residue-residue contact prediction using mean-field Direct Coupling Analysis
      and PSICOV sparse inverse covariance methods. The tool explicitly supports BioXSD
      as an output format to facilitate integration into bioinformatics workflows
      and web services, with planned BioXSD input support. This demonstrates BioXSD
      serving as the standardized exchange format for machine learning tool outputs,
      enabling incorporation of ML-based contact predictions into interoperable analysis
      pipelines.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1186/1471-2105-15-85
  - id: B2AI_APP:107
    category: B2AI:Application
    name: OntoDT Datatype Ontology for ML Dataset Repositories
    description: The OntoDT generic ontology of datatypes uses BioXSD to improve representation
      of basic bioinformatics datatypes in exchange formats, enabling construction
      of taxonomies for datasets, data mining tasks, generalizations, and algorithms.
      OntoDT can be used for annotation and querying of machine learning dataset repositories
      and for constructing data mining workflows. This links BioXSD datatype representations
      to ML and data mining ontologies, supporting standardized workflow assembly
      through consistent bioinformatics datatype semantics.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1016/j.ins.2015.08.006
- id: B2AI_STANDARD:33
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:22
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Brain Imaging Data Structure
  formal_specification: https://github.com/bids-standard/bids-specification
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:3
  is_open: true
  name: BIDS
  publication: doi:10.1038/sdata.2016.44
  purpose_detail: The Brain Imaging Data Structure (BIDS) is a simple and intuitive
    way to organize and describe data. This document defines the BIDS specification,
    which provides many details to help implement the standard. It includes the core
    specification as well as many extensions to specific brain imaging modalities,
    and increasingly also to other kinds of data.
  requires_registration: false
  url: https://bids-specification.readthedocs.io/en/stable/
  has_application:
  - id: B2AI_APP:8
    category: B2AI:Application
    name: BIDS Apps for ML-Ready Preprocessing and Feature Extraction
    description: Containerized BIDS Apps such as fMRIPrep, MRIQC, and MRtrix3 Connectome
      accept BIDS datasets and emit standardized derivatives including preprocessed
      time series, image quality metrics, and connectivity matrices that function
      directly as inputs or labels for ML workflows. The BIDS Apps ecosystem uses
      containerization to provide reproducible preprocessing pipelines across systems
      including HPC environments via Singularity, enabling consistent feature extraction
      for training machine learning models on neuroimaging data.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1371/journal.pcbi.1005209
  - id: B2AI_APP:108
    category: B2AI:Application
    name: PyBIDS and MNE-BIDS Programmatic Dataset Assembly
    description: PyBIDS provides programmatic access to query BIDS datasets and their
      metadata and to organize derivatives in BIDS-Derivatives format, facilitating
      reproducible train-validation splits and feature assembly for ML pipelines.
      MNE-BIDS provides an integration layer for MNE-Python enabling standardized
      ingestion of EEG, MEG, and iEEG BIDS data into analysis pipelines that feed
      ML methods. These tools enable automated dataset selection, metadata-driven
      curation, and scalable learning across neuroimaging studies.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1162/imag_a_00103
  - id: B2AI_APP:109
    category: B2AI:Application
    name: BIDS Derivatives and Connectivity Specifications for ML Features
    description: The BIDS Connectivity extension and Derivatives specifications define
      interoperable formats for structural and functional connectivity matrices, seed-based
      maps, tractograms, and tractometry across modalities including sMRI, fMRI, DWI,
      PET, EEG, iEEG, and MEG. These standardized derivative schemas enable common
      ML features to be shared and allow benchmarking of ML models on consistent feature
      representations, supporting reproducible radiomics and connectomics analyses.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1162/imag_a_00103
  - id: B2AI_APP:110
    category: B2AI:Application
    name: XGBoost-Based DICOM to BIDS Automated Conversion
    description: An XGBoost classifier trained on DICOM metadata achieves 99.5% accuracy
      in classifying MRI acquisition types and automatically transforms unstructured
      clinical imaging into BIDS datasets with little to no user intervention. This
      ML-enabled conversion tool reduces manual curation burden, accelerates creation
      of standardized training corpora, and enables clinical imaging data to be rapidly
      prepared for downstream machine learning analyses in BIDS format.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1007/s12021-024-09659-5
- id: B2AI_STANDARD:34
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - diagnosticinstrument
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Brief Fatigue Inventory
  formal_specification: http://www.npcrc.org/files/news/brief_fatigue_inventory.pdf
  is_open: true
  name: BFI
  purpose_detail: The Brief Fatigue Inventory (BFI) is used to rapidly assess the
    severity and impact of cancer-related fatigue. An increasing focus on cancer-related
    fatigue emphasized the need for sensitive tools to assess this most frequently
    reported symptom. The six interference items correlate with standard quality-of-life
    measures.
  requires_registration: false
  url: https://www.mdanderson.org/research/departments-labs-institutes/departments-divisions/symptom-research/symptom-assessment-tools/brief-fatigue-inventory.html
- id: B2AI_STANDARD:35
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - diagnosticinstrument
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Brief Pain Inventory
  formal_specification: http://www.npcrc.org/files/news/briefpain_short.pdf
  is_open: true
  name: BPI
  purpose_detail: The Brief Pain Inventory (BPI) rapidly assesses the severity of
    pain and its impact on functioning. The BPI has been translated into dozens of
    languages, and it is widely used in both research and clinical settings.
  requires_registration: false
  url: https://www.mdanderson.org/research/departments-labs-institutes/departments-divisions/symptom-research/symptom-assessment-tools/brief-pain-inventory.html
- id: B2AI_STANDARD:36
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Browser Extensible Data Format
  formal_specification: https://github.com/samtools/hts-specs/blob/master/BEDv1.pdf
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:53
  is_open: true
  name: BED
  purpose_detail: The Browser Extensible Data (BED) format is a flexible, tab-delimited
    text format designed for defining annotation tracks in genome browsers, particularly
    the UCSC Genome Browser. BED format provides a standardized way to represent genomic
    features with positional information, supporting both simple coordinate-based
    annotations and complex multi-exon structures. The format consists of 12 possible
    fields, with the first three (chromosome, start position, end position) being
    mandatory and nine additional optional fields providing detailed feature information
    including name, score, strand orientation, thick start/end coordinates for coding
    regions, RGB color values for visualization, and block structures for representing
    discontinuous features like exons and introns. BED files use zero-based, half-open
    coordinate system where the start position is inclusive and the end position is
    exclusive, enabling precise genomic interval representation. The format supports
    various annotation types from simple genomic intervals to complex gene models,
    making it essential for genomic data visualization, comparative genomics, and
    functional annotation workflows in bioinformatics research.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:119
  url: https://genome.ucsc.edu/FAQ/FAQformat.html#format1
- id: B2AI_STANDARD:37
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Business Process Modeling Notation
  has_relevant_organization:
  - B2AI_ORG:10
  is_open: true
  name: BPMN
  purpose_detail: A graphical notation that depicts the steps in a business process.
    BPMN depicts the end-to-end flow of a business process. The notation has been
    specifically designed to coordinate the sequence of processes and the messages
    that flow between different process participants in a related set of activities.
  requires_registration: false
  url: https://www.omg.org/bpmn/index.htm
- id: B2AI_STANDARD:38
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Case Management Model and Notation
  has_relevant_organization:
  - B2AI_ORG:10
  is_open: true
  name: CMMN
  purpose_detail: A common meta- model and notation for modeling and graphically expressing
    a case as well as an interchange format for exchanging case models among different
    tools.
  requires_registration: false
  url: https://www.omg.org/cmmn/
- id: B2AI_STANDARD:39
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Case Report guidelines
  is_open: true
  name: CARE
  purpose_detail: The CARE guidelines provide a framework that supports transparency
    and accuracy in the publication of case reports and the reporting of information
    from patient encounters.
  requires_registration: false
  url: https://www.care-statement.org/
- id: B2AI_STANDARD:40
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDISC Case Report Tabulation Data Definition Specification
  is_open: true
  name: Define
  purpose_detail: The CDISC Case Report Tabulation Data Definition Specification (define.xml)
    Version 1.0 reflects changes from a comment period through the Health Level 7
    (HL7) Regulated Clinical Research Information Management Technical Committee (RCRIM)
    in December 2003 (www.hl7.org) and CDISC's website in September 2004 as well as
    the work done by the define.xml team in conjunction with the CDISC ODM team to
    add functionality, features, and additional documentation. This document specifies
    the standard for providing Case Report Tabulations Data Definitions in an XML
    format for submission to regulatory authorities (e.g., FDA). The XML schema used
    to define the expected structure for these XML files is based on an extension
    to the CDISC Operational Data Model (ODM). The 1999 FDA electronic submission
    (eSub) guidance and the electronic Common Technical Document (eCTD) documents
    specify that a document describing the content and structure of the included data
    should be provided within a submission. This document is known as the Data Definition
    Document (e.g., define.pdf in the 1999 guidance). The Data Definition Document
    provides a list of the datasets included in the submission along with a detailed
    description of the contents of each dataset. To increase the level of automation
    and improve the efficiency of the Regulatory Review process, define.xml can be
    used to provide the Data Definition Document in a machine-readable format.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:15
  url: https://www.cdisc.org/standards/data-exchange/define-xml
- id: B2AI_STANDARD:41
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDISC Controlled Terminology for Analysis Dataset Model
  formal_specification: https://evs.nci.nih.gov/ftp1/CDISC/ADaM/
  is_open: true
  name: CDISC ADaM
  purpose_detail: Controlled terminology for data analysis and assessments.
  related_to:
  - B2AI_STANDARD:42
  - B2AI_STANDARD:43
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:15
  url: https://evs.nci.nih.gov/ftp1/CDISC/ADaM/ADaM%20Terminology.html
- id: B2AI_STANDARD:42
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDISC Controlled Terminology for Data Collection for Protocol
  formal_specification: https://evs.nci.nih.gov/ftp1/CDISC/Protocol/
  is_open: true
  name: CDISC Protocol
  purpose_detail: Controlled terminology for biomedical protocols.
  related_to:
  - B2AI_STANDARD:41
  - B2AI_STANDARD:43
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:15
  url: https://evs.nci.nih.gov/ftp1/CDISC/Protocol/Protocol%20Terminology.html
- id: B2AI_STANDARD:43
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDISC Controlled Terminology for Therapeutic Area Standards
  is_open: true
  name: CDISC TAUGs
  purpose_detail: Disease-specific metadata, examples and guidance on implementing
    CDISC standards.
  related_to:
  - B2AI_STANDARD:41
  - B2AI_STANDARD:42
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:15
  url: https://www.cdisc.org/standards/therapeutic-areas
- id: B2AI_STANDARD:44
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDISC Controlled Terminology Standards for Data Aggregation through
    Study Data Tabulation Model (including QRS, Medical Device and Pharmacogenomics
    Data)
  formal_specification: https://evs.nci.nih.gov/ftp1/CDISC/SDTM/
  is_open: true
  name: SDTM
  purpose_detail: A standard for organizing and formatting data to streamline processes
    in collection, management, analysis and reporting.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:15
  url: https://www.cdisc.org/standards/foundational/sdtm
- id: B2AI_STANDARD:45
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDISC Controlled Terminology Standards for Data Collection through
    Clinical Data Acquisition Standards Harmonization
  formal_specification: https://evs.nci.nih.gov/ftp1/CDISC/SDTM/
  is_open: true
  name: CDASH
  purpose_detail: The Clinical Data Acquisition Standards Harmonization (CDASH) is
    a CDISC foundational standard that establishes consistent data collection practices
    across clinical studies and sponsors. CDASH provides a standardized framework
    for designing case report forms (CRFs) and electronic data capture (eCRF) systems
    by defining a common set of data collection fields and structures. The standard
    specifies field names, labels, prompt questions, and controlled terminology for
    capturing clinical observations, ensuring that data can be directly traced to
    the Study Data Tabulation Model (SDTM) without extensive post-collection transformation.
    By harmonizing data collection at the source, CDASH reduces data cleaning efforts,
    improves data quality, and accelerates the transition from collection to analysis.
    The standard covers various therapeutic areas and data domains including demographics,
    vital signs, adverse events, concomitant medications, and laboratory results.
    CDISC provides a library of ready-to-use, CDASH-compliant annotated eCRFs in multiple
    formats (PDF, HTML, XML) that can be implemented as-is or customized for specific
    study needs, facilitating regulatory submission and data review processes.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:15
  url: https://www.cdisc.org/standards/foundational/cdash
- id: B2AI_STANDARD:46
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDISC Dataset-XML
  is_open: true
  name: CDISC Dataset
  purpose_detail: CDISC Dataset-XML, which was released for comment under the name
    StudyDataSet-XML but was renamed to avoid confusion with the CDISC SDS team,
    is a new standard used to exchange study datasets in an XML format. The purpose
    of Dataset-XML is to support the interchange of tabular data for clinical research
    applications using ODM-based XML technologies. The Dataset-XML model is based
    on the CDISC Operational Data Model (ODM) standard and should follow the metadata
    structure defined in the CDISC Define-XML standard. Dataset-XML can represent
    any tabular dataset including SDTM, ADaM, SEND, or non-standard legacy datasets.
    Some noteworthy items relating to Dataset-XML v1.0 include alternative to SAS
    Version 5 Transport (XPT) format for datasets ODM-based model for representation
    of SEND, SDTM, ADaM or legacy datasets Capable of supporting CDISC regulatory
    data submissions Based on Define-XML v2 or v1 metadata, easy to reference Dataset-XML
    supports all language encodings supported by XML.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:15
  url: https://www.cdisc.org/standards/data-exchange/dataset-xml
- id: B2AI_STANDARD:47
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDISC Laboratory Data Model
  is_open: true
  name: CDISC LAB
  purpose_detail: LAB provides a standard model for the acquisition and exchange of
    laboratory data, primarily between labs and sponsors or CROs. The LAB standard
    was specifically designed for the interchange of lab data acquired in clinical
    trials.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:15
  url: https://www.cdisc.org/standards/data-exchange/lab
  has_application:
  - id: B2AI_APP:9
    category: B2AI:Application
    name: HL7 FHIR to CDISC Laboratory Mapping for ML-Ready EHR Data
    description: The HL7 FHIR to CDISC Joint Mapping Implementation Guide includes
      mappings for the Laboratory domain that translate FHIR lab resources into CDISC
      variables, linked with CDISC LB-to-LOINC mapping guidance to leverage real-world
      data for clinical trials. This harmonization enables EHR laboratory observations
      to be transformed into standardized, ML-ready lab features with consistent semantic
      coding via LOINC, supporting downstream machine learning analyses, pooled studies,
      and data mining across clinical trial and real-world datasets.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.47912/jscdm.162
  - id: B2AI_APP:111
    category: B2AI:Application
    name: Metadata-Driven ETL for Scalable Lab Dataset Construction
    description: CDISC Laboratory Model implementations use metadata-driven ETL approaches
      to produce standards-compliant lab datasets from source systems, reducing manual
      data preparation through automated transformation logic that operates at the
      metadata level rather than requiring code changes. This metadata-driven approach
      is foundational for building scalable, ML-ready dataset construction pipelines
      that can consistently process laboratory data across multiple studies and sites,
      enabling reproducible preprocessing for machine learning workflows.
    used_in_bridge2ai: false
    references:
    - https://www.lexjansen.com/pharmasug/2002/proceed/DM/dm01.pdf
  - id: B2AI_APP:112
    category: B2AI:Application
    name: Integrated SDTM Laboratory Data for Pooled ML Analytics
    description: Integration of clinical trial and real-world data using CDISC standards
      including the Laboratory domain produces harmonized SDTM and ADaM datasets that
      enable pooled analyses and data mining across multiple studies and sponsors.
      Standardized laboratory variables with CDISC Controlled Terminology facilitate
      dataset aggregation, warehousing, and reuse, supporting pooled machine learning
      analyses and real-world evidence research by providing consistent lab feature
      representations that serve as inputs for predictive modeling and downstream
      analytics.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.47912/jscdm.128
- id: B2AI_STANDARD:48
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDISC Operational Data Model
  is_open: true
  name: CDISC ODM
  purpose_detail: The CSDIC ODM is a vendor-neutral, platform-independent format for
    exchanging and archiving clinical and translational research data, along with
    their associated metadata, administrative data, reference data, and audit information.
    ODM facilitates the regulatory-compliant acquisition, archival and exchange of
    metadata and data.
  related_to:
  - B2AI_STANDARD:689
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:15
  url: https://www.cdisc.org/standards/data-exchange/odm
  has_application:
  - id: B2AI_APP:10
    category: B2AI:Application
    name: Clinical Trial Data Integration and Predictive Modeling
    description: CDISC ODM (Operational Data Model) is used in AI applications for
      standardizing clinical trial data exchange, enabling machine learning models
      to train on multi-study datasets and predict trial outcomes, patient enrollment,
      and safety events. AI systems leverage ODM's XML-based representation of study
      metadata, case report forms, and clinical data to automatically harmonize data
      from different trials, perform cross-study analyses, and develop predictive
      models for trial design optimization. The standard enables AI applications that
      forecast patient dropout rates, identify optimal sites for recruitment based
      on historical data, and detect protocol deviations through anomaly detection.
      ODM's structured format facilitates automated quality control and regulatory
      submission preparation through AI-driven validation.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1016/j.cct.2019.105820
- id: B2AI_STANDARD:49
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDISC Protocol Representation Model
  is_open: true
  name: CDISC PRM
  purpose_detail: The CDISC Protocol Representation Model Version 1.0 (PRM V1.0) is
    intended for those involved in the planning and design of a research protocol.
    The model focuses on the characteristics of a study and the definition and association
    of activities within the protocols, including arms and epochs. PRM V1.0 also includes
    the definitions of the roles that participate in those activities. The scope of
    this model includes protocol content including Study Design, Eligibility Criteria,
    and the requirements from the ClinicalTrials.gov and World Health Organization
    (WHO) registries. The majority of business requirements were provided by subject
    matter experts in clinical trial protocols. PRM V1.0 is based on the BRIDG Release
    3.0 Protocol Representation sub-domain. It includes all classes in the BRIDG Protocol
    Representation sub-domain plus some classes from other BRIDG sub-domains, generally
    classes required for ClinicalTrials.gov and the WHO registries.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:15
  url: https://www.cdisc.org/standards/foundational/protocol
- id: B2AI_STANDARD:50
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDISC Standard for the Exchange of Nonclinical Data
  is_open: true
  name: SEND
  purpose_detail: The CDISC SEND is intended to guide the organization, structure,
    and format of standard nonclinical tabulation datasets for interchange between
    organizations such as sponsors and CROs and for submission to the US Food and
    Drug Administration (FDA)
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:15
  url: https://www.cdisc.org/standards/foundational/send
- id: B2AI_STANDARD:51
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDISC Study Design Model in XML
  is_open: true
  name: CDISC SDM
  purpose_detail: The CDISC Study Design Model in XML (SDM-XML) version 1.0 allows
    organizations to provide rigorous, machine-readable, interchangeable descriptions
    of the designs of their clinical studies, including treatment plans, eligibility
    and times and events. As an extension to the existing CDISC Operational Data Model
    (ODM) specification, SDM-XML affords implementers the ease of leveraging existing
    ODM concepts and re-using existing ODM definitions. SDM-XML defines three key
    sub-modules  Structure, Workflow, and Timing  permitting various levels of detail
    in any representation of a clinical studys design, while allowing a high degree
    of authoring flexibility. The specification document is available for download
    as a PDF file. A ZIP file containing the XML Schemas, several examples, and an
    SDM-XML element and attribute reference also is available.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:15
  url: https://www.cdisc.org/standards/data-exchange/sdm-xml
- id: B2AI_STANDARD:52
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CHADO XML interchange Format
  formal_specification: https://github.com/GMOD/Chado
  has_relevant_organization:
  - B2AI_ORG:125
  is_open: true
  name: CHADO
  publication: doi:10.1093/bioinformatics/btm189
  purpose_detail: Chado is a modular schema covering many aspects of biology, not
    just sequence data. Chado-XML has exactly the same scope as the Chado schema.
  requires_registration: false
  url: https://github.com/GMOD/Chado
- id: B2AI_STANDARD:53
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Chain Format for pairwise alignment
  is_open: true
  name: chain
  purpose_detail: The Chain format is a compact file format developed by UCSC for
    representing pairwise genomic sequence alignments that permit gaps in both the
    reference and query sequences simultaneously. Each chain alignment consists of
    a header line specifying score, chromosome identifiers, sizes, strands, and coordinates
    for both sequences, followed by alignment data lines describing ungapped blocks
    and the gaps between them. The format uses zero-based half-open intervals for
    coordinates and includes support for reverse-complement alignments through strand
    indicators. Chain files are particularly useful for comparing whole genomes or
    large genomic regions, representing syntenic relationships, structural variations,
    and evolutionary rearrangements. They are commonly used in genome browsers and
    liftOver tools for mapping genomic coordinates between different genome assemblies
    or between species. The format supports "snake" or rearrangement display modes
    that visualize complex structural variants including inversions, duplications,
    and translocations. Chain alignments can be produced by tools like BLASTZ and
    are widely used in comparative genomics workflows.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:119
  url: http://genome.ucsc.edu/goldenPath/help/chain.html
- id: B2AI_STANDARD:54
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:27
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CHARMM Card File Format
  is_open: true
  name: CARD
  purpose_detail: The CARD file format is the standard means in CHARMM for providing
    a human readable and writable coordinate file.
  requires_registration: false
  url: https://charmm-gui.org/charmmdoc/io.html
- id: B2AI_STANDARD:55
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:3
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Chemical Markup Language
  is_open: true
  name: CML
  purpose_detail: CML (Chemical Markup Language) is an XML language designed to hold
    most of the central concepts in chemistry. It was the first language to be developed
    and plays the same role for chemistry as MathML for mathematics and GML for geographical
    systems. CML covers most mainstream chemistry and especially molecules, reactions,
    solid-state, computation and spectroscopy. Since it has a special flexible approach
    to numeric science it also covers a very wide range of chemical properties, parameters
    and experimental observation. It is particularly concerned with the communication
    between machines and humans, and machines to machines. It has been heavily informed
    by the current chemical scholarly literature and chemical databases. XML is a
    mainstream approach providing semantics for science, such as MathML, SBML/BIOPAX
    (biology), GML and KML (geo) SVG (graphics) and NLM-DTD, ODT and OOXML (documents).
    CML provides support for most chemistry, especially molecules, compounds, reactions,
    spectra, crystals and computational chemistry (compchem). CML has been developed
    by Peter Murray-Rust and Henry Rzepa since 1995. It is the de facto XML for chemistry,
    accepted by publishers and with more than 1 million lines of Open Source code
    supporting it. CML can be validated and built into authoring tools (for example
    the Chemistry Add-in for Microsoft Word). A list of CML-compliant and CML-aware
    software can be found on the software page. The infrastructure includes legacy
    converters, dictionaries and conventions, Semantic Web and Linked Open Data. There
    are several versions of the CML schema. The most recent schema is schema 3. This
    essentially relaxes many of the constraints imposed in the previous stable release
    (schema 2.4), allowing users to put together the elements and attributes in a
    more flexible manner to fit the data that they want to represent more easily.
  requires_registration: false
  url: https://www.xml-cml.org/
- id: B2AI_STANDARD:56
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:4
  - B2AI_TOPIC:35
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ClinGen Interpretation Model
  formal_specification: https://github.com/clingen-data-model/clingen-interpretation
  is_open: true
  name: ClinGen Interpretation
  purpose_detail: The ClinGen Interpretation Model is a comprehensive data model and
    exchange format designed to capture, structure, and communicate the clinical interpretation
    of genetic variants. It supports the representation of evidence, reasoning, provenance,
    and contextual information underlying variant pathogenicity assessments, aligning
    with ACMG/AMP guidelines and community standards. The model enables detailed documentation
    of what evidence was used, how it was applied, who performed each interpretive
    act, and how interpretations build upon prior knowledge. It is flexible enough
    to represent both simple assertions and fully evidence-based interpretations,
    and is implemented using industry-standard JSON-LD for interoperability. The ClinGen
    Interpretation Model is closely aligned with the Monarch Initiative's SEPIO ontology,
    supporting integration with other biomedical data models and facilitating automated
    reasoning, reproducibility, and data sharing in clinical genomics.
  requires_registration: false
  url: https://dataexchange.clinicalgenome.org/interpretation/
  has_application:
  - id: B2AI_APP:11
    category: B2AI:Application
    name: LitGen Semi-Supervised Literature Recommendation and Evidence Classification
    description: LitGen uses semi-supervised deep learning trained on ClinGen VCI-curated
      papers annotated with ACMG/AMP-aligned evidence types, incorporating curator
      explanations and unlabeled ClinGen-linked papers to improve proxy labels for
      literature recommendation and evidence-type classification. The system leverages
      ClinGen Variant Curation Interface annotations to predict which papers support
      specific ACMG/AMP evidence criteria, achieving 7.9-12.6% relative performance
      improvement over supervised-only models and reducing manual curation burden
      for variant interpretation workflows.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1142/9789811215636_0007
  - id: B2AI_APP:113
    category: B2AI:Application
    name: ACMG/AMP Feature-Based Variant Pathogenicity Classification
    description: Machine learning models explicitly encode ACMG/AMP evidence criteria
      as features in penalized logistic regression to produce probabilistic pathogenicity
      scores and rank variants of uncertain significance for prioritization. The approach
      treats ClinGen/ACMG guideline-based evidence levels as structured ML features,
      enabling guidelines-informed classification that slightly outperforms some in
      silico scores on certain VUS datasets and improves prioritization for clinical
      decision support in hereditary disease gene panels.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1038/s41598-022-06547-3
  - id: B2AI_APP:114
    category: B2AI:Application
    name: ClinGen SVI Calibration of Computational Predictors to PP3/BP4
    description: A probabilistic framework maps continuous scores from missense variant
      prediction tools to ACMG/AMP PP3 and BP4 evidence strength levels using ClinGen/ACMG
      standards, establishing score intervals for Supporting, Moderate, and Strong
      evidence. This calibration enables automated, standardized assignment of computational
      evidence to ClinGen-aligned interpretation workflows, with most tools achieving
      Supporting level and several reaching Moderate or Strong evidence thresholds
      for pathogenic or benign classification.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1016/j.ajhg.2022.10.013
  - id: B2AI_APP:115
    category: B2AI:Application
    name: CGBench LLM Benchmarking for ClinGen Evidence Reasoning
    description: CGBench uses ClinGen VCI and GCI entries with expert explanations
      as ground truth to evaluate large language models on evidence extraction, strength
      scoring, and explanation concordance for variant interpretation. The benchmark
      shows moderate extraction performance with GPT-4o achieving 49% precision and
      79% recall, improved explanation concordance with few-shot prompting reaching
      70%, but limited strength-change prediction accuracy, revealing both capabilities
      and gaps in LLM reasoning for ClinGen-aligned genomic curation tasks.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.48550/arxiv.2510.11985
- id: B2AI_STANDARD:57
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CLSI Next-Generation In Vitro Diagnostic Interface
  is_open: false
  name: CLSI AUTO16
  purpose_detail: Exchange of data about and produced by in vitro diagnostic tests.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:16
  url: https://clsi.org/standards/products/automation-and-informatics/documents/auto16/
- id: B2AI_STANDARD:58
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  - B2AI_TOPIC:26
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CLUSTAL-W Alignment Format
  is_open: true
  name: MSF
  purpose_detail: The MSF (Multiple Sequence Format) is a standardized file format
    produced by CLUSTAL-W for representing multiple sequence alignments of proteins
    or nucleic acids. It includes sequence metadata, alignment length, checksums for
    data integrity verification, and displays aligned sequences in blocks with position
    numbering. The format supports gap characters and ambiguity codes, making it widely
    compatible with bioinformatics tools for phylogenetic analysis, sequence conservation
    studies, and comparative genomics. MSF files preserve alignment quality information
    and are human-readable while maintaining machine-parseable structure for automated
    processing.
  requires_registration: false
  url: https://bioinfo.nhri.edu.tw/gcg/doc/11.0/clustalw+.html
- id: B2AI_STANDARD:59
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  - B2AI_TOPIC:26
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CLUSTAL-W Dendrogram Guide File Format
  is_open: true
  name: DND
  purpose_detail: Format for the tree (or dendrogram) used to guide the a multiple
    sequence alignment process.
  requires_registration: false
  url: https://bioinfo.nhri.edu.tw/gcg/doc/11.0/clustalw+.html
- id: B2AI_STANDARD:60
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CODE-EHR best-practice framework for the use of structured electronic
    health-care records in clinical research
  is_open: true
  name: CODE-EHR
  publication: doi:10.1016/S2589-7500(22)00151-0
  purpose_detail: '...we propose the CODE-EHR minimum standards framework to be used
    by researchers and clinicians to improve the design of studies and enhance transparency
    of study methods. The CODE-EHR framework aims to develop robust and effective
    utilisation of health-care data for research purposes.'
  requires_registration: false
  url: https://www.bigdata4betterhearts.eu/News/ID/146/More-about-the-CODE-EHR-approach-and-framework
- id: B2AI_STANDARD:61
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Coherent X-ray Imaging Data Bank format
  is_open: true
  name: CXI
  purpose_detail: The CXI format was created as common format for all the data in
    the Coherent X-ray Imaging Data Bank (CXIDB). Naturally its scope is all experimental
    data collected during Coherent X-ray Imaging experiments as well as all data generated
    during the analysis of the experimental data.
  requires_registration: false
  url: https://www.cxidb.org/cxi.html
- id: B2AI_STANDARD:62
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Collaborative Computing Project for the NMR community data model
  is_open: true
  name: CCPN
  publication: doi:10.1002/prot.20449
  purpose_detail: The CCPN Data Model for macromolecular NMR is intended to cover
    all data needed for macromolecular NMR spectroscopy from the initial experimental
    data to the final validation. It serves for exchange of data between programs,
    for storage, data harvesting, and database deposition. The data model proper is
    an abstract description of the relevant data and their relationships - it is implemented
    in the modeling language UML. From this CCPN autogenerates interfaces (APIs) for
    various languages, format description and I/O routines, and documentation.
  requires_registration: false
  url: https://sites.google.com/site/ccpnwiki/home/documentation/ccpnmr-analysis/core-concepts/the-ccpn-data-model
  has_application:
  - id: B2AI_APP:12
    category: B2AI:Application
    name: CcpNmr-Integrated Deep Learning for HNCA Backbone Assignment
    description: Deep neural networks integrated with the CcpNmr AnalysisAssign software
      analyze HNCA spectrum line shapes to yield amino acid type probabilities for
      automated protein backbone assignment. The networks are trained on synthetic
      databases with realistic instrumental artifacts and noise, taking 2D slices
      of pyruvate-patterned HNCA spectra as input and outputting probability tables
      that combine with primary sequence information for rapid assignment. This ML
      integration within the CCPN software framework accelerates NMR analysis workflows,
      though networks require retraining when experimental parameters change.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1126/sciadv.ado0403
- id: B2AI_STANDARD:63
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Common Fund Data Ecosystem Crosscut Metadata Model
  formal_specification: https://osf.io/bq6k9/
  is_open: true
  name: CFDE C2M2
  purpose_detail: The Common Fund Data Ecosystem (CFDE) Crosscut Metadata Model (C2M2)
    is a flexible, extensible data model designed to harmonize metadata across NIH
    Common Fund data coordinating centers (DCCs) to enable integrated search, discovery,
    and analysis. C2M2 provides a standardized framework for describing biomedical
    research assets including files, biosamples, subjects, and collections with their
    associated metadata using controlled vocabularies and ontologies. The model consists
    of core entities (file, biosample, subject, project, collection) connected through
    relationships that capture data provenance, experimental context, and biological
    associations. C2M2 submissions include data tables describing these entities along
    with controlled vocabulary term usage tables that link to standard ontologies
    for enhanced semantic interoperability. The model supports supra-schematic constraints
    validated by dedicated tools, and includes ontology reference files for adding
    display names, descriptions, and synonyms to controlled terms. C2M2 enables the
    CFDE Workbench portal to provide unified search and browsing across heterogeneous
    Common Fund datasets, facilitating cross-program data discovery and promoting
    FAIR (Findable, Accessible, Interoperable, Reusable) principles for biomedical
    research data.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:68
  url: https://docs.nih-cfde.org/en/latest/c2m2/draft-C2M2_specification/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:13
    category: B2AI:Application
    name: CFDE Workbench ML-Ready Data Harmonization and Knowledge Graph Integration
    description: The CFDE Workbench uses C2M2 to harmonize metadata and host processed
      data in standardized, AI-ready formats that can be loaded as data frames and
      integrated into ML workflows. C2M2-derived metadata feeds PostgreSQL databases
      supporting complex queries for ML dataset preparation, while planned conversion
      to Neo4j knowledge graphs will align database structure with the metadata model
      to enhance search performance and facilitate ML applications. This harmonization
      across Common Fund programs reduces barriers to assembling training corpora
      and interoperable inputs for cross-program AI/ML analyses.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2025.02.04.636535
  - id: B2AI_APP:116
    category: B2AI:Application
    name: LLM-Powered Chatbot with C2M2-Grounded Workflow Automation
    description: The CFDE Workbench integrates an LLM chatbot using OpenAI assistants
      API that answers Common Fund program and data questions while executing API
      calls based on standardized workflow specifications. The chatbot leverages C2M2-harmonized
      resources and codified ETL workflows through the Playbook Workflow Builder to
      automate discovery and retrieval, using function calling to ensure comprehensive
      and reproducible results. This direct AI application is enabled by C2M2-backed
      infrastructure that provides the structured metadata and APIs necessary for
      the LLM to interact with heterogeneous datasets.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2025.02.04.636535
  - id: B2AI_APP:117
    category: B2AI:Application
    name: FAIRshake Assessment of C2M2-Derived Assets for AI-Readiness
    description: The CFDE Workbench subjects metadata, processed data, and code assets
      derived from C2M2 submissions to automated FAIR assessments via FAIRshake. These
      assessments evaluate findability, accessibility, interoperability, and reusability
      of C2M2-derived assets that support AI/ML tools including the Playbook Workflow
      Builder, GeneSetCart, DD-KG-UI, and CFDE-GSE. By operationalizing AI-readiness
      through systematic FAIRness evaluation, this application ensures that C2M2-structured
      datasets meet the accessibility and interoperability requirements necessary
      for downstream ML consumption.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2025.02.04.636535
  - id: B2AI_APP:118
    category: B2AI:Application
    name: Ecosystem-Level Metadata Harmonization for Cross-Program ML Dataset Discovery
    description: The CFDE catalog uses C2M2 to ingest diverse Data Coordinating Center
      metadata into a unified model supporting centralized indexing and search across
      NIH Common Fund programs. This harmonization with controlled vocabularies and
      ontologies minimizes barriers to cross-program dataset discovery and reuse,
      which is prerequisite for assembling training corpora and interoperable inputs
      for AI/ML analyses. C2M2's evolving model adapts to community standards and
      emerging AI/ML technologies, enabling effective search and retrieval mechanisms
      essential for training AI models on biomedical data while promoting FAIR principles.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2021.11.05.467504
- id: B2AI_STANDARD:64
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Common Metadata Elements for Cataloging Biomedical Datasets
  formal_specification: https://figshare.com/articles/dataset/Common_Metadata_Elements_for_Cataloging_Biomedical_Datasets/1496573?file=3377663
  is_open: true
  name: Common Metadata
  purpose_detail: This dataset outlines a proposed set of core, minimal metadata elements
    that can be used to describe biomedical datasets, such as those resulting from
    research funded by the National Institutes of Health. It can inform efforts to
    better catalog or index such data to improve discoverability. The proposed metadata
    elements are based on an analysis of the metadata schemas used in a set of NIH-supported
    data sharing repositories. Common elements from these data repositories were identified,
    mapped to existing data-specific metadata standards from to existing multidisciplinary
    data repositories, DataCite and Dryad, and compared with metadata used in MEDLINE
    records to establish a sustainable and integrated metadata schema.
  requires_registration: false
  url: https://figshare.com/articles/dataset/Common_Metadata_Elements_for_Cataloging_Biomedical_Datasets/1496573
- id: B2AI_STANDARD:65
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - diagnosticinstrument
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: COmprehensive Score for financial Toxicity A FACIT Measure of Financial
    Toxicity
  has_relevant_organization:
  - B2AI_ORG:30
  is_open: true
  name: FACIT-COST
  publication: doi:10.1002/cncr.30369
  purpose_detail: Developed in conjunction with the University of Chicago, the COST
    is a patient-reported outcome measure that describes the financial distress experienced
    by cancer patients. Since its initial publication, an additional item from the
    FACIT System has been included to screen for financial toxicity and to provide
    a good global summary item for financial toxicity.
  requires_registration: false
  url: https://www.facit.org/measures/FACIT-COST
- id: B2AI_STANDARD:66
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Consolidated Clinical Document Architecture
  formal_specification: http://www.hl7.org/implement/standards/product_brief.cfm?product_id=492
  is_open: true
  name: C-CDA
  purpose_detail: A widely-used, XML-based format for electronic health records. Superceded
    by FHIR document standards.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:40
  url: https://www.healthit.gov/topic/standards-technology/consolidated-cda-overview
- id: B2AI_STANDARD:67
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Consolidated criteria for reporting qualitative research
  formal_specification: http://www.cnfs.net/modules/module2/story_content/external_files/13_COREQ_checklist_000017.pdf
  is_open: true
  name: COREQ
  publication: doi:10.1093/intqhc/mzm042
  purpose_detail: 'COREQ (Consolidated Criteria for Reporting Qualitative Research)
    is a comprehensive 32-item reporting checklist specifically designed to improve
    the transparency, rigor, and completeness of qualitative research reports, particularly
    those involving interviews and focus groups. This evidence-based guideline addresses
    three major domains essential for qualitative research reporting: research team
    and reflexivity (covering the researcher''s credentials, occupation, experience,
    gender, and relationship with participants), study design (including theoretical
    framework, participant selection, setting, and data collection methods), and analysis
    and findings (encompassing data analysis approaches, verification procedures,
    and presentation of findings). COREQ serves as a critical quality assessment tool
    for researchers, editors, reviewers, and readers, helping ensure that qualitative
    studies provide sufficient detail for proper evaluation and potential replication.
    The checklist promotes methodological transparency and supports the credibility
    and trustworthiness of qualitative health research by standardizing reporting
    expectations across the research community.'
  requires_registration: false
  url: http://www.cnfs.net/modules/module2/story_content/external_files/13_COREQ_checklist_000017.pdf
- id: B2AI_STANDARD:68
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Consolidated Health Economic Evaluation Reporting Standards
  formal_specification: https://www.ispor.org/heor-resources/good-practices/article/consolidated-health-economic-evaluation-reporting-standards-2022-cheers-2022-statement-updated-reporting-guidance-for-health-economic-evaluations
  is_open: true
  name: CHEERS
  publication: doi:10.1136/bmj.f1049
  purpose_detail: The Consolidated Health Economic Evaluation Reporting Standards
    (CHEERS) statement is an attempt to consolidate and update previous health economic
    evaluation guidelines efforts into one current, useful reporting guidance. The
    primary audiences for the CHEERS statement are researchers reporting economic
    evaluations and the editors and peer reviewers assessing them for publication.
  requires_registration: true
  url: https://www.ispor.org/heor-resources/good-practices/article/consolidated-health-economic-evaluation-reporting-standards-2022-cheers-2022-statement-updated-reporting-guidance-for-health-economic-evaluations
- id: B2AI_STANDARD:69
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Consolidated Standards of Reporting Trials
  is_open: true
  name: CONSORT
  publication: doi:10.1136/bmj.c332
  purpose_detail: CONSORT (Consolidated Standards of Reporting Trials) is an evidence-based
    minimum set of recommendations for reporting randomized controlled trial (RCT)
    results, developed to address inadequate reporting that prevents proper assessment
    of trial quality and generalizability. The CONSORT 2025 Statement consists of
    a 30-item checklist and flow diagram covering how trials are designed, analyzed,
    and interpreted, with detailed recommendations for reporting participant recruitment,
    randomization procedures, interventions, outcomes, sample size calculations, statistical
    methods, baseline characteristics, results for each outcome, harms, interpretation,
    and generalizability. The flow diagram tracks progress of all participants through
    each trial stage (enrollment, allocation, follow-up, analysis) with numbers and
    reasons for exclusions and losses. CONSORT is companion to SPIRIT (Standard Protocol
    Items - Recommendations for Interventional Trials) which provides a 34-item checklist
    for protocol reporting. Both guidelines emphasize transparent and complete reporting
    to enable readers to critically appraise, interpret, and apply research findings.
    CONSORT includes numerous extensions for specific trial designs (cluster randomized,
    non-inferiority, pragmatic trials), interventions (herbal, acupuncture, non-pharmacological
    treatment), and data types. The guidelines are supported by Explanation and Elaboration
    documents with detailed rationale and examples, online tools (COBWEB for report
    writing, SEPTRE for protocol management), translations in multiple languages,
    and endorsement by hundreds of medical journals worldwide as part of the EQUATOR
    Network research reporting standards.
  requires_registration: false
  url: https://www.consort-statement.org/
- id: B2AI_STANDARD:70
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Continua Design Guidelines
  formal_specification: https://members.pchalliance.org/document/dl/2148
  is_open: true
  name: Continua
  purpose_detail: A suite of open industry standards and specifications that provide
    several means to end-to-end interoperability between personal medical devices
    and health information systems.
  requires_registration: false
  url: https://www.pchalliance.org/continua-design-guidelines
- id: B2AI_STANDARD:71
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Critical care data exchange format
  formal_specification: https://github.com/autonlab/auviewer
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:16
  is_open: true
  name: CCDEF
  publication: doi:10.1088/1361-6579/abfc9b
  purpose_detail: HDF5-based critical care data exchange format which stores multiparametric
    data in an efficient, self-describing, hierarchical structure and supports real-time
    streaming and compression. In addition to cardiorespiratory and laboratory data,
    the format can, in future, accommodate other large datasets such as imaging and
    genomics.
  related_to:
  - B2AI_STANDARD:339
  requires_registration: false
  url: https://ccdef.org/
- id: B2AI_STANDARD:72
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Crypt4GH format
  formal_specification: https://github.com/samtools/hts-specs
  has_relevant_organization:
  - B2AI_ORG:34
  is_open: true
  name: Crypt4GH
  purpose_detail: Crypt4GH is a file format specification developed by the Global
    Alliance for Genomics and Health (GA4GH) for secure storage and transmission of
    genomic and health-related data. The format provides end-to-end encryption and
    authentication, allowing sensitive genomic data to be stored in public repositories
    while maintaining confidentiality. It uses modern cryptographic standards including
    Curve25519 for key agreement, ChaCha20 for encryption, and Poly1305 for authentication.
    The format supports multiple recipients through per-recipient encrypted headers,
    enabling controlled data sharing where different users can decrypt the same file
    using their own private keys. Crypt4GH is designed for streaming access, allowing
    applications to decrypt and process data incrementally without loading entire
    files into memory. The format includes metadata protection and supports selective
    decryption of file segments, making it suitable for large-scale genomic datasets.
    Existing bioinformatics tools can be adapted with minimal modifications to read
    and write Crypt4GH-encrypted data, facilitating adoption in genomics workflows.
  requires_registration: false
  url: https://samtools.github.io/hts-specs/crypt4gh.pdf
- id: B2AI_STANDARD:73
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:27
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Crystallographic Information File format
  is_open: true
  name: CIF
  purpose_detail: The acronym CIF is used both for the Crystallographic Information
    File, the data exchange standard file format of Hall, Allen & Brown (1991) (see
    Documentation), and for the Crystallographic Information Framework, a broader
    system of exchange protocols based on data dictionaries and relational rules expressible
    in different machine-readable manifestations, including, but not restricted to,
    Crystallographic Information File and XML.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/Crystallographic_Information_File
- id: B2AI_STANDARD:74
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - codesystem
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Current Procedural Terminology
  formal_specification: https://www.cms.gov/Medicare/Fraud-and-Abuse/PhysicianSelfReferral
  is_open: false
  name: CPT
  purpose_detail: Code set used to bill outpatient & office procedures.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:3
  url: https://www.ama-assn.org/amaone/cpt-current-procedural-terminology
  used_in_bridge2ai: true
- id: B2AI_STANDARD:75
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Data Documentation Initiative Lifecycle
  is_open: true
  name: DDI-Lifecycle
  purpose_detail: The freely available international DDI standard describes data that
    result from observational methods in the social, behavioral, economic, and health
    sciences. DDI is used to document data in over 60 countries of the world.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:23
  url: https://ddialliance.org/Specification/DDI-Lifecycle/3.3/
- id: B2AI_STANDARD:76
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Dead simple owl design pattern exchange format
  formal_specification: https://github.com/INCATools/dead_simple_owl_design_patterns
  has_relevant_organization:
  - B2AI_ORG:58
  is_open: true
  name: DOS-DP
  publication: doi:10.1186/s13326-017-0126-0
  purpose_detail: A simple design pattern system that can easily be consumed, whatever
    your code base, for OWL ontologies.
  requires_registration: false
  url: https://oboacademy.github.io/obook/tutorial/dosdp-template/
- id: B2AI_STANDARD:77
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Decision Model and Notation
  has_relevant_organization:
  - B2AI_ORG:10
  is_open: true
  name: DMN
  purpose_detail: A modeling language and notation for the precise specification of
    business decisions and business rules.
  requires_registration: false
  url: https://www.omg.org/dmn/
- id: B2AI_STANDARD:78
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Description Language for Taxonomy
  is_open: true
  name: DELTA
  purpose_detail: When taxonomic descriptions are prepared for input to computer programs,
    the form of the coding is usually dictated by the requirements of a particular
    program or set of programs. This restricts the type of data that can be represented,
    and the number of other programs that can use the data. Even when working with
    a particular program, it is frequently necessary to set up different versions
    of the same basic data, for example, when using restricted sets of taxa or characters
    to make special-purpose keys. The potential advantages of automation, especially
    in connexion with large groups, cannot be realized if the data have to be restructured
    by hand for every operation. The DELTA (DEscription Language for TAxonomy) system
    was developed to overcome these problems. It was designed primarily for easy use
    by people rather than for convenience in computer programming, and is versatile
    enough to replace the written description as the primary means of recording data.
    Consequently, it can be used as a shorthand method of recording data, even if
    computer processing of the data is not envisaged.
  requires_registration: false
  url: https://www.delta-intkey.com/
- id: B2AI_STANDARD:79
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - audiovisual
  - fileformat
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 10 Media Storage and File Format for Media Interchange
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part10.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOMDIR
  purpose_detail: This Part of the DICOM Standard specifies a general model for the
    storage of Medical Imaging information on removable media. The purpose of this
    Part is to provide a framework allowing the interchange of various types of medical
    images and related information on a broad range of physical storage media.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:80
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 11 Media Storage Application Profiles
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part11.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 11
  purpose_detail: This Part of the DICOM Standard specifies application specific subsets
    of the DICOM Standard to which an implementation may claim conformance. Such a
    conformance statement applies to the interoperable interchange of medical images
    and related information on storage media for specific clinical uses.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:81
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 12 Media Formats and Physical Media for Media Interchange
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part12.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 12
  purpose_detail: This Part of the DICOM Standard facilitates the interchange of information
    between digital imaging computer systems in medical environments. This interchange
    will enhance diagnostic imaging and potentially other clinical applications. The
    multi-part DICOM Standard defines the services and data that shall be supplied
    to achieve this interchange of information.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:82
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 14 Grayscale Standard Display Function
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part14.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 14
  purpose_detail: PS3.14 specifies a standardized Display Function for display of
    grayscale images. It provides examples of methods for measuring the Characteristic
    Curve of a particular Display System for the purpose of either altering the Display
    System to match the Grayscale Standard Display Function, or for measuring the
    conformance of a Display System to the Grayscale Standard Display Function. Display
    Systems include such things as monitors with their associated driving electronics
    and printers producing films that are placed on light-boxes or alternators.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:83
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 15 Security and System Management Profiles
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part15.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 15
  purpose_detail: This Part of the DICOM Standard specifies Security and System Management
    Profiles to which implementations may claim conformance. Security and System Management
    Profiles are defined by referencing externally developed standard protocols, such
    as TLS, ISCL, DHCP, and LDAP, with attention to their use in a system that uses
    DICOM Standard protocols for information interchange.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:84
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 16 Content Mapping Resource
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part16.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DCMR
  purpose_detail: DICOM Part 16 Content Mapping Resource (DCMR) is a comprehensive
    component of the DICOM standard that provides the foundational vocabulary and
    semantic structures used throughout medical imaging systems worldwide. DCMR defines
    the Templates and Context Groups that standardize the representation of clinical
    concepts, measurements, and observations within DICOM objects, ensuring consistent
    interpretation of medical imaging data across different systems, vendors, and
    healthcare institutions. The Content Mapping Resource includes structured templates
    for various types of medical imaging reports, measurements, and annotations, covering
    specialties such as radiology, cardiology, ophthalmology, and oncology. These
    templates define the specific data elements, their relationships, and the controlled
    terminologies that should be used when creating structured reports and enhanced
    imaging objects. DCMR plays a crucial role in enabling interoperability, supporting
    artificial intelligence applications in medical imaging, and facilitating the
    exchange of semantically rich imaging information in clinical practice and research
    environments.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:85
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 17 Explanatory Information
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part17.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 17
  purpose_detail: 'DICOM Part 17: Explanatory Information serves as the comprehensive
    educational and interpretive companion to the DICOM (Digital Imaging and Communications
    in Medicine) Standard, providing normative annexes that clarify implementation
    requirements and extensive informative annexes that explain the rationale, context,
    and practical application of DICOM specifications across the complete multi-part
    standard maintained by NEMA (National Electrical Manufacturers Association) and
    the Medical Imaging & Technology Alliance. This part functions as the bridge between
    the formal, technically precise language of the normative DICOM specifications
    (Parts 1-21) and the practical needs of implementers, including medical device
    manufacturers, PACS vendors, healthcare IT developers, clinical engineers, radiologists,
    and AI/ML researchers who must correctly interpret and apply DICOM protocols in
    real-world medical imaging systems. Part 17 contains normative annexes that define
    mandatory implementation rules, unique identifier (UID) allocation schemes, conformance
    statement structures, and security profiles referenced throughout other DICOM
    parts, making compliance verification and interoperability testing possible. The
    informative annexes provide contextual explanations, use case scenarios, implementation
    guidelines, and best practices that illuminate the "why" behind DICOM''s design
    decisions, including historical context for legacy features maintained for backward
    compatibility, clinical workflow considerations that shaped information model
    choices, and trade-offs between flexibility and interoperability in attribute
    definitions. For AI and machine learning applications in medical imaging, Part
    17''s explanatory material is invaluable for understanding how to correctly encode
    AI-generated annotations (segmentations, structured reports, parametric maps)
    in DICOM formats, interpret manufacturer-specific private tags that may contain
    proprietary imaging parameters needed for model training, navigate the complexities
    of coordinate system transformations between image acquisition space and patient
    anatomical space, and ensure that AI model outputs maintain proper DICOM conformance
    for integration into clinical PACS workflows. The part includes detailed explanations
    of DICOM''s information object definitions (IODs), service-object pair (SOP) classes,
    and application entities that form the foundation for network communication between
    imaging modalities, archives, workstations, and AI inference servers. Part 17
    clarifies the distinction between DICOM''s composite and normalized information
    models, explains the role of Type 1 (required), Type 2 (required but may be empty),
    and Type 3 (optional) attributes in ensuring semantic interoperability, and provides
    guidance on handling multi-frame images, enhanced MR/CT IODs, and whole slide
    imaging pyramids that are increasingly common in pathology AI applications. For
    researchers building AI training datasets from clinical DICOM archives, Part 17''s
    explanatory material helps interpret DICOM metadata for proper cohort selection,
    understand acquisition parameter variations that affect image characteristics
    and model generalizability, and recognize quality control indicators embedded
    in DICOM headers that may signal artifacts or protocol deviations requiring exclusion
    from training sets. The part also explains DICOM''s approach to encoding patient
    orientation, anatomical landmarks, and spatial registration information critical
    for multi-modal image fusion and longitudinal studies where AI models must track
    anatomical changes over time or correlate findings across different imaging modalities
    (CT, MRI, PET, ultrasound). Part 17''s annexes document the rationale behind DICOM''s
    privacy and security frameworks, including de-identification profiles and attribute
    confidentiality levels essential for creating shareable AI training datasets that
    comply with HIPAA, GDPR, and institutional review board requirements. The explanatory
    information supports regulatory submissions by providing the interpretive context
    needed to demonstrate DICOM conformance in FDA 510(k) applications and EU Medical
    Device Regulation (MDR) technical documentation for AI-enabled medical imaging
    products. Updated with each DICOM standard revision, Part 17 incorporates explanations
    of new supplements and correction proposals, ensuring that implementers understand
    the evolution of the standard and can plan migration paths for existing systems
    while maintaining backward compatibility with legacy imaging equipment and archives.'
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:86
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 18 Web Services
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part18.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOMweb
  purpose_detail: PS3.18 specifies web services (using the HTTP family of protocols)
    for managing and distributing DICOM (Digital Imaging and Communications in Medicine)
    Information Objects, such as medical images, annotations, reports, etc. to healthcare
    organizations, providers, and patients. The term DICOMweb is used to designate
    the RESTful Web Services described here.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:87
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 19 Application Hosting
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part19.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 19
  purpose_detail: This Part of the DICOM Standard defines an interface between two
    software applications. One application, the Hosting System, provides the second
    application with data, such as a set of images and related data. The second application,
    the Hosted Application, analyzes that data, potentially returning the results
    of that analysis, for example in the form of another set of images and/or structured
    reports, to the first application. Such an Application Program Interface (API)
    differs in scope from other portions of the DICOM Standard in that it standardizes
    the data interchange between software components on the same system, instead of
    data interchange between different systems.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:88
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 2 Conformance
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part02.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 2
  purpose_detail: An implementation need not employ all the optional components of
    the DICOM Standard. After meeting the minimum general requirements, a conformant
    DICOM implementation may utilize whatever SOP Classes, communications protocols,
    Media Storage Application Profiles, optional (Type 3) Attributes, codes and controlled
    terminology, etc., needed to accomplish its designed task.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:89
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 20 Imaging Reports using HL7 Clinical Document Architecture
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part20.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_relevant_organization:
  - B2AI_ORG:40
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 20
  purpose_detail: This Part of the DICOM Standard specifies templates for the encoding
    of imaging reports using the HL7 Clinical Document Architecture Release 2 (CDA
    R2, or simply CDA) Standard. Within this scope are clinical procedure reports
    for specialties that use imaging for screening, diagnostic, or therapeutic purposes.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:90
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:15
  - standards_process_maturity_final
  - implementation_maturity_production
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 21 Transformations between DICOM and other Representations
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part21.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 21
  purpose_detail: This Part of the DICOM Standard specifies the transformations between
    DICOM and other representations of the same information.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:91
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 22 Real-Time Communication
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part22.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 22
  purpose_detail: This Part of the DICOM Standard specifies an SMPTE ST 2110-10 based
    service, relying on RTP, for the real-time transport of DICOM metadata. It provides
    a mechanism for the transport of DICOM metadata associated with a video or an
    audio flow based on the SMPTE ST 2110-20 and SMPTE ST 2110-30, respectively.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:92
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 3 Information Object Definitions
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part03.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 3
  purpose_detail: This Part of the DICOM Standard specifies the set of Information
    Object Definitions (IODs) that provide an abstract definition of real-world objects
    applicable to communication of digital medical information. For each IOD, this
    Part specifies any necessary information for the semantic description of the IOD,
    relationships to associated real-world objects relevant to the IOD, Attributes
    that describe the characteristics of the IOD.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:93
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 4 Service Class Specifications
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part04.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 4
  purpose_detail: This Part of the DICOM Standard specifies the set of Service Class
    Definitions that provide an abstract definition of real-world activities applicable
    to communication of digital medical information.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:94
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 5 Data Structures and Encoding
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part05.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 5
  purpose_detail: In this Part of the Standard the structure and encoding of the Data
    Set is specified. In the context of Application Entities communicating over a
    network, a Data Set is that portion of a DICOM Message that conveys information
    about real world objects being managed over the network. A Data Set may have other
    contexts in other applications of this Standard; e.g., in media exchange the Data
    Set translates to file content structure.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:95
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 6 Data Dictionary
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part06.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 6
  purpose_detail: This Part of the DICOM Standard is PS 3.6 of a multi-part standard
    produced to facilitate the interchange of information between digital imaging
    computer systems in medical environments. This interchange will enhance diagnostic
    imaging and potentially other clinical applications. The multi-part DICOM Standard
    covers the protocols and data that shall be supplied to achieve this interchange
    of information.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:96
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 7 Message Exchange
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part07.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DIMSE
  purpose_detail: This Part of the DICOM Standard specifies the DICOM Message Service
    Element (DIMSE). The DIMSE defines an Application Service Element (both the service
    and protocol) used by peer DICOM Application Entities for the purpose of exchanging
    medical images and related information.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:97
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 8 Network Communication Support for Message Exchange
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part08.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 8
  purpose_detail: The Communication Protocols specified in this Part of PS3 closely
    fit the ISO Open Systems Interconnection Basic Reference Model (ISO 7498-1, see
    Figure 1-1). They relate to the following layers - Physical, Data Link, Network,
    Transport, Session, Presentation and the Association Control Services (ACSE) of
    the Application layer. The communication protocols specified by this Part are
    general purpose communication protocols (TCP/IP) and not specific to this Standard.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:98
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Digital Imaging And Communications In Medicine
  formal_specification: https://www.dicomstandard.org/current
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  - B2AI_SUBSTRATE:19
  has_relevant_organization:
  - B2AI_ORG:115
  - B2AI_ORG:114
  - B2AI_ORG:117
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM
  purpose_detail: Digital Imaging and Communications in Medicine (DICOM) is the international
    standard for medical imaging information and related data. DICOM defines the
    formats and communication protocols for medical images (CT, MRI, ultrasound, X-ray,
    etc.) and associated metadata including patient demographics, study information,
    equipment parameters, and image acquisition details. The standard encompasses
    file formats, network protocols, and information models that enable interoperability
    across imaging devices, PACS (Picture Archiving and Communication Systems), workstations,
    and clinical information systems. DICOM supports structured reporting through
    DICOM-SR for encoding measurements, annotations, and AI-generated findings in
    a standardized machine-readable format. The standard includes specialized modules
    for radiation dose tracking, workflow management, and integration of AI inference
    results. DICOM's comprehensive metadata structure and pixel data encoding make
    it essential for training and deploying AI models in medical imaging, while its
    widespread clinical adoption ensures that AI tools can integrate seamlessly into
    existing radiological workflows and health IT infrastructure.
  related_to:
  - B2AI_STANDARD:691
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  url: https://www.dicomstandard.org/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:33
    category: B2AI:Application
    name: NCI Imaging Data Commons FAIR Dataset Harmonization for ML Training
    description: The NCI Imaging Data Commons uses DICOM to harmonize large-scale
      public imaging collections with AI-derived annotations, supporting transparent
      cohort selection and reproducible ML development. IDC encodes images and derived
      artifacts (volumetric segmentations, slice-level annotations, radiomics features)
      consistently using DICOM Segmentation objects and Structured Reports, enabling
      FAIR (findable, accessible, interoperable, reusable) datasets. Cloud-ready tools
      and libraries including highdicom lower barriers for ML developers to produce
      and consume DICOM-compliant training labels and outputs directly from Python
      workflows, facilitating dataset quality control and reproducibility for AI model
      development.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1038/s41597-023-02864-y
  - id: B2AI_APP:119
    category: B2AI:Application
    name: DICOM Segmentation and Structured Report Encoding for Supervised Learning
      Labels
    description: Research datasets are re-encoded into DICOM annotation objects to
      make labels directly consumable by ML pipelines. LIDC-IDRI lung nodule annotations
      were converted to DICOM Segmentation (SEG) objects and DICOM Structured Reports
      following template TID1500, providing interoperable nodular masks and qualitative/quantitative
      assessments. Disease-specific datasets like HCC-TACE distribute DICOM-SEG tumor/liver
      labels for algorithm training and evaluation. DICOM-SEG metadata tags capture
      algorithm provenance (Segment algorithm type, Segmentation type) essential for
      data curation, reproducibility, and supervised ML workflows, while software
      stacks (DCMTK, ITK, dcmqi, pydicom-seg) enable conversion and pipeline integration.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1186/s13244-021-01081-8
  - id: B2AI_APP:120
    category: B2AI:Application
    name: Vendor-Agnostic PACS-Integrated Shadow Testing Pipeline for Near-Real-Time
      Segmentation
    description: A containerized, DICOM-compatible pipeline integrates PACS with ML
      inference for near-real-time clinical shadow testing of segmentation algorithms.
      PACS sends studies via classic DICOM services (C-STORE/C-FIND/C-MOVE) to an
      on-premises GPU host that performs nnU-Net inference and converts results to
      DICOM Segmentation objects and Structured Reports encoding volumetry. Results
      are stored in a DCM4CHEE archive and visualized in OHIF web viewer, enabling
      clinical evaluation, human-in-the-loop corrections, and dataset curation. This
      demonstrates concrete PACS-to-inference-to-reporting workflows using established
      DICOM network protocols for AI integration into radiology operations.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.3389/fmed.2023.1241570
  - id: B2AI_APP:121
    category: B2AI:Application
    name: DICOM Structured Report TID1500 for Machine-Readable AI Results and Radiomics
    description: Quantitative AI outputs including radiomics features and imaging
      biomarkers are standardized via DICOM Structured Report template TID1500 measurement
      reports, providing machine-readable, auditable AI results that facilitate downstream
      research and clinical decision support. DICOM SR encodes semantic measurements
      with standardized format and lexicon, acting as a big-data container for multimodal
      patient data integration. DICOM Parametric Map IODs preserve derived pixelwise
      feature maps (kurtosis, texture) as DICOM objects for reproducible feature extraction.
      Integration guidance from standards bodies emphasizes encoding AI outputs into
      SR to scale deployment across vendors and sites.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1186/s13244-021-01081-8
    - https://doi.org/10.1117/1.jmi.7.1.016502
  - id: B2AI_APP:122
    category: B2AI:Application
    name: DICOMweb RESTful Services for Scalable Cloud and Web-Based ML Pipelines
    description: DICOMweb RESTful services (WADO-RS, QIDO-RS, STOW-RS) connect external
      ML applications and viewers to enterprise archives over HTTP, enabling scalable
      web and cloud AI pipelines. DICOMweb client APIs allow external apps to retrieve,
      query, and store DICOM objects without classic DIMSE protocols, facilitating
      integration with modern web architectures. Frameworks stream DICOM and metadata
      from hospital PACS into research compute clusters to power real-time ML and
      operational analytics. Examples include Niffler for PACS-to-research pipelines
      with metadata extraction and ML analytics, and EMPAIA decentralized platform
      for running third-party AI on DICOM data at scale.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1016/j.cmpb.2024.108113
  - id: B2AI_APP:123
    category: B2AI:Application
    name: DICOM-WSI and DICOMweb for Computational Pathology ML Development
    description: DICOM Whole Slide Imaging (DICOM-WSI) and DICOMweb enable interoperable
      storage, visualization, and annotation of whole-slide images in web viewers
      for computational pathology. These standards support collecting standardized
      annotations and displaying AI inference outputs including segmentation masks,
      heat maps, and image-derived measurements within the DICOM ecosystem. DICOM-WSI
      brings pathology images into uniform DICOM workflows to facilitate ML model
      development and validation, while web viewers (e.g., Visilab Viewer) provide
      platforms for annotating training data and reviewing AI results in digital pathology
      contexts.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1038/s41597-023-02864-y
    - https://doi.org/10.1016/j.cmpb.2024.108113
  - id: B2AI_APP:124
    category: B2AI:Application
    name: IODeep Proposed IOD for DNN Model Management and Federated Learning in
      PACS
    description: The proposed IODeep DICOM Information Object Definition stores deep
      neural network architectures and weights inside PACS as non-patient objects,
      enabling model selection via DICOM tag metadata (modality, anatomical region,
      disease). Inference runs client-side or on dedicated servers proximal to viewers,
      with predicted ROIs saved as RT Structure Sets for physician validation and
      retraining. IODeep supports model lifecycle management including fine-tuning
      with hospital data and federated learning scenarios by sharing model metadata
      without moving patient data. The design uses Unlimited Text VR for JSON architecture
      descriptions and weight URIs, with parsers for TensorFlow/PyTorch instantiation,
      demonstrating an emerging pattern for managing AI models within DICOM-compliant
      workflows.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1016/j.cmpb.2024.108113
  - id: B2AI_APP:125
    category: B2AI:Application
    name: DICOM GSPS and RT Structure Sets for AI Annotation Presentation and Clinical
      Feedback
    description: DICOM Grayscale Softcopy Presentation State (GSPS) ensures consistent
      overlay and display of AI annotations across workstations, while RT Structure
      Set (RTSS) objects store physician-validated contours after AI suggestion, supporting
      clinical quality assurance and retraining workflows. RTSS is widely supported
      for storing ROI contours and labels for model training, enabling human-in-the-loop
      correction loops where radiologists review AI predictions, validate or correct
      them, and create ground-truth datasets. Integration of CAD results into PACS
      using DICOM SR and GSPS, combined with DICOM routers for communication and correction,
      supports maturity levels from investigational AI display through continuously
      updated deployed models driven by radiologist interactions.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1117/1.jmi.7.1.016502
    - https://doi.org/10.1016/j.cmpb.2024.108113
- id: B2AI_STANDARD:99
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Direct Applicability Statement for Secure Health Transport
  formal_specification: https://wiki.directproject.org/w/images/e/e6/Applicability_Statement_for_Secure_Health_Transport_v1.2.pdf
  has_relevant_organization:
  - B2AI_ORG:26
  is_open: true
  name: Direct Standard
  purpose_detail: Describes how to use SMTP, S/MIME, and X.509 certificates to securely
    transport health information over the Internet.
  requires_registration: false
  url: https://wiki.directproject.org/w/images/e/e6/Applicability_Statement_for_Secure_Health_Transport_v1.2.pdf
- id: B2AI_STANDARD:100
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Distributed Sequence Annotation System
  formal_specification: https://static-content.springer.com/esm/art%3A10.1186%2F1471-2105-2-7/MediaObjects/12859_2001_8_MOESM1_ESM.pdf
  is_open: true
  name: DAS
  publication: doi:10.1186/1471-2105-2-7
  purpose_detail: Allows sequence annotations to be decentralized among multiple third-party
    annotators and integrated on an as-needed basis by client-side software.
  requires_registration: false
  url: https://static-content.springer.com/esm/art%3A10.1186%2F1471-2105-2-7/MediaObjects/12859_2001_8_MOESM1_ESM.pdf
- id: B2AI_STANDARD:101
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ecological Metadata Language
  is_open: true
  name: EML
  publication: doi:10.5063/F11834T2
  purpose_detail: A metadata standard developed for the earth, environmental and ecological
    sciences.
  requires_registration: false
  url: https://eml.ecoinformatics.org/
- id: B2AI_STANDARD:102
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Economic Botany Data Collection Standard
  is_open: true
  name: EBDCS
  purpose_detail: This standard provides a system whereby uses of plants (in their
    cultural context) can be described, using standardised descriptors and terms,
    and attached to taxonomic data sets. It resulted from discussions at the International
    Working Group on Taxonomic Databases for Plant Sciences (TDWG) between 1989 and
    1992. Users and potential users of the standard include economic botanists and
    ethnobotanists whose purpose is to record all known information about the uses
    of a taxon; educationalists, taxonomists, biochemists, anatomists etc. who wish
    to record plant use, often at a broad level; economic botany collection curators
    who need to describe accurately the uses and values of specimens in their collections;
    bibliographers who need to describe plant uses referred to in publications and
    to apply keywords consistently for ease of data retrieval. While this standard
    is still in use, it is no longer actively maintained (labelled as prior on the
    TDWG website).
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:93
  url: https://www.tdwg.org/standards/economic-botany/
- id: B2AI_STANDARD:103
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - codesystem
  concerns_data_topic:
  - B2AI_TOPIC:26
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Enzyme Commission Number
  is_open: true
  name: EC
  publication: doi:10.1126/science.150.3697.719
  purpose_detail: A system for classification of enzymes that also serves as a basis
    for assigning code numbers to them.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:61
  url: https://iubmb.qmul.ac.uk/enzyme/rules.html
- id: B2AI_STANDARD:104
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - diagnosticinstrument
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: EORTC Quality of Life questionnaires
  is_open: true
  name: EORTC QLQ
  purpose_detail: Instruments designed to assess (some of) the different aspects that
    define the QoL of (a specific group of) cancer patients.
  requires_registration: true
  url: https://qol.eortc.org/questionnaires/
- id: B2AI_STANDARD:105
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: European Data Format
  is_open: true
  name: EDF
  purpose_detail: The European Data Format (EDF) is a simple and flexible format for
    exchange and storage of multichannel biological and physical signals, particularly
    in clinical neurophysiology, sleep medicine, and cardiology. Originally developed
    in 1990 by European biomedical engineers through the "Methodology for the Analysis
    of the Sleep-Wakefulness Continuum" project funded by the European Community,
    EDF became the de-facto standard for EEG and PSG recordings. The format is hardware
    and software independent, supporting various montages, transducers, prefiltering
    options, and sampling frequencies. EDF enables creation of common databases for
    sleep records, comparative analysis of algorithms across laboratories, and standardized
    evaluation of manual and automatic analysis methods. The format has been extended
    to EDF+, which maintains backward compatibility while adding support for interrupted
    recordings, annotations, stimuli, events, and analysis results such as deltaplots,
    QRS parameters, and sleep stages. EDF+ can store any medical recording including
    EMG, evoked potentials, and ECG data, with stricter specifications that enable
    automatic electrode localization and calibration. Both formats are freely available
    and widely adopted in commercial equipment and multicenter research projects.
  requires_registration: false
  url: https://www.edfplus.info/
- id: B2AI_STANDARD:106
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - diagnosticinstrument
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: European Laryngological Society guidelines
  is_open: true
  name: ELS
  publication: doi:10.1007/s004050000299
  purpose_detail: A multidimensional set of minimal basic measurements suitable for
    all common dysphonias is proposed. It includes five different approaches - perception
    (grade, roughness, breathiness), videostroboscopy (closure, regularity, mucosal
    wave and symmetry), acoustics (jitter, shimmer, Fo-range and softest intensity),
    aerodynamics (phonation quotient), and subjective rating by the patient.
  requires_registration: false
  url: https://doi.org/10.1007/s004050000299
- id: B2AI_STANDARD:107
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - diagnosticinstrument
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: EuroQol Five Dimension Three Level descriptive system
  is_open: true
  name: EQ-5D-3L
  purpose_detail: The EQ-5D-3L descriptive system comprises the following five dimensions
    - mobility, self-care, usual activities, pain/discomfort and anxiety/depression.
    Each dimension has 3 levels - no problems, some problems, and extreme problems.
    The patient is asked to indicate his/her health state by ticking the box next
    to the most appropriate statement in each of the five dimensions. This decision
    results into a 1-digit number that expresses the level selected for that dimension.
    The digits for the five dimensions can be combined into a 5-digit number that
    describes the patients health state.
  requires_registration: true
  url: https://euroqol.org/eq-5d-instruments/eq-5d-3l-about/
- id: B2AI_STANDARD:108
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FAIR Cookbook
  has_relevant_organization:
  - B2AI_ORG:28
  is_open: true
  name: FAIR Cookbook
  publication: doi:10.5281/zenodo.7156792
  purpose_detail: An online, open and live resource for the Life Sciences with recipes
    that help you to make and keep data Findable, Accessible, Interoperable and Reusable;
    in one word FAIR.
  requires_registration: false
  url: https://faircookbook.elixir-europe.org/
- id: B2AI_STANDARD:109
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Fast Healthcare Interoperability Resources
  has_relevant_organization:
  - B2AI_ORG:114
  - B2AI_ORG:117
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: FHIR
  purpose_detail: Fast Healthcare Interoperability Resources (FHIR) is a standard
    for exchanging healthcare information electronically, developed by HL7. FHIR defines
    a set of modular components called "resources" that represent discrete clinical
    and administrative concepts (patients, observations, medications, procedures,
    diagnostic reports, etc.) with well-defined data structures, terminology bindings,
    and RESTful API interactions. Each resource can be retrieved, created, updated,
    and searched via standard HTTP operations, enabling flexible system integration
    and mobile/web application development. FHIR supports multiple exchange paradigms
    including RESTful APIs, messaging, documents, and services, with extensibility
    mechanisms (profiles, extensions) to adapt to local requirements while maintaining
    interoperability. The standard leverages modern web technologies (JSON, XML),
    common terminologies (SNOMED CT, LOINC, RxNorm), and implementation guides to
    facilitate rapid deployment across EHR systems, clinical decision support tools,
    patient-facing applications, and research platforms. FHIR's structured, granular
    data representation and standardized APIs make it particularly suitable for AI/ML
    applications that require consistent access to longitudinal patient data, clinical
    observations, and healthcare workflows for training predictive models and deploying
    real-time decision support at the point of care.
  related_to:
  - B2AI_STANDARD:720
  - B2AI_STANDARD:688
  - B2AI_STANDARD:693
  - B2AI_STANDARD:694
  - B2AI_STANDARD:697
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:40
  url: https://www.hl7.org/fhir/overview.html
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:34
    category: B2AI:Application
    name: Cumulus Federated EHR Learning with Bulk FHIR and AI/NLP
    description: The Cumulus platform operationalizes SMART/HL7 Bulk FHIR Access API
      for standardized data export across multiple healthcare institutions, then applies
      AI and natural language processing for computable phenotyping to define cohorts
      and outcomes from both structured and unstructured EHR data. The SMART Text2FHIR
      pipeline extracts insights from clinical texts and converts them into structured
      FHIR data elements for analysis. Only aggregate outputs leave each institution,
      enabling privacy-preserving federated learning across sites for public health
      monitoring and research while maintaining data sovereignty and interoperability
      through standardized FHIR exchange.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2024.02.02.24301940
  - id: B2AI_APP:126
    category: B2AI:Application
    name: FHIR Observation Encoding of Radiology AI Findings for Interoperable Integration
    description: A framework models radiology AI outputs (such as pulmonary nodule
      characteristics from automated detection algorithms) as FHIR Observation resources
      embedded within DiagnosticReport objects, standardizing AI-generated findings
      alongside radiologist reports. This FHIR-based encoding enables interoperable
      downstream use, long-term tracking of imaging findings, and integration with
      electronic health records for clinical decision-making. The structured FHIR
      representation facilitates reproducible data management, cohort identification,
      and longitudinal analysis of AI-detected imaging biomarkers across healthcare
      systems.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1093/jamia/ocae134
  - id: B2AI_APP:127
    category: B2AI:Application
    name: MyDigiTwin FHIR Harmonization for Federated Learning Cohort Studies
    description: The MyDigiTwin federated learning research infrastructure uses a
      FHIR-based data harmonization framework to standardize cohort study variables
      across multiple sites for cardiovascular risk prediction modeling. The pipeline
      successfully generated approximately 150,000 FHIR bundles from Lifelines cohort
      variable data, demonstrating practical large-scale FHIR use for multi-center
      ML. This FHIR harmonization ensures interoperability and facilitates the structuring,
      standardization, and exchange of healthcare data across federated learning nodes,
      enabling distributed model training while data remains at local institutions.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.3233/shti240735
  - id: B2AI_APP:128
    category: B2AI:Application
    name: SMART on FHIR Integration for AI-Driven Clinical Decision Support Systems
    description: SMART on FHIR provides a modular framework enabling third-party AI-driven
      clinical decision support applications to integrate seamlessly into health information
      systems. Implementations include automated breast cancer diagnosis via ultrasound
      with FHIR-standardized diagnostic metadata, ECG stream analysis frameworks for
      cloud-native AI processing, and machine learning-enhanced architectures that
      use FHIR to standardize clinical and imaging data flow for AI-based risk assessment.
      FHIR's standardized APIs and resource models enable deployment of predictive
      models across clinical domains, improving adherence to guidelines and enhancing
      diagnostic accessibility through interoperable, scalable CDSS implementations.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.20944/preprints202509.0818.v1
  - id: B2AI_APP:129
    category: B2AI:Application
    name: FHIR-Based Analytics Platforms for Clinical Predictive Model Deployment
    description: FHIR data models and APIs support analytics frameworks and deployment
      of clinical predictive models including sepsis prediction, distributed phenotyping
      analytics platforms, and personalized medicine applications. Implementations
      provide patterns for converting FHIR resources to analysis-ready formats, hosting
      predictive models as web services, and automating management of audit files
      and structured medical data. Platforms like doc.ai leverage FHIR analytical
      capabilities for personalized medicine, while distributed phenotyping systems
      use FHIR to enable clinical decision-making across multiple healthcare sites
      with standardized data access and model integration.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.3390/healthcare11121729
- id: B2AI_STANDARD:110
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FASTA Sequence Format
  is_open: true
  name: FASTA
  purpose_detail: A text-based data format for nucleotide and amino acid sequences.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/FASTA_format
  used_in_bridge2ai: true
- id: B2AI_STANDARD:111
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FASTQ Sequence and Sequence Quality Format
  has_relevant_organization:
  - B2AI_ORG:116
  - B2AI_ORG:114
  is_open: true
  name: FASTQ
  purpose_detail: FASTQ is a text-based file format for storing nucleotide sequences
    along with their corresponding per-base quality scores, making it the de facto
    standard for raw sequencing data from high-throughput sequencing platforms (Illumina,
    PacBio, Oxford Nanopore, etc.). Each sequence record consists of four lines containing
    a sequence identifier, the raw nucleotide sequence, a separator line, and ASCII-encoded
    quality scores (Phred scores) representing the confidence of each base call. The
    quality scores enable downstream tools to weight bases appropriately during alignment,
    variant calling, and assembly operations. FASTQ supports various quality encoding
    schemes (Phred+33, Phred+64) and accommodates reads of variable length from different
    sequencing technologies. The format's simultaneous representation of sequence
    and quality information is essential for quality control, error correction, read
    filtering, and AI/ML applications that require confidence metrics for training
    models on sequencing data quality assessment, base calling improvement, contamination
    detection, and metagenomic classification tasks.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/FASTQ_format
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:35
    category: B2AI:Application
    name: seqQscorer Automated FASTQ Quality Control Using Machine Learning
    description: seqQscorer computes features from FASTQ files including per-base
      and per-read quality metrics, overrepresented sequences, k-mer counts, and mapping-derived
      statistics, then trains tree-based (Random Forest) and deep learning classifiers
      to automatically flag low- versus high-quality NGS files. The system uses FASTQ-derived
      quality scores as core predictive features with labels from ENCODE curation,
      creating generalizable models across assays (RNA-seq, ChIP-seq, DNase-seq, ATAC-seq)
      and species. This ML-based automated QC provides decision support for curators
      and researchers, saving resources by identifying poor-quality experiments early
      in analysis pipelines.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1186/s13059-021-02294-2
  - id: B2AI_APP:130
    category: B2AI:Application
    name: MiniScrub CNN-Based Nanopore Read Error Scrubbing Using FASTQ Quality Pileups
    description: MiniScrub encodes Oxford Nanopore read-to-read overlaps and per-base
      qualities from FASTQ into image-like pileup channels that capture minimizer
      matches, base quality scores, and inter-minimizer distances. A convolutional
      neural network trained on these FASTQ-derived representations identifies and
      removes low-quality read segments de novo, without requiring reference alignment
      or hybrid error correction. This CNN-based scrubbing improves downstream assembly
      quality by reducing misassemblies and indel errors, demonstrating how FASTQ
      sequence and quality information can be transformed into ML-friendly image representations
      for quality classification tasks.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1186/s12859-019-3103-z
  - id: B2AI_APP:131
    category: B2AI:Application
    name: MAC-ErrorReads Supervised Classification for Filtering Erroneous NGS Reads
    description: MAC-ErrorReads formulates erroneous-read filtration as supervised
      binary classification using features extracted from FASTQ sequences via TF-IDF
      encoding (treating sequence tokens/k-mers like text). Multiple supervised algorithms
      including Naive Bayes models are trained to distinguish erroneous versus accurate
      reads, retaining higher-quality reads and improving downstream assembly quality
      and genomic coverage. This ML approach addresses increasing sequencing throughput
      by providing automated, scalable read filtering that complements traditional
      k-mer-based and alignment-based error-handling methods across both short-read
      and long-read NGS platforms.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1186/s12859-024-05681-1
  - id: B2AI_APP:132
    category: B2AI:Application
    name: SeqTagger CTC-CRF Basecaller for Nanopore Barcode Demultiplexing with Quality
      Filtering
    description: SeqTagger demultiplexes direct RNA nanopore datasets by training
      a CTC-CRF DNA basecalling model (using Bonito software) to decode barcode sequences
      from signal data, producing FASTQ basecalls that are then mapped to reference
      barcodes. Per-read median base quality from FASTQ is used to filter out potential
      misassignments, achieving high precision (approximately 99%) and recall (approximately
      95%) for demultiplexing. This workflow demonstrates how FASTQ basecalled sequences
      and their quality scores enable accurate barcode assignment and quality-based
      filtering in direct RNA sequencing, contrasting with image-based CNN approaches
      by operating in sequence space.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2024.10.29.620808
  - id: B2AI_APP:133
    category: B2AI:Application
    name: Deep Learning Variant Calling from FASTQ-Aligned Read Pileups
    description: Deep learning variant callers including DeepVariant, Clair3, and
      DNAscope operate on pileup tensors built from FASTQ-aligned reads, incorporating
      per-base quality scores as input features alongside base identity. These models
      transform FASTQ-derived alignments into image-like or tensor representations
      that preserve sequence context and quality information, then apply CNNs or specialized
      architectures for genotype prediction. Benchmarks demonstrate that these deep
      learning approaches outperform traditional algorithms for SNP and indel calling
      across platforms (Illumina, PacBio HiFi, Oxford Nanopore), with quality scores
      explicitly used to weight evidence and calibrate predictions through methods
      like GATK's variant quality score recalibration (VQSR).
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2024.03.15.585313
    - https://doi.org/10.1002/0471250953.bi1110s43
  - id: B2AI_APP:134
    category: B2AI:Application
    name: CNN-Based eDNA Read Classification and rRNA Detection from FASTQ
    description: Deep learning models for metagenomics and environmental DNA analysis
      map FASTQ reads directly to taxonomic labels using k-mer embeddings or sequence
      representations, enabling fast and scalable classification with comparable accuracy
      to conventional pipelines. CNN-based workflows process raw FASTQ at high throughput
      for taxonomic assignment, while tools like RiboDetector use bidirectional LSTM
      models to identify ribosomal RNA reads in short-read data for contaminant removal.
      These approaches transform FASTQ sequences (and optionally quality-derived features)
      into token/k-mer embeddings or sequence inputs for classification tasks, demonstrating
      rapid and accurate read-level identification that reduces misclassification
      in downstream RNA-seq and metagenomic workflows.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2024.03.15.585313
  - id: B2AI_APP:135
    category: B2AI:Application
    name: fastp FASTQ Preprocessing for ML-Ready Feature Generation
    description: fastp performs ultra-fast all-in-one FASTQ preprocessing including
      quality control, adapter trimming, quality filtering, UMI processing, and base
      correction in a single scan, generating standardized outputs used as inputs
      or targets for ML workflows. The tool produces per-base quality profiles, k-mer
      occurrence tables, overrepresented-sequence positions, adapter detection results,
      and paired-end overlap corrections that serve as features for ML quality prediction
      models, denoising/error-correction training data, and metadata for classifiers.
      UMI-processed consensus reads reduce noise for ML training, while comprehensive
      pre- and post-filtering metrics enable automated quality assessment and artifact
      detection across NGS workflows, facilitating large-scale dataset generation
      for ML model development.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1093/bioinformatics/bty560
- id: B2AI_STANDARD:112
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - policy
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Federal Policy for the Protection of Human Subjects
  has_relevant_organization:
  - B2AI_ORG:39
  is_open: true
  name: Common Rule
  purpose_detail: The Federal Policy for the Protection of Human Subjects or the Common
    Rule was published in 1991 and codified in separate regulations by 15 Federal
    departments and agencies, as listed below. The HHS regulations, 45 CFR part 46,
    include four subparts - subpart A, also known as the Federal Policy or the Common
    Rule; subpart B, additional protections for pregnant women, human fetuses, and
    neonates; subpart C, additional protections for prisoners; and subpart D, additional
    protections for children. Each agency includes in its chapter of the Code of Federal
    Regulations [CFR] section numbers and language that are identical to those of
    the HHS codification at 45 CFR part 46, subpart A. For all participating departments
    and agencies the Common Rule outlines the basic provisions for IRBs, informed
    consent, and Assurances of Compliance. Human subject research conducted or supported
    by each federal department/agency is governed by the regulations of that department/agency.
  requires_registration: false
  url: https://www.hhs.gov/ohrp/regulations-and-policy/regulations/common-rule/index.html
  used_in_bridge2ai: true
- id: B2AI_STANDARD:113
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FHIR Common Data Models Harmonization
  formal_specification: https://build.fhir.org/ig/HL7/cdmh/profiles.html
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: CDMH
  purpose_detail: The Common Data Models Harmonization (CDMH) FHIR Implementation
    Guide is an HL7-sponsored specification that provides standardized mappings and
    transformation rules for converting observational clinical data from multiple
    common data models (CDMs) into HL7 FHIR format, enabling interoperability between
    patient-centered outcomes research networks, federated data networks, and learning
    health systems that use different data representation schemas while preserving
    semantic fidelity and supporting cross-network queries for comparative effectiveness
    research, quality improvement initiatives, and post-market surveillance studies.
    CDMH focuses on harmonizing four major CDMs widely adopted in the United States
    research infrastructurePCORNet (Patient-Centered Outcomes Research Network),
    i2b2/ACT (Informatics for Integrating Biology and Bedside / Accrual to Clinical
    Trials), OMOP (Observational Medical Outcomes Partnership), and FDA Sentineleach
    of which employs distinct data schemas, terminology bindings, and structural conventions
    that historically impeded data sharing and federated analysis across research
    consortia. The Implementation Guide defines FHIR resource profiles, ConceptMaps,
    StructureMaps, and transformation logic that specify how core clinical entities
    (patient demographics, encounters, diagnoses, procedures, medications, laboratory
    results, vital signs) represented in PCORNet's relational model, i2b2/ACT's star
    schema with ontology-driven observations, OMOP's standardized vocabularies and
    fact tables, or Sentinel's distributed query architecture are mapped to corresponding
    FHIR resources (Patient, Encounter, Condition, Procedure, MedicationRequest,
    Observation) with appropriate code system translations and cardinality adjustments.
    By providing bidirectional mappings, CDMH enables research networks to expose
    their existing CDM data through FHIR APIs without requiring complete data migration,
    allowing external applications, quality measure engines, and clinical decision
    support tools built on FHIR to access multi-network datasets while preserving
    local data governance and privacy controls. The harmonization approach addresses
    semantic challenges including vocabulary alignment (mapping between SNOMED CT,
    RxNorm, ICD-10-CM, LOINC, and CDM-specific code systems like PCORNet value sets
    or OMOP concept codes), temporal representation differences (event dates versus
    observation periods versus time spans), and structural mismatches (flat tables
    versus hierarchical resources, normalized schemas versus denormalized views).
    CDMH supports use cases in comparative effectiveness research where investigators
    need to pool patient cohorts from multiple networks to achieve sufficient sample
    sizes for detecting treatment effects, rare disease studies, or subgroup analyses,
    with the FHIR-based harmonization layer enabling federated queries that execute
    locally at each network site and aggregate de-identified summary statistics without
    transferring patient-level data across organizational boundaries. For regulatory
    applications, CDMH facilitates FDA Sentinel's integration with FHIR-enabled clinical
    data sources, enabling post-market safety surveillance queries to span both claims-based
    data (traditional Sentinel domain) and EHR-based clinical observations (accessible
    via FHIR), improving signal detection for adverse events that require clinical
    context beyond billing codes. The Implementation Guide provides validation rules,
    conformance testing criteria, and quality metrics (data completeness, mapping
    coverage, terminology alignment rates) to assess the fidelity of CDM-to-FHIR
    transformations and identify data quality issues requiring remediation. CDMH integration
    with SMART on FHIR enables patient-facing applications (patient-reported outcomes
    capture, longitudinal health records, research participation platforms) to interact
    with research network data through standardized APIs, supporting patient engagement
    in research and transparent data sharing with participants. The harmonization
    framework supports machine learning and AI applications by providing unified access
    to training datasets aggregated from multiple CDMs, with FHIR's structured representation
    facilitating feature engineering, phenotype definition, and model validation across
    diverse source systems. As research networks adopt FHIR natively (via FHIR-native
    CDMs or dual-model architectures), CDMH serves as a transition pathway preserving
    investments in existing CDM implementations while enabling incremental migration
    toward standards-based interoperability. The IG is maintained by HL7's Clinical
    Interoperability Council with input from PCORnet, i2b2, OMOP/OHDSI, and FDA Sentinel
    stakeholder communities, ensuring alignment with evolving CDM specifications and
    FHIR release cycles.
  related_to:
  - B2AI_STANDARD:109
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: https://build.fhir.org/ig/HL7/cdmh/
- id: B2AI_STANDARD:114
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:13
  - B2AI_TOPIC:35
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FHIR Genomics Operations
  formal_specification: https://github.com/FHIR/genomics-operations
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: Genomics Operations
  publication: doi:10.1093/jamia/ocac246
  purpose_detail: A standardized suite of genomics operations in FHIR designed to
    support a wide range of clinical scenarios, such as variant discovery; clinical
    trial matching; hereditary condition and pharmacogenomic screening; and variant
    reanalysis.
  related_to:
  - B2AI_STANDARD:109
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: http://build.fhir.org/ig/HL7/genomics-reporting/operations.html
- id: B2AI_STANDARD:115
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FHIR Provenance Resource
  formal_specification: https://www.hl7.org/fhir/provenance-definitions.html
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: Provenance
  purpose_detail: '"The FHIR Provenance resource provides comprehensive tracking of
    information about activities that create, revise, delete, or sign healthcare resources,
    describing the entities and agents involved in these processes. Based on the W3C
    Provenance specification, it establishes a critical foundation for assessing authenticity,
    enabling trust, and allowing reproducibility in healthcare data systems. The resource
    supports three key components from W3C Provenance: Entity (physical, digital,
    or conceptual things with fixed aspects), Agent (persons, devices, systems, organizations,
    or care teams bearing responsibility for activities), and Activity (time-bound
    processes that act upon entities). Provenance resources serve as record-keeping
    assertions that capture context information for quality, reliability, and trustworthiness
    assessments. The standard supports multiple use cases including tracking data
    lineage in clinical workflows, enabling audit trails for regulatory compliance,
    supporting clinical decision-making through data origin verification, and facilitating
    data integration from multiple healthcare systems. It includes digital signature
    capabilities for integrity verification and non-repudiation, supports versioning
    and unique identification requirements, and provides mechanisms for recording
    import and transformation activities in healthcare interoperability scenarios."'
  related_to:
  - B2AI_STANDARD:109
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: https://www.hl7.org/fhir/provenance.html
- id: B2AI_STANDARD:116
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FHIR Resource CodeSystem
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: CodeSystem
  purpose_detail: The CodeSystem resource is used to declare the existence of and
    describe a code system or code system supplement and its key properties, and optionally
    define a part or all of its content.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: http://hl7.org/fhir/codesystem.html
  used_in_bridge2ai: true
- id: B2AI_STANDARD:117
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  - B2AI_TOPIC:32
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FHIR Resource ConceptMap
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: ConceptMap
  purpose_detail: A statement of relationships from one set of concepts to one or
    more other concepts - either concepts in code systems, or data element/data element
    concepts, or classes in class models.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: http://hl7.org/fhir/conceptmap.html
- id: B2AI_STANDARD:118
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FHIR Resource NamingSystem
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: NamingSystem
  purpose_detail: A curated namespace that issues unique symbols within that namespace
    for the identification of concepts, people, devices, etc. Represents a System
    used within the Identifier and Coding data types.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: http://hl7.org/fhir/namingsystem.html
- id: B2AI_STANDARD:119
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FHIR Resource TerminologyCapabilities
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: TerminologyCapabilities
  purpose_detail: A TerminologyCapabilities resource documents a set of capabilities
    (behaviors) of a FHIR Terminology Server that may be used as a statement of actual
    server functionality or a statement of required or desired server implementation.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: http://hl7.org/fhir/terminologycapabilities.html
- id: B2AI_STANDARD:120
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FHIR Resource ValueSet
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: ValueSet
  purpose_detail: A ValueSet resource instance specifies a set of codes drawn from
    one or more code systems, intended for use in a particular context. Value sets
    link between CodeSystem definitions and their use in coded elements.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: http://hl7.org/fhir/valueset.html
- id: B2AI_STANDARD:121
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:2
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Flow Cytometry Standard format
  is_open: true
  name: FCS
  publication: doi:10.1002/cyto.a.20825
  purpose_detail: The flow cytometry data file standard provides the specifications
    needed to completely describe flow cytometry data sets within the confines of
    the file containing the experimental data. In 1984, the first Flow Cytometry Standard
    format for data files was adopted as FCS 1.0. This standard was modified in 1990
    as FCS 2.0 and again in 1997 as FCS 3.0. We report here on the next generation
    Flow Cytometry Standard data file format. FCS 3.1 is a minor revision based on
    suggested improvements from the community. The unchanged goal of the Standard
    is to provide a uniform file format that allows files created by one type of acquisition
    hardware and software to be analyzed by any other type.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/Flow_Cytometry_Standard
- id: B2AI_STANDARD:122
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FORCE11 Data Citation Principles
  is_open: true
  name: FORCE11 DC
  purpose_detail: The Data Citation Principles cover the purpose, function and attributes
    of citations. These principles recognize the dual necessity of creating citation
    practices that are both human understandable and machine-actionable.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:32
  url: https://force11.org/info/joint-declaration-of-data-citation-principles-final/
- id: B2AI_STANDARD:123
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - diagnosticinstrument
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Functional Assessment of Cancer Therapy - General
  is_open: true
  name: FACT-G
  purpose_detail: The Functional Assessment of Cancer Therapy - General (FACT-G) is
    a 27-item questionnaire designed to measure four domains of HRQOL in cancer patients
    - Physical, social, emotional, and functional well-being. Original development
    and validation involved 854 patients with cancer and 15 oncology specialists.
    An initial pool of 370 overlapping items for breast, lung, and colorectal cancer
    was generated by open-ended interviews with patients experienced with the symptoms
    of cancer and oncology professionals. Using preselected criteria, items were reduced
    to a 38-item general version. Factor and scaling analyses of these 38 items on
    545 patients with mixed cancer diagnoses resulted in the 27-item FACT-General
    (FACT-G). Coefficients of reliability and validity were uniformly high. The scale's
    ability to discriminate patients on the basis of stage of disease, performance
    status rating (PSR), and hospitalization status supports its sensitivity. It has
    also demonstrated sensitivity to change over time.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:30
  url: https://www.facit.org/measures/FACT-G
- id: B2AI_STANDARD:124
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - diagnosticinstrument
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Functional Assessment of Chronic Illness Therapy - Dyspnea
  is_open: true
  name: FACIT-Dyspnea
  publication: doi:10.1016/j.jval.2010.06.001
  purpose_detail: The Functional Assessment of Chronic Illness Therapy - Dyspnea
    (FACIT-Dyspnea) is a validated, patient-reported outcome (PRO) instrument designed
    to comprehensively assess the multidimensional impact of dyspnea (shortness of
    breath) on patients' functional status and health-related quality of life across
    chronic respiratory diseases, cardiovascular conditions, and oncologic illnesses
    where breathlessness is a primary symptom affecting daily activities, emotional
    well-being, and social participation. Developed by the FACIT measurement system
    group and published with rigorous psychometric validation in Value in Health (2010),
    the instrument comprises a 10-item short form (FACIT-Dyspnea-10) that captures
    physical limitations imposed by dyspnea (ability to walk, climb stairs, perform
    household tasks, exercise), functional impairments in activities of daily living
    (bathing, dressing, eating), emotional consequences (anxiety, frustration, fear
    related to breathlessness), and social impacts (ability to participate in social
    activities, work, and recreational pursuits). Each item uses a 5-point Likert
    scale (0 = not at all, 4 = very much) referencing the past 7 days, with total
    scores ranging from 0-40 where higher scores indicate greater dyspnea-related
    impairment and lower quality of life. The FACIT-Dyspnea-10 demonstrates strong
    internal consistency (Cronbach's alpha typically >0.85), test-retest reliability,
    convergent validity with other dyspnea measures (Medical Research Council dyspnea
    scale, Borg scale, visual analog scales), and responsiveness to clinical change
    following therapeutic interventions, making it suitable for both cross-sectional
    assessment and longitudinal monitoring of treatment response. Clinical applications
    span chronic obstructive pulmonary disease (COPD) management where dyspnea is
    the cardinal symptom driving healthcare utilization and disability, interstitial
    lung diseases (idiopathic pulmonary fibrosis, sarcoidosis, connective tissue
    disease-associated ILD) where progressive breathlessness correlates with disease
    severity and survival, heart failure populations where dyspnea on exertion guides
    functional classification and treatment escalation decisions, and cancer-related
    breathlessness in lung cancer patients or those experiencing treatment-related
    pulmonary toxicity (radiation pneumonitis, chemotherapy-induced lung injury, immunotherapy-related
    pneumonitis). The instrument is particularly valuable in clinical trials evaluating
    pharmacologic interventions (bronchodilators, anti-fibrotics, diuretics, oxygen
    therapy, opioids for refractory dyspnea), pulmonary rehabilitation programs, exercise
    training interventions, and palliative care approaches for advanced disease, where
    patient-centered outcomes and subjective symptom burden are critical endpoints
    complementing objective measures like spirometry, six-minute walk distance, or
    imaging findings. FACIT-Dyspnea scores correlate with healthcare resource utilization
    patterns (emergency department visits, hospitalizations for dyspnea exacerbations,
    long-term oxygen prescriptions), support risk stratification models predicting
    disease progression and mortality, and inform shared decision-making conversations
    about treatment goals, advance care planning, and hospice transitions when breathlessness
    becomes refractory to disease-modifying therapies. The instrument's brevity (10
    items, ~2-3 minute administration time) facilitates integration into routine clinical
    encounters, remote patient monitoring programs using electronic PRO (ePRO) platforms,
    and large-scale registry studies or epidemiological cohorts examining dyspnea
    prevalence and burden across populations. FACIT-Dyspnea has been translated into
    multiple languages with cultural adaptation and validation in international populations,
    supporting multi-national clinical trials and cross-cultural comparative effectiveness
    research. The instrument complements physiologic measures and biomarkers by capturing
    the lived experience of dyspnea from the patient perspective, addressing FDA and
    EMA guidance emphasizing patient-reported outcomes as essential endpoints in regulatory
    submissions for symptom-focused therapies. FACIT-Dyspnea data support health economic
    evaluations by quantifying the quality-of-life burden of dyspnea for cost-utility
    analyses (QALY calculations) and inform healthcare policy decisions regarding
    coverage of symptomatic therapies, pulmonary rehabilitation services, and palliative
    interventions for breathlessness management.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:30
  url: https://www.facit.org/measures/facit-dyspnea
- id: B2AI_STANDARD:125
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - diagnosticinstrument
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Functional Assessment of Non-Life Threatening Conditions
  is_open: true
  name: FANLTC
  purpose_detail: The Functional Assessment of Non-Life Threatening Conditions (FANLTC)
    is a 26-item version of the FACT-G designed to be administered to patients with
    non-life threatening conditions. The item from the FACT-G making reference to
    anxiety about death has been removed, and the instrument measures four domains
    of HRQOL - Physical, social/family, emotional and functional well-being.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:30
  url: https://www.facit.org/measures/FANLTC
- id: B2AI_STANDARD:126
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:13
  - B2AI_TOPIC:35
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Functional equivalence of genome sequencing analysis pipelines enables
    harmonized variant calling across human genetics projects
  formal_specification: https://github.com/CCDG/Pipeline-Standardization
  is_open: true
  name: Regier2018
  publication: doi:10.1038/s41467-018-06159-4
  purpose_detail: Regier2018 defines whole genome sequencing (WGS) data processing
    standards that enable functional equivalence (FE) across different analysis pipelines
    used by various research groups. These standards establish best practices for
    alignment, variant calling, and quality control steps in genomic analysis workflows,
    allowing different computational implementations to produce concordant results
    while still permitting innovation in pipeline optimization. The framework addresses
    key challenges in large-scale genomic studies by defining expected outputs, quality
    metrics, and validation procedures that ensure harmonized variant calling across
    projects. This standardization is particularly important for collaborative efforts
    like the Centers for Common Disease Genomics (CCDG), where multiple centers must
    produce comparable data that can be combined for downstream analysis. The standards
    cover reference genome usage, read alignment parameters, variant detection algorithms,
    and filtering criteria, providing a foundation for reproducible genomics research.
  requires_registration: false
  url: https://github.com/CCDG/Pipeline-Standardization
- id: B2AI_STANDARD:127
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Functional Genomics Experiment Markup Language
  is_open: true
  name: FuGE-ML
  purpose_detail: Functional genomics experiments present many challenges in data
    archiving, sharing and querying. As the size and complexity of data generated
    from such experiments grows, so does the requirement for standard data formats.
    To address these needs, the Functional Genomics Experiment [Object Model / Markup-Language]
    (FuGE-OM, FuGE-ML) has been created to facilitate the development of data standards.FuGE
    is a model of the shared components in different functional genomics domains.
    FuGE facilitates the development of data standards in functional genomics in two
    ways. 1. FuGE provides a model of common components in functional genomics investigations,
    such as materials, data, protocols, equipment and software. These models can be
    extended to develop modular data formats with consistent structure. 2. FuGE provides
    a framework for capturing complete laboratory workflows, enabling the integration
    of pre-existing data formats. In this context, FuGE allows the capture of additional
    metadata that gives formats a context within the complete workflow. FuGE is available
    as a UML model and an XML Schema
  requires_registration: false
  url: https://fuge.sourceforge.net/index.php
- id: B2AI_STANDARD:128
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Functional Genomics Experiment model for flow cytometry
  is_open: true
  name: FuGEFlow
  publication: doi:10.1186/1471-2105-10-184
  purpose_detail: FuGEFlow is a specialized extension of the Functional Genomics Experiment
    (FuGE) data model designed to provide comprehensive, machine-readable representation
    of flow cytometry experimental workflows, metadata, and data provenance in compliance
    with the Minimum Information about a Flow Cytometry Experiment (MIFlowCyt) reporting
    guidelines, developed through collaborative efforts between the ISAC Data Standards
    Task Force, British Columbia Cancer Agency, and NIH/NIAID to enable standardized
    data exchange, repository integration, and computational reproducibility in high-throughput
    flow cytometry research. The model comprises Flow-OM (Flow Cytometry Object Model)
    defined in UML and Flow-ML (Flow Markup Language) implemented as an XML schema
    generated via AndroMDA, extending FuGE's core classes to capture flow cytometry-specific
    entities including cytometer instrumentation (flow cells, lasers, optical detectors
    with voltage parameters), sample preparation protocols (staining, fixation, permeabilization),
    acquisition parameters, and analytical workflows while maintaining references
    to external standards including FCS (Flow Cytometry Standard) list-mode data
    files and Gating-ML gating transformation descriptions rather than duplicating
    raw fluorescence measurements. FuGEFlow maps all MIFlowCyt mandatory and optional
    metadata elements to FuGE classes or newly defined FuGEFlow extensions (e.g.,
    ListModeDataFile, extended Material and Equipment classes), enabling complete
    experiment documentation from biological source through data acquisition to computational
    analysis with explicit ontology term bindings for standardized vocabularies.
    The framework supports conversion to ISA-TAB (Investigation-Study-Assay) tabular
    format via XSL stylesheets, facilitating integration with broader functional
    genomics data repositories and cross-domain meta-analyses that combine flow cytometry
    with transcriptomics, proteomics, or metabolomics datasets. FuGEFlow's design
    enables software tool interoperability through the FuGE toolkit which generates
    programmatic interfaces from the object model, and its flexible architecture
    accommodates diverse experimental designs including time-course studies, dose-response
    assays, and complex multi-parametric analyses that were not anticipated during
    initial specification development. In the context of artificial intelligence and
    machine learning applications, FuGEFlow serves as an enabling infrastructure standard
    rather than a direct ML analysis frameworkwhile peer-reviewed literature does
    not document specific cases where ML models were trained or validated using FuGEFlow-encoded
    datasets, the standard's comprehensive metadata capture and standardized experiment
    representation provide the foundation for several potential ML use cases including
    cross-study dataset integration for training population-scale classification
    models, reproducible feature extraction pipelines that link raw FCS signals through
    Gating-ML transformations to derived cellular phenotypes, automated quality control
    systems that detect instrument drift or protocol deviations through metadata
    pattern analysis, and meta-learning approaches that generalize gating strategies
    or cell population identification algorithms across diverse experimental conditions
    documented in standardized metadata. The ISA-TAB conversion capability enables
    export to tabular formats commonly consumed by ML frameworks (pandas, scikit-learn,
    TensorFlow), while FuGEFlow's provenance tracking supports reproducible ML workflows
    where training data lineage, preprocessing steps, and analytical transformations
    must be documented for regulatory compliance or scientific validation. Public
    flow cytometry data repositories built on Flow-ML infrastructure (as planned in
    the original FuGEFlow publication with anticipated ImmPort integration) could
    serve as curated training corpora for supervised learning of cell type classifiers,
    automated gating algorithms, or anomaly detection systems, though such repositories'
    ML utilization has not been systematically reported in subsequent literature.
    The standard's ontology-based knowledge representation aligns with broader data
    mining best practices articulated by the minimum information community, where
    machine-readable experiment metadata enables computational hypothesis generation,
    experimental design optimization, and integration of biological knowledge with
    primary analytical resultscapabilities that form the semantic foundation for
    AI systems even when FuGEFlow itself is not explicitly invoked in ML method descriptions.
  requires_registration: false
  url: https://doi.org/10.1186/1471-2105-10-184
- id: B2AI_STANDARD:129
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - diagnosticinstrument
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Functionality Assessment Flowchart
  is_open: true
  name: FAF
  publication: doi:10.1186/s12885-015-1526-0
  purpose_detail: The Functionality Assessment Flowchart (FAF) is a structured clinical
    assessment tool designed to evaluate cancer patients' performance status through
    a standardized flowchart-based methodology. FAF provides a systematic approach
    to measuring functional capacity and activities of daily living in oncology patients,
    yielding performance status scores with high inter-observer agreement. The flowchart
    format guides clinicians through a series of binary decision points based on patient
    capabilities (ability to work, walk, self-care, etc.), resulting in consistent
    and reproducible assessments. FAF addresses limitations of traditional performance
    status scales by reducing subjectivity and improving reliability across different
    observers and clinical settings. The standardized scoring enables comparison of
    patient functional status over time, supports clinical decision-making for treatment
    planning, facilitates stratification in clinical trials, and provides structured
    data suitable for integration into electronic health records and outcomes research
    databases.
  requires_registration: false
  url: https://doi.org/10.1186/s12885-015-1526-0
- id: B2AI_STANDARD:130
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: GA4GH metadata model
  formal_specification: https://github.com/ga4gh-metadata/metadata-schemas
  is_open: true
  name: GA4GH
  purpose_detail: The Global Alliance for Genomics and Health (GA4GH) metadata model
    provides a standardized framework for representing and sharing genomic and clinical
    data across institutions and international borders, developed by a coalition of
    public and private stakeholders including research institutions, healthcare organizations,
    and technology companies. The model defines schemas for core genomic data types
    including variants, reads, references, annotations, and associated clinical phenotypes,
    using protocol buffers (protobuf) and JSON for interoperable data exchange. GA4GH
    schemas cover reference genomes and sequences, variant calls and annotations (compatible
    with VCF), sequencing reads and alignments (supporting BAM/CRAM), RNA quantification,
    continuous-valued genomic signals, genomic features and annotations, metadata
    about biosamples and individuals, phenotypic information using ontologies, and
    provenance tracking. The framework enables federated data access through APIs
    including hts-get for streaming genomic data, Beacon for discovery queries across
    datasets, Data Repository Service (DRS) for accessing data objects, Task Execution
    Service (TES) for running analysis workflows, and Workflow Execution Service (WES)
    for managing computational pipelines. GA4GH standards facilitate large-scale collaborative
    research initiatives, support FAIR principles (Findable, Accessible, Interoperable,
    Reusable), enable secure data sharing with privacy-preserving technologies, and
    provide the foundation for international genomics research networks, clinical
    genomics implementations, and precision medicine programs.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:34
  collection:
  - deprecated
  url: https://github.com/ga4gh-metadata/metadata-schemas?tab=readme-ov-file
- id: B2AI_STANDARD:131
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Gating-ML specification
  is_open: true
  name: Gating-ML
  purpose_detail: The Gating-ML specification represents a proposal on how to form
    unambiguous XML-based gate definitions that may be used independently as well
    as included as one of the components of ACS. Such a description of gates can facilitate
    the interchange and validation of data between different software packages with
    the potential of significant increase of hardware and software interoperability.
    The specification supports rectangular gates in n dimensions (i.e., from one-dimensional
    range gates up to n-dimensional hyper-rectangular regions), polygon gates in two
    (and more) dimensions, ellipsoid gates in n dimensions, decision tree structures,
    and Boolean collections of any of the types of gates. Gates can be uniquely identified
    and may be ordered into a hierarchical structure to describe a gating strategy.
    Gates may be applied on parameters as in list mode data files (e.g., FCS files)
    or on transformed parameters as described by any explicit parameter transformation.
    Therefore, since version 1.5, parameter transformation and compensation description
    are included as part of the Gating-ML specification.
  requires_registration: false
  url: http://flowcyt.sourceforge.net/gating/
- id: B2AI_STANDARD:132
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:26
  - B2AI_TOPIC:33
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: GenBank Sequence Format
  is_open: true
  name: GB
  purpose_detail: The GenBank format is a comprehensive text-based annotation format
    for nucleotide and protein sequences maintained by NCBI as part of the GenBank
    sequence database, one of the three components of the International Nucleotide
    Sequence Database Collaboration (INSDC) alongside EMBL and DDBJ. Each GenBank
    record contains a sequence along with extensive structured metadata organized
    into standardized fields including locus name and length, definition line, accession
    and version numbers, keywords, source organism with taxonomic classification,
    literature references with PubMed links, and feature table annotations. The feature
    table uses a hierarchical key-value structure to annotate biological features
    such as genes, coding sequences (CDS) with translation, regulatory elements, binding
    sites, variations, and experimental evidence, with controlled qualifiers and ontology
    terms. GenBank format supports protein translations, codon usage information,
    cross-references to other databases (UniProt, PDB, Gene, etc.), and provenance
    tracking of sequence submissions and updates. The rich annotation structure enables
    integration of genomic context, functional annotations, phylogenetic information,
    and experimental validation data in a single standardized record, making GenBank
    essential for comparative genomics, gene annotation pipelines, sequence analysis
    tools, and machine learning applications that leverage both sequence content and
    biological metadata for training predictive models of gene structure, function,
    and regulation.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/GenBank
- id: B2AI_STANDARD:133
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Gene Prediction File Format
  is_open: true
  name: GenePred
  purpose_detail: The genePred (Gene Prediction) format is a flexible tab-delimited
    table format developed by UCSC Genome Browser for representing gene structure
    predictions and annotations, widely used for storing transcript models, gene predictions
    from ab initio tools, and curated gene annotations. The basic genePred format
    contains 10 required fields - gene name, chromosome, strand, transcription start/end
    positions, coding sequence (CDS) start/end positions, exon count, comma-separated
    exon start positions, and comma-separated exon end positions. Extended versions
    (genePredExt) add fields for unique identifiers, CDS start/end status (complete/incomplete),
    exon frame information for each coding exon, and gene symbols or descriptions.
    The format efficiently represents complex transcript structures including alternative
    splicing isoforms, UTRs (untranslated regions), introns, and partial gene models.
    GenePred files support various gene prediction algorithms (GeneMark, Augustus,
    SNAP), RefSeq annotations, Ensembl gene builds, and GENCODE comprehensive gene
    sets. UCSC provides utilities including genePredToBed for BED format conversion,
    genePredToGtf for GTF conversion, and genePredToPsl for PSL format output. The
    format's compact structure and explicit exon coordinates make it ideal for genome
    browsers, annotation pipelines, and comparative genomics analyses requiring efficient
    storage and rapid retrieval of gene structure information across whole genomes.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:119
  url: https://genome.ucsc.edu/FAQ/FAQformat.html#format9
- id: B2AI_STANDARD:134
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:26
  - B2AI_TOPIC:33
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Gene Product Annotation Data format
  is_open: true
  name: GPAD
  purpose_detail: The Gene Ontology project provides annotations describing attributes
    of biological entities such as genes and gene products. The Gene Ontology has
    historically provided annotations via Gene Association Format (GAF), including
    GAF-1 and GAF-2. Ontologies are distributed separately, using an OWL serialization
    or OBO format. The use of GAF has some drawbacks. Combined representation of gene/gene
    product data and annotations leads to redundancy/repetition No way to represent
    gene/gene product metadata for unannotated genes Requirement to maintain backward
    compatibility makes it harder to introduce enhancements such as use of an ontology
    for evidence types GAF formats will continue to be supported, but the need for
    a way to represent genes/gene products separately from annotations, as well as
    the need to use the evidence ontology has lead to the creation of the GPAD (Gene
    Product Annotation Data) and GPI (Gene Product Information) formats, defined here.
    Whilst GPAD and GPI have been defined for use within the Gene Ontology Consortium
    for GO annotation, this specification is designed to be reusable for analagous
    ontology-based annotation - for example, gene phenotype annotation.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:36
  url: http://geneontology.org/docs/gene-product-association-data-gpad-format/
- id: B2AI_STANDARD:135
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Gene Transfer Format
  is_open: true
  name: GTF
  purpose_detail: GTF (Gene Transfer Format, GTF2.2) is an extension to, and backward
    compatible with, GFF2. The first eight GTF fields are the same as GFF. The feature
    field is the same as GFF, with the exception that it also includes the following
    optional values, 5UTR, 3UTR, inter, inter_CNS, and intron_CNS.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:119
  url: http://genome.ucsc.edu/FAQ/FAQformat.html#format4
- id: B2AI_STANDARD:136
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: GenePattern GeneSet Table Format
  is_open: true
  name: GMT
  purpose_detail: The GMT (Gene Matrix Transposed) format is a tab-delimited text
    file format developed by the Broad Institute for representing gene sets used in
    gene set enrichment analysis (GSEA) and related computational biology applications.
    Each line in a GMT file represents a single gene set with a simple structure containing
    the gene set name, description (or URL), and a tab-separated list of gene identifiers
    (typically gene symbols, Entrez IDs, or other standard identifiers). The format's
    simplicity and human readability make it ideal for storing and sharing curated
    gene set collections such as pathway databases (KEGG, Reactome), Gene Ontology
    term associations, disease gene signatures, transcription factor targets, and
    experimentally derived co-expression modules. GMT files are widely used by enrichment
    analysis tools (GSEA, Enrichr, WebGestalt), enable integration of custom gene
    sets into analysis workflows, support reproducible research through standardized
    gene set definitions, and facilitate machine learning applications that use gene
    set membership as features for classification, dimension reduction, and biological
    interpretation of high-throughput genomic data.
  requires_registration: false
  url: https://www.genepattern.org/file-formats-guide#GMT
- id: B2AI_STANDARD:137
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: General Feature Format
  formal_specification: https://github.com/The-Sequence-Ontology/Specifications
  is_open: true
  name: GFF
  purpose_detail: The General Feature Format (GFF) is a tab-delimited text file format
    developed by the Sequence Ontology Consortium for representing genomic features
    and annotations. GFF3, the current version, provides a flexible structure for
    describing genes, transcripts, exons, regulatory elements, and other sequence
    features with their genomic coordinates, strand orientation, phase information,
    and hierarchical relationships. Each line represents a single feature with nine
    standardized fields including sequence ID, source, feature type (from Sequence
    Ontology terms), start/end positions, score, strand, and attributes. The format
    supports parent-child relationships through ID and Parent attributes, enabling
    representation of complex gene structures with multiple transcripts and alternative
    splicing. GFF3 includes directives for metadata and embedded FASTA sequences.
    The format is widely used by genome browsers (UCSC, Ensembl, JBrowse), annotation
    pipelines, and analysis tools for visualizing and processing genomic data. Its
    human-readable structure and extensive tool support make it a standard choice
    for genome annotation exchange and long-term data archiving.
  requires_registration: false
  url: https://github.com/The-Sequence-Ontology/Specifications
- id: B2AI_STANDARD:138
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Genome Annotation File
  is_open: true
  name: GAF
  purpose_detail: The Gene Ontology Annotation File (GAF) format is a standardized,
    tab-delimited text file specification developed by the Gene Ontology Consortium
    for representing gene product annotations using GO terms. GAF 2.1 defines 17 fields
    per line, capturing essential information such as database identifiers, gene symbols,
    GO terms, evidence codes, references, taxonomic information, and annotation extensions.
    Each line encodes a single association between a gene product and a GO term, supported
    by evidence and references, enabling precise tracking of functional, process,
    and cellular component annotations. The format supports both required and optional
    fields, allowing for rich annotation detail and interoperability across databases.
    GAF is widely used for large-scale functional genomics, comparative biology, and
    integrative bioinformatics, providing a foundation for computational analysis,
    data sharing, and automated reasoning about gene function across species.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:36
  url: http://geneontology.org/docs/go-annotation-file-gaf-format-2.1/
- id: B2AI_STANDARD:139
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Genome Beacons
  formal_specification: https://github.com/ga4gh-beacon/beacon-v2
  is_open: true
  name: Beacon
  publication: doi:10.1002/humu.24369
  purpose_detail: Beacon v2 is a protocol/specification established by the Global
    Alliance for Genomics and Health initiative (GA4GH) that defines an open standard
    for federated discovery of genomic (and phenoclinic) data in biomedical research
    and clinical applications.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:34
  url: http://docs.genomebeacons.org/
- id: B2AI_STANDARD:140
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:13
  - B2AI_TOPIC:35
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Genome Variation Format
  formal_specification: https://github.com/The-Sequence-Ontology/Specifications/blob/master/gvf.md
  is_open: true
  name: GVF
  purpose_detail: "The Genome Variation Format (GVF) is a specialized extension of
    the Generic Feature Format version 3 (GFF3) designed specifically for representing
    sequence variants, structural variations, and genomic alterations at nucleotide
    resolution relative to a reference genome assembly, developed and maintained by
    the Sequence Ontology Consortium. GVF inherits GFF3's tab-delimited, nine-column
    structure (seqid, source, type, start, end, score, strand, phase, attributes)
    while adding variant-specific conventions and controlled vocabularies for describing
    single nucleotide variants (SNVs), insertions, deletions, inversions, translocations,
    copy number variations (CNVs), and complex genomic rearrangements with precise
    breakpoint coordinates and allelic information. The format uses Sequence Ontology
    (SO) terms in the 'type' column to classify variant types (SO:0001483 for SNV,
    SO:1000032 for indel, SO:0001019 for CNV, SO:0000199 for translocation), ensuring
    semantic consistency and computational interoperability across variant calling
    pipelines, annotation systems, and genomic databases. GVF's attribute column supports
    standardized key-value pairs including Variant_seq for alternate alleles, Reference_seq
    for reference alleles, Genotype for individual genotypes, Variant_freq for population
    allele frequencies, Variant_effect for functional consequences predicted by tools
    like VEP or SnpEff, and Dbxref for cross-references to variant databases (dbSNP,
    ClinVar, COSMIC, gnomAD). The format was designed to address limitations of earlier
    variant formats (such as ambiguities in coordinate systems and lack of standardized
    variant classification) while maintaining backward compatibility with existing
    GFF3 parsers and genome browsers. GVF files include structured pragma directives
    that specify format version,
    reference genome assembly (e.g., GRCh38, GRCm39), and chromosome/scaffold boundaries,
    enabling unambiguous interpretation of variant coordinates across different reference
    assemblies and coordinate systems. The format supports multi-sample variant data
    through the Genotype attribute, allowing representation of diploid or polyploid
    genotypes (homozygous reference, heterozygous, homozygous alternate) and phasing
    information for linked variants on the same haplotype. GVF is particularly valuable
    for documenting structural variations and complex genomic events that involve
    multiple breakpoints, nested features, or imprecise boundaries, where VCF's linear
    representation becomes cumbersome or ambiguous. Research applications include
    population genetics studies reporting allele frequencies and linkage disequilibrium
    patterns, clinical genomics workflows annotating pathogenic variants with ACMG
    classifications and penetrance estimates, cancer genomics projects cataloging
    somatic mutations and driver events, and comparative genomics analyses tracking
    orthologous variants across species. GVF files integrate with genome browsers
    (UCSC Genome Browser, Ensembl, IGV) for visualization of variant landscapes alongside
    gene annotations, regulatory elements, and conservation scores. The format's standardized
    structure enables systematic variant aggregation and meta-analysis, supporting
    databases like Ensembl Variation, NCBI dbSNP, and model organism databases that
    curate and distribute genome-wide variant catalogs. While VCF (Variant Call Format)
    has become the dominant standard for high-throughput sequencing variant calls
    due to its compact representation and extensive tool support, GVF remains relevant
    for scenarios requiring richer semantic annotations, complex structural variants
    with feature hierarchies, or integration with GFF3-based annotation pipelines
    where variant features are treated as genomic intervals in unified analytical
    workflows. The Sequence Ontology Consortium maintains the GVF specification and
    provides reference implementations, validation tools, and mappings to related
    standards (VCF, HGVS nomenclature) to facilitate format interconversion and data
    harmonization across genomic resources."
  requires_registration: false
  url: https://github.com/The-Sequence-Ontology/Specifications/blob/master/gvf.md
- id: B2AI_STANDARD:141
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Genomic Contextual Data Markup Language
  is_open: true
  name: GCDML
  publication: doi:10.1089/omi.2008.0A10
  purpose_detail: The Genomic Contextual Data Markup Language (GCDML) is an XML Schema
    developed by the Genomic Standards Consortium (GSC) to implement the Minimum Information
    about a Genome Sequence (MIGS), Minimum Information about a Metagenome Sequence
    (MIMS), and Minimum Information about a MARKer gene Sequence (MIMARKS) specifications.
    This sample-centric, strongly-typed schema provides a comprehensive set of descriptors
    for documenting the complete provenance of biological samples, from initial collection
    and environmental context through sequencing and subsequent analysis. GCDML enables
    standardized reporting of genomic and metagenomic data by defining required metadata
    fields that capture sampling conditions, laboratory processing methods, sequencing
    parameters, and analytical workflows. The schema facilitates data exchange between
    research groups, repositories, and databases, ensuring that essential contextual
    information needed to interpret sequence data is preserved and communicated effectively.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:38
  url: https://doi.org/10.1089/omi.2008.0A10
- id: B2AI_STANDARD:142
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Global Genome Biodiversity Network Data Standard
  is_open: true
  name: GGBN
  publication: doi:10.1093/database/baw125
  purpose_detail: In order to facilitate exchange of information on genomic samples
    and their derived data, the Global Genome Biodiversity Network (GGBN) Data Standard
    is intended to provide a platform based on a documented agreement to promote the
    efficient sharing and usage of genomic sample material and associated specimen
    information in a consistent way.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:93
  url: https://www.tdwg.org/standards/ggbn/
- id: B2AI_STANDARD:143
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:20
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: GlycoCT encoding scheme
  formal_specification: https://github.com/glycoinfo/GlycoCT
  is_open: true
  name: GlycoCT
  publication: doi:10.1016/j.carres.2008.03.011
  purpose_detail: '"GlycoCT (Glyco Connection Table) format is an advanced, comprehensive
    encoding scheme specifically designed to describe complex carbohydrate sequences
    with unprecedented precision and consistency in glycobioinformatics. Developed
    as version 4 (KIROI) by Stephan Herget and Rene Ranzinger in 2007, this format
    employs a controlled vocabulary based on IUPAC nomenclature rules to systematically
    name monosaccharides and utilizes a connection table approach rather than linear
    encoding, enabling accurate representation of branched and complex glycan structures.
    GlycoCT incorporates a sophisticated block concept to efficiently handle frequently
    occurring structural features such as repeating units, which are common in biological
    carbohydrates. The format exists in two complementary variants: a condensed form
    optimized for database applications with strict sorting rules ensuring uniqueness
    for use as primary keys, and a more verbose XML syntax that provides enhanced
    readability and machine processing capabilities. Released under Creative Commons
    Attribution 4.0 International License, GlycoCT represents a significant advancement
    toward achieving a unified and broadly accepted sequence format in glycobioinformatics,
    encompassing the capabilities of the heterogeneous landscape of existing digital
    encoding schemata while providing the foundation for developing ontological relationships
    between glycan structural terms and supporting automated glycan structure analysis."'
  requires_registration: false
  url: https://github.com/glycoinfo/GlycoCT
- id: B2AI_STANDARD:144
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: GTrack genomic data format
  formal_specification: https://github.com/gtrack/gtrack
  is_open: true
  name: GTrack
  publication: doi:10.1186/1471-2105-12-494
  purpose_detail: GTrack is a tabular format that was developed as part of the Genomic
    HyperBrowser system to provide a uniform representation of most types of genomic
    datasets. GTrack is able to replace common formats such as WIG, GFF, BED, FASTA,
    in addition to represent chromatin capture datasets, such as Hi-C and ChIA-PET.
  requires_registration: false
  url: https://github.com/gtrack/gtrack
- id: B2AI_STANDARD:145
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:20
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: GUID and Life Sciences Identifiers Applicability Statements
  formal_specification: https://github.com/tdwg/guid-as
  is_open: true
  name: GUID-AS
  purpose_detail: GUIDs are Globally Unique Identifiers which should be referentially
    consistent and resolvable in order to support tests of uniqueness and the acquisition
    of associated metadata. Further, permanent and robust resolution services need
    to be available. The TDWG Globally Unique Identifiers Task Group (TDWG GUID),
    after meeting twice in 2006, recommended the use of the Life Sciences Identifiers
    (LSID) to uniquely identify shared data objects in the biodiversity domain.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:93
  url: http://www.tdwg.org/standards/150
- id: B2AI_STANDARD:146
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Guidelines for Information About Therapy Experiments
  is_open: true
  name: GIATE
  publication: doi:10.1186/1756-0500-5-10
  purpose_detail: The Guidelines for Information About Therapy Experiments (GIATE)
    is a minimum information checklist developed to establish a consistent framework
    for transparently reporting the purpose, methods, and results of therapeutic experiments,
    particularly in preclinical and translational research contexts. GIATE provides
    structured reporting requirements covering experimental design, intervention details
    (therapeutic agents, dosing, administration routes, timing), subject characteristics,
    outcome measures, statistical methods, and results presentation. The guidelines
    aim to improve reproducibility, enable meta-analyses, facilitate comparison across
    studies, and enhance the quality of preclinical therapeutic research by ensuring
    complete documentation of experimental parameters that affect interpretation and
    translatability. GIATE addresses the need for standardized reporting of therapy
    experiments to support evidence synthesis, reduce publication bias, and improve
    the rigor and transparency of preclinical studies that inform clinical trial design
    and therapeutic development pipelines.
  requires_registration: false
  url: https://doi.org/10.1186/1756-0500-5-10
- id: B2AI_STANDARD:147
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - policy
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Health Insurance Portability and Accountability Act of 1996
  formal_specification: https://aspe.hhs.gov/reports/health-insurance-portability-accountability-act-1996
  is_open: true
  name: HIPAA
  purpose_detail: The Health Insurance Portability and Accountability Act of 1996
    (HIPAA) is a federal law that required the creation of national standards to protect
    sensitive patient health information from being disclosed without the patient's
    consent or knowledge. The US Department of Health and Human Services (HHS) issued
    the HIPAA Privacy Rule to implement the requirements of HIPAA. The HIPAA Security
    Rule protects a subset of information covered by the Privacy Rule.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:39
  url: https://www.hhs.gov/hipaa/index.html
  used_in_bridge2ai: true
- id: B2AI_STANDARD:148
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - codesystem
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Healthcare Common Procedure Coding System
  is_open: true
  name: HCPCS
  purpose_detail: The Healthcare Common Procedure Coding System (HCPCS) is a standardized
    code set maintained by the Centers for Medicare & Medicaid Services (CMS) for
    reporting medical procedures, products, supplies, and services to Medicare, Medicaid,
    and private health insurers for claims processing and reimbursement. HCPCS consists
    of two levels - Level I comprises CPT (Current Procedural Terminology) codes maintained
    by the American Medical Association for physician services and procedures, while
    Level II contains alphanumeric codes (A through V series) for items and services
    not covered by CPT including durable medical equipment (DME), prosthetics, orthotics,
    supplies, ambulance services, drugs administered via methods other than oral, and
    temporary procedures. Each HCPCS code is accompanied by modifiers that provide
    additional information about the service or item (laterality, level of service,
    unusual circumstances). The code set is updated quarterly to reflect new technologies,
    procedures, and supplies, and includes detailed descriptions, Medicare coverage
    policies, and pricing information. HCPCS enables standardized billing, facilitates
    claims adjudication, supports healthcare cost analysis and utilization research,
    and provides the foundation for healthcare payment systems and revenue cycle management
    across payers.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:17
  url: https://www.cms.gov/Medicare/Coding/HCPCSReleaseCodeSets
  used_in_bridge2ai: true
- id: B2AI_STANDARD:149
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Herbarium Information Standards and Protocols for Interchange of Data
  is_open: true
  name: HISPID3
  purpose_detail: The 'Herbarium Information Standards and Protocols for Interchange
    of Data' (HISPID) is a standard format for the interchange of electronic herbarium
    specimen information. HISPID has been developed by a committee of representatives
    from all major Australian herbaria. This interchange standard was first published
    in 1989, with a revised version published in 1993./nHISPID3 (version 3) is an
    accession-based interchange standard. Although many fields refer to attributes
    of the taxon they should be construed as applying to the specimen represented
    by the record, not to the taxon per se. The interchange of taxonomic, nomenclatural,
    bibliographic, typification, rare and endangered plant conservation, and other
    related information is not dealt with in this standard, unless it specifically
    refers to a particular accession (record). While this standard is still in use,
    it is no longer actively maintained (labelled as prior on the TDWG website).
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:93
  url: https://www.tdwg.org/standards/hispid3/
- id: B2AI_STANDARD:150
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Histoimmunogenetics Markup Language
  formal_specification: https://github.com/nmdp-bioinformatics/hml
  is_open: true
  name: HML
  purpose_detail: Histoimmunogenetics Markup Language (HML) is intended as a potentially
    general-purpose XML format for exchanging genetic typing data. This format supports
    NGS based genotyping methods, raw sequence reads, registered methodologies, reference
    data, complete reporting of allele and genotype ambiguity and MIRING compliant
    reporting.
  requires_registration: false
  url: https://bioinformatics.bethematchclinical.org/hla-resources/hml/
- id: B2AI_STANDARD:151
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HL7 Arden Syntax
  formal_specification: https://www.hl7.org/login/index.cfm?next=/implement/standards/product_brief.cfm?product_id=2
  is_open: true
  name: Arden
  publication: doi:10.1016/j.jbi.2012.02.001
  purpose_detail: The Arden Syntax for Medical Logic Systems Version 2.10 is the latest
    version of a formalism for clinical knowledge representation that can be used
    by clinicians, knowledge engineers, administrators and others to implement clinical
    decision support (CDS) solutions to help improve the quality and safety of care.
    Arden Syntax can be used to create a knowledge base for CDS systems that, when
    coupled with patient data, can generate patient-specific CDS interventions for
    improving patient care. The key change in Version 2.10 over Version 2.9 is inclusion
    of a normative XML representation for Arden Syntax. This was done because the
    use of XML facilitates the development of tools such as syntax checkers and editors
    that can help increase the correctness of executable knowledge modules, and this
    in turn will foster the augmentation of the development and production environments
    for Arden, thereby increasing its utility.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:40
  url: http://www.hl7.org/implement/standards/product_brief.cfm?product_id=372
- id: B2AI_STANDARD:152
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HL7 Clinical Document Architecture Implementation Guide - Data Provenance
  is_open: true
  name: HL7 CDA Data Provenance
  purpose_detail: The HL7 Clinical Document Architecture (CDA) Data Provenance Implementation
    Guide provides standardized guidance and templates for capturing and exchanging
    comprehensive provenance information about clinical and care-related information
    within CDA documents. The guide addresses the fundamental provenance questions
    - who created the information (author, organization, device), when it was created
    (timestamps, versioning), where it was created (facility, location), how it was
    created (method, procedure, source system), and why it was created (purpose, context).
    The implementation guide defines structured elements and coded values for documenting
    data transformations, aggregations, derivations, and chains of custody that are
    critical for clinical decision-making, regulatory compliance, and legal accountability.
    Provenance metadata supports trust and transparency in health information exchange,
    enables auditability and traceability of clinical data across systems and care
    settings, facilitates detection of data quality issues, and provides essential
    context for interpreting clinical information in research, quality improvement,
    and AI/ML applications where understanding data lineage and reliability is paramount.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:40
  url: http://www.hl7.org/implement/standards/product_brief.cfm?product_id=420
- id: B2AI_STANDARD:153
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HL7 FHIR Resource DiagnosticReport
  formal_specification: http://hl7.org/fhir/diagnosticreport-definitions.html
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: DiagnosticReport
  purpose_detail: Resource for findings and interpretation of diagnostic tests performed
    on patients, groups of patients, devices, and locations, and/or specimens derived
    from these. The report includes clinical context such as requesting and provider
    information, and some mix of atomic results, images, textual and coded interpretations,
    and formatted representation of diagnostic reports.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: http://hl7.org/fhir/diagnosticreport.html
- id: B2AI_STANDARD:154
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:9
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HL7 FHIR Resource GenomicStudy
  formal_specification: https://build.fhir.org/genomicstudy-definitions.html
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: GenomicStudy
  purpose_detail: GenomicStudy resource aims at delineating relevant information of
    a genomic study. A genomic study might comprise one or more analyses, each serving
    a specific purpose. These analyses may vary in method (e.g., karyotyping, CNV,
    or SNV detection), performer, software, devices used, or regions targeted.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: https://build.fhir.org/genomicstudy.html
- id: B2AI_STANDARD:155
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  - B2AI_TOPIC:20
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HL7 FHIR Resource MolecularSequence
  formal_specification: http://hl7.org/implement/standards/fhir/molecularsequence-definitions.html
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: MolecularSequence
  purpose_detail: The FHIR MolecularSequence resource provides a standardized structure
    for representing raw and processed biological sequence data including DNA, RNA,
    and amino acid sequences within the FHIR ecosystem, enabling integration of genomic
    information with clinical data. The resource captures observed sequences, reference
    sequences, sequence variations (SNPs, indels, structural variants), quality scores,
    read coverage information, and repository links to external sequence databases
    (GenBank, EMBL, RefSeq). MolecularSequence supports representation of sequence
    coordinates using zero-based or one-based numbering systems, strand orientation,
    and relationships between sequences (such as transcript-to-protein mappings). The
    resource can reference associated specimens, patients, or other subjects, and links
    to variant calling results, expression data, and functional annotations. MolecularSequence
    enables clinical genomics workflows including variant interpretation, pharmacogenomics
    decision support, molecular diagnostics reporting, and precision medicine applications
    by providing a FHIR-native way to exchange sequence data alongside phenotypic,
    diagnostic, and treatment information in electronic health records and clinical
    information systems.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: http://hl7.org/implement/standards/fhir/molecularsequence.html
- id: B2AI_STANDARD:156
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HL7 FHIR Resource Observation
  formal_specification: http://hl7.org/implement/standards/fhir/observation-definitions.html
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: Observation
  purpose_detail: The FHIR Observation resource is a central, versatile structure
    for representing measurements, assessments, and assertions made about a patient,
    device, location, or other subject in healthcare contexts. Observations encompass
    a wide range of clinical and research data including vital signs (blood pressure,
    heart rate, temperature), laboratory results (chemistry, hematology, microbiology),
    imaging measurements, clinical assessments (pain scores, functional status), device
    readings (glucose monitors, pulse oximeters), social determinants of health, genomic
    findings, and quality metrics. The resource uses coded values from standard terminologies
    (LOINC, SNOMED CT) to identify what was measured, supports numeric values with
    units (UCUM), coded results, textual findings, and multimedia attachments. Observation
    includes metadata for effective time, status, performer, method, specimen reference,
    reference ranges, and interpretation flags. The resource supports panels and components
    for organizing related observations (complete blood count, metabolic panel), enables
    longitudinal tracking through sequences and trends, and links to supporting evidence
    and derivations. Observations are fundamental to AI/ML applications requiring structured
    clinical phenotypes, time-series analytics, predictive modeling, and integration
    of diverse data types for clinical decision support and population health analysis.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: http://hl7.org/implement/standards/fhir/observation.html
- id: B2AI_STANDARD:157
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:6
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HL7 FHIR Resource Patient
  formal_specification: http://hl7.org/implement/standards/fhir/patient-definitions.html
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: Patient
  purpose_detail: The FHIR Patient resource provides a standardized structure for
    representing demographics and administrative information about individuals or animals
    receiving care or health-related services. The resource captures core demographic
    data including name (with multiple name types and use contexts), birth date, gender,
    address (with structured components and period of validity), telecom contact information
    (phone, email, with use and priority indicators), and multiple identifiers (medical
    record numbers, social security numbers, insurance IDs) with system and type codes.
    Patient supports marital status, multiple languages with proficiency levels, photo
    attachments, contact persons and their relationships, communication preferences,
    general practitioners, managing organization, and links to related patients (merged
    records, see-also relationships). The resource includes deceased indicator, multiple
    birth information, and animal-specific extensions for species and breed. Patient
    serves as a central hub linking to all other clinical resources (observations,
    conditions, procedures, medications) and enables patient matching, record linkage,
    demographic analytics, cohort identification, and population health management.
    The standardized demographics are essential for AI/ML applications requiring patient
    stratification, bias detection, social determinants analysis, and fair representation
    across diverse populations in clinical prediction models and decision support systems.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: http://hl7.org/implement/standards/fhir/patient.html
- id: B2AI_STANDARD:158
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HL7 FHIR Resource Specimen
  formal_specification: http://hl7.org/fhir/specimen-definitions.html
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: Specimen
  purpose_detail: The FHIR Specimen resource provides a standardized structure for
    representing any material sample taken from a biological entity (living or dead)
    or from a physical object or the environment, essential for laboratory testing,
    biobanking, and clinical diagnostics. The resource captures comprehensive specimen
    metadata including type (blood, tissue, urine, etc.), subject/patient association,
    collection details (method, body site, quantity, date/time), identifiers (accession
    number, container ID), processing and handling procedures (centrifugation, fixation,
    storage), parent-child specimen relationships for aliquots and derivatives, storage
    conditions (temperature, humidity), and current status (available, unavailable,
    unsatisfactory, entered-in-error). The resource supports complex laboratory workflows
    by tracking specimen provenance through collection, transport, processing, and
    analysis stages, with fields for collection procedure, additive substances, container
    types, and special handling requirements. Specimen integrates with other FHIR
    resources including Patient, Practitioner, ServiceRequest, DiagnosticReport, and
    Observation to create complete clinical laboratory information workflows. The
    resource enables biobank specimen catalogs, clinical trial sample tracking, public
    health surveillance specimen management, and research biorepository operations.
    FHIR Specimen supports interoperability between laboratory information systems
    (LIS), electronic health records (EHR), biobank management systems, and research
    databases, facilitating standardized specimen data exchange across healthcare
    and research ecosystems while maintaining compliance with regulations for specimen
    handling, consent, and data privacy.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: http://hl7.org/fhir/specimen.html
- id: B2AI_STANDARD:159
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:6
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HL7 Gender Harmony Project
  is_open: true
  name: GHP
  publication: doi:10.1093/jamia/ocab196
  purpose_detail: Currently, it is common that a single data element is used to capture
    both sex and gender information, often assuming these two items are one unified
    idea. This specification challenges that notion and proposes that independent
    consideration of sex and gender, and the assessment of their differences promotes
    the health of women, men, and people of diverse gender identities of all ages,
    avoiding systematic errors that generate results with a low validity (if any)
    in clinical studies. The Gender Harmony model describes an approach that can improve
    data accuracy for sex and gender information in health care systems.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:40
  url: http://www.hl7.org/implement/standards/product_brief.cfm?product_id=564
- id: B2AI_STANDARD:160
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HUPO-PSI TraML format
  is_open: true
  name: TraML
  purpose_detail: The HUPO PSI Mass Spectrometry Standards Working Group (MSS WG)
    has developed a specification for a standardized format for the exchange and transmission
    of transition lists for selected reaction monitoring (SRM) experiments.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:41
  url: https://www.psidev.info/traml
- id: B2AI_STANDARD:161
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:18
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: IEEE 360-2022 IEEE Standard for Wearable Consumer Electronic Devices
  is_open: false
  name: IEEE P360
  purpose_detail: An overview, terminology, and categorization for Wearable Consumer
    Electronic Devices (Wearables). It further outlines an architecture for a series
    of standard specifications that define technical requirements and testing methods
    for different aspects of Wearables, from basic security and suitableness of wearing
    to various functional areas like health, fitness, and infotainment, etc.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:44
  url: https://standards.ieee.org/ieee/360/6244/
- id: B2AI_STANDARD:162
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: IHE Basic Patient Privacy Consents
  formal_specification: https://profiles.ihe.net/ITI/TF/Volume1/ch-19.html
  is_open: true
  name: BPPC
  purpose_detail: A mechanism to record the patient privacy consent(s) and a method
    for Content Consumers to use to enforce the privacy consent appropriate to the
    use.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:46
  url: https://profiles.ihe.net/ITI/TF/Volume1/ch-19.html
- id: B2AI_STANDARD:163
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: IHE Clinical Decision Support Order Appropriateness Tracking
  is_open: true
  name: IHE CDS-OAT
  purpose_detail: Profile for Clinical Decision Support and Appropriate Use Criteria
    (AUC) information as received from the CDS Mechanism 145 (CDSM) on the order and
    charge transaction to the revenue cycle application that is responsible to create
    a claim.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:46
  url: https://www.ihe.net/uploadedFiles/Documents/Radiology/IHE_Rad_Suppl_CDS-OAT.pdf
- id: B2AI_STANDARD:164
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: IHE Clinical Research Document standard
  formal_specification: https://www.ihe.net/uploadedFiles/Documents/QRPH/IHE_QRPH_Suppl_CRD.pdf
  is_open: true
  name: CRD
  purpose_detail: The Clinical Research Document Profile (CRD) specifies a standard
    way to generate a clinical research document from EHR data provided in the CDA
    standard.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:46
  url: https://www.ihe.net/uploadedFiles/Documents/QRPH/IHE_QRPH_Suppl_CRD.pdf
- id: B2AI_STANDARD:165
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: IHE Cross-Community Patient Discovery Profile
  is_open: true
  name: IHE XCPD
  purpose_detail: Standardized means to locate communities that hold patient relevant
    health data and the translation of patient identifiers across communities holding
    the same patients data.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:46
  url: https://profiles.ihe.net/ITI/TF/Volume1/ch-27.html
- id: B2AI_STANDARD:166
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: IHE IT Infrastructure Technical Framework
  formal_specification: https://profiles.ihe.net/ITI/TF/index.html
  is_open: true
  name: IHE ITI
  purpose_detail: Technical profiles and definitions for IT use cases, transactions,
    content, and metadata.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:46
  url: https://profiles.ihe.net/ITI/index.html
- id: B2AI_STANDARD:167
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: IHE Laboratory Analytical Workflow
  formal_specification: https://www.ihe.net/uploadedFiles/Documents/PaLM/IHE_PaLM_TF_Vol1.pdf
  is_open: true
  name: LAW
  purpose_detail: Profile to support the analytical workflow between analyzers of
    the clinical laboratory and the IT systems managing their work
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:46
  url: https://www.ihe.net/resources/technical_frameworks/#PaLM
- id: B2AI_STANDARD:168
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: IHE Patient Care Device Profiles
  is_open: true
  name: IHE PCD
  purpose_detail: IHE Patient Care Device (PCD) Profiles are standards-based interoperability
    specifications developed by Integrating the Healthcare Enterprise (IHE) to address
    medical device communication challenges in clinical settings. The profiles follow
    IHE's technical framework development process including proposal, supplement development,
    public comment, trial implementation at Connectathon events, and finalization.
    Key PCD profiles include ACM (Alert Communication Management) for standardized
    alarm handling, DEC (Device Enterprise Communication) for device-to-enterprise
    integration, IDCO (Implantable Device Cardiac Observations) for cardiac device
    data, IPEC (Infusion Pump Event Communication) for infusion pump safety, PIV (Point
    of Care Infusion Verification) for medication verification, PCIM (Point of Care
    Identity Management) for device authentication, RDQ (Retrospective Data Query)
    for historical data retrieval, RTM (Rosetta Terminology Mapping) for standardized
    device terminology, and WCM (Waveform Communication Module) for physiological
    waveform data. These profiles leverage existing standards like HL7, DICOM, and
    IEEE 11073 to enable vendor-neutral device interoperability, supporting critical
    use cases such as automated vital signs documentation in electronic health records,
    integrated alarm management across monitoring systems, and safe medication administration
    workflows.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:46
  url: https://wiki.ihe.net/index.php?title=PCD_Profiles
- id: B2AI_STANDARD:169
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: IHE Patient Demographics Query Integration Profile
  is_open: true
  name: IHE PDQ
  purpose_detail: Standardized ways for multiple distributed applications to query
    a patient information server for a list of patients, based on user-defined search
    criteria, and retrieve a patients demographic (and, optionally, visit or visit-related)
    information directly into the application.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:46
  url: https://profiles.ihe.net/ITI/TF/Volume1/ch-8.html
- id: B2AI_STANDARD:170
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: IHE Retrieve Form for Data Capture
  is_open: true
  name: RFD
  purpose_detail: The RFD Profile provides a generic polling mechanism to allow an
    external agency to indicate issues with data that have been captured and enable
    the healthcare provider to correct the data. The profile does not dictate the
    mechanism employed or content required to achieve such corrections.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:46
  url: https://profiles.ihe.net/ITI/TF/Volume1/ch-17.html
- id: B2AI_STANDARD:171
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: IICC Digital Format for Publication of LOINC to Vendor IVD Test Results
  has_relevant_organization:
  - B2AI_ORG:53
  has_training_resource:
  - B2AI_STANDARD:848
  is_open: true
  name: LIVD
  purpose_detail: Industry-defined format to facilitate the publication and exchange
    of LOINC codes for vendor IVD test results.
  requires_registration: false
  url: https://ivdconnectivity.org/livd/
- id: B2AI_STANDARD:172
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:19
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Image Cytometry Standard
  is_open: true
  name: ICS
  publication: doi:10.1002/cyto.990110502
  purpose_detail: The Image Cytometry Standard (ICS) is a digital multidimensional
    image file format used in life sciences microscopy. It stores not only the image
    data, but also the microscopic parameters describing the optics during the acquisition.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/Image_Cytometry_Standard
- id: B2AI_STANDARD:173
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:15
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: imzML format
  has_relevant_organization:
  - B2AI_ORG:20
  is_open: true
  name: imzML
  publication: doi:10.1016/j.jprot.2012.07.026
  purpose_detail: The purpose of imzML is to facilitate the exchange and processing
    of mass spectrometry imaging data. This website is intended to provide all information
    neccesary to implement imzML.imzML was developed in the framework of the EU funded
    project COMPUTIS. The main goals during the development were complete description
    of MS imaging experiments and efficient storage of (very large) data sets. imzML
    is it not limited to MS imaging, but is also useful for other MS applications
    generating large data sets such as LC-FTMS. The current version is mzML 1.1.0.
    The metadata part of imzML is based on the mzML format by HUPO-PSI
  requires_registration: false
  url: https://ms-imaging.org/imzml/
- id: B2AI_STANDARD:174
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - codesystem
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: International Classification of Diseases 10th Revision Clinical Modification
  is_open: true
  name: ICD-10-CM
  purpose_detail: "ICD-10-CM (International Classification of Diseases, 10th Revision,
    Clinical Modification) is the United States' clinical modification of the WHO's
    ICD-10 classification system, mandated for reporting diagnoses and health conditions
    in all healthcare settings for billing, epidemiology, quality measurement, and
    public health surveillance since October 2015. Maintained by the National Center
    for Health Statistics (NCHS) in collaboration with CMS, ICD-10-CM provides over
    70,000 diagnosis codes with significantly expanded granularity compared to ICD-9-CM,
    enabling more precise documentation of disease severity, anatomical location,
    episode of care (initial encounter, subsequent encounter, sequela), and laterality
    (left, right, bilateral). The coding structure uses alphanumeric format with 3-7
    characters where the first character is alphabetic, second is numeric, third through
    seventh provide increasing specificity of body site, etiology, severity, and clinical
    details, with optional seventh characters as extensions for encounter context
    or injury staging. Major chapters include infectious diseases (A00-B99), neoplasms
    (C00-D49), endocrine and metabolic disorders (E00-E89), mental and behavioral
    disorders (F01-F99), nervous system (G00-G99), circulatory (I00-I99), respiratory
    (J00-J99), digestive (K00-K95), musculoskeletal (M00-M99), and external causes
    of morbidity (V00-Y99). Critical features include combination codes that capture
    multiple conditions or manifestations in single codes, placeholder 'x' characters
    to allow for future expansion, and seventh character extensions for obstetric
    outcomes, fracture healing status, and diabetes complications. ICD-10-CM drives
    healthcare reimbursement through diagnosis-related groups (DRGs), enables clinical
    decision support systems through structured diagnosis data, supports population
    health analytics identifying disease trends and risk factors, powers epidemiological
    surveillance for CDC reportable conditions tracking disease outbreaks, and provides
    standardized terminology for electronic health record (EHR) systems ensuring interoperability.
    Annual updates released October 1st incorporate new diseases, refined definitions,
    and code revisions reflecting medical advances, essential for medical billing
    specialists, clinical coders, health informaticians, public health researchers,
    and healthcare quality officers."
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:14
  url: https://www.cdc.gov/nchs/icd/icd-10-cm.htm
  used_in_bridge2ai: true
- id: B2AI_STANDARD:175
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - codesystem
  - standards_process_maturity_final
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: International Classification of Diseases 11th Revision
  is_open: true
  name: ICD-11
  purpose_detail: "ICD-11 (International Classification of Diseases, 11th Revision)
    is the World Health Organization's latest global standard for diagnostic health
    information, officially adopted by the World Health Assembly in 2019 and effective
    from January 2022, representing the most comprehensive revision in over three
    decades with fully digital-native architecture designed for electronic health
    systems and multilingual implementation. Developed through extensive international
    collaboration involving thousands of health professionals across 90 countries,
    ICD-11 contains over 17,000 diagnostic categories with significantly enhanced
    granularity, clinical detail, and scientific accuracy compared to ICD-10, incorporating
    advances in medical knowledge including genomic medicine, patient safety, traditional
    medicine integration, and sexual health classifications. The classification uses
    a multi-hierarchical structure with foundation component enabling flexible content
    management, allowing multiple parent categories and post-coordination where complex
    conditions are composed by combining codes for anatomy, etiology, severity, temporality,
    and other clinical dimensions. Major structural improvements include entirely
    new chapters for traditional medicine (validated practices from Chinese, Ayurvedic,
    and other systems), extension codes for functional assessment using WHO Disability
    Assessment Schedule 2.0 (WHODAS), detailed antimicrobial resistance documentation,
    gaming disorder and other emerging conditions, and significantly expanded mental
    health classifications with dimensional assessments. ICD-11 features built-in
    multilingual support covering Arabic, Chinese, English, French, Russian, Spanish
    with machine translation capabilities, linearizations tailored for different use
    cases (mortality reporting, morbidity statistics, primary care, clinical documentation),
    and modern URI-based coding enabling semantic web integration and FHIR compatibility.
    Critical technical features include embedded logical definitions using description
    logic enabling automated classification and consistency checking, standardized
    application programming interfaces (APIs) for EHR integration, and continuous
    online updating mechanism replacing decennial revision cycles with regular incremental
    updates. ICD-11 supports global health surveillance enabling real-time disease
    outbreak tracking, international health statistics comparability across countries
    and regions, research data standardization for epidemiological studies and clinical
    trials, health service planning through accurate disease burden measurement, and
    AI/machine learning applications through structured, semantically rich diagnostic
    data. Adoption varies globally with some countries implementing immediately while
    others transition gradually, essential for medical informaticians, public health
    authorities, clinical coders, WHO collaborating centers, and healthcare system
    planners."
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:100
  url: https://icd.who.int/en
- id: B2AI_STANDARD:176
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - codesystem
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: International Classification of Diseases 9th Revision Clinical Modification
  is_open: true
  name: ICD-9-CM
  purpose_detail: "ICD-9-CM (International Classification of Diseases, 9th Revision,
    Clinical Modification) was the official medical classification system used in
    the United States from 1979 until September 2015 for coding diagnoses and procedures
    in healthcare billing, epidemiological research, and health statistics, representing
    the U.S. clinical modification of WHO's ICD-9 with additional detail for morbidity
    classification. Developed by the National Center for Health Statistics (NCHS)
    and used by CMS for Medicare/Medicaid reimbursement, ICD-9-CM contained approximately
    13,000 diagnosis codes and 3,000 procedure codes, structured with 3-5 digit numeric
    format where first three digits represent disease category and subsequent digits
    add specificity for anatomical location, etiology, or manifestations. Major diagnostic
    chapters included infectious and parasitic diseases (001-139), neoplasms (140-239),
    endocrine and metabolic diseases (240-279), mental disorders (290-319), nervous
    system (320-389), circulatory system (390-459), respiratory system (460-519),
    digestive system (520-579), genitourinary system (580-629), and external causes
    (E codes E800-E999) documenting injury circumstances. The procedure classification
    (Volume 3) used 2-4 digit numeric codes organized by anatomical system, covering
    surgical operations, diagnostic procedures, and therapeutic interventions primarily
    for inpatient hospital settings. Despite decades of widespread adoption creating
    massive legacy datasets and establishing institutional coding workflows, ICD-9-CM
    became outdated due to limited specificity insufficient for modern medicine's
    diagnostic precision, exhausted code capacity with no room for new diseases or
    procedures, lack of laterality designation, and outdated medical terminology inconsistent
    with current clinical practice. The mandated transition to ICD-10-CM/PCS in October
    2015 required extensive healthcare industry preparation including EHR system updates,
    coder retraining, and billing system modifications, yet ICD-9-CM remains critically
    important for historical medical records analysis, longitudinal epidemiological
    studies spanning pre-2015 data, retrospective cohort studies, trend analysis requiring
    crosswalk mappings between ICD-9 and ICD-10, and legacy system maintenance. Applications
    include historical disease surveillance tracking public health trends, health
    services research analyzing treatment patterns, medical informatics developing
    code translation algorithms, and archival medical data management, essential for
    health informaticians managing legacy data, epidemiologists conducting temporal
    studies, medical archivists, and researchers working with pre-2015 healthcare
    datasets."
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:14
  url: https://www.cdc.gov/nchs/icd/icd9cm.htm
  used_in_bridge2ai: true
- id: B2AI_STANDARD:177
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: INTRPRT guidelines for transparent machine learning for medical image
    analysis
  is_open: true
  name: INTRPRT
  publication: doi:10.1038/s41746-022-00699-2
  purpose_detail: A design directive for transparent ML systems in medical image analysis.
    The INTRPRT guideline suggests human-centered design principles, recommending
    formative user research as the first step to understand user needs and domain
    requirements.
  requires_registration: false
  url: https://doi.org/10.1038/s41746-022-00699-2
- id: B2AI_STANDARD:178
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: IPSM Alignment Format
  formal_specification: https://github.com/INTER-IoT/ipsm-alignments
  is_open: true
  name: IPSM-AF
  purpose_detail: Alignment can be interpreted as a set of uni-directional mappings
    for transforming input RDF graph into output RDF graph. It is persisted in IPSM
    Alignment Format (IPSM-AF) that is based on a well known Alignment API Format.
  requires_registration: false
  url: https://inter-iot.readthedocs.io/projects/ipsm/en/latest/Configuration/Alignment-format/IPSM-alignment-format/
- id: B2AI_STANDARD:179
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ISA-Tab-Nano format
  has_relevant_organization:
  - B2AI_ORG:47
  is_open: true
  name: ISA-TAB-Nano
  publication: doi:10.1186/1472-6750-13-2
  purpose_detail: ISA-TAB-Nano specifies the format for representing and sharing information
    about nanomaterials, small molecules and biological specimens along with their
    assay characterization data (including metadata, and summary data) using spreadsheet
    or TAB-delimited files.
  requires_registration: false
  url: https://wiki.nci.nih.gov/display/icr/isa-tab-nano
- id: B2AI_STANDARD:180
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ISO 13606 standard for Electronic Health Record Communication
  formal_specification: https://www.iso.org/standard/67868.html
  has_relevant_organization:
  - B2AI_ORG:49
  is_open: false
  name: EN13606
  purpose_detail: A means for communicating part or all of the electronic health record
    (EHR) of one or more identified subjects of care between EHR systems, or between
    EHR systems and a centralised EHR data repository. It can also be used for EHR
    communication between an EHR system or repository and clinical applications or
    middleware components (such as decision support components), or personal health
    applications and devices, that need to access or provide EHR data, or as the representation
    of EHR data within a distributed (federated) record system.
  requires_registration: true
  url: http://www.en13606.org/
- id: B2AI_STANDARD:181
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ITU H.810, H.811, H.812, H.812.5, and H.813
  is_open: true
  name: ITU-T E-health
  purpose_detail: Standards for medical device communication.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:50
  url: https://www.itu.int/en/ITU-T/studygroups/2013-2016/16/Pages/rm/ehealth.aspx
- id: B2AI_STANDARD:182
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:3
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Joint Committee on Atomic and Molecular Physical Data standard
  is_open: true
  name: JCAMP-DX
  purpose_detail: The JCAMP-DX was one of the earliest specifications providing a
    standard file format for data exchange in mass spectrometry. It was initially
    developed for infrared spectrometry and related chemical and physical information
    between spectrometer data systems of different manufacture. It was also used later
    for nuclear magnetic resonance spectroscopy. JCAMP-DX is an ASCII based format
    and therefore not very compact even though it includes standards for file compression.
    All data are stored as labeled fields of variable length using printable ASCII
    characters. JCAMP-DX was officially released in 1988. JCAMP-DX was found impractical
    for today's large MS data sets, but it is still used for exchanging moderate numbers
    of spectra. IUPAC is currently in charge of its maintenance and the latest protocol
    is from 2005.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:51
  url: https://iupac.org/what-we-do/digital-standards/jcamp-dx/
- id: B2AI_STANDARD:183
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: K-mer countgraph
  is_open: true
  name: Countgraph
  publication: doi:10.12688/f1000research.6924.1
  purpose_detail: A format used by the khmer tool to represent k-mer counts and their
    occurences.
  related_to:
  - B2AI_STANDARD:782
  requires_registration: false
  url: https://khmer.readthedocs.io/en/v2.0/dev/binary-file-formats.html#countgraph
- id: B2AI_STANDARD:184
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - diagnosticinstrument
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Karnofsky Performance Scale
  is_open: true
  name: KPS
  publication: doi:10.1200/JCO.1984.2.3.187
  purpose_detail: A standard way of measuring the ability of cancer patients to perform
    ordinary tasks. The Karnofsky Performance Status scores range from 0 to 100. A
    higher score means the patient is better able to carry out daily activities. Karnofsky
    Performance Status may be used to determine a patient's prognosis, to measure
    changes in a patients ability to function, or to decide if a patient could be
    included in a clinical trial. Also called KPS.
  requires_registration: false
  url: http://www.npcrc.org/files/news/karnofsky_performance_scale.pdf
- id: B2AI_STANDARD:185
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: KEGG Markup Language
  is_open: true
  name: KGML
  purpose_detail: The KEGG Markup Language (KGML) is an exchange format of the KEGG
    pathway maps, which is converted from internally used KGML+ (KGML+SVG) format.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:52
  url: https://www.kegg.jp/kegg/xml/
- id: B2AI_STANDARD:186
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - deprecated
  concerns_data_topic:
  - B2AI_TOPIC:20
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Life sciences domain analysis model
  is_open: true
  name: LS-DAM
  publication: doi:10.1136/amiajnl-2011-000763
  purpose_detail: The LS DAM v2.2.1 is comprised of 130 classes and covers several
    core areas including Experiment, Molecular Biology, Molecular Databases and Specimen.
    Nearly half of these classes originate from the BRIDG model, emphasizing the semantic
    harmonization between these models. Validation of the LS DAM against independently
    derived information models, research scenarios and reference databases supports
    its general applicability to represent life sciences research.
  requires_registration: false
  url: https://doi.org/10.1136/amiajnl-2011-000763
- id: B2AI_STANDARD:187
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - codesystem
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Logical Observation Identifier Names and Codes
  has_relevant_organization:
  - B2AI_ORG:114
  - B2AI_ORG:115
  has_training_resource:
  - B2AI_STANDARD:848
  is_open: true
  name: LOINC
  purpose_detail: Tests, observations, diagnostics, and other clinical procedures.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:53
  url: loinc.org
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:37
    category: B2AI:Application
    name: Automated ML-Based Mapping of Free-Text Laboratory Codes to LOINC
    description: Machine learning classifiers including Random Forest, logistic regression,
      and ensemble methods automatically map free-text laboratory descriptors (test
      names, units, panels) from local EHR systems to standardized LOINC codes. Novel
      encoding methods vectorize heterogeneous lab terminology, and trained models
      achieve high accuracy (Random Forest approximately 94.5% top-1 accuracy; ensemble
      approaches up to 99% accuracy) on nationwide oncology EHR data from approximately
      280 US clinics. This automation dramatically reduces manual terminologist effort
      (which typically requires 6-8 hours per 1000 terms) and enables research-ready
      datasets by harmonizing laboratory data across institutions for downstream ML
      applications.
    used_in_bridge2ai: false
    references:
    - https://pmc.ncbi.nlm.nih.gov/articles/PMC8861721/
  - id: B2AI_APP:136
    category: B2AI:Application
    name: LLM-Based LOINC Standardization Using Pre-Trained T5 Embeddings
    description: Pre-trained T5 large language models with contrastive learning and
      few-shot fine-tuning retrieve and suggest top-k LOINC code candidates from local
      laboratory descriptions, supporting human-in-the-loop curation workflows. The
      approach uses contextual embeddings and data augmentation to handle acronyms,
      synonyms, misspellings, and missing metadata across LOINC's six dimensions (component,
      property, time, system, scale, method), generalizing to unseen LOINC targets
      without retraining. Training and evaluation use source-target pairs from MIMIC-III
      EHR data, demonstrating improved top-k retrieval performance for scalable cross-site
      laboratory harmonization given LOINC's large code space (over 80,000 codes).
    used_in_bridge2ai: false
    references:
    - https://proceedings.mlr.press/v193/tu22a/tu22a.pdf
  - id: B2AI_APP:137
    category: B2AI:Application
    name: Value-Based Laboratory Code Mapping to LOINC Using Result Distributions
    description: Statistical feature engineering on laboratory test result distributions
      (mean, median, quartiles, variance, skewness) combined with unit normalization
      enables AI mapping of in-house codes to LOINC via intermediate standards (JLAC10)
      without relying on test-name NLP. Applied to the J-DREAMS diabetes database
      (955,011 entries across 15 facilities, 51 analytes), classifiers achieved at
      least 70% mapping accuracy for 80.4% of analytes. This value-centric approach
      is particularly useful where NLP resources or medical corpora are limited, demonstrating
      that unit and value harmonization tied to LOINC groupings is critical for accurate
      automated mapping and international data sharing.
    used_in_bridge2ai: false
    references:
    - https://pmc.ncbi.nlm.nih.gov/articles/PMC12150744/
  - id: B2AI_APP:138
    category: B2AI:Application
    name: LOINC-Coded Laboratory Features for Improved Multi-Site Predictive Modeling
    description: Using LOINC-standardized laboratory features in predictive models
      significantly improves performance and cross-site transferability compared to
      unmapped local codes. In a 13-hospital UPMC heart failure cohort (2008-2012)
      predicting 30-day readmission, models trained with manually LOINC-mapped lab
      features consistently achieved significantly higher AUCs despite modest overall
      performance. LOINC aggregation and grouping procedures materially affect model
      reproducibility and external validation across institutions, demonstrating that
      explicit LOINC standardization should be reported as a critical data preprocessing
      step in multi-site ML studies to ensure interpretability and portability.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1093/jamiaopen/ooy063
  - id: B2AI_APP:139
    category: B2AI:Application
    name: LOINC-to-HPO Semantic Integration for Deep Phenotyping and Biomarker Discovery
    description: A widely adopted pipeline maps LOINC-coded laboratory test results
      transmitted via FHIR to Human Phenotype Ontology (HPO) terms, enabling computational
      deep phenotyping and biomarker discovery. The system manually biocurated mappings
      for 2,923 commonly used LOINC tests, handling numeric, ordinal, and nominal
      results using standardized FHIR interpretation codes, and leveraging HPO's hierarchical
      structure to aggregate heterogeneous tests with comparable interpretations.
      Validated on 15,681 UNC EHR patients with respiratory complaints and identifying
      known asthma biomarkers, the approach supports association studies, cohort analysis,
      and was released as a SMART on FHIR application for EHR integration and downstream
      ML applications requiring ontology-based phenotype embeddings.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1038/s41746-019-0110-4
  - id: B2AI_APP:140
    category: B2AI:Application
    name: N3C Unit and Value Harmonization Using LOINC Concept Grouping for ML-Ready
      Datasets
    description: The National COVID Cohort Collaborative (N3C) unit harmonization
      pipeline groups laboratory measurements by LOINC concepts, selects canonical
      units with UCUM conventions and conversion formulas, and infers missing units
      using Kolmogorov-Smirnov distributional comparisons across pooled multi-site
      data. Applied to billions of OMOP-mapped EHR lab records, the pipeline harmonized
      88.1% of values and imputed units for 78.2% of records missing units (41% of
      contributors' records), reclaiming large data fractions otherwise unavailable
      for analysis. This LOINC-based harmonization directly enables creation of ML-ready
      datasets for predictive modeling, phenotyping, and analytics across heterogeneous
      EHR sources.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1093/jamia/ocac054
  - id: B2AI_APP:141
    category: B2AI:Application
    name: High-Throughput LOINC Document Ontology Mapping Using Metadata-Based Methods
    description: A scalable pipeline maps clinical document metadata to LOINC Document
      Ontology (LOINC DO) codes at large scale using bag-of-words and vector distance
      methods applied to structured EHR fields (event titles, tags, encounter types)
      rather than full-text NLP. Applied to University of Missouri Cerner database
      (over 130 million decompressed notes), the metadata-driven approach achieved
      73.4% document coverage and mapped 132 million notes in under 2 hours, claimed
      to be an order of magnitude more efficient than NLP-based methods. This LOINC
      DO standardization supports downstream computable phenotyping, documentation
      quality assessment, and potential ML applications requiring standardized clinical
      document classification.
    used_in_bridge2ai: false
    references:
    - https://pmc.ncbi.nlm.nih.gov/articles/PMC10785913/pdf/1116.pdf
- id: B2AI_STANDARD:188
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:27
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Macromolecular Crystallographic Information File
  is_open: true
  name: mmCIF
  purpose_detail: PDBx/mmCIF became the standard PDB archive format in 2014. All PDB
    data processing and annotation will be performed using PDBx/mmCIF at all wwPDB
    sites. PDBx/mmCIF consists of categories of information represented as tables
    and keyword value pairs.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:82
  url: https://mmcif.wwpdb.org/
- id: B2AI_STANDARD:189
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:23
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Markup Components for Describing Multiple Sequence Alignments
  is_open: true
  name: MSAML
  purpose_detail: MSAML was formulated to make manipulation and extraction of multiple
    sequence alignment information easier by logically defining the parts of an alignment
    for use in an XML conformant application.
  requires_registration: false
  url: http://xml.coverpages.org/msaml-desc-dec.html
- id: B2AI_STANDARD:190
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:3
  - B2AI_TOPIC:27
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MDL molfile Format
  is_open: true
  name: MDL
  purpose_detail: An MDL Molfile is a file format for holding information about the
    atoms, bonds, connectivity and coordinates of a molecule.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/Chemical_table_file
- id: B2AI_STANDARD:191
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:3
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MDL reaction Format
  is_open: true
  name: RXN
  purpose_detail: The MDL reaction format is used to store information on chemical
    reactions.
  requires_registration: false
  url: https://open-babel.readthedocs.io/en/latest/FileFormats/MDL_RXN_format.html
- id: B2AI_STANDARD:192
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:6
  - B2AI_TOPIC:31
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Measuring Sex, Gender Identity, and Sexual Orientation
  is_open: true
  name: MSGISO
  publication: doi:10.17226/26424
  purpose_detail: Measuring Sex, Gender Identity, and Sexual Orientation recommends
    that the National Institutes of Health (NIH) adopt new practices for collecting
    data on sex, gender, and sexual orientation - including collecting gender data
    by default, and not conflating gender with sex as a biological variable. The report
    recommends standardized language to be used in survey questions that ask about
    a respondent's sex, gender identity, and sexual orientation. Better measurements
    will improve data quality, as well as the NIH's ability to identify LGBTQI+ populations
    and understand the challenges they face.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:60
  url: https://nap.nationalacademies.org/catalog/26424/measuring-sex-gender-identity-and-sexual-orientation
- id: B2AI_STANDARD:193
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:22
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Medical Imaging NetCDF (Minc) format
  is_open: true
  name: MNC
  publication: doi:10.3389/fninf.2016.00035
  purpose_detail: A system for flexible, self-documenting representation of neuroscientific
    imaging data with arbitrary orientation and dimensionality.
  requires_registration: false
  url: https://en.wikibooks.org/wiki/MINC/SoftwareDevelopment/MINC2.0_File_Format_Reference
- id: B2AI_STANDARD:194
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - codesystem
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Medicare Severity Diagnosis Related Groups codes
  is_open: true
  name: MS-DRG
  purpose_detail: Medical cases in the US are classified into Medicare Severity Diagnosis
    Related Groups (MS-DRGs) for payment based on the following information reported
    by the hospital - the principal diagnosis, up to 24 additional diagnoses, and
    up to 25 procedures performed during the stay. In a small number of MS-DRGs, classification
    is also based on the age, sex, and discharge status of the patient. Effective
    October 1, 2015, the diagnosis and procedure information is reported by the hospital
    using codes from the International Classification of Diseases, Tenth Revision,
    Clinical Modification (ICD-10-CM) and the International Classification of Diseases,
    Tenth Revision, Procedure Coding System (ICD-10-PCS).
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:17
  url: https://www.cms.gov/Medicare/Medicare-Fee-for-Service-Payment/AcuteInpatientPPS/MS-DRG-Classifications-and-Software
- id: B2AI_STANDARD:195
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:11
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MEDIN Discovery Metadata Standard
  formal_specification: https://github.com/medin-marine/Discovery-Standard-public-content
  is_open: true
  name: MEDIN
  purpose_detail: MEDIN Discovery Metadata Standard is a UK marine profile of the
    UK GEMINI (GEo-spatial Metadata Interoperability iNItiative) standard that also
    complies with the EU INSPIRE Directive and ISO19115. It provides comprehensive
    guidance for creating discovery metadata that accompanies marine datasets, describing
    what the data contains, where it was collected, and how to access it. The standard
    supports both geospatial (v3.1.2) and non-spatial marine data types. MEDIN provides
    multiple tools including the web-based Discovery Metadata Editor for creating,
    validating, exporting, and publishing records to the MEDIN Data Discovery Portal,
    and the standalone Metadata Maestro desktop application with user-friendly interface
    and offline capability. The standard includes XML Schema Definition (XSD) and
    Schematron files for validation, enabling systematic quality control of metadata
    records within organizational workflows.
  requires_registration: false
  url: https://www.medin.org.uk/medin-discovery-metadata-standard
- id: B2AI_STANDARD:196
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - diagnosticinstrument
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Memorial Symptom Assessment Scale
  is_open: true
  name: MSAS
  publication: doi:10.1016/0959-8049(94)90182-1
  purpose_detail: The Memorial Symptom Assessment Scale (MSAS) is a new patient-rated
    instrument that was developed to provide multidimensional information about a
    diverse group of common symptoms.
  requires_registration: false
  url: http://www.npcrc.org/files/news/memorial_symptom_assessment_scale.pdf
- id: B2AI_STANDARD:197
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:13
  - B2AI_TOPIC:28
  - B2AI_TOPIC:34
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Meta-omics Data and Collection Objects
  is_open: true
  name: MOD-CO
  purpose_detail: The conceptual and procedural meta-omics schema is developed as
    part of the MOD-CO project. It is entitled with MOD-CO schema, a conceptual schema
    for processing sample data in meta-omics research and published in various kind
    of schema representations. With that, the MOD-CO schema is a generic and comprehensive
    schema providing specifications useful for later software implementation and facilitates
    international standardisation processes.
  requires_registration: false
  url: https://www.mod-co.net/wiki/Schema_Representations
- id: B2AI_STANDARD:198
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:33
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831'
  description: MIAME Notation in Markup Language
  has_relevant_organization:
  - B2AI_ORG:74
  is_open: true
  name: MINiML
  purpose_detail: MINiML (MIAME Notation in Markup Language, pronounced 'minimal')
    is a data exchange format optimized for microarray gene expression data, as well
    as many other types of high-throughput molecular abundance data. MINiML assumes
    only very basic relations between objects - Platform (e.g., array), Sample (e.g.,
    hybridization), and Series (experiment). MINiML captures all components of the
    MIAME checklist, as well as any additional information that the submitter wants
    to provide. MINiML uses XML Schema as syntax.
  requires_registration: false
  url: https://www.ncbi.nlm.nih.gov/geo/info/MINiML.html
- id: B2AI_STANDARD:199
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:33
  - B2AI_TOPIC:34
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MicroArray Gene Expression Markup Language
  is_open: true
  name: MAGE-ML
  purpose_detail: This document is a standard that addresses the representation of
    gene expression data and relevant annotations, as well as mechanisms for exchanging
    these data. The field of gene expression experiments has several distinct technologies
    that a standard must include (e.g., single vs. dual channel experiments, cDNA
    vs. oligonucleotides). Because of these different technologies and different types
    of gene expression experiments, it is not expected that all aspects of the standard
    will be used by all organizations. With the acceptance of XML Metadata Interchange
    as an OMG standard it is possible to specify a normative UML model using a tool
    such as Rational Rose that describes the data structures for Gene Expression
  requires_registration: false
  url: http://scgap.systemsbiology.net/standards/mage_miame.php
- id: B2AI_STANDARD:200
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:33
  - B2AI_TOPIC:34
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MicroArray Gene Expression Markup Language Tab format
  is_open: true
  name: MAGE-TAB
  purpose_detail: Sharing of microarray data within the research community has been
    greatly facilitated by the development of the disclosure and communication standards
    MIAME and MAGE-ML by the FGED Society. However, the complexity of the MAGE-ML
    format has made its use impractical for laboratories lacking dedicated bioinformatics
    support. We propose a simple tab-delimited, spreadsheet-based format, MAGE-TAB,
    which will become a part of the MAGE microarray data standard and can be used
    for annotating and communicating microarray data in a MIAME compliant fashion.
    MAGE-TAB will enable laboratories without bioinformatics experience or support
    to manage, exchange and submit well-annotated microarray data in a standard format
    using a spreadsheet. The MAGE-TAB format is self-contained, and does not require
    an understanding of MAGE-ML or XML
  requires_registration: false
  url: http://scgap.systemsbiology.net/standards/mage_miame.php
- id: B2AI_STANDARD:201
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Microbiological Common Language
  is_open: true
  name: MCL
  publication: doi:10.1016/j.resmic.2010.02.005
  purpose_detail: MCL is a data exchange standard for microbiological information.
    In short, MCL defines terms which can be used to reference and describe microorganisms.
    It is designed to form a simple and generic framework leveraging the electronical
    exchange of information about microorganisms. MCL is loosely coupled from its
    actual representation technologies and is currently used to structure XML and
    RDF files (see examples).
  requires_registration: false
  url: https://doi.org/10.1016/j.resmic.2010.02.005
- id: B2AI_STANDARD:202
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MIMIC Waveform Database Format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:9
  has_relevant_organization:
  - B2AI_ORG:57
  - B2AI_ORG:115
  is_open: true
  name: WFDB Format
  purpose_detail: Format for MIMIC Waveform Database records.
  requires_registration: false
  url: https://wfdb.io/mimic_wfdb_tutorials/mimic/formatting.html
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:39
    category: B2AI:Application
    name: MIMIC-BP Curated Dataset for Blood Pressure Estimation Benchmarking
    description: The Python wfdb package reads MIMIC-III waveform signals directly,
      converts samples to double precision, and preserves signals exactly as recorded
      for ML workflows. MIMIC-BP organizes 1,524 patients into fixed 30-second segments
      (30 segments per patient) at 125 Hz with ABP, ECG, PPG, and RESP waveforms
      stored as NumPy files, providing per-segment median SBP/DBP labels for supervised
      learning. Pre-defined per-subject train/validation/test splits prevent data
      leakage under calibration-free protocols. This WFDB-enabled standardized segmentation,
      labeling, and splitting supports reproducible benchmarking of deep learning
      (ResNet, LSTM, Transformer) and classical ML (SVR, Random Forest) models for
      cuff-less blood pressure estimation across approximately 380 hours of ICU data.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1038/s41597-024-04041-1
  - id: B2AI_APP:142
    category: B2AI:Application
    name: PulseDB Large-Scale BP Estimation Corpus Using WFDB Toolbox
    description: The WFDB toolbox retrieves and validates MIMIC-III matched subset
      records, filtering for simultaneous ECG (lead II), PPG, and ABP channels while
      removing NaN samples by extracting the longest valid intervals per record (10
      seconds to 10 hours). Signals are resampled to 125 Hz and segmented into standardized
      10-second windows with beat-level annotations and per-segment SBP/DBP labels,
      yielding 5.2 million segments. This WFDB-enabled quality control, multi-channel
      synchronization, and standardized segmentation provides a large benchmarking
      corpus for training and evaluating ML/DL cuff-less blood pressure estimators
      with cross-study comparability and reproducible preprocessing pipelines.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.3389/fdgth.2022.1090854
  - id: B2AI_APP:143
    category: B2AI:Application
    name: Intracranial Hypertension Forecasting Using Multi-Scale WFDB Waveforms
    description: MIMIC-III WFDB provides high-frequency waveforms (125 Hz) and derived
      1 Hz time series for ML early-warning systems. Specific channels (ICP, CPP,
      ABP variants, HR, PLETH, RESP, ECG) are selected via WFDB, with cohort filtering
      enforcing minimum 24-hour recording length and maximum 25% per-channel missing
      values, reducing to 123 usable segments. Preprocessing produces 1-minute blocks
      with artifact deletion, and supervised labels are derived from WFDB ICP time
      series (median ICP greater than 20 mmHg for five consecutive minutes defines
      intracranial hypertension events). This WFDB-based channel selection, quality
      filtering, segmentation, and labeling enables training of logistic regression
      and feature-based predictive models for forecasting intracranial hypertension
      up to 8 hours ahead.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1088/1361-6579/ab6360
  - id: B2AI_APP:144
    category: B2AI:Application
    name: Deep Learning ABP Waveform Imputation from WFDB Multi-Channel Signals
    description: MIMIC-III waveform records accessed via WFDB provide synchronized
      ECG (lead II), PPG, and invasive ABP for training deep learning models to impute
      continuous arterial blood pressure waveforms from non-invasive signals. Preprocessing
      includes downsampling to 100 Hz, low-pass filtering (16 Hz cutoff), robust
      per-window scaling, and cross-correlation-based clock drift correction (up to
      plus or minus 4 seconds). Sliding 32-second windows with 16-second steps are
      quality-filtered (variance and peak-count thresholds), and labels are derived
      from invasive ABP medians over 4-second windows. This WFDB-enabled preprocessing,
      segmentation, inter-signal alignment, and label derivation supports training
      U-Net-like segmentation models with transfer learning to external (UCLA) cohorts
      for generalizable waveform-to-waveform prediction.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1038/s41598-021-94913-y
  - id: B2AI_APP:145
    category: B2AI:Application
    name: Comparative Deep Learning for ABP Prediction from WFDB Physiological Signals
    description: MIMIC waveform records provide co-recorded ECG (lead V), PPG, and
      ABP for comparative evaluation of deep learning architectures including ResNet,
      WaveNet, and LSTM for blood pressure prediction. WFDB-sourced data undergoes
      artifact detection and removal (flat lines, missing heartbeats, negative BP
      values), signal filtering (8th-order Chebyshev bandpass for ECG at 2-59 Hz),
      and segmentation into 10-minute recordings. Target ABP is normalized with physiological
      bounds (15-300 mmHg) for model training using RMSE and Huber loss, requiring
      de-normalization for evaluation. This WFDB-based data selection, preprocessing,
      segmentation, and target normalization pipeline supports systematic comparison
      of CNN and RNN architectures for continuous blood pressure estimation from ICU
      waveforms.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1007/s12559-021-09910-0
- id: B2AI_STANDARD:203
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Minimal Information about a high throughput SEQuencing Experiment
  formal_specification: https://drive.google.com/file/d/1YyvWT02puzMG_UgNmfAEJwVr60-pMvIE/view?usp=sharing
  is_open: true
  name: MINSEQE
  purpose_detail: Information needed to enable the unambiguous interpretation and
    facilitate reproduction of the results of a high throughput sequencing experiment.
  requires_registration: false
  url: https://zenodo.org/record/5706412
- id: B2AI_STANDARD:204
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Minimal Information about a Self-Monitoring Experiment
  is_open: false
  name: MISME
  publication: doi:10.3233/978-1-61499-423-7-79
  purpose_detail: This is a reporting guideline for self-monitoring and quantified-self
    experiments and their use for research purposes.
  requires_registration: false
  url: https://doi.org/10.3233/978-1-61499-423-7-79
- id: B2AI_STANDARD:205
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Minimal Information Required In the Annotation of Models
  is_open: true
  name: MIRIAM
  purpose_detail: MIRIAM (Minimum Information Required In the Annotation of biochemical
    Models) is a set of guidelines and standards developed by the computational systems
    biology community to ensure consistent annotation and curation of computational
    models in biology, particularly those encoded in SBML (Systems Biology Markup
    Language) and CellML formats. MIRIAM defines requirements for model attribution
    including reference publications, creator information, creation dates, and modification
    history to ensure proper provenance tracking and reproducibility. The guidelines
    mandate the use of controlled vocabularies and standardized identifiers from MIRIAM
    Resources (identifiers.org) to annotate biological entities such as genes, proteins,
    metabolites, reactions, and pathways with unambiguous references to external databases
    like UniProt, ChEBI, Gene Ontology, KEGG, and Reactome. MIRIAM specifies the use
    of RDF (Resource Description Framework) and qualified annotations to encode biological
    semantics and relationships between model components and biological knowledge.
    The standard promotes model reusability by requiring clear licensing information
    (Creative Commons, etc.), encoded parameter units using standardized ontologies,
    and comprehensive documentation of model assumptions and limitations. MIRIAM compliance
    is supported by tools including the MIRIAM annotation editor in systems biology
    software, libSBML annotation functions, and BioModels Database validation workflows.
    The guidelines have been widely adopted by model repositories (BioModels, CellML
    Model Repository), enhancing model discoverability, integration into larger modeling
    frameworks, and facilitating quantitative comparison of alternative models representing
    the same biological system.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:19
  url: http://co.mbine.org/standards/miriam
- id: B2AI_STANDARD:206
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:2
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Minimum Information about a Flow Cytometry Experiment
  is_open: true
  name: MIFlowCyt
  publication: doi:10.1002/cyto.a.20623
  purpose_detail: The fundamental tenet of scientific research is that the published
    results of any study have to be open to independent validation or refutation.
    The Minimum Information about a Flow Cytometry Experiment (MIFlowCyt) establishes
    criteria for recording and reporting information about the flow cytometry experiment
    overview, samples, instrumentation and data analysis. It promotes consistent annotation
    of clinical, biological and technical issues surrounding a flow cytometry experiment
    by specifying the requirements for data content and by providing a structured
    framework for capturing information.
  requires_registration: false
  url: https://isac-net.org/page/MIFlowCyt
- id: B2AI_STANDARD:207
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Minimum Information About a Proteomics Experiment
  is_open: true
  name: MIAPE
  purpose_detail: Information from whole proteomics experiments; where samples came
    from, and how analyses of them were performed.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:41
  url: https://www.psidev.info/miape
- id: B2AI_STANDARD:208
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:33
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Minimum Information About a RNAi Experiment
  is_open: true
  name: MIARE
  purpose_detail: Minimum Information About an RNAi Experiment (MIARE) is a set of
    reporting guidelines that describes the minimum information that should be reported
    about an RNAi experiment to enable the unambiguous interpretation and reproduction
    of the results. MIARE forms part of a larger effort to develop RNAi data standards
    that include a data model, data exchange format, controlled vocabulary and supporting
    software tools.
  requires_registration: false
  url: http://miare.sourceforge.net/HomePage
- id: B2AI_STANDARD:209
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Minimum information about any sequence
  formal_specification: https://github.com/GenomicsStandardsConsortium/mixs/
  is_open: true
  name: MIxS
  publication: doi:10.1038/nbt.1823
  purpose_detail: The Minimum Information about any (x) Sequence (MIxS) is a unified
    standard developed by the Genomic Standards Consortium (GSC) for describing sequence
    data from diverse environments and organisms. MIxS provides a single point of
    entry for the scientific community to access and use GSC checklists, which include
    11 distinct sequence types covering bacterial/archaeal genomes (MigsBa), eukaryotes
    (MigsEu), organelles (MigsOrg), plasmids (MigsPl), viruses (MigsVi), marker sequences
    (MimarksC/S), metagenomes (Mims), metagenome-assembled genomes (Mimag), single
    amplified genomes (Misag), and uncultivated virus genomes (Miuvig). The standard
    is complemented by 21 environmental extensions tailored to specific sampling contexts
    including agriculture, air, built environment, food production, host-associated,
    human-associated (with specific gut, oral, skin, and vaginal extensions), hydrocarbon
    resources, microbial mat/biofilm, plant-associated, sediment, soil, symbiont-associated,
    wastewater/sludge, and water environments. The specification is maintained as
    a YAML-formatted LinkML schema serving as the authoritative source for generating
    downstream GSC artifacts.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:38
  url: https://genomicsstandardsconsortium.github.io/mixs/
- id: B2AI_STANDARD:210
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Minimum Information About Plant Phenotyping Experiments
  is_open: true
  name: MIAPPE
  purpose_detail: A reporting guideline for plant phenotyping experiments. Comprises
    a checklist, i.e., a list of attributes that may be necessary to fully describe
    an experiment so that it is understandable and replicable. Should be consulted
    by people recording and depositing the data. Covers description of the following
    aspects of plant phenotyping experiment - study, environment, experimental design,
    sample management, biosource, treatment and phenotype. To read more, please visit
    http://cropnet.pl/phenotypes
  requires_registration: false
  url: https://www.miappe.org/
- id: B2AI_STANDARD:211
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:35
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Minimum Information About Somatic Mutation
  is_open: true
  name: MIASM
  publication: doi:10.1002/humu.20832
  purpose_detail: MIASM was developed for the collection of somatic variations to
    promote standards for annotations of somatic variation data, and to promote data
    integration with other data resources.
  requires_registration: false
  url: http://structure.bmc.lu.se/MIASM/miasm.html
- id: B2AI_STANDARD:212
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:35
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Minimum Information for QTLs and Association Studies
  is_open: true
  name: MIQAS
  purpose_detail: The MIQAS set of rules accompanied with the standardized XML and
    tab-delimited file formats will serve two goals - to encourage research groups
    that wish to publish a QTL paper to provide and submit the necessary information
    that would make meta-analysis possible and to allow easy interchange of data between
    different QTL and association analysis databases. Databases that implement the
    standardized XML format will typically write an import and an export filter to
    read data from and dump data into that an XML file. This is the same approach
    as used for the exchange of sequences between NCBI, Ensembl and DDBJ at the early
    stages of the Human Genome Project.
  requires_registration: false
  url: http://miqas.sourceforge.net/
- id: B2AI_STANDARD:213
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Minimum information required to initiate collaborations between biobanks
  is_open: true
  name: MIABIS
  publication: doi:10.1089/bio.2015.0070
  purpose_detail: MIABIS represents the minimum information required to initiate collaborations
    between biobanks and to enable the exchange of biological samples and data. The
    aim is to facilitate the reuse of bio-resources and associated data by harmonizing
    biobanking and biomedical research.
  requires_registration: false
  url: https://doi.org/10.1089/bio.2015.0070
- id: B2AI_STANDARD:214
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Minimum information required to report the use of quantification techniques
    in a proteomics experiment
  is_open: true
  name: MIAPE-Quant
  purpose_detail: This module identifies the minimum information required to report
    the use of quantification techniques in a proteomics experiment, sufficient to
    support both the effective interpretation and assessment of the data and the potential
    recreation of the results of the data analysis.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:41
  url: https://www.psidev.info/attachments/miape-quant-091-documents
- id: B2AI_STANDARD:215
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:27
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ModelCIF
  formal_specification: http://github.com/ihmwg/ModelCIF
  is_open: true
  name: ModelCIF
  publication: doi:10.1101/2022.12.06.518550
  purpose_detail: An extension of the Protein Data Bank Exchange / macromolecular
    Crystallographic Information Framework (PDBx/mmCIF); provides an extensible data
    representation for deposition, archiving, and public dissemination of predicted
    3D models of proteins.
  requires_registration: false
  url: http://github.com/ihmwg/ModelCIF
- id: B2AI_STANDARD:216
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - multimodal
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Multilevel Healthcare Information Modeling specifications
  is_open: true
  name: MLHIM
  purpose_detail: The Multilevel Healthcare Information Modeling (MLHIM) specifications
    enables the exchange of syntactically and semantically interoperable data extracts
    between distributed, independently developed, biomedical databases and clinical
    applications, promoting syntactic and semantic integration of Translational Research
    data. The Semantic MedWeb is an implementation of a MLHIM-based database development
    platform (open source code available at https://github.com/mlhim/SemanticMedWeb)
  requires_registration: false
  url: https://mlhim-specifications.readthedocs.io/en/master/
- id: B2AI_STANDARD:217
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:23
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Multiple Alignment Format
  is_open: true
  name: MAF
  purpose_detail: The multiple alignment format stores a series of multiple alignments
    in a format that is easy to parse and relatively easy to read. This format stores
    multiple alignments at the DNA level between entire genomes. Previously used formats
    are suitable for multiple alignments of single proteins or regions of DNA without
    rearrangements, but would require considerable extension to cope with genomic
    issues such as forward and reverse strand directions, multiple pieces to the alignment,
    and so forth.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:119
  url: http://genome.ucsc.edu/FAQ/FAQformat.html#format5
- id: B2AI_STANDARD:218
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: mz5 format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:16
  is_open: true
  name: mz5
  publication: doi:10.1074/mcp.O111.011379
  purpose_detail: The mz5 format is a complete reimplementation of the mzML data
    model and ontology built on the HDF5 (Hierarchical Data Format 5) storage backend,
    designed to address performance and scalability limitations of XML-based mass
    spectrometry formats. mz5 preserves the semantic structure and controlled vocabulary
    terms of mzML while leveraging HDF5's binary format, chunked storage, and native
    compression capabilities to achieve significantly faster read/write operations
    and reduced file sizes compared to mzML. The format organizes mass spectrometry
    data into hierarchical HDF5 groups and datasets representing spectra, chromatograms,
    instrument configurations, and metadata, with support for efficient random access
    to individual spectra and parallel I/O operations. mz5 maintains compatibility
    with the Proteomics Standards Initiative controlled vocabularies and supports
    the same rich metadata and data provenance as mzML, while providing superior
    performance for large-scale proteomics and metabolomics datasets. The HDF5 foundation
    enables integration with high-performance computing workflows, supports multiple
    programming languages (C, Python, Java, R), and facilitates efficient processing
    of high-resolution and high-throughput mass spectrometry data for downstream
    analysis pipelines and machine learning applications.
  related_to:
  - B2AI_STANDARD:339
  requires_registration: false
  url: https://doi.org/10.1074/mcp.O111.011379
- id: B2AI_STANDARD:219
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: mzData format
  is_open: true
  name: mzData
  purpose_detail: mzData is an XML format for representing mass spectrometry data
    in such a way as to completely describe the instrumental aspects of the experiment.
    This format is deprecated and has been superseded by mzML.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:41
  url: https://psidev.info/mass-spectrometry-workgroup#mzdata
- id: B2AI_STANDARD:220
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: mzIndentML format
  is_open: true
  name: mzIndentML
  purpose_detail: A large number of different proteomics search engines are available
    that produce output in a variety of different formats. It is intended that mzIdentML
    will provide a common format for the export of identification results from any
    search engine. The format was originally developed under the name AnalysisXML
    as a format for several types of computational analyses performed over mass spectra
    in the proteomics context.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:41
  url: https://www.psidev.info/mzidentml
- id: B2AI_STANDARD:221
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: mzML format
  formal_specification: http://www.peptideatlas.org/tmp/mzML1.1.0.html
  is_open: true
  name: mzML
  publication: doi:10.1007/978-1-60761-444-9_22
  purpose_detail: From 2005-2008 there has existed two separate XML formats for encoding
    raw spectrometer output, mzData developed by the PSI and mzXML developed at the
    Seattle Proteome Center at the Institute for Systems Biology. It was recognized
    that the existence of two separate formats for essentially the same thing generated
    confusion and required extra programming effort. Therefore the PSI, with full
    participation by ISB, has developed a new format by taking the best aspects of
    each of the precursor formats to form a single one. It is intended to replace
    the previous two formats. This new format was originally given a working name
    of dataXML. The final name is mzML.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:41
  url: https://psidev.info/mzML
- id: B2AI_STANDARD:222
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: mzQuantML format
  is_open: true
  name: mzQuantML
  purpose_detail: The mzQuantML standard format is intended to store the systematic
    description of workflows quantifying molecules (principly peptides and proteins)
    by mass spectrometry. A large number of different software packages are available
    that produce output in a variety of different formats. It is intended that mzQuantML
    will provide a common format for the export of identification results from any
    software package. The format was originally developed under the name AnalysisXML
    as a format for several types of computational analyses performed over mass spectra
    in the proteomics context. It has been decided to split development into two formats,
    mzIdentML for peptide and protein identification and mzQuantML (described here),
    covering quantitative proteomic data derived from MS. The development of mzQuantML
    is driven by some general principles, specific use cases and the goal of supporting
    specific techniques, as listed below. These were discussed and agreed at the development
    meeting in Tubingen in July 2011.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:41
  url: https://www.psidev.info/mzquantml
- id: B2AI_STANDARD:223
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - deprecated
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: mzXML format
  is_open: true
  name: mzXML
  publication: doi:10.1038/nbt1031
  purpose_detail: mzXML was a pioneering open, generic XML (extensible markup language)
    representation specifically developed for mass spectrometry (MS) data exchange
    and storage in proteomics research. As one of the first standardized formats for
    MS data, mzXML provided a vendor-neutral way to represent complex mass spectrometry
    information including mass-to-charge ratios, intensity values, scan parameters,
    and instrument metadata in a structured, machine-readable format. The format supported
    both profile and centroid data representation modes and could accommodate various
    types of MS experiments including MS/MS fragmentation spectra. Despite its historical
    significance in establishing data standardization practices in proteomics, mzXML
    is now deprecated and has been superseded by the more comprehensive mzML format,
    which offers enhanced features, better compression, controlled vocabularies, and
    broader community support. While legacy systems may still encounter mzXML files,
    current best practices recommend migration to mzML for new data processing workflows
    and long-term data preservation in mass spectrometry applications.
  requires_registration: false
  url: https://doi.org/10.1038/nbt1031
- id: B2AI_STANDARD:224
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: National Cancer Institute Enterprise Vocabulary Service
  is_open: true
  name: NCI EVS
  purpose_detail: Terminology content, tools, and services to accurately code, analyze
    and share cancer and biomedical research, clinical and public health information.
    Includes NCI Thesaurus and Metathesaurus.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:71
  url: https://evs.nci.nih.gov/
- id: B2AI_STANDARD:225
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: National Emergency Medical Services Information System
  has_relevant_organization:
  - B2AI_ORG:66
  is_open: true
  name: NEMSIS
  purpose_detail: Standard for the collection and transmission of emergency medical
    services (EMS) operations and patient care data.
  requires_registration: false
  url: https://nemsis.org/technical-resources/
- id: B2AI_STANDARD:226
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: National Patient-Centered Clinical Research Network Common Data Model
  formal_specification: https://pcornet.org/wp-content/uploads/2022/01/PCORnet-Common-Data-Model-v60-2020_10_221.pdf
  is_open: true
  name: PCORNet CDM
  purpose_detail: This guiding principle is expressed in the CDM design through prioritization
    of analytic functionality, and a parsimonious approach based upon analytic utility.
    At times, this results in decisions that are not based in relational database
    modeling principles such as normalization. The model is designed to facilitate
    routine and rapid execution of distributed complex analytics. To meet this design
    requirement, some fields are duplicated across multiple tables to support faster
    analytic operations for distributed querying. The PCORnet CDM is based on the
    FDA Mini-Sentinel CDM. This allows PCORnet to more easily leverage the large array
    of analytic tools and expertise developed for the MSCDM v4.0, including data characterization
    approaches and the various tools for complex distributed analytics.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:81
  url: http://pcornet.org/pcornet-common-data-model/
- id: B2AI_STANDARD:227
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Natural Collections Description standard
  is_open: true
  name: NCD
  purpose_detail: Natural Collections Description (NCD) (A data standard for exchanging
    data describing natural history collections) is a proposed data standard for describing
    collections of natural history materials at the collection level; one NCD record
    describes one entire collection. Collection descriptions are electronic records
    that document the holdings of an organisation as groups of items, which complement
    the more traditional item-level records such as are produced for a single specimen
    or a library book. NCD is tailored to natural history. It lies between general
    resource discovery standards such as Dublin Core (DC) and rich collection description
    standards such as the Encoded Archival Description (EAD). The NCD standard covers
    all types of natural history collections, such as specimens, original artwork,
    archives, observations, library materials, datasets, photographs or mixed collections
    such as those that result from expeditions and voyages of discovery.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:93
  url: https://www.tdwg.org/standards/ncd/
- id: B2AI_STANDARD:228
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: NCBI BioProject XML Schema
  formal_specification: https://www.ncbi.nlm.nih.gov/data_specs/schema/other/bioproject/
  has_relevant_organization:
  - B2AI_ORG:74
  is_open: true
  name: BioProject Schema
  publication: doi:10.1093/nar/gkr1163
  purpose_detail: 'The NCBI BioProject Schema is a comprehensive XML Schema Definition
    (XSD) that specifies the structure and validation rules for metadata describing
    biological research projects within NCBI''s BioProject database. A BioProject
    serves as an organizing principle for diverse experimental data types, providing
    a single accession identifier that links raw sequence reads (Sequence Read Archive),
    assembled genomes (GenBank), proteomics data, annotations, publications, and
    supplementary materials generated by a specific research initiative. The schema
    formally defines the metadata elements, data types, cardinalities, and controlled
    vocabularies required for representing project scope, organism information, experimental
    design, objectives, data types generated, participating organizations, funding
    sources, and publication references. This standardization enables consistent programmatic
    submission, retrieval, and computational analysis of project-level metadata across
    NCBI''s interconnected databases, supporting reproducible research and data reuse. 
    The BioProject Schema is organized hierarchically with core elements including
    ProjectID (unique accession), ProjectDescr (descriptive metadata with title, description,
    relevance, objectives), ProjectType (classification as primary submission, umbrella,
    or peer-reviewed), Organism (taxonomic identifiers with NCBI Taxonomy ID bindings
    and strain/isolate details), ProjectTypeSubmission (methodology-specific elements
    for genome sequencing, metagenome, transcriptome, epigenome, proteome, targeted
    locus, or other experimental types), LocusTagPrefix (for genome annotation submission),
    Targets (genomic regions, genes, or variants of interest), Publications (PubMed
    ID cross-references), and ExternalLinks (resources hosted outside NCBI). Complex
    types and enumerated values enforce validation rules, ensuring submitted data
    meets minimal information standards. For example, the ''MethodType'' enumeration
    restricts sequencing methodologies to predefined categories (eSequencing, eAssembly,
    eAnnotation, etc.), while taxonomic identifiers must resolve to valid NCBI Taxonomy
    entries. The schema supports nested project relationships, enabling umbrella projects
    (coordinated multi-center initiatives or consortia) to aggregate related sub-projects
    while maintaining individual accessions and metadata granularity. Temporal elements
    capture project timelines, release dates, and data embargos, coordinating with
    NCBI''s controlled-access data policies (dbGaP) for human subject research.
    BioProject metadata plays a critical role in organizing multi-omics datasets and
    enabling integrative computational analyses. The schema''s structured representation
    of experimental intent, organism context, and data provenance facilitates federated
    searches across NCBI resources. For instance, a single BioProject accession might
    link whole-genome sequencing reads in SRA, assembled contigs in GenBank, RNA-Seq
    profiles in Gene Expression Omnibus (GEO), and variant calls in ClinVar, providing
    a unified entry point for researchers investigating the same biological system.
    NCBI''s Entrez query system indexes BioProject metadata fields, enabling complex
    queries such as "all bacterial genome projects funded by NIH from 2020-2023 with
    associated antimicrobial resistance phenotypes." Programmatic access through NCBI
    E-utilities (ESearch, EFetch, ELink) and Datasets API returns XML documents conforming
    to the BioProject Schema, which can be parsed using standard XML libraries (lxml
    in Python, xml2 in R) for downstream analysis. The schema''s formal validation
    ensures data quality at submission, reducing curation burden and improving machine-readability
    for automated pipelines.
    In AI and machine learning workflows, BioProject metadata serves multiple functions.
    The schema''s structured project descriptions, experimental design features, and
    data type annotations enable large-scale meta-analyses and dataset discovery for
    model training. Text mining applications extract organism names, methodologies,
    and objectives from BioProject titles and descriptions to classify research domains
    or predict experimental outcomes. Recommendation systems leverage project similarities
    (taxonomic overlap, methodology concordance, temporal proximity, publication co-authorship)
    to suggest relevant datasets or collaborators. Scientometric analyses use BioProject
    metadata to track research trends, funding allocation impacts, and reproducibility
    rates across domains (e.g., examining how many genome projects result in peer-reviewed
    publications or reusable annotations). Knowledge graphs integrate BioProject metadata
    with ontologies (Gene Ontology for functional annotations, Disease Ontology for
    clinical relevance), enabling semantic queries and hypothesis generation. For
    training cohort assembly in supervised learning, structured project metadata allows
    filtering by taxonomic scope (human vs model organism), data types (RNA-Seq vs
    whole-genome sequencing), experimental conditions (disease vs control), and sample
    sizes, ensuring datasets meet minimal requirements for statistical power and biological
    relevance. The schema''s temporal and versioning information supports longitudinal
    studies of data growth and model performance degradation over time. As a broadly
    adopted NCBI standard with over 1.2 million BioProject records spanning prokaryotes,
    eukaryotes, viruses, and metagenomes, it provides a rich corpus for training natural
    language processing models on scientific text and developing AI systems for automated
    experimental design and data integration in biological research.'
  requires_registration: false
  url: https://www.ncbi.nlm.nih.gov/data_specs/schema/other/bioproject/
  has_application:
  - id: B2AI_APP:40
    category: B2AI:Application
    name: Research Project Metadata Mining and Dataset Discovery
    description: BioProject schema is used in AI applications for mining metadata
      about biological research projects, enabling automated discovery of relevant
      datasets, prediction of experimental outcomes, and analysis of research trends.
      Machine learning systems parse BioProject records to identify related studies
      for meta-analyses, recommend similar projects to researchers, and predict which
      experimental approaches are likely to succeed based on project descriptions.
      AI applications leverage structured project metadata to train models that classify
      research projects by methodology, extract experimental design features, and
      identify collaborative networks in biological research. The schema enables large-scale
      scientometric analyses and AI-driven research prioritization based on historical
      project outcomes and resource allocation.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:229
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: NCPDP Formulary and Benefit Standard
  is_open: false
  name: NCPDP F&B
  purpose_detail: The NCPDP Formulary and Benefit Standard provides a standardized
    means for pharmacy benefit payers, including health plans and Pharmacy Benefit
    Managers (PBMs), to electronically communicate formulary and benefit information
    to prescribers via technology vendor systems. This standard enables real-time
    access to critical medication coverage information at the point of prescribing,
    including patient eligibility verification, product coverage status, benefit financial
    details (copayments, deductibles, out-of-pocket costs), coverage restrictions
    such as prior authorization requirements, step therapy protocols, quantity limits,
    and therapeutic alternatives when restrictions exist. By standardizing this data
    exchange, the standard reduces prescription abandonment, improves medication adherence,
    decreases prior authorization processing time, and enhances clinical decision-making
    by providing prescribers with transparent, actionable benefit information. The
    standard supports integration with Electronic Health Record (EHR) systems and
    e-prescribing platforms in accordance with HIPAA, MMA, HITECH, and Meaningful
    Use requirements, ultimately contributing to reduced healthcare costs and improved
    patient safety through more informed prescribing decisions.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:63
  url: https://standards.ncpdp.org/Access-to-Standards.aspx
- id: B2AI_STANDARD:230
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: NESTcc Data Quality Framework
  formal_specification: https://mdic.org/wp-content/uploads/2020/02/NESTcc-Data-Quality-Framework.pdf
  is_open: true
  name: NESTcc DQF
  purpose_detail: Guiding principles and a foundation for the capture and use of high-quality
    data for post-market evaluation of medical devices
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:64
  url: https://nestcc.org/data-quality-and-methods/
- id: B2AI_STANDARD:231
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Neurodata Without Borders
  formal_specification: https://github.com/NeurodataWithoutBorders
  is_open: true
  name: NWB
  publication: doi:10.1101/523035
  purpose_detail: Neurodata Without Borders (NWB) is a comprehensive data standard
    and software ecosystem for neurophysiology and behavioral data, developed by a
    collaborative team of neuroscientists and software developers to break down barriers
    to data sharing in neuroscience. NWB supports diverse neurophysiology data types
    including intracellular and extracellular electrophysiology (single-unit recordings,
    local field potentials, patch-clamp), optical physiology (calcium imaging, two-photon
    microscopy, optogenetics), behavioral tracking, stimulus presentation metadata,
    and experimental trial structures. The format is built on HDF5, providing efficient
    storage and access to large-scale datasets while maintaining human-readable metadata.
    NWB's extensibility mechanism through neurodata extensions allows researchers
    to adapt the standard for novel recording modalities and experimental paradigms
    without breaking compatibility. The ecosystem includes PyNWB and MatNWB APIs for
    data creation and access, validation tools, and integration with the DANDI Archive
    for public data sharing. NWB adoption is growing across major neuroscience initiatives
    with support from Allen Institute, HHMI, Kavli Foundation, Simons Foundation,
    and INCF, facilitating reproducible research, cross-laboratory data integration,
    and development of standardized analysis pipelines across cellular, systems, and
    computational neuroscience.
  requires_registration: false
  url: https://www.nwb.org/
- id: B2AI_STANDARD:232
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:22
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Neuroimaging Data Model
  has_relevant_organization:
  - B2AI_ORG:99
  is_open: true
  name: NIDM
  purpose_detail: The Neuroimaging Data Model (NIDM) is a collection of specification
    documents and examples that outline a domain specific extension to the W3C Provenance
    Data Model (PROV-DM) for the exchange and sharing of human brain imaging data.
    The goal of the data model is to capture data, information about the data and
    processes that generated the data (i.e. provenance). This information can be converted
    to RDF and therefore queried using SPARQL. This representation allows machine
    accessible representations of brain imaging data and will provide links to related
    resources such as publications, virtual machines, people and funding agencies.
  requires_registration: false
  url: http://nidm.nidash.org/
  has_application:
  - id: B2AI_APP:41
    category: B2AI:Application
    name: Neuroimaging Results Sharing and Meta-Analytic AI
    description: NIDM (NeuroImaging Data Model) is used in AI applications for sharing
      and aggregating neuroimaging analysis results across studies, enabling meta-analytic
      machine learning and improving reproducibility in computational neuroscience.
      AI systems leverage NIDM's semantic representation of imaging workflows, statistical
      maps, and analysis provenance to train models that learn from aggregated results
      rather than raw images, respecting data sharing constraints while enabling large-scale
      analyses. The model supports AI applications that perform automated quality
      assessment of imaging studies, detect inconsistencies in reported results, and
      synthesize findings across diverse analysis pipelines. NIDM enables machine
      learning systems to understand the complete analytical context of neuroimaging
      results, improving reproducibility and meta-analytic power.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:233
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:22
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Neuroimaging Informatics Technology Initiative file format
  formal_specification: https://nifti.nimh.nih.gov/nifti-2
  is_open: true
  name: NIFTI
  purpose_detail: The Neuroimaging Informatics Technology Initiative (nifti) file
    format was envisioned about a decade ago as a replacement to the then widespread,
    yet problematic, analyze 7.5 file format. The main problem with the previous format
    was perhaps the lack of adequate information about orientation in space, such
    that the stored data could not be unambiguously interpreted. Although the file
    was used by many different imaging software, the lack of adequate information
    on orientation obliged some, most notably spm, to include, for every analyze file,
    an accompanying file describing the orientation, such as a file with extension
    .mat.
  requires_registration: false
  url: https://nifti.nimh.nih.gov/
- id: B2AI_STANDARD:234
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: NeuroML
  is_open: true
  name: NeuroML
  purpose_detail: NeuroML is a model description language developed in XML (extensible
    Markup Language) that was created to facilitate data archiving, data and model
    exchange, database creation, and model publication in the neurosciences. One of
    the goals of the NeuroML project is to develop standards for model specification
    that will allow for greater simulator interoperability and model exchange.
  requires_registration: false
  url: https://neuroml.org/
- id: B2AI_STANDARD:235
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:48
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Neurophysiology Data Translation Format
  has_relevant_organization:
  - B2AI_ORG:11
  is_open: true
  name: NDF
  publication: doi:10.3389/conf.fnins.2010.13.00118
  purpose_detail: The purpose of the Neurophysiology Data Translation Format (NDF)
    is to provide a means of sharing neurophysiology experimental data and derived
    data between services and tools developed within the CARMEN project (www.carmen.org.uk).
    This document specifes the NDF. The specification supports the types of data that
    are currently used by members of the CARMEN consortium and provides a capability
    to support future data types. It is capable of accommodating external data file
    formats as well as metadata such as user defined experimental descriptions and
    the history (provenance) of derived data.
  requires_registration: false
  url: https://doi.org/10.3389/conf.fnins.2010.13.00118
- id: B2AI_STANDARD:236
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:26
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: New Hampshire eXtended Format
  is_open: true
  name: NHX
  purpose_detail: NHX is based on the New Hampshire (NH) standard (also called Newick
    tree format).
  requires_registration: false
  url: http://www.phylosoft.org/NHX/
- id: B2AI_STANDARD:237
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:26
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Newick tree Format
  is_open: true
  name: Newick
  purpose_detail: The Newick Standard for representing trees in computer-readable
    form makes use of the correspondence between trees and nested parentheses, noticed
    in 1857 by the famous English mathematician Arthur Cayley.
  requires_registration: false
  url: http://evolution.genetics.washington.edu/phylip/newicktree.html
- id: B2AI_STANDARD:238
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:25
  - B2AI_TOPIC:26
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: NeXML format
  formal_specification: https://github.com/nexml/nexml
  is_open: true
  name: NeML
  purpose_detail: To facilitate interoperability in evolutionary comparative analysis,
    we present NeXML, an XML standard (inspired by the current standard, NEXUS) that
    supports exchange of richly annotated comparative data. NeXML defines syntax for
    operational taxonomic units, character-state matrices, and phylogenetic trees
    and networks. Documents can be validated unambiguously. Importantly, any data
    element can be annotated, to an arbitrary degree of richness, using a system that
    is both flexible and rigorous. We describe how the use of NeXML by the TreeBASE
    and Phenoscape projects satisfies user needs that cannot be satisfied with other
    available file formats
  requires_registration: false
  url: https://github.com/nexml/nexml
- id: B2AI_STANDARD:239
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Nibble sequence format
  is_open: true
  name: nib
  purpose_detail: The nibble (.nib) format is a compact binary file format for storing
    DNA sequences, originally developed for the UCSC Genome Browser and BLAT alignment
    tool. The format achieves 4-fold compression by packing two nucleotide bases per
    byte using 2-bit encoding (A=00, C=01, G=10, T=11), significantly reducing storage
    requirements compared to text-based FASTA format. Each .nib file contains a single
    sequence record with a simple header specifying sequence length and format version,
    followed by the packed sequence data. The format stores sequences in a form optimized
    for rapid access by genome browsers and alignment algorithms, supporting efficient
    memory mapping for large-scale genomic analyses. While .nib files provide space-efficient
    storage, they lack the flexibility of indexed formats like 2bit (which can store
    multiple sequences) and have been largely superseded by more modern compressed
    formats. The format remains in use for legacy applications and continues to be
    supported by UCSC Genome Browser utilities including nibFrag for sequence extraction
    and faToNib/nibToFa for format conversion, primarily for maintaining compatibility
    with older genome browser implementations and analysis pipelines.
  requires_registration: false
  url: https://genomebrowser.wustl.edu/goldenPath/help/blatSpec.html
- id: B2AI_STANDARD:240
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:3
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: NMR Self-defining Text Archive and Retrieval format
  is_open: true
  name: NMR-STAR
  publication: doi:10.1007/s10858-018-0220-3
  purpose_detail: Format and ontology used to represent experiments, spectral and
    derived data, and supporting metadata.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:9
  url: https://bmrb.io/standards/
- id: B2AI_STANDARD:241
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:17
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: nmrML
  has_relevant_organization:
  - B2AI_ORG:21
  is_open: true
  name: nmrML
  purpose_detail: nmrML is an open mark-up language for NMR data. It is currently
    under heavy development and is not yet ready for public use. The development of
    this standard is coordinated by Workpackage 2 of the COSMOS - COordination Of
    Standards In MetabOlomicS Project. COSMOS is a global effort to enable free and
    open sharing of metabolomics data. Coordinated by Dr Christoph Steinbeck of the
    EMBL-European Bioinformatics Institute, COSMOS brings together European data providers
    to set and promote community standards that will make it easier to disseminate
    metabolomics data through life science e-infrastructures. This Coordination Action
    has been financed with 2 million by the European Commission's Seventh Framework
    Programme. The nmrML data standard will be approved by the Metabolomics Standards
    Initiative and was derived from an earlier nmrML that was developed by the Metabolomics
    Innovation Centre (TMIC).
  requires_registration: false
  url: https://nmrml.org/
- id: B2AI_STANDARD:242
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:25
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Observ-Tab format
  is_open: true
  name: Observ-Tab
  publication: doi:10.1002/humu.22070
  purpose_detail: Observ-Tab is a simple spreadsheet format to represent and exchange
    phenotype data.
  requires_registration: false
  url: https://doi.org/10.1002/humu.22070
- id: B2AI_STANDARD:243
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Observational Medical Outcomes Partnership Common Data Model
  formal_specification: https://github.com/OHDSI/CommonDataModel
  has_relevant_organization:
  - B2AI_ORG:115
  - B2AI_ORG:114
  has_training_resource:
  - B2AI_STANDARD:844
  is_open: true
  name: OMOP CDM
  publication: doi:10.3233/978-1-61499-564-7-574
  purpose_detail: The Observational Medical Outcomes Partnership (OMOP) Common Data
    Model is an open community data standard designed to standardize the structure
    and content of observational healthcare data from electronic health records, administrative
    claims, registries, and other sources to enable efficient, large-scale analyses
    that produce reliable evidence. OMOP CDM organizes patient-level data into standardized
    tables including person, visit, condition occurrence, drug exposure, procedure
    occurrence, measurement, observation, and device exposure, with relationships
    linking events to patients and care episodes. The model uses standardized vocabularies
    (SNOMED CT, RxNorm, LOINC, etc.) mapped through the OHDSI Vocabulary for semantic
    interoperability, ensuring consistent representation of clinical concepts across
    diverse healthcare systems. OMOP CDM supports longitudinal patient histories,
    enables reproducible network studies where the same analytic code runs on data
    from multiple institutions without sharing patient-level information, and provides
    the foundation for the OHDSI open-science community's analytical tools including
    cohort definition, characterization, population-level effect estimation, and patient-level
    prediction. The standardized schema facilitates federated learning, multi-site
    clinical research, comparative effectiveness studies, pharmacovigilance, and machine
    learning applications requiring harmonized features across heterogeneous EHR
    systems for training predictive models, phenotyping algorithms, and decision support
    tools.
  related_to:
  - B2AI_STANDARD:733
  - B2AI_STANDARD:692
  - B2AI_STANDARD:695
  - B2AI_STANDARD:698
  - B2AI_STANDARD:703
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:76
  url: https://ohdsi.github.io/CommonDataModel/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:42
    category: B2AI:Application
    name: Multi-Database Patient-Level Risk Prediction with External Validation
    description: OMOP-standardized features and the OHDSI Patient-Level Prediction
      (PLP) pipeline enabled development and extensive external validation of clinical
      risk models across international sites. A Lasso logistic regression model predicting
      symptomatic hemorrhagic transformation after ischemic stroke was developed
      on OMOP-mapped EHR and externally validated across 10 databases spanning the
      US, Europe, and Asia (internal AUC 0.75; mean external AUC approximately 0.71,
      range 0.60-0.78). OMOP's standardized covariate definitions and PLP tooling
      enabled identical feature extraction and cross-site code portability for reproducible,
      multi-database risk modeling.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1371/journal.pone.0226718
  - id: B2AI_APP:146
    category: B2AI:Application
    name: Federated Diabetes Heart Failure Risk Modeling
    description: OMOP CDM enabled federated patient-level prediction for 1-year incident
      heart failure risk in type 2 diabetes patients across five US databases. Shared
      cohort and covariate definitions in OMOP, combined with OHDSI tools, enabled
      reproducible distributed model development and validation using multiple classifiers
      (Lasso logistic regression, Random Forest, Gradient Boosting, XGBoost), achieving
      external AUCs of approximately 0.72-0.80 across validation sites. OMOP's standardization
      facilitated consistent model portability without patient-level data sharing.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1371/journal.pone.0226718
  - id: B2AI_APP:147
    category: B2AI:Application
    name: Hospital Length-of-Stay Prediction with Explainable AI
    description: OMOP v5.3-standardized features from condition, drug, procedure,
      and measurement tables supported operational length-of-stay prediction for
      planned hospital admissions using gradient-boosting methods (XGBoost, LightGBM)
      with SHAP explainability. A single-site OMOP implementation (South Korea) achieved
      internal AUROC up to 0.891, with external validation at a separate OMOP-mapped
      hospital yielding AUROC approximately 0.804. OMOP's standardized feature representation
      allowed reproducible training and external testing for operational forecasting.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2024.08.23.24311950
  - id: B2AI_APP:148
    category: B2AI:Application
    name: Portable NLP-Based Clinical Phenotyping
    description: A portable NLP phenotyping system stored NLP outputs and rule artifacts
      in OMOP tables (notes and annotation mapping) to enable cross-institutional
      reuse. The system combined rule-based extractions with statistical machine
      learning classifiers for phenotype identification (e.g., obesity and comorbidities)
      and demonstrated competitive performance on i2b2 challenge discharge summaries.
      OMOP's common schema for text-derived concepts enabled portable NLP pipelines
      and downstream machine learning for cohort discovery and trial recruitment
      across multiple sites.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1016/j.jbi.2023.104343
  - id: B2AI_APP:149
    category: B2AI:Application
    name: Clinical Knowledge Graphs for Explainable AI
    description: OMOP CDM served as the data source for constructing clinical knowledge
      graphs in FHIR RDF format (FHIR-Ontop-OMOP) to support explainable AI workflows.
      OMOP's standardized vocabularies and relational structure enabled consistent
      semantic linking and query over patient-level data, transforming OMOP tables
      into RDF knowledge graphs that provide semantic features and enable explainable
      AI by linking standardized clinical concepts through ontological relationships.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2024.08.23.24311950
  - id: B2AI_APP:150
    category: B2AI:Application
    name: Process Mining of Clinical Workflows
    description: Methods to derive event logs from OMOP tables (visit, procedure,
      measurement) enabled process mining of inpatient, outpatient, and emergency
      workflows and patient care pathways. Real-world surgical cases at a tertiary
      hospital were analyzed to construct clinical pathway models, and artificial
      neural networks were demonstrated for pathway variance prediction. OMOP CDM
      provided a reproducible source for process-aware analytics and downstream predictive
      tasks from standardized event sequences.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2024.08.23.24311950
  - id: B2AI_APP:151
    category: B2AI:Application
    name: Imaging AI Enablement via MI-CDM Extension
    description: The Medical Imaging CDM (MI-CDM) extends OMOP with imaging metadata
      and feature-provenance tables to link DICOM data and imaging-derived biomarkers
      with clinical OMOP data, enabling multimodal phenotyping and imaging AI workflows.
      A prototype use case demonstrated longitudinal CT lung nodule tracking, and
      implementations for prostate cancer research (ProCAncer-I) captured imaging
      metadata and curation processes. MI-CDM makes image-derived features computable
      within OMOP, supporting reproducible imaging AI pipelines and phenotype definitions
      that include imaging biomarkers.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1007/s10278-024-00982-6
  - id: B2AI_APP:152
    category: B2AI:Application
    name: Cross-Country Model Generalizability and Feature Selection
    description: OMOP-standardized features from EHRs mapped across the US, UK, Finland,
      and Korea enabled cross-site feature evaluation to improve external validity
      of prolonged opioid use prediction after surgery. Independent cross-site feature
      selection workflows using Lasso logistic regression improved generalizability,
      with local AUROC approximately 0.75 and averaged external AUROC approximately
      0.69 after cross-site feature selection. OMOP's consistent feature representation
      enabled generalizable machine learning across countries and healthcare systems.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2024.08.23.24311950
  - id: B2AI_APP:153
    category: B2AI:Application
    name: Oncology-Specific AI with Genomic and Imaging Vocabularies
    description: OMOP oncology extensions incorporating genomic vocabularies (ClinVar,
      CIVic, OncoKB), HemOnc chemotherapy regimen vocabularies, and radiology CDM
      (R-CDM) with RadLex-mapped imaging tables enable cancer-specific AI applications.
      Use cases include case identification from clinical notes using support vector
      machines and tree-based models, predictive modeling with the ATLAS Patient-Level
      Prediction module on genomically-enriched OMOP data, and standardized imaging-AI
      workflows. OMOP's oncology-specific vocabularies and modules facilitate AI
      model development for precision oncology across harmonized multicenter datasets.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.3390/ijms231911834
    - https://doi.org/10.1101/2024.08.23.24311950
- id: B2AI_STANDARD:244
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Observational Medical Outcomes Partnership Common Evidence Model
  formal_specification: https://github.com/OHDSI/CommonEvidenceModel
  has_training_resource:
  - B2AI_STANDARD:844
  is_open: true
  name: OMOP CEM
  publication: doi:10.1007/s40264-014-0189-0
  purpose_detail: An improved replacement to the previously reported LAERTES system.
    One of the initial uses of CEM has been its use in generating lists of negative
    control concepts to be used in empirical calibration.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:76
  url: https://github.com/OHDSI/CommonEvidenceModel/wiki/Postprocessing-Negative-Controls
  has_application:
  - id: B2AI_APP:43
    category: B2AI:Application
    name: Causal Inference and Observational Study AI
    description: OMOP Common Evidence Model is used in AI applications for standardizing
      the representation of evidence from observational studies, enabling machine
      learning models to learn causal relationships from real-world data and generate
      reliable evidence for treatment effectiveness. AI systems leverage CEM's structured
      representation of study designs, populations, exposures, and outcomes to train
      models that estimate treatment effects from observational data, adjust for confounding
      using propensity score methods, and validate predictions through negative controls.
      The model enables AI applications in comparative effectiveness research, pharmacovigilance,
      and evidence synthesis where distinguishing correlation from causation is critical.
      Machine learning approaches use CEM to automate evidence quality assessment
      and synthesize findings across heterogeneous observational studies.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:245
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Open Biomedical Ontology Flat File Format
  formal_specification: https://owlcollab.github.io/oboformat/doc/obo-syntax.html
  is_open: true
  name: OBO
  purpose_detail: The Open Biomedical Ontology (OBO) Flat File Format is a human-readable,
    line-oriented syntax for representing biomedical ontologies that originated with
    the Gene Ontology and has become a widely adopted standard within the OBO Foundry
    community. OBO format provides a simplified, tag-value structure for defining
    ontology terms, their hierarchical relationships (is_a), logical definitions,
    synonyms, cross-references, and metadata, with each term enclosed in a [Term]
    stanza containing fields like id, name, namespace, def (text definition with
    citations), and relationship tags. The format has a defined mapping to OWL (Web
    Ontology Language), allowing OBO files to be converted to OWL/RDF for semantic
    web applications while maintaining a more accessible syntax for biologists and
    curators. OBO format supports rich relationship types beyond simple hierarchies
    (part_of, regulates, develops_from), structured synonym types (exact, broad,
    narrow, related), obsoletion workflows with replaced_by and consider tags, and
    cross-references to external databases. The format is designed for version control
    systems (line-oriented changes), supports modular ontology development through
    import statements, and enables community-driven collaborative ontology construction.
    OBO format files are processed by standard tools (ROBOT, Owltools, OWL API with
    OBO parser) and underpin hundreds of biomedical ontologies including GO, Uberon,
    ChEBI, Disease Ontology, and Cell Ontology. The format balances human readability
    for manual curation with machine-parseability for computational workflows, semantic
    reasoning, data annotation, and integration into knowledge graphs supporting AI/ML
    applications in biomedicine.
  requires_registration: false
  url: https://owlcollab.github.io/oboformat/doc/GO.format.obo-1_4.html
- id: B2AI_STANDARD:246
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:18
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Open mHealth
  formal_specification: https://github.com/openmhealth
  has_relevant_organization:
  - B2AI_ORG:114
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: Open mHealth
  purpose_detail: Open mHealth (OMH) is a comprehensive ecosystem of data standards,
    schemas, and tools designed to make patient-generated health data from mobile
    devices, wearables, and sensors interoperable, meaningful, and actionable across
    healthcare and research applications. The OMH data standards provide JSON-based
    schemas for over 75 common health data types including physical activity (steps,
    distance, calories burned), vital signs (heart rate, blood pressure, body temperature),
    sleep metrics, body composition, medications, survey responses, and environmental
    data. Each schema includes metadata for data provenance (acquisition source, temporal
    relationship, user notes) enabling proper contextualization and interpretation.
    OMH tools include Shimmer for integrating data from diverse sources (Fitbit, Apple
    HealthKit, Google Fit, Withings, iHealth), libraries for data validation and storage,
    visualization components for detecting health patterns, and FHIR integration capabilities
    for aligning mobile health data with electronic health records. The platform supports
    diverse use cases including randomized controlled trials with standardized mobile
    data collection, remote patient monitoring programs, n-of-1 clinical trial analyses,
    machine learning algorithm development on normalized health datasets, and patient-reported
    outcomes integrated with biometric data. OMH is used by over 6,000 developers
    and health organizations including Cornell Tech, Kaiser Permanente, Stanford Medicine,
    UCSF, and Copenhagen Center for Health Technology, enabling research reproducibility
    and clinical care integration by transforming heterogeneous mobile health data
    into a unified, queryable format.
  requires_registration: false
  url: https://www.openmhealth.org/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:44
    category: B2AI:Application
    name: Mobile Health Data Integration for Behavioral AI
    description: Open mHealth schemas are used in AI applications for standardizing
      and integrating data from wearable devices, mobile health apps, and patient-generated
      health data streams for behavioral pattern recognition, activity classification,
      and health prediction models. Machine learning systems leverage Open mHealth's
      JSON-based data schemas to process heterogeneous data from fitness trackers,
      sleep monitors, medication adherence apps, and symptom tracking tools, enabling
      AI models for real-time health monitoring, early disease detection, and personalized
      intervention recommendations. The standardized format facilitates training of
      deep learning models on multi-modal time-series data including heart rate, physical
      activity, sleep patterns, and self-reported symptoms, supporting applications
      in chronic disease management, mental health monitoring, and precision behavioral
      medicine.
    used_in_bridge2ai: false
  - id: B2AI_APP:167
    category: B2AI:Application
    name: LSTM Time-Series Prediction for Athlete Readiness Monitoring
    references:
    - https://doi.org/10.1145/3395035.3425300
    description: Open mHealth data standards are used in sports analytics systems
      for storing and processing athlete self-reported data as training datasets for
      LSTM-based prediction models. In the PMSys elite soccer analytics platform,
      a Reporter App synchronized player self-reported "readiness" assessments to
      an OMH-compliant Data Storage Unit (DSU) as Open mHealth JSON datapoint objects,
      which served as the training dataset for an LSTM model predicting next-day athlete
      readiness. The implementation used a 4-layer LSTM architecture (input layer,
      2 hidden layers, output layer) with sequence length of 36, trained for 30 epochs
      with batch size 4 using the rmsprop optimizer. Data were collected from 19 elite
      soccer players over January-August, with two training regimes evaluated. Training
      on a single athlete's data proved insufficient for reliable predictions, while
      training on team-wide data yielded substantially more accurate predictions with
      clearly visible peaks in readiness patterns. This application demonstrates how
      OMH's standardized JSON datapoint format enables reproducible time-series machine
      learning workflows in sports science, where longitudinal self-reported assessments
      combined with team-level data aggregation support predictive modeling for performance
      optimization and injury prevention.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:247
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:19
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Open Microscopy Environment TIFF specification
  is_open: true
  name: OME-TIFF
  purpose_detail: OME-TIFF (Open Microscopy Environment TIFF) is a file format specification
    that combines standard TIFF image files with OME-XML metadata embedded in the
    TIFF header, enabling storage of complex multi-dimensional microscopy datasets
    including multi-channel fluorescence, z-stacks, time series, and multi-position
    acquisitions. Each OME-TIFF file contains one or more standard TIFF images with
    rich microscopy metadata encoded as OME-XML in the ImageDescription tag of the
    first IFD (Image File Directory), describing acquisition parameters, instrument
    settings, channel information, dimensions, physical pixel sizes, timestamps, and
    experimental context. The format supports large datasets by allowing data to span
    multiple TIFF files while maintaining metadata consistency through the OME-XML
    master file that references all constituent files. OME-TIFF balances the advantages
    of widespread TIFF support in image processing software with the semantic richness
    of OME-XML metadata, making microscopy data accessible to both OME-aware applications
    (Bio-Formats, OMERO, ImageJ/Fiji) and standard image viewers. This dual compatibility
    facilitates data sharing, long-term archiving, interoperability across microscopy
    platforms, and integration into computational workflows including machine learning
    pipelines for image analysis, segmentation, and feature extraction in biological
    imaging applications.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:77
  url: https://docs.openmicroscopy.org/ome-model/5.6.3/ome-tiff/#
- id: B2AI_STANDARD:248
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:19
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Open Microscopy Environment XML format
  is_open: true
  name: OME-XML
  publication: doi:10.1186/gb-2005-6-5-r47
  purpose_detail: OME-XML (Open Microscopy Environment XML) is a comprehensive XML-based
    file format for storing both microscopy image pixels and associated metadata using
    the OME Data Model, a rich schema that describes biological imaging experiments
    with semantic precision. The format encodes multi-dimensional image data (x, y,
    z, channel, time) as Base64-encoded binary pixel arrays within XML elements, alongside
    extensive metadata including instrument configuration (microscope, objectives,
    detectors, light sources), acquisition parameters (exposure times, wavelengths,
    filters), experimental context (annotations, regions of interest, overlays), specimen
    information, and structured annotations. OME-XML uses a controlled vocabulary
    and hierarchical schema (defined by XSD) that ensures consistent representation
    of microscopy concepts across diverse imaging modalities including widefield,
    confocal, super-resolution, high-content screening, and light-sheet microscopy.
    The format serves as the foundation for OME-TIFF and is the native format for
    Bio-Formats library, enabling interoperability across proprietary microscope file
    formats through standardized conversion. OME-XML supports FAIR principles by providing
    rich, machine-readable metadata essential for data sharing, long-term preservation,
    reproducible analysis, and integration into computational workflows including
    image processing pipelines and machine learning applications requiring comprehensive
    contextual information about imaging experiments.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:77
  url: https://docs.openmicroscopy.org/ome-model/5.6.3/ome-xml/
- id: B2AI_STANDARD:249
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:27
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Open Modeling EXchange format
  formal_specification: http://co.mbine.org/specifications/omex.version-1.pdf
  is_open: true
  name: OMEX
  purpose_detail: OMEX (Open Modeling EXchange format) is a ZIP-based container file
    format developed by the COMBINE (Computational Modeling in Biology Network) community
    for bundling all components of a computational modeling and simulation experiment
    into a single, self-contained, exchangeable archive. An OMEX archive contains
    model descriptions (SBML, CellML, NeuroML, etc.), simulation experiment specifications
    (SED-ML for defining simulation protocols, analysis steps, and visualization),
    associated data files (initial conditions, parameters, experimental observations),
    metadata (OMEX Metadata describing provenance, authorship, annotations using RDF),
    and a manifest file that catalogs all contents with their roles and formats. The
    format ensures reproducibility by packaging model structure, simulation configuration,
    analysis workflows, and contextual information together, enabling complete recreation
    of computational experiments across different simulation tools and platforms.
    OMEX supports FAIR principles for computational models through standardized packaging,
    facilitates model exchange and reuse across the systems biology and computational
    physiology communities, enables archiving in model repositories (BioModels, PMR),
    and provides the foundation for reproducible in silico experiments essential for
    model validation, parameter estimation, and integration into larger computational
    workflows including systems biology pipelines and machine learning applications
    for biological modeling.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:19
  url: https://github.com/combine-org/combine-specifications/blob/main/specifications/omex.md
- id: B2AI_STANDARD:250
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Open-Source Integrated Clinical Environment Standard
  formal_specification: https://github.com/mdpnp/mdpnp
  is_open: true
  name: OpenICE
  purpose_detail: OpenICE is an initiative to create a community implementation of
    an Integrated Clinical Environment. The initiative encompasses not only software
    implementation but also an architecture for a wider clinical ecosystem to enable
    new avenues of clinical research. OpenICE seeks to integrate an inclusive framework
    of healthcare devices and clinical applications to existing Healthcare IT ecosystems.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:54
  url: https://www.openice.info/
- id: B2AI_STANDARD:251
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: openEHR Archetype Object Model
  formal_specification: https://specifications.openehr.org/releases/AM/latest/AOM1.4.html
  is_open: true
  name: AOM14
  purpose_detail: This document contains the definitive statement of archetype semantics,
    in the form of an object model for archetypes. The AOM provides a comprehensive
    formal specification enabling software to process archetypes independent of their
    persistent representation (ADL, XML, or other formats). The model defines constraint-based
    domain entities through hierarchical structures alternating between object and
    attribute constraints (C_COMPLEX_OBJECT, C_ATTRIBUTE, C_PRIMITIVE_OBJECT classes).
    It supports archetype specialization with explicit depth tracking, composition
    through archetype slots with assertion-based constraints, and domain-specific
    extensions via C_DOMAIN_TYPE classes. The AOM includes comprehensive primitive
    type constraints (C_DATE, C_TIME, C_STRING, C_INTEGER), reference mechanisms (ARCHETYPE_INTERNAL_REF,
    CONSTRAINT_REF), and assertion capabilities using first-order predicate logic.
    It provides the semantic foundation for archetype-enabled kernels, ADL parsers,
    and archetype validation systems in clinical information modeling, enabling programmatic
    manipulation of constraint structures and serving as the API specification for
    archetype-based software applications.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:79
  url: https://specifications.openehr.org/releases/AM/latest/AOM1.4.html
  has_application:
  - id: B2AI_APP:45
    category: B2AI:Application
    name: Clinical Archetypes and Semantic Interoperability for AI
    description: AOM14 (Archetype Object Model) is used in AI applications for defining
      reusable clinical information models that enable semantic interoperability of
      health data across systems and support consistent feature extraction for machine
      learning. AI systems leverage archetypes to understand the clinical meaning
      and constraints of health data elements, enabling models to learn from data
      collected using openEHR archetypes across different implementations. The standardized
      clinical models support AI applications that require portable feature definitions,
      enable transfer learning across healthcare systems using archetypes, and ensure
      that AI models interpret clinical concepts consistently regardless of underlying
      database structures. Archetypes provide machine-readable clinical semantics
      that improve AI model interpretability and facilitate automated validation of
      model inputs.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:252
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: openEHR Architecture
  formal_specification: https://specifications.openehr.org/releases/BASE/latest/architecture_overview.html
  is_open: true
  name: openEHR
  purpose_detail: openEHR is the name of a technology for e-health, consisting of
    open specifications, clinical models and software that can be used to create standards,
    and build information and interoperability solutions for healthcare. The various
    artefacts of openEHR are produced by the openEHR community and managed by openEHR
    International, an international non-profit organisation originally established
    in 2003 and previously managed by the openEHR Foundation.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:79
  url: https://www.openehr.org/about/what_is_openehr
- id: B2AI_STANDARD:253
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Outbreak Reports and Intervention Studies Of Nosocomial infection
  formal_specification: https://www.ucl.ac.uk/drupal/site_antimicrobial-resistance/sites/antimicrobial-resistance/files/checklist_authors.pdf
  is_open: true
  name: ORION
  publication: doi:10.1016/S1473-3099(07)70082-8
  purpose_detail: The quality of research in hospital epidemiology (infection control)
    must be improved to be robust enough to influence policy and practice. In order
    to raise the standards of research and publication, a CONSORT equivalent for these
    largely quasi-experimental studies has been prepared by the authors of two relevant
    systematic reviews undertaken for the HTA and the Cochrane Collaboration. The
    statement was revised following widespread consultation with learned societies,
    editors of journals and researchers. It consists of a 22 item checklist, and a
    summary table. The emphasis is on transparency to improve the quality of reporting
    and on the use of appropriate statistical techniques.The statement has been endorsed
    and welcomed by a number of professional special interest groups and societies
    including the Association of Medical Microbiologists (AMM), Bristish Society for
    Antimicrobial Chemotherapy (BSAC) and the Infection Control Nurses' Association
    (ICNA) Research and Development Group. Like CONSORT, ORION considers itself a
    work in progress, which requires ongoing dialogue for successful promotion and
    dissemination. The statement is therefore offered for further public discussion
    and journals are encouraged to trial it as part of their reviewing and editing
    process and feedback to the authors.
  requires_registration: false
  url: https://www.ucl.ac.uk/antimicrobial-resistance/reporting-guidelines/orion-statement-consort-equivalent-infection-control-intervention-studies
- id: B2AI_STANDARD:254
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Patient-Centric Integrated Clinical Environment Standard
  formal_specification: https://www.astm.org/f2761-09r13.html
  is_open: false
  name: ICE
  purpose_detail: This standard specifies the characteristics necessary for the safe
    integration of MEDICAL DEVICES and other equipment, via an electronic interface,
    from different MANUFACTURERS into a single medical system for the care of a single
    high acuity PATIENT. This standard establishes requirements for a medical system
    that is intended to have greater error resistance and improved PATIENT safety,
    treatment efficacy and workflow efficiency than can be achieved with independently
    used MEDICAL DEVICES.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:54
  url: https://mdpnp.org/mdice.html
- id: B2AI_STANDARD:255
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:26
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Pfam / HMMER Profile file format
  formal_specification: https://www.genome.jp/tools/motif/hmmformat.htm
  is_open: true
  name: HMMER Format
  purpose_detail: The profile HMM calculated from multiple sequnce alignment data
    in this service is stored in Profile HMM save format (usually with .hmm extension).
    It is an ASCII file containing a lot of header and descriptive records followed
    by large numerical matrix which holds probabilistic model of the motif. The file
    of this format is useful to search against sequnce databases to find out other
    proteins which share the same motif. This HMM file should not be edited manually
    (especially the matrix part) because it contains consistent numerical model as
    a whole.
  requires_registration: false
  url: http://hmmer.org/
- id: B2AI_STANDARD:256
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Phenopackets schema
  formal_specification: https://github.com/phenopackets/phenopacket-schema
  has_relevant_organization:
  - B2AI_ORG:58
  is_open: true
  name: Phenopackets
  purpose_detail: Phenopackets is a schema for exchanging computable
    representations of patient clinical phenotypes, diseases, genomic information,
    and associated metadata in a standardized, machine-readable format. The schema
    uses Protocol Buffers (protobuf) and JSON to encode clinical observations as
    Human Phenotype Ontology (HPO) terms, diseases as OMIM/MONDO identifiers, age
    and temporal information, biosample details, genomic interpretations (variants,
    genes), pedigree information, measurements, medical actions, and provenance metadata.
    Phenopackets supports multiple use cases including individual patient records,
    family pedigrees, and cohort descriptions, enabling seamless exchange of phenotypic
    data between clinical systems, research databases, and diagnostic platforms. The
    format bridges clinical phenotyping with genomics by linking patient phenotypes
    to genetic variants, facilitating variant interpretation, genotype-phenotype correlation
    studies, and rare disease diagnosis. Phenopackets promotes FAIR principles for
    clinical data through standardized vocabularies (HPO, LOINC, UCUM), supports
    federated analysis across rare disease registries without sharing raw patient
    data, enables AI/ML applications for automated phenotyping and diagnostic support,
    and provides the foundation for international data sharing initiatives including
    matchmaking services that connect patients with similar phenotypes for clinical
    research and trial recruitment.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:34
  url: http://phenopackets.org/
  has_application:
  - id: B2AI_APP:46
    category: B2AI:Application
    name: ML-Ready Rare Disease Corpus for Benchmarking and Training
    description: A curated corpus of 4,916 case-level phenopackets spanning 277 Mendelian
      and chromosomal diseases was released explicitly as an analysis-ready, AI-ready
      dataset to enable machine learning analyses of clinical phenotype data. The
      corpus supports gene and disease prioritization pipelines, patient stratification
      studies, and genotype-phenotype correlation analyses by providing standardized
      HPO-encoded phenotypes linked to genomic diagnoses. Phenopackets' uniform format
      enables benchmarking of diagnostic software performance, testing and tuning
      of algorithms on standardized inputs, and evaluation of ML models for rare
      disease genomic diagnostics across diverse disease presentations.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2024.05.29.24308104
  - id: B2AI_APP:154
    category: B2AI:Application
    name: Phenotype-Driven Diagnostic Tools Integration
    description: Widely used phenotype-driven diagnostic and gene/variant prioritization
      tools including Exomiser, LIRICAL, Phen2Gene, and CADA accept Phenopackets
      as standardized input files, enabling integration of patient-level HPO-encoded
      phenotypes with genomic data for AI-enabled diagnostics. Phenopackets facilitate
      computational pipeline integration by providing a consistent format for representing
      clinical observations, supporting aggregation across sites, and enabling interoperability
      with electronic health record (EHR) systems and rare disease registries. This
      standardization allows diagnostic AI tools to process patient data from diverse
      sources using a common schema, improving diagnostic accuracy and enabling federated
      diagnostic workflows.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2021.11.27.21266944
    - https://doi.org/10.1038/s41587-022-01357-4
  - id: B2AI_APP:155
    category: B2AI:Application
    name: Clinical Annotation System with Enhanced ML Performance
    description: SAMS (Symptom Annotation Made Simple), an HPO-integrated clinical
      annotation system that imports and exports Phenopackets, demonstrated measurable
      improvements in data quality that benefit downstream AI/ML pipelines. SAMS
      reported a 10% increase in recall for scientific publication annotations and
      a 20% increase in recall for EHR-derived annotations through improved entity
      linking and standardized symptom representation. These quality improvements
      in structured phenotype data directly enhance the performance of machine learning
      models that rely on high-quality, standardized clinical phenotypes for training
      and inference in diagnostic and research applications.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1093/nar/gkad1005
  - id: B2AI_APP:156
    category: B2AI:Application
    name: Large-Scale Federated Rare Disease Data Sharing
    description: Within the Solve-RD consortium, Phenopackets support standardized
      clinical data sharing for large federated cohorts with 11,349+ individuals
      (growing to over 19,000), facilitating AI/ML-ready data integration for rare
      disease diagnostics and research. The European Joint Programme on Rare Diseases
      (EJP RD) utilizes Phenopackets for federated data discovery across rare disease
      resources while adhering to FAIR principles. Phenopackets enable distributed
      analyses, patient matchmaking services, and cohort identification across international
      sites without requiring patient-level data sharing, providing a computable
      substrate for training and validating machine learning models on multi-institutional
      rare disease datasets.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2021.11.27.21266944
  - id: B2AI_APP:157
    category: B2AI:Application
    name: Genotype-Phenotype Correlation Analytics
    description: The GPSEA (GenoPheno Statistical Evidence Assessment) framework uses
      Phenopackets to represent adverse phenotype data and compute genotype-phenotype
      correlations across 6,613 individuals spanning 85 cohorts. Phenopackets' standardized
      representation of patient-level phenotypes, variants, and biosample information
      enables systematic computational analysis of genotype-phenotype relationships,
      supporting downstream AI/ML analytics for variant interpretation, phenotype
      prediction from genotypes, and discovery of novel genotype-phenotype associations.
      The format facilitates aggregation of case-level data from diverse sources
      into analysis-ready datasets that machine learning models can use to learn
      patterns between genetic variants and clinical presentations.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2024.05.29.24308104
  - id: B2AI_APP:158
    category: B2AI:Application
    name: Patient Matchmaking and Cohort Discovery Systems
    description: Phenopackets serve as the computable representation underlying data-driven
      patient matchmaking services that connect individuals with similar phenotypes
      across international rare disease networks for clinical research, trial recruitment,
      and collaborative diagnosis. The standardized format enables AI-powered differential
      diagnosis systems and automated cohort identification tools that match patient
      phenotypes to disease profiles, identify suitable clinical trial candidates,
      and discover similar cases in distributed databases. Phenopackets' consistent
      encoding of clinical observations using HPO terms allows machine learning algorithms
      to compute phenotypic similarity scores, cluster patients by presentation,
      and support precision medicine initiatives through data-driven patient stratification
      across global rare disease registries.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.17863/cam.87963
    - https://doi.org/10.1038/s41587-022-01357-4
- id: B2AI_STANDARD:257
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: PHIN Messaging Guide for Syndromic Surveillance Emergency Department,
    Urgent Care, Inpatient and Ambulatory Care Settings
  formal_specification: https://www.cdc.gov/nssp/documents/guides/syndrsurvmessagguide2_messagingguide_phn.pdf
  has_relevant_organization:
  - B2AI_ORG:40
  is_open: true
  name: PHIN Guide
  purpose_detail: An HL7 messaging and content reference standard for national, syndromic
    surveillance electronic health record technology certification; A basis for local
    and state syndromic surveillance messaging implementation guides; A resource for
    planning for the increasing use of electronic health record technology and for
    providing details on health data elements that may become a part of future public
    health syndromic surveillance messaging requirements; Optional elements of interest
    for adding laboratory results to syndromic surveillance messages using ORU^R01
    message structure (see details in the PHIN messaging Standard, National Condition
    Reporting case Notification, ORU^R01 message Structure Specification profile,
    Version 2.1, 2014)
  requires_registration: false
  url: https://knowledgerepository.syndromicsurveillance.org/hl7-version-251-phin-messaging-guide-syndromic-surveillance-emergency-department-urgent-care-and
- id: B2AI_STANDARD:258
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  - B2AI_TOPIC:12
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: phyloXML
  is_open: true
  name: phyloXML
  publication: doi:10.1186/1471-2105-10-356
  purpose_detail: phyloXML is a specialized XML-based language for representing phylogenetic
    trees and networks, along with rich associated metadata. It provides standardized
    elements for encoding taxonomic information, gene names, sequence identifiers,
    branch lengths, support values, gene duplication and speciation events, and other
    evolutionary attributes. The extensible structure of phyloXML enables interoperability
    between evolutionary biology and comparative genomics tools, supporting both simple
    and complex tree annotations. Its schema allows for domain-specific extensions
    and integration with other bioinformatics resources, making it a widely adopted
    standard for sharing, visualizing, and analyzing phylogenetic data in research
    and database applications.
  requires_registration: false
  url: http://www.phyloxml.org/
- id: B2AI_STANDARD:259
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:18
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Physical Activity Monitoring for Fitness Wearables Step Counting
  is_open: true
  name: ANSI/CTA-2056
  purpose_detail: ANSI/CTA-2056 is a voluntary consensus standard developed by the
    Consumer Technology Association (CTA) that establishes standardized definitions,
    testing methodologies, and minimum performance criteria for measuring step counting
    accuracy on consumer wearable devices and smartphone applications used for physical
    activity monitoring and health tracking. The standard defines a "step" as a single
    stride during human locomotion and specifies validation protocols using controlled
    laboratory testing on treadmills at various speeds (slow walking 2.0 mph, normal
    walking 3.0 mph, brisk walking 4.0 mph) as well as free-living wear tests in real-world
    conditions to evaluate device performance across diverse user populations and
    activity patterns. ANSI/CTA-2056 requires manufacturers to report step counting
    accuracy as mean absolute percentage error (MAPE) and specifies acceptable error
    thresholds - devices should achieve less than 10% error for controlled walking
    tests and less than 20% error for free-living conditions to meet the standard's
    performance benchmarks. The standard addresses measurement challenges including
    false positive step detection from upper body movements, vehicle vibrations, and
    daily living activities, as well as false negative errors from slow shuffling
    gaits or irregular walking patterns. ANSI/CTA-2056 promotes transparency by requiring
    clear disclosure of test conditions, participant demographics, reference measurement
    systems (ActiGraph accelerometers, manual observation), and statistical analysis
    methods used for validation. The standard facilitates comparability across commercial
    fitness trackers, smartwatches, and pedometer applications from manufacturers
    like Fitbit, Apple Watch, Garmin, and Samsung, supporting consumer informed decision-making,
    clinical research applications requiring validated activity metrics, and workplace
    wellness programs utilizing step count goals for employee health initiatives.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:4
  url: https://webstore.ansi.org/standards/ansi/cta20562016ansi
- id: B2AI_STANDARD:260
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Portable Encapsulated Project specification
  formal_specification: https://github.com/pepkit/pepspec
  is_open: true
  name: PEP
  publication: doi:10.1093/gigascience/giab077
  purpose_detail: The Portable Encapsulated Project (PEP) specification is a standardized,
    machine-readable format for organizing and describing biological sample metadata
    and associated computational analysis configurations in data-intensive bioinformatics
    projects. PEP uses a YAML-based configuration file (project_config.yaml) that
    points to a sample annotation table (typically CSV or TSV) containing sample attributes,
    along with optional subsample tables for complex hierarchical relationships. The
    specification defines a formal structure for representing sample metadata with
    arbitrary attributes, derived attributes computed from other columns, sample modifiers
    for conditional processing, and implied attributes inferred from organizational
    context. PEP supports amendments for alternative project configurations, subanotations
    for linking multiple data files to single samples, and project-level metadata
    including descriptions, keywords, and namespace information. The format is designed
    to make projects portable across compute environments, reproducible through version-controlled
    configurations, and interoperable across analysis tools through a growing ecosystem
    of PEP-compatible software (looper, pypiper, pepr, geofetch, pephub). PEP's structured
    metadata representation enables systematic queries across sample collections, facilitates
    batch processing and parallelization of analyses, supports complex experimental
    designs with multiple data types per sample, and provides a foundation for FAIR
    data practices in genomics and multi-omics research. The specification is particularly
    valuable for managing large-scale projects with hundreds or thousands of samples,
    enabling metadata-driven workflow execution, automated data retrieval, and standardized
    documentation that supports reproducible computational research and machine learning
    applications requiring well-annotated training datasets.
  related_to:
  - B2AI_STANDARD:761
  requires_registration: false
  url: http://pep.databio.org/
- id: B2AI_STANDARD:261
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Portable Format for Bioinformatics
  formal_specification: https://github.com/uc-cdis/pypfb
  is_open: true
  name: PFB
  publication: doi:10.1101/2022.07.19.500678
  purpose_detail: A self-describing serialized format for bulk biomedical data called
    the Portable Format for Biomedical (PFB) data. The Portable Format for Biomedical
    data is based upon Avro and encapsulates a data model, a data dictionary, the
    data itself, and pointers to third party controlled vocabularies. In general,
    each data element in the data dictionary is associated with a third party controlled
    vocabulary to make it easier for applications to harmonize two or more PFB files.
  requires_registration: false
  url: https://anvilproject.org/ncpi/technologies#portable-format-for-bioinformatics-pfb
- id: B2AI_STANDARD:262
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Preferred Reporting Items for Systematic Reviews and Meta-Analyses
  is_open: true
  name: PRISMA
  publication: doi:10.1136/bmj.n71
  purpose_detail: An evidence-based minimum set of items for reporting in systematic
    reviews and meta-analyses.The aim of the PRISMA Statement is to help authors improve
    the reporting of systematic reviews and meta-analyses. We have focused on randomized
    trials, but PRISMA can also be used as a basis for reporting systematic reviews
    of other types of research, particularly evaluations of interventions. PRISMA
    may also be useful for critical appraisal of published systematic reviews, although
    it is not a quality assessment instrument to gauge the quality of a systematic
    review.
  requires_registration: false
  url: https://www.prisma-statement.org/
- id: B2AI_STANDARD:263
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:27
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Protein Data Bank Format
  has_relevant_organization:
  - B2AI_ORG:82
  is_open: true
  name: PDB
  purpose_detail: An exchange format for reporting experimentally determined three-dimensional
    structures of biological macromolecules that serves a global community of researchers,
    educators, and students. The data contained in the archive include atomic coordinates,
    bibliographic citations, primary and secondary structure, information, and crystallographic
    structure factors and NMR experimental data
  requires_registration: false
  url: https://www.cgl.ucsf.edu/chimera/docs/UsersGuide/tutorials/pdbintro.html
- id: B2AI_STANDARD:264
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:26
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Proteomics Standards Initiative Protein Affinity Reagent format
  is_open: true
  name: PSI-PAR
  publication: doi:10.1074/mcp.M900185-MCP200
  purpose_detail: The work on PSI-PAR was initiated as part of the ProteomeBinders
    project and carried out by EMBL-EBI and the PSI-MI work group. The Proteomics
    Standards Initiative (PSI) aims to define community standards for data representation
    in proteomics to facilitate data comparison, exchange and verification. For detailed
    information on all PSI activities, please see PSI Home Page. The PSI-PAR format
    is a standardized means of representing protein affinity reagent data and is designed
    to facilitate the exchange of information between different databases and/or LIMS
    systems. PSI-PAR is not a proposed database structure. The PSI-PAR format consists
    of the PSI-MI XML2.5 schema (originally designed for molecular interactions) and
    the PSI-PAR controlled vocabulary. In addition, PSI-PAR documentation and examples
    are available on this web page. The scope of PSI-PAR is PAR and target protein
    production and characterization.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:41
  url: https://www.psidev.info/psi-par
- id: B2AI_STANDARD:265
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:26
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: PSI GelML
  is_open: true
  name: PSI GelML
  purpose_detail: GelML is a data exchange format for describing the results of gel
    electrophoresis experiments. GelML is developed as a HUPO-PSI working group.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:41
  url: https://www.psidev.info/gelml/1.0
- id: B2AI_STANDARD:266
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: PSI-MI XML
  is_open: true
  name: PSI-MI XML
  purpose_detail: The PSI-MI XML 2.5 is a community standard for molecular interactions
    which has been jointly developed by major data providers (BIND, CellZome, DIP,
    GSK, HPRD, Hybrigenics, IntAct, MINT, MIPS, Serono, U. Bielefeld, U. Bordeaux,
    U. Cambridge, and others).This format is stable and used for several years now
    - published in October 2007 (Broadening the Horizon  Level 2.5 of the HUPO-PSI
    Format for Molecular Interactions; Samuel Kerrien et al. BioMed Central. 2007.),
    it has been adapted for many different usages. It can be used for storing any
    kind of molecular interaction data - complexes and binary interactions not only
    protein-protein interactions, can describe nucleic acids interactions and others
    hierarchical complexes modelling by using interactionRef in participants instead
    of an interactor Data representation in PSI-MI 2.5 XML relies heavily on the use
    of controlled vocabularies. They can be accessed easily via the Ontology Lookup
    Service, PSI-MI, PSI-MOD.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:41
  url: https://www.psidev.info/mif
- id: B2AI_STANDARD:267
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: qcML format
  is_open: true
  name: qcML
  publication: doi:10.1074/mcp.M113.035907
  purpose_detail: An XML format for quality-related data of mass spectrometry and
    other high-throughput experiments. Quality control is increasingly recognized
    as a crucial aspect of mass spectrometry based proteomics. Several recent papers
    discuss relevant parameters for quality control and present applications to extract
    these from the instrumental raw data. What has been missing, however, is a standard
    data exchange format for reporting these performance metrics. We therefore developed
    the qcML format, an XML-based standard that follows the design principles of the
    related mzML, mzIdentML, mzQuantML, and TraML standards from the HUPO-PSI (Proteomics
    Standards Initiative). In addition to the XML format, we also provide tools for
    the calculation of a wide range of quality metrics as well as a database format
    and interconversion tools, so that existing LIMS systems can easily add relational
    storage of the quality control data to their existing schema. We here describe
    the qcML specification, along with possible use cases and an illustrative example
    of the subsequent analysis possibilities. All information about qcML is available
    at http://code.google.com/p/qcml
  requires_registration: false
  url: https://doi.org/10.1074/mcp.M113.035907
- id: B2AI_STANDARD:268
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:33
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Real-time PCR Data Markup Language
  is_open: true
  name: RDML
  purpose_detail: The RDML file format is developed by the RDML consortium (http://www.rdml.org)
    and can be used free of charge. The RDML file format was created to encourage
    the exchange, publication, revision and re-analysis of raw qPCR data. The core
    of an RDML file is an experiment, not a PCR run. Therefore all the information
    is collected which is required to understand an experiment. The structure of the
    file format was inspired by a database structure. In the file are several master
    elements, which are then referred to in other parts of the file. This structure
    allows to reduce the amount of redundant information and encourages the user to
    provide useful information. The Real-time PCR Data Markup Language (RDML) is a
    structured and universal data standard for exchanging quantitative PCR (qPCR)
    data. The data standard should contain sufficient information to understand the
    experimental setup, re-analyse the data and interpret the results. The data standard
    is a compressed text file in Extensible Markup Language (XML) and enables transparent
    exchange of annotated qPCR data between instrument software and third-party data
    analysis packages, between colleagues and collaborators, and between authors,
    peer reviewers, journals and readers. To support the public acceptance of this
    standard, both an on-line RDML file generator is available for end users, as well
    as RDML software libraries to be used by software developers, enabling import
    and export of RDML data files.
  requires_registration: false
  url: http://www.rdml.org
- id: B2AI_STANDARD:269
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Reporting guidelines for randomized controlled trials for livestock
    and food safety
  is_open: true
  name: REFLECT
  publication: doi:10.4315/0362-028x-73.3.579
  purpose_detail: REFLECT (Reporting guidElines For randomized controLled trials for
    livEstoCk and food safeTy) is a specialized, evidence-based minimum set of 22
    reporting items specifically designed to improve the transparency, reproducibility,
    and quality of randomized controlled trials in livestock production, animal health,
    and food safety research. This comprehensive reporting guideline addresses the
    unique challenges and complexities inherent in agricultural and veterinary research,
    including both field trials conducted in real-world farm settings and controlled
    challenge studies in laboratory environments. REFLECT covers essential aspects
    of trial design, implementation, and reporting including study population characteristics,
    randomization procedures, intervention details, outcome measurements, statistical
    analyses, and results presentation. The guideline is specifically tailored for
    trials evaluating therapeutic or preventive interventions that impact production
    outcomes, animal health parameters, and food safety measures. Available in both
    MS Word and PDF formats, REFLECT serves as a dynamic, evolving document that is
    periodically updated as new evidence emerges, helping researchers, editors, reviewers,
    and regulatory agencies ensure that livestock and food safety trials are reported
    with sufficient detail for proper scientific evaluation and evidence-based decision
    making.
  requires_registration: false
  url: https://meridian.cvm.iastate.edu/reflect/
- id: B2AI_STANDARD:270
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Reporting recommendations for tumour Marker prognostic studies
  formal_specification: https://www.equator-network.org/wp-content/uploads/2016/10/REMARK-checklist-for-EQUATOR-website-002.docx
  is_open: true
  name: REMARK
  publication: doi:10.1038/sj.bjc.6602678
  purpose_detail: Despite years of research and hundreds of reports on tumor markers
    in oncology, the number of markers that have emerged as clinically useful is pitifully
    small. Often initially reported studies of a marker show great promise, but subsequent
    studies on the same or related markers yield inconsistent conclusions or stand
    in direct contradiction to the promising results. It is imperative that we attempt
    to understand the reasons why multiple studies of the same marker lead to differing
    conclusions. A variety of methodological problems have been cited to explain these
    discrepancies. Unfortunately, many tumor marker studies have not been reported
    in a rigorous fashion, and published articles often lack sufficient information
    to allow adequate assessment of the quality of the study or the generalizability
    of study results. The development of guidelines for the reporting of tumor marker
    studies was a major recommendation of the National Cancer Institute-European Organisation
    for Research and Treatment of Cancer (NCI-EORTC) First International Meeting on
    Cancer Diagnostics in 2000. As for the successful CONSORT initiative for randomized
    trials and for the STARD statement for diagnostic studies, we suggest guidelines
    to provide relevant information about the study design, preplanned hypotheses,
    patient and specimen characteristics, assay methods, and statistical analysis
    methods. In addition, the guidelines provide helpful suggestions on how to present
    data and important elements to include in discussions. The goal of these guidelines
    is to encourage transparent and complete reporting so that the relevant information
    will be available to others to help them to judge the usefulness of the data and
    understand the context in which the conclusions apply.
  requires_registration: false
  url: https://www.equator-network.org/reporting-guidelines/reporting-recommendations-for-tumour-marker-prognostic-studies-remark/
- id: B2AI_STANDARD:271
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:31
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ReproSchema
  formal_specification: https://github.com/ReproNim/reproschema
  is_open: true
  name: ReproSchema
  purpose_detail: A common schema that encodes how the different elements of assessment
    data and / or the metadata relate to one another.
  requires_registration: false
  url: https://www.repronim.org/reproschema/
- id: B2AI_STANDARD:272
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Sentinel Common Data Model
  formal_specification: https://dev.sentinelsystem.org/projects/SCDM/repos/sentinel_common_data_model/browse
  is_open: true
  name: SCDM
  purpose_detail: A standard data structure that allows Sentinel Data Partners to
    quickly execute distributed programs against local data.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:89
  url: https://www.sentinelinitiative.org/methods-data-tools/sentinel-common-data-model
  has_application:
  - id: B2AI_APP:47
    category: B2AI:Application
    name: FDA Sentinel System and Distributed AI for Drug Safety
    description: Sentinel Common Data Model is used in AI applications for active
      surveillance of medical product safety across distributed healthcare databases,
      enabling privacy-preserving machine learning for adverse event detection and
      risk assessment. AI systems leverage SCDM's standardization of claims data,
      electronic health records, and registries to develop models that identify safety
      signals, predict adverse events, and characterize treatment patterns across
      the FDA Sentinel System network. The common data model enables federated learning
      where AI algorithms execute locally at each data partner without sharing patient-level
      data, supporting rapid assessment of emerging safety concerns. Machine learning
      applications use SCDM to train models on massive populations for rare adverse
      event detection and comparative safety analysis.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:273
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Sequence Alignment/Map Format
  is_open: true
  name: SAM
  publication: doi:10.1093/bioinformatics/btp352
  purpose_detail: The Sequence Alignment/Map (SAM) format is a TAB-delimited text
    format for storing sequence alignments against reference sequences, developed
    as part of the samtools project and maintained by the GA4GH Large Scale Genomics
    work stream. SAM consists of an optional header section (beginning with @ symbols)
    containing metadata about reference sequences, read groups, programs, and comments,
    followed by an alignment section with one line per aligned read. Each alignment
    line contains 11 mandatory fields including query name (QNAME), bitwise FLAG encoding
    mapping properties, reference sequence name (RNAME), 1-based leftmost mapping
    position (POS), mapping quality (MAPQ), CIGAR string describing alignment operations,
    mate pair information (RNEXT, PNEXT, TLEN), sequence (SEQ), and ASCII-encoded
    base quality scores (QUAL). The format supports optional fields as TAG:TYPE:VALUE
    triplets for storing additional information such as edit distance, alternative
    alignments, and aligner-specific metadata. SAM serves as the text-based companion
    to the binary BAM and compressed CRAM formats, with bidirectional conversion tools
    (samtools view) enabling interoperability. The format accommodates various alignment
    types including mapped, unmapped, secondary, supplementary, and chimeric alignments
    with proper-pair relationships. SAM files are human-readable for debugging and
    inspection, widely supported by aligners (BWA, Bowtie2, STAR), variant callers,
    and genomics toolkits, forming the foundation for standardized sequencing data
    exchange in next-generation sequencing workflows.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:34
  url: http://samtools.sourceforge.net/
- id: B2AI_STANDARD:274
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  - B2AI_TOPIC:33
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Sequence Read Archive Metadata XML
  has_relevant_organization:
  - B2AI_ORG:74
  is_open: true
  name: SRA-XML
  publication: doi:10.1093/nar/gkq1019
  purpose_detail: SRA-XML is an XML-based metadata format used by the Sequence Read
    Archive (SRA) and the European Nucleotide Archive (ENA) to describe and exchange
    metadata for raw sequencing data submissions from next-generation sequencing platforms.
    The format captures detailed information about sequencing experiments, sample
    attributes, library preparation, instrument models, and data files, enabling standardized
    data submission, validation, and integration across international repositories.
    SRA-XML supports the reproducibility and discoverability of sequencing datasets
    by providing a structured, machine-readable representation of experimental context
    and provenance, facilitating large-scale genomics research and data sharing.
  requires_registration: false
  url: https://www.ncbi.nlm.nih.gov/sra/docs/submitmeta/
  has_application:
  - id: B2AI_APP:48
    category: B2AI:Application
    name: Sequence Data Metadata Mining and Dataset Discovery
    description: SRA-XML (Sequence Read Archive metadata format) is used in AI applications
      for automated mining of experimental metadata from genomic studies, enabling
      dataset discovery, quality assessment, and training of models that learn from
      experimental design information. Machine learning systems parse SRA metadata
      to identify relevant datasets for specific research questions, predict data
      quality issues before download, and extract experimental conditions that inform
      downstream analysis. AI applications leverage structured metadata to train models
      for automated experiment type classification, sample relationship inference,
      and detection of metadata quality issues. The format enables large-scale meta-analyses
      where AI systems integrate findings across thousands of sequencing experiments
      by understanding their experimental context.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:275
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Simple Omnibus Format in Text
  is_open: true
  name: SOFT
  purpose_detail: Simple Omnibus Format in Text (SOFT) is a line-based, plain text
    format originally developed by NCBI Gene Expression Omnibus (GEO) for batch submission
    and download of genomic data. Though GEO discontinued accepting SOFT submissions
    in early 2024, all records remain available for download in SOFT format. The format
    uses four line types distinguished by first-character markers - caret lines (^)
    indicate entity types (PLATFORM, SAMPLE, SERIES), bang lines (!) specify entity
    attributes as label-value pairs, hash lines (#) describe data table headers, and
    data lines contain tab-delimited table rows. A single SOFT file can concatenate
    multiple Platform (GPL), Sample (GSM), and Series (GSE) records with their data
    tables and descriptive metadata. Platform tables require unique row identifiers
    and trackable sequence identifiers (GenBank/RefSeq accessions, clone IDs, oligo
    sequences) with standard headers for sequences, organism source, and various accession
    types. Sample tables must include ID_REF columns matching Platform identifiers
    and VALUE columns containing normalized, comparable measurements (scaled signals
    for single-channel or log ratios for dual-channel data). The format is compatible
    with common spreadsheet and database applications and supports MIAME standards
    for comprehensive data interpretation.
  requires_registration: false
  url: https://www.ncbi.nlm.nih.gov/geo/info/soft-seq.html
- id: B2AI_STANDARD:276
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:3
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Simplified Molecular Input Line Entry Specification Format
  formal_specification: http://opensmiles.org/opensmiles.html
  is_open: true
  name: SMILES
  purpose_detail: A typographical line notation for specifying chemical structure.
  requires_registration: false
  url: http://opensmiles.org/
- id: B2AI_STANDARD:277
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Simulation Experiment Description Markup Language
  is_open: true
  name: SED-ML
  purpose_detail: SED-ML is an XML-based format for encoding simulation setups, to
    ensure exchangeability and reproducibility of simulation experiments. It follows
    the requirements defined in the MIASE guidelines.
  requires_registration: false
  url: https://sed-ml.org/
- id: B2AI_STANDARD:278
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  - B2AI_TOPIC:35
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: SPDI data model
  has_relevant_organization:
  - B2AI_ORG:74
  is_open: true
  name: SPDI
  publication: doi:10.1093/bioinformatics/btz856
  purpose_detail: Sequence variant data model. Represents all variants as a sequence
    of four operations. Start at the boundary before the first nucleotide in the sequence
    S, advance P nucleotides, delete D nucleotides, then Insert the nucleotides in
    the string.
  requires_registration: false
  url: https://doi.org/10.1093/bioinformatics/btz856
  has_application:
  - id: B2AI_APP:49
    category: B2AI:Application
    name: Unambiguous Variant Representation for Clinical AI
    description: SPDI (Sequence-Position-Deletion-Insertion) notation is used in AI
      applications for creating precise, unambiguous representations of genetic variants
      that eliminate nomenclature inconsistencies affecting model training and clinical
      interpretation. AI systems leverage SPDI's standardized four-element format
      to normalize variant representations from diverse sources (clinical labs, research
      databases, literature), enabling consistent feature engineering for machine
      learning models predicting variant pathogenicity. The notation resolves ambiguities
      in variant left-normalization and reference sequence specification that can
      cause the same biological variant to appear as different features to AI models,
      improving model accuracy and reproducibility. SPDI is particularly valuable
      for clinical AI systems that must reconcile variants reported in different formats
      across laboratories.
    used_in_bridge2ai: false
    references:
    - https://www.ncbi.nlm.nih.gov/variation/notation/
- id: B2AI_STANDARD:279
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Standard EEG Data Structure
  is_open: false
  name: SEDS
  publication: doi:10.1016/j.softx.2021.100933
  purpose_detail: A new and more flexible data structure, named the Standard EEG Data
    Structure (SEDS), was proposed to meet the needs of both small-scale EEG data
    batch processing in single-site studies and large-scale EEG data sharing and analysis
    in single-/multisite studies (especially on cloud platforms).
  requires_registration: false
  url: https://doi.org/10.1016/j.softx.2021.100933
- id: B2AI_STANDARD:280
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - codesystem
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Standard PREanalytical Code
  is_open: true
  name: SPREC
  publication: doi:10.1089/bio.2017.0109
  purpose_detail: The Standard PREanalytical Code (SPREC) is a comprehensive coding
    system developed by the ISBER Biospecimen Science Working Group to standardize
    documentation of pre-analytical variables that affect biospecimen quality. SPREC
    uses a seven-element alphanumeric code to capture critical factors including specimen
    type, primary container, time to processing (warm and cold ischemia), centrifugation
    parameters (speed and temperature), and long-term storage conditions. This systematic
    encoding enables biobanks and biorepositories to consistently track and communicate
    how specimens were collected, processed, and preserved, which directly impacts
    molecular analyte stability and experimental reproducibility. By providing a standardized
    language for pre-analytical variation, SPREC facilitates data harmonization across
    biobanks, enables quality comparisons between specimen collections, supports regulatory
    compliance, and enhances the value of biological samples for translational research.
    The code is particularly valuable for multi-center studies where specimen provenance
    must be tracked and controlled. SPREC version 3.0 covers various specimen types
    including blood, tissue, urine, and other biological fluids, with specific codes
    for derivatives like DNA, RNA, and proteins.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:48
  url: https://www.isber.org/page/SPREC
- id: B2AI_STANDARD:281
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Standards for Reporting of Diagnostic Accuracy Studies
  is_open: true
  name: STARD
  publication: doi:10.1136/bmjopen-2016-012799
  purpose_detail: The objective of the STARD initiative is to improve the accuracy
    and completeness of reporting of studies of diagnostic accuracy, to allow readers
    to assess the potential for bias in the study (internal validity) and to evaluate
    its generalisability (external validity).The STARD statement consist of a checklist
    of 25 items and recommends the use of a flow diagram which describe the design
    of the study and the flow of patients.
  requires_registration: false
  url: https://www.equator-network.org/reporting-guidelines/stard/
- id: B2AI_STANDARD:282
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:26
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Stockholm Multiple Alignment Format
  is_open: true
  name: Stockholm
  purpose_detail: The Stockholm format is a comprehensive system for marking up and
    annotating features in multiple sequence alignments, widely used by HMMER, Pfam,
    and Belvu bioinformatics tools. The format supports detailed annotation of secondary
    structure (SS), surface accessibility (SA), transmembrane regions (TM), posterior
    probability (PP), ligand binding sites (LI), active sites (AS), and intron positions
    (IN). The format includes structured headers with STOCKHOLM 1.0 identifier, sequence
    alignment blocks with name/start-end notation, and comprehensive markup capabilities
    supporting database references, organism classification, phylogenetic trees in
    New Hampshire format, and various structural and functional annotations. It provides
    flexible annotation of aligned sequences with exact one-character-per-column markup
    for positional features and free-text annotations for sequence and file-level
    metadata.
  requires_registration: false
  url: https://sonnhammer.sbc.su.se/Stockholm.html
- id: B2AI_STANDARD:283
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Strengthening the Reporting of Genetic Association studies
  is_open: true
  name: STREGA
  publication: doi:10.1002/gepi.20410
  purpose_detail: Making sense of rapidly evolving evidence on genetic associations
    is crucial to making genuine advances in human genomics and the eventual integration
    of this information in the practice of medicine and public health. Assessment
    of the strengths and weaknesses of this evidence, and hence the ability to synthesize
    it, has been limited by inadequate reporting of results. The STrengthening the
    REporting of Genetic Association studies (STREGA) initiative builds on the Strengthening
    the Reporting of Observational Studies in Epidemiology (STROBE) Statement and
    provides additions to 12 of the 22 items on the STROBE checklist. The additions
    concern population stratification, genotyping errors, modelling haplotype variation,
    Hardy-Weinberg equilibrium, replication, selection of participants, rationale
    for choice of genes and variants, treatment effects in studying quantitative traits,
    statistical methods, relatedness, reporting of descriptive and outcome data, and
    the volume of data issues that are important to consider in genetic association
    studies. The STREGA recommendations do not prescribe or dictate how a genetic
    association study should be designed but seek to enhance the transparency of its
    reporting, regardless of choices made during design, conduct, or analysis.
  requires_registration: false
  url: https://www.equator-network.org/reporting-guidelines/strobe-strega/
- id: B2AI_STANDARD:284
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Strengthening the Reporting of Observational studies in Epidemiology
  is_open: true
  name: STROBE
  publication: doi:10.1016/j.jclinepi.2007.11.008
  purpose_detail: STROBE stands for an international, collaborative initiative of
    epidemiologists, methodologists, statisticians, researchers and journal editors
    involved in the conduct and dissemination of observational studies, with the common
    aim of STrengthening the Reporting of OBservational studies in Epidemiology. The
    STROBE Statement is being endorsed by a growing number of biomedical journals.
    Incomplete and inadequate reporting of research hampers the assessment of the
    strengths and weaknesses of the studies reported in the medical literature. Readers
    need to know what was planned (and what was not), what was done, what was found,
    and what the results mean. Recommendations on the reporting of studies that are
    endorsed by leading medical journals can improve the quality of reporting.Observational
    research comprises several study designs and many topic areas. We aimed to establish
    a checklist of items that should be included in articles reporting such research
    - the STROBE Statement. We considered it reasonable to initially restrict the
    recommendations to the three main analytical designs that are used in observational
    research - cohort, case-control, and cross-sectional studies. We want to provide
    guidance on how to report observational research well. Our recommendations are
    not prescriptions for designing or conducting studies. Also, the checklist is
    not an instrument to evaluate the quality of observational research.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:91
  url: https://www.strobe-statement.org/
- id: B2AI_STANDARD:285
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:27
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Structure Data Format
  is_open: true
  name: SDF
  purpose_detail: SDF (Structure-Data File) is a widely adopted chemical file format
    developed by Molecular Design Limited (MDL, now part of BIOVIA/Dassault Systmes)
    for storing and exchanging molecular structure information along with associated
    chemical, biological, and pharmaceutical data in a single text-based file. The
    format extends the MDL Molfile specification by wrapping individual molecular
    structures with delimited data fields, enabling storage of multiple compounds
    with their properties, experimental measurements, and metadata in a concatenated
    format suitable for chemical databases and compound libraries. Each SDF record
    begins with a Molfile block containing the connection table representation of
    a molecule including atom coordinates (2D or 3D), bond connectivity, atom types,
    bond orders, stereochemistry (wedge/dash bonds, chiral centers), and charge states,
    followed by a data block with tagged fields containing arbitrary properties such
    as chemical names, CAS registry numbers, molecular weights, melting points, solubility
    measurements, biological activity values (IC50, EC50, Ki), ADMET properties, patent
    identifiers, or custom annotations. The format uses delimiter lines ($$$$) to
    separate individual compound records, allowing files to contain from single molecules
    to millions of structures in vendor catalogs, virtual screening libraries, or
    high-throughput screening datasets. SDF supports representation of organic and
    inorganic molecules, polymers, organometallics, and molecular complexes with features
    including isotopic labels, radical species, multi-component systems (salts, mixtures),
    and query structures with variable attachment points or R-groups for combinatorial
    libraries. The text-based format ensures human readability and cross-platform
    compatibility, though binary variants exist for improved storage efficiency. SDF
    files are extensively used in cheminformatics workflows for compound registration
    systems storing chemical inventory with associated metadata, structure-activity
    relationship (SAR) databases linking molecular structures to biological screening
    results, virtual screening campaigns where docking scores and predicted properties
    are stored alongside structures, chemical vendor catalogs (ChemBridge, Enamine,
    ZINC database) distributing purchasable compounds, patent chemistry databases
    documenting novel chemical matter, and chemical property prediction where computed
    descriptors (logP, TPSA, hydrogen bond donors/acceptors) are appended to structures.
    Machine learning applications in drug discovery rely heavily on SDF format for
    training datasets where molecular structures are converted to fingerprints, graph
    representations, or SMILES strings while associated SDF data fields provide activity
    labels, potency values, or ADMET endpoints for supervised learning models predicting
    bioactivity, toxicity, or pharmacokinetic properties. SDF readers and writers
    are implemented in all major cheminformatics toolkits including RDKit (Python),
    Open Babel, ChemAxon, CDK (Chemistry Development Kit), OEChem (OpenEye), and commercial
    platforms (Pipeline Pilot, KNIME, Schrodinger Suite), ensuring interoperability
    across drug discovery informatics pipelines. Limitations include lack of formal
    standardization for data field naming conventions leading to inconsistent property
    tags across different data sources, limited support for complex molecular features
    like polymers or biologics compared to newer formats, and verbosity for large
    datasets where binary or compressed formats may be preferred. Despite these limitations,
    SDF remains the de facto standard for molecular structure exchange in pharmaceutical
    research, academic chemistry, and commercial chemical databases due to its simplicity,
    extensibility, widespread tool support, and ability to bundle molecular structures
    with rich experimental and computed data in a single portable file format.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/Chemical_table_file#SDF
- id: B2AI_STANDARD:286
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: SummarizedExperiment container
  formal_specification: https://github.com/Bioconductor/SummarizedExperiment
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:40
  is_open: true
  name: SummarizedExperiment
  purpose_detail: SummarizedExperiment is a foundational S4 class in the Bioconductor
    ecosystem providing a standardized container for storing and manipulating rectangular
    matrices of experimental results (assays) alongside associated genomic coordinates
    (row data) and sample annotations (column data), enabling coordinated subsetting,
    transformation, and analysis of high-throughput biological experiments. The data
    structure organizes assay data as one or more matrix-like objects (supporting
    dense matrices, sparse matrices, HDF5-backed arrays, and DelayedArray for out-of-memory
    computation) where rows typically represent genomic features (genes, transcripts,
    exons, peaks, CpG sites, genomic ranges) and columns represent biological samples
    (patients, conditions, replicates, time points). Each SummarizedExperiment object
    contains three primary components coordinated through shared row and column names
    ensuring data integrity through guaranteed synchronization during subsetting operations.
    The assays component stores primary experimental measurements as a list of matrices
    (raw counts, normalized counts, log-transformed values, TPM, FPKM, p-values, effect
    sizes) allowing multiple representations of the same experiment in parallel. The
    rowData (or rowRanges for genomic coordinates) component stores feature-level
    metadata as a DataFrame or GRanges object including gene symbols, Ensembl IDs,
    chromosome positions, strand information, gene biotypes, functional annotations,
    statistical summaries, or quality metrics for each measured feature. The colData
    component stores sample-level metadata as a DataFrame capturing experimental design
    variables (treatment groups, batch information, sequencing depth, clinical covariates,
    patient demographics, tissue types) essential for downstream statistical analysis.
    Additional metadata slots include the metadata list for experiment-level information
    (protocol descriptions, processing parameters, references), names for assay identifiers,
    and elementMetadata for slot-specific annotations. SummarizedExperiment supports
    advanced genomic operations through integration with GenomicRanges, enabling coordinate-based
    subsetting to extract features overlapping genomic regions of interest, interval
    arithmetic for computing coverage and enrichment, and annotation with regulatory
    elements or variant positions. The design ensures memory efficiency through support
    for HDF5-backed storage via HDF5Array allowing analysis of datasets exceeding
    RAM capacity, DelayedArray for lazy evaluation deferring computation until results
    are needed, and sparse matrix formats for single-cell data with predominantly
    zero values. Extensions include RangedSummarizedExperiment for genomic coordinate-aware
    operations, SingleCellExperiment adding reduced dimension embeddings and alternative
    experiments for single-cell transcriptomics, MultiAssayExperiment for multi-omics
    integration coordinating RNA-seq, proteomics, metabolomics, and clinical data
    from the same samples, and specialized derivatives for specific assay types (SpatialExperiment
    for spatial transcriptomics, TreeSummarizedExperiment for microbiome phylogenetic
    data). The format is extensively used across Bioconductor packages with over 400
    packages building upon SummarizedExperiment for RNA-seq analysis (DESeq2, edgeR,
    limma), ChIP-seq peak calling, DNA methylation profiling, variant calling workflows,
    single-cell genomics (Seurat conversion, scater, scran), spatial transcriptomics
    (Giotto, Seurat spatial), and machine learning applications where coordinated
    feature-sample matrices with rich annotations enable supervised learning, batch
    correction, dimensionality reduction, and multi-omics integration. The standardized
    structure facilitates interoperability across analysis tools, reproducible research
    through self-contained data objects preserving experimental context, and scalable
    workflows handling datasets from bulk RNA-seq to million-cell single-cell atlases.
  requires_registration: false
  url: https://bioconductor.org/packages/release/bioc/html/SummarizedExperiment.html
- id: B2AI_STANDARD:287
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Synthetic Biology Open Language
  formal_specification: https://sbolstandard.org/#specifications
  is_open: true
  name: SBOL
  purpose_detail: The Synthetic Biology Open Language (SBOL) is a data standard for
    representing synthetic biology designs in a machine-readable format, enabling
    the electronic exchange, storage, and computational manipulation of genetic constructs,
    circuits, and engineered biological systems. SBOL provides a formal, structured
    representation of DNA components, genetic parts (promoters, ribosome binding sites,
    coding sequences, terminators), hierarchical assemblies, functional annotations,
    and design constraints using XML or RDF serialization formats with precisely defined
    semantics based on community ontologies. The standard supports description of
    both natural and synthetic genetic elements with their sequences, functional roles,
    interactions, and provenance metadata, facilitating communication between synthetic
    biologists, genetic engineers, software tools, and biofabrication facilities.
    SBOL enables key workflows including electronic submission of designs to DNA synthesis
    providers and contract manufacturers, storage and versioning of genetic constructs
    in biological design repositories, computational design space exploration and optimization,
    model-based design linking genetic circuits to mathematical models (via integration
    with SBML), and embedding machine-readable genetic designs in scientific publications
    for reproducibility. The standard encompasses multiple levels of abstraction from
    abstract functional specifications to detailed sequence-level implementations,
    supporting combinatorial design where multiple variants are systematically generated,
    modular composition of reusable genetic parts from standardized libraries (BioBricks,
    iGEM Registry), and design-build-test-learn cycles common in synthetic biology
    workflows. SBOL 3.0, the current major version, introduced significant enhancements
    including representation of combinatorial designs with template-based assembly,
    support for protein and chemical entities beyond nucleic acids, constraints and
    design rules for automated validation, measures and parameters for quantitative
    specifications, improved provenance tracking with activities and agents, and topological
    descriptors for complex genetic architectures. Software ecosystem integration
    includes design tools (Benchling, GenoCAD, Eugene, Cello for genetic circuit design),
    simulation platforms coupling SBOL designs with dynamical models, repository systems
    (SynBioHub for federated part repositories, iGEM Registry integration), and workflow
    automation platforms connecting design, synthesis ordering, and experimental validation.
    Applications span metabolic pathway engineering for biomanufacturing, genetic
    circuit design for biosensors and diagnostics, genome-scale engineering projects,
    cell-free synthetic biology systems, and educational synthetic biology where standardized
    parts facilitate teaching and reproducible student projects. SBOL supports collaborative
    research by enabling teams to share complex multi-level designs unambiguously,
    computational design automation where genetic algorithms explore design spaces,
    and quality control ensuring designs meet specifications before expensive synthesis
    and testing, making it essential infrastructure for the reproducible, scalable
    practice of modern synthetic biology.
  requires_registration: false
  url: https://sbolstandard.org/
- id: B2AI_STANDARD:288
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Systems Biology Graphical Notation
  formal_specification: https://sbgn.github.io/downloads/specifications/pd_level1_version2.pdf
  is_open: true
  name: SBGN
  publication: doi:10.1038/nbt.1558
  purpose_detail: Systems Biology Graphical Notation (SBGN) is a standardized visual
    language for representing biological processes and relationships in maps. SBGN
    defines the precise meaning of all graphical symbols through three complementary
    languages - Process Description (PD) for biochemical reactions and molecular interactions,
    Activity Flow (AF) for the flow of information between biochemical entities, and
    Entity Relationship (ER) for relationships between biological entities independent
    of temporal aspects. Each language has specific symbols, syntax rules, and semantic
    definitions to ensure unambiguous interpretation across different tools and users.
    SBGN-ML, an XML-based exchange format, enables storage and transfer of SBGN diagrams
    between software applications, supported by the standard LibSBGN library. The
    notation is widely adopted in major biological databases including Reactome, PANTHER
    Pathway, BioModels, and Pathway Commons. Multiple software tools support SBGN
    diagram creation and editing (CellDesigner, Newt Editor, VANTED/SBGN-ED, PathVisio,
    yEd), format conversion from KEGG, BioPAX, and SBML, and visualization of pathway
    models, facilitating standardized communication of complex biological knowledge
    across the systems biology community.
  requires_registration: false
  url: https://sbgn.github.io/
- id: B2AI_STANDARD:289
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Systems Biology Markup Language
  has_relevant_organization:
  - B2AI_ORG:19
  is_open: true
  name: SBML
  publication: doi:10.1515/jib-2017-0081
  purpose_detail: The Systems Biology Markup Language (SBML) is a comprehensive XML-based
    exchange format specifically designed for representing computational models of
    biological processes in a machine-readable, standardized form. Developed and maintained
    by the international SBML community, SBML enables interoperability between diverse
    systems biology software tools by providing a common model representation language
    that eliminates translation errors and ensures consistent model interpretation
    across different platforms. While SBML excels at representing biochemical reaction
    networks, metabolic pathways, gene regulatory networks, and signal transduction
    cascades, its flexible architecture supports modeling of various biological phenomena
    from molecular to cellular scales. The format includes sophisticated features
    for describing reaction kinetics, species concentrations, compartmentalization,
    parameter sensitivity, and dynamic behaviors through mathematical expressions
    and differential equations. SBML's modular design supports extensions for specialized
    modeling requirements including spatial modeling, flux balance analysis, and multi-scale
    simulations. Supported by over 270 software tools, SBML serves as the de facto
    standard for systems biology model exchange, enabling reproducible research, model
    sharing, database integration, and collaborative development in computational
    systems biology.
  related_to:
  - B2AI_STANDARD:829
  requires_registration: false
  url: http://sbml.org/
- id: B2AI_STANDARD:290
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Systems Biology Results Markup Language
  is_open: false
  name: SBRML
  purpose_detail: Systems Biology has benefited tremendously from the development
    and use of SBML, which is a markup language to specify models composed of molecular
    species, and their interactions (including reactions). SBML is a common format
    that many systems biology software understand and thus it has become the way in
    which models are shared and communicated. Despite the popularity of SBML, that
    has resulted in many models being available in electronic format, there is currently
    no standard way of communicating the results of the operations carried out with
    such models (e.g. simulations). Here we propose a new markup language which is
    complementary to SBML and which is intended to specify results from operations
    carried out on models SBRML. In fact, this markup language is useful also to communicate
    experimental data as long as it is possible to express the data in terms of a
    reference SBML model. Thus SBRML is a means of specifying quantitative results
    in the context of a systems biology model.
  requires_registration: false
  url: https://sbrml.sourceforge.net/SBRML/Welcome.html
- id: B2AI_STANDARD:291
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Tabix index file format
  formal_specification: https://samtools.github.io/hts-specs/tabix.pdf
  is_open: true
  name: Tabix
  publication: doi:10.1093/bioinformatics/btq671
  purpose_detail: A tab-delimited genome position index file format. The format can
    handle individual chromosomes up to 512 Mbp (2^29 bases) in length.
  requires_registration: false
  url: http://www.htslib.org/doc/tabix.html
- id: B2AI_STANDARD:292
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Taxonomic Concept Transfer Schema
  formal_specification: https://github.com/tdwg/tcs
  is_open: true
  name: TCS
  purpose_detail: The development of an abstract model for a taxonomic concept, which
    can capture the various models represented and understood by the various data
    providers, is central to this project. This model is presented as an XML schema
    document that is proposed as a standard to allow exchange of data between different
    data models. It aims to capture data as understood by the data owners without
    distortion, and facilitate the query of different data resources according to
    the common schema model. The TCS schema was conceived to allow the representation
    of taxonomic concepts as defined in published taxonomic classifications, revisions
    and databases. As such, it specifies the structure for XML documents to be used
    for the transfer of defined concepts. Valid transfer documents may either explicitly
    detail the defining components of taxon concepts, transfer GUIDs referring to
    defined taxon concepts (if and when these are available) or a mixture of the two.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:93
  url: http://www.tdwg.org/standards/117
  has_application:
  - id: B2AI_APP:50
    category: B2AI:Application
    name: Taxonomic Data Integration and Species Classification AI
    description: TCS (Taxonomic Concept Schema) is used in AI applications for managing
      taxonomic concepts and classifications, enabling machine learning models to
      understand species relationships, resolve taxonomic synonyms, and integrate
      biodiversity data across different classification systems. AI systems leverage
      TCS to train models that automatically map species names across taxonomies,
      predict taxonomic placement of newly discovered organisms, and reconcile conflicting
      classifications. The schema supports natural language processing applications
      that extract taxonomic information from scientific literature, automated quality
      control of species occurrence records, and machine learning approaches to phylogenetic
      inference. TCS enables AI systems to reason about taxonomic hierarchies and
      evolutionary relationships when analyzing biological data.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:293
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Taxonomic Databases Working Group Standards Documentation Standard
  is_open: true
  name: TDWG SDS
  purpose_detail: This document defines how TDWG standards should be presented. Each
    standard is a logical directory or folder containing two or more files - a cover
    page outlining basic meta data for the standard and one or more normative files
    specifying the standard itself. Rules are specified for the naming of standards
    and files. Human readable files should be in English, follow basic layout principles
    and be marked up in XHTML. The legal statements that all documents must contain
    are defined.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:93
  url: http://www.tdwg.org/standards/147
- id: B2AI_STANDARD:294
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:3
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ToxML standard
  is_open: true
  name: ToxML
  publication: doi:10.1080/1062936X.2013.783506
  purpose_detail: ToxML is an open data exchange standard that allows the representation
    and communication of toxicological and related data in a well-structured electronic
    format.
  requires_registration: false
  url: https://doi.org/10.1080/1062936X.2013.783506
- id: B2AI_STANDARD:295
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Transparent Reporting of Evaluations with Nonrandomized Designs
  formal_specification: https://www.cdc.gov/trendstatement/pdf/trendstatement_TREND_Checklist.pdf
  is_open: true
  name: TREND
  publication: doi:10.2105/ajph.94.3.361
  purpose_detail: Evidence-based public health decisions are based on evaluations
    of intervention studies with randomized and nonrandomized designs. Transparent
    reporting is crucial for assessing the validity and efficacy of these intervention
    studies, and, it facilitates synthesis of the findings for evidence-based recommendations.
    Therefore, the mission of the Transparent Reporting of Evaluations with Nonrandomized
    Designs (TREND) group is to improve the reporting standards of nonrandomized evaluations
    of behavioral and public health intervention
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:12
  url: https://www.cdc.gov/trendstatement/index.html
- id: B2AI_STANDARD:296
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Trusted Instant Messaging Plus Applicability Statement
  formal_specification: https://directtrust.app.box.com/s/p3vp3g4bv52cyi4nbpyfpdal6t7z2qdz
  has_relevant_organization:
  - B2AI_ORG:26
  is_open: true
  name: TIM+
  purpose_detail: Trusted Instant Messaging (TIM+) defines a protocol that facilitates
    real-time communication and incorporates secure messaging concepts to ensure information
    is transmitted securely between known, trusted entities both within and across
    enterprises. TIM+ will determine the availability or presence of trusted endpoints
    and support text-based communication and file transfers.
  requires_registration: false
  url: https://directtrust.app.box.com/s/p3vp3g4bv52cyi4nbpyfpdal6t7z2qdz
- id: B2AI_STANDARD:297
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: TumorML standard
  is_open: true
  name: TumorML
  publication: doi:10.1145/2544063.2544064
  purpose_detail: Originally developed as part of the FP7 Transatlantic Tumor Model
    Repositories project, TumorML has been developed as an XML-based domain-specific
    vocabulary that includes elements from existing vocabularies, to deal with storing
    and transmitting existing cancer models among research communities.
  requires_registration: false
  url: https://doi.org/10.1145/2544063.2544064
- id: B2AI_STANDARD:298
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - codesystem
  concerns_data_topic:
  - B2AI_TOPIC:3
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Unique Ingredient Identifiers
  is_open: true
  name: UNII
  purpose_detail: Unique Ingredient Identifiers (UNIIs) are alphanumeric codes generated
    by the FDA's Global Substance Registration System (GSRS) to uniquely identify
    substances based on their scientific identity characteristics using ISO 11238
    data elements. Each UNII is derived from a substance's molecular structure and/or
    descriptive information, ensuring that the same substance always receives the
    same identifier regardless of when or where it is registered in the regulatory
    lifecycle. UNIIs enable efficient and accurate exchange of substance information
    across regulatory submissions, product labeling, adverse event reporting, and
    drug databases without ambiguity from varying nomenclature or trade names. The
    system covers diverse substance types including chemical compounds, proteins,
    nucleic acids, polymers, mixtures, structurally diverse materials, and specified
    substances (salts, stereoisomers, defined mixtures). UNIIs are generated at any
    time in the regulatory process and do not imply FDA review or approval. The GSRS
    database provides searchable access to UNIIs along with associated substance names,
    codes (CAS, INN, EC), structural representations, and attribute data. UNIIs are
    extensively used in SPL (Structured Product Labeling) documents, NDC (National
    Drug Code) directory, FAERS (FDA Adverse Event Reporting System), and international
    regulatory systems, facilitating interoperability between FDA databases and harmonization
    with global substance identification standards.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:31
  url: https://precision.fda.gov/uniisearch
- id: B2AI_STANDARD:299
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  - markuplanguage
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:13
  - B2AI_TOPIC:35
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Variant Call Format
  formal_specification: https://github.com/samtools/hts-specs
  has_relevant_organization:
  - B2AI_ORG:117
  is_open: true
  name: VCF
  purpose_detail: Variant Call Format (VCF) is a text file format (most likely stored
    in a compressed manner). It contains meta-information lines, a header line, and
    then data lines each containing information about a position in the genome.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:34
  url: https://en.wikipedia.org/wiki/Variant_Call_Format
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:51
    category: B2AI:Application
    name: Variant Pathogenicity Prediction and Genomic AI
    description: VCF format is essential for AI applications in genomics, particularly
      for training machine learning models to predict variant pathogenicity, disease
      associations, and functional impacts. Deep learning models parse VCF files to
      extract genetic variants and their annotations for tasks such as rare disease
      diagnosis, cancer genome interpretation, and pharmacogenomic prediction. AI
      systems leverage VCF's structured representation of genomic variants, quality
      scores, and population frequencies to build models that integrate variant-level
      data with clinical phenotypes, enabling precision medicine applications and
      automated variant classification according to ACMG guidelines.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:300
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Variant Effect Predictor format
  has_relevant_organization:
  - B2AI_ORG:29
  is_open: true
  name: VEP
  purpose_detail: A text format devised by Ensembl for the eponymous Variant Effect
    Predictor tool.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:124
  url: https://useast.ensembl.org/info/docs/tools/vep/vep_formats.html
- id: B2AI_STANDARD:301
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Variation Representation Specification
  has_relevant_organization:
  - B2AI_ORG:117
  is_open: true
  name: VRS
  publication: doi:10.1016/j.xgen.2021.100027
  purpose_detail: The Variation Representation Specification (VRS, pronounced verse)
    is a standard developed by the Global Alliance for Genomics and Health (GA4GH)
    to facilitate and improve sharing of genetic information. The Specification consists
    of a JSON Schema for representing many classes of genetic variation, conventions
    to maximize the utility of the schema, and a Python implementation that promotes
    adoption of the standard.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:34
  url: https://vrs.ga4gh.org/en/latest/index.html
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:52
    category: B2AI:Application
    name: Standardized Variant Representation for ML Interoperability
    description: VRS (Variation Representation Specification) is used in AI applications
      to create unambiguous, computationally accessible representations of genetic
      variants that enable machine learning model interoperability across different
      genomic databases and variant calling pipelines. AI systems leverage VRS to
      normalize variant representations, resolve ambiguities in variant nomenclature,
      and create consistent feature representations for training models that predict
      variant effects, interpret clinical significance, or perform variant prioritization.
      VRS's precise coordinate system and allele representation enable AI models to
      correctly match variants across different genome builds, integrate data from
      multiple sources, and generate reproducible predictions that can be shared across
      institutions and validated against standardized variant benchmarks.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:302
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:35
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: VarioML format
  formal_specification: https://github.com/VarioML/VarioML
  is_open: true
  name: VarioML
  purpose_detail: A Locus-specific Database (LSDB) describes the variants discovered
    on a single gene or members of a gene family and other related functional elements.
    LSDBs are curated by experts on their respective loci, and as such are typically
    the best resources of such information available. But LSDBs vary widely in format
    and completeness, making data integration and exchange among them difficult and
    time-consuming. To address these difficulties, the VarioML format has been developed
    for the full range of variation data use-cases, providing semantically well-defined
    components which can be easily composed to fit specific needs. Using VarioML,
    data owners can now efficiently enable the integration, federation, and exchange
    of their variant data. The discoverabiliaty, extensibility, and quality of variation
    data is immediately enhanced. Critical new avenues of research and knowledge discovery
    are opened, as data using the VarioML standard can be integrated with the global
    library of purely genetic data. VarioML is a central prerequisite for effective
    modelling of phenotype data and genotype-to-phenotype relationships. It removes
    the long-standing technical obstacles to the effective passing of variant data
    from discovery laboratories into the biomedical database world. Now all that is
    needed is the broad participation of the genotype-to-phenotype research community.
  requires_registration: false
  url: https://github.com/VarioML/VarioML
- id: B2AI_STANDARD:303
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Vocabulary Maintenance Standard
  has_relevant_organization:
  - B2AI_ORG:93
  is_open: true
  name: VMS
  purpose_detail: The Vocabulary Maintenance Standard (VMS) is a TDWG (Biodiversity
    Information Standards) specification that defines formal procedures for maintaining
    and evolving controlled vocabularies used in biodiversity informatics. Unlike
    traditional standards that remain relatively static, VMS recognizes that vocabulary
    standards must adapt incrementally to meet evolving community needs without requiring
    the full standards ratification process for every change. The specification categorizes
    types of vocabulary modifications (additions, modifications, deprecations) and
    establishes governance mechanisms for implementing these changes through designated
    vocabulary maintenance groups. VMS defines roles and responsibilities for stakeholders
    including vocabulary maintainers, Interest Groups, and the broader TDWG community,
    specifying how change proposals are submitted, reviewed, and approved. The standard
    ensures that vocabulary evolution remains transparent, traceable, and backward-compatible
    where possible, while maintaining the stability needed for production systems.
    VMS applies to major TDWG vocabularies including Darwin Core terms and is essential
    for maintaining semantic interoperability across biodiversity data systems as
    scientific understanding and data practices evolve.
  requires_registration: false
  url: https://www.tdwg.org/standards/vms/
- id: B2AI_STANDARD:304
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - diagnosticinstrument
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Voice Handicap Index
  is_open: false
  name: VHI
  publication: doi:10.1044/1058-0360.0603.66
  purpose_detail: A statistically robust Voice Handicap Index (VHI). An 85-item version
    of this instrument was administered to 65 consecutive patients seen in the Voice
    Clinic at Henry Ford Hospital. The data were subjected to measures of internal
    consistency reliability and the initial 85-item version was reduced to a 30-item
    final version. This final version was administered to 63 consecutive patients
    on two occasions in an attempt to assess test-retest stability, which proved to
    be strong. The findings of the latter analysis demonstrated that a change between
    two administrations of 18 points represents a significant shift in psychosocial
    function.
  requires_registration: false
  url: https://doi.org/10.1044/1058-0360.0603.66
- id: B2AI_STANDARD:305
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: W3C HCLS Dataset Description
  is_open: true
  name: HCLS
  purpose_detail: Access to consistent, high-quality metadata is critical to finding,
    understanding, and reusing scientific data. This document describes a consensus
    among participating stakeholders in the Health Care and the Life Sciences domain
    on the description of datasets using the Resource Description Framework (RDF).
    This specification meets key functional requirements, reuses existing vocabularies
    to the extent that it is possible, and addresses elements of data description,
    versioning, provenance, discovery, exchange, query, and retrieval.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:99
  url: https://www.w3.org/TR/hcls-dataset/
- id: B2AI_STANDARD:306
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Wiggle Format
  is_open: true
  name: WIG
  purpose_detail: Wiggle files and its bedgraph variant allow you to plot quantitative
    data as either shades of color (dense mode) or bars of varying height (full and
    pack mode) on the genome.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:119
  url: https://genome.ucsc.edu/goldenPath/help/wiggle.html
- id: B2AI_STANDARD:307
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - audiovisual
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:19
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Zeiss LSM series confocal microscope image
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:19
  is_open: false
  name: LSM
  purpose_detail: A proprietary image format based on TIFF.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:101
  url: https://openwetware.org/wiki/Dissecting_LSM_files
- id: B2AI_STANDARD:308
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  concerns_data_topic:
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Adaptive Multi-Rate Speech Codec
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:49
  is_open: true
  name: AMR
  purpose_detail: Audio data compression scheme optimized for speech coding, adopted
    in October 1998 as the standard speech codec by 3GPP (3d Generation Partnership
    Project) and now widely used in GSM (Global System for Mobile Communications).
  requires_registration: false
  url: https://www.loc.gov/preservation/digital/formats/fdd/fdd000254.shtml
- id: B2AI_STANDARD:309
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Analyst native acquisition file format
  is_open: false
  name: WIFF
  publication: doi:10.1074/mcp.R112.019695
  purpose_detail: Mass spectra output format used by AB SCIEX intstruments.
  requires_registration: false
  url: https://doi.org/10.1074/mcp.R112.019695
- id: B2AI_STANDARD:310
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Analyze file format
  formal_specification: https://analyzedirect.com/documents/AD_AnalyzeImage75_File_Format.pdf
  is_open: true
  name: HDR/IMG
  purpose_detail: Analyze 7.5 can store voxel-based volumes and consists of two files
    - One file with the actual data in a binary format with the filename extension
    .img and another file (header with filename extension .hdr) with information about
    the data such as voxel size and number of voxel in each dimension.
  requires_registration: false
  url: https://analyzedirect.com/documents/AD_AnalyzeImage75_File_Format.pdf
- id: B2AI_STANDARD:311
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Apache Arrow
  formal_specification: https://github.com/apache/arrow
  has_relevant_organization:
  - B2AI_ORG:5
  is_open: true
  name: Arrow
  purpose_detail: A language-independent columnar memory format for flat and hierarchical
    data, organized for efficient analytic operations on modern hardware like CPUs
    and GPUs. The Arrow memory format also supports zero-copy reads for lightning-fast
    data access without serialization overhead.
  requires_registration: false
  url: https://arrow.apache.org/
- id: B2AI_STANDARD:312
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Apache Avro
  formal_specification: https://github.com/apache/avro
  has_relevant_organization:
  - B2AI_ORG:5
  is_open: true
  name: Avro
  purpose_detail: Avro is a row-oriented remote procedure call and data serialization
    framework developed within Apache's Hadoop project. It uses JSON for defining
    data types and protocols, and serializes data in a compact binary format.
  requires_registration: false
  url: https://avro.apache.org/
- id: B2AI_STANDARD:313
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Asset Description Metadata Schema
  has_relevant_organization:
  - B2AI_ORG:99
  is_open: true
  name: ADMS
  purpose_detail: ADMS is a profile of DCAT, used to describe semantic assets (or
    just 'Assets'), defined as highly reusable metadata (e.g. xml schemata, generic
    data models) and reference data (e.g. code lists, taxonomies, dictionaries, vocabularies)
    that are used for eGovernment system development.
  requires_registration: false
  url: https://www.w3.org/TR/vocab-adms/
- id: B2AI_STANDARD:314
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Audio Interchange File Format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:49
  is_open: true
  name: AIFF
  purpose_detail: File format for sound that wraps various sound bitstreams, ranging
    from uncompressed waveform to MIDI.
  requires_registration: false
  url: https://www.loc.gov/preservation/digital/formats/fdd/fdd000005.shtml
- id: B2AI_STANDARD:315
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:15
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Audio Video Interleave digital multimedia container format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:49
  is_open: true
  name: AVI
  purpose_detail: AVI files can contain both audio and video data in a file container
    that allows synchronous audio-with-video playback.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/Audio_Video_Interleave
- id: B2AI_STANDARD:316
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Bitmap format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:19
  is_open: true
  name: BMP
  purpose_detail: 'BMP (Bitmap) is a raster graphics file format developed by Microsoft for storing
    device-independent bitmaps (DIBs) in Windows and OS/2 systems. The format consists
    of a file header, DIB header (with multiple versions: BITMAPCOREHEADER, BITMAPINFOHEADER,
    BITMAPV4HEADER, BITMAPV5HEADER), optional color palette, and pixel array storing
    uncompressed or RLE-compressed bitmap data. BMP supports 1, 4, 8, 16, 24, and
    32 bits per pixel, accommodating indexed color (palettes), RGB color, and alpha
    channels for transparency. The format uses little-endian byte ordering and stores
    pixel rows bottom-to-top by default, with each row padded to 4-byte boundaries.
    BMP files can contain color profiles (ICC) for color management and support various
    compression methods including BI_RGB (uncompressed), BI_RLE4/BI_RLE8 (run-length
    encoding), and BI_BITFIELDS (custom RGB bit masks). The simplicity, widespread
    familiarity, and open format specification make BMP common in Windows applications,
    image processing software, and scientific imaging. While BMP files are typically
    large due to minimal compression, they compress efficiently with lossless algorithms
    (ZIP, RAR). BMP is used in GDI (Graphics Device Interface) subsystems, icons (ICO),
    cursors (CUR), and as an intermediate format for image processing. Software support
    is extensive: Adobe Photoshop, GIMP, Microsoft Office, browsers (Chrome, Edge),
    and programming libraries across platforms. In AI/ML imaging applications, BMP
    serves as a raw, lossless format for medical imaging (preserving pixel accuracy),
    training data preparation (avoiding compression artifacts), image annotation workflows,
    and intermediate processing steps where format simplicity facilitates pixel-level
    manipulation for computer vision tasks, though typically converted to compressed
    formats for efficient storage and transmission.'
  requires_registration: false
  url: https://en.wikipedia.org/wiki/BMP_file_format
- id: B2AI_STANDARD:317
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Cap'n'Proto
  formal_specification: https://github.com/capnproto/capnproto
  is_open: true
  name: Cap'n'Proto
  purpose_detail: Capn Proto is an insanely fast data interchange format and capability-based
    RPC system. Think JSON, except binary. Or think Protocol Buffers, except faster.
  requires_registration: false
  url: https://capnproto.org/
- id: B2AI_STANDARD:318
  category: B2AI_STANDARD:DataStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CellML language
  is_open: true
  name: CellML
  publication: doi:10.1186/1471-2105-11-178
  purpose_detail: CellML language is an open standard based on the XML markup language.
    CellML is being developed by the Auckland Bioengineering Institute at the University
    of Auckland and affiliated research groups. The purpose of CellML is to store
    and exchange computer-based mathematical models. CellML allows scientists to share
    models even if they are using different model-building software. It also enables
    them to reuse components from one model in another, thus accelerating model building.
  requires_registration: false
  url: https://www.cellml.org/
- id: B2AI_STANDARD:319
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: citation-file-format
  formal_specification: https://github.com/citation-file-format/citation-file-format
  is_open: true
  name: CFF
  purpose_detail: A file format for providing citation metadata for software or datasets
    in plaintext files that are easy to read by both humans and machines.
  requires_registration: false
  url: https://citation-file-format.github.io/
- id: B2AI_STANDARD:320
  category: B2AI_STANDARD:DataStandard
  collection:
  - workflowlanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Common Workflow Language
  formal_specification: https://github.com/common-workflow-language/common-workflow-language
  is_open: true
  name: CWL
  publication: doi:10.48550/arXiv.2105.07028
  purpose_detail: specification for describing data analysis workflows and tools
  requires_registration: false
  url: https://www.commonwl.org/
- id: B2AI_STANDARD:321
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CURATE(D) Checklists
  formal_specification: https://docs.google.com/document/d/1RWt2obXOOeJRRFmVo9VAkl4h41cL33Zm5YYny3hbPZ8/edit
  is_open: true
  name: CURATE(D)
  purpose_detail: The CURATE(D) steps are a teaching and representation tool. This
    model is useful for onboarding data curators and orienting researchers preparing
    to share their data. It serves as a demonstration for the type of work involved
    in robust data curation, and was created to fit within institution-specific data
    repository workflows.
  requires_registration: false
  url: https://datacurationnetwork.org/outputs/workflows/
- id: B2AI_STANDARD:322
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DAMA Data Management Body of Knowledge
  has_relevant_organization:
  - B2AI_ORG:22
  is_open: false
  name: DAMA-DMBOK
  purpose_detail: Reference guide for processes, best practices, and principles in
    data management.
  requires_registration: true
  url: https://www.dama.org/cpages/body-of-knowledge
- id: B2AI_STANDARD:323
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Data Catalog Vocabulary
  has_relevant_organization:
  - B2AI_ORG:99
  is_open: true
  name: DCAT
  purpose_detail: An RDF vocabulary designed to facilitate interoperability between
    data catalogs published on the Web.
  requires_registration: false
  url: https://www.w3.org/TR/vocab-dcat-1/
- id: B2AI_STANDARD:324
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DataCite Metadata Schema
  formal_specification: https://schema.datacite.org/meta/kernel-4.3/metadata.xsd
  is_open: true
  name: DataCite
  purpose_detail: The DataCite Metadata Schema is a list of core metadata properties
    chosen for the accurate and consistent identification of a resource for citation
    and retrieval purposes, along with recommended use instructions. The resource
    that is being identified can be of any kind, but it is typically a dataset. We
    use the term dataset in its broadest sense. We mean it to include not only numerical
    data, but any other research data outputs.
  requires_registration: false
  url: https://schema.datacite.org/
- id: B2AI_STANDARD:325
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Dataset Cards
  is_open: true
  name: Dataset Cards
  purpose_detail: 'Dataset Cards are structured documentation templates for describing machine
    learning datasets, introduced by Google and adopted widely in the ML community,
    particularly through Hugging Face''s Datasets library. Dataset Cards provide standardized
    metadata capturing dataset purpose, structure, creation methodology, intended uses,
    limitations, and ethical considerations. The documentation includes: dataset summary
    and description; language coverage and data sources; supported ML tasks and features;
    dataset structure (splits, configurations, data fields); creation process (annotations,
    quality control, collection methodology); considerations for using the data (social
    impact, biases, privacy concerns); additional resources (papers, repositories,
    licenses). Dataset Cards promote responsible AI by making dataset characteristics
    transparent, helping practitioners assess fitness for purpose, understand potential
    biases, and evaluate dataset limitations before use. The structured format enables
    dataset discovery on ML platforms, facilitates reproducibility by documenting provenance,
    and encourages accountability in dataset creation and curation. Hugging Face hosts
    100,000+ dataset cards in their Hub, standardizing documentation for NLP, computer
    vision, audio, and multimodal datasets. Dataset Cards complement Model Cards by
    extending documentation principles to training data, addressing data quality, collection
    practices, annotation procedures, and data ethics. In AI/ML pipelines, Dataset
    Cards support informed dataset selection, bias mitigation through transparency,
    reproducible research by documenting data versions, and regulatory compliance (AI
    Act, GDPR) by clarifying data provenance, consent, and usage restrictions.'
  requires_registration: false
  url: https://huggingface.co/docs/datasets/dataset_card
- id: B2AI_STANDARD:326
  category: B2AI_STANDARD:DataStandard
  collection:
  - datasheets
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Datasheets for Datasets
  is_open: true
  name: Datasheets
  publication: https://arxiv.org/abs/1803.09010
  purpose_detail: '...we propose that every dataset be accompanied with a datasheet
    that documents its motivation, composition, collection process, recommended uses,
    and so on.'
  requires_registration: false
  url: https://arxiv.org/abs/1803.09010
- id: B2AI_STANDARD:327
  category: B2AI_STANDARD:DataStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: eXtensible Graph Markup and Modeling Language
  is_open: true
  name: XGMML
  purpose_detail: eXtensible Graph Markup and Modeling Language is an XML application
    based on GML which is used for graph description. XGMML uses tags to describe
    nodes and edges of a graph. The purpose of XGMML is to make possible the exchange
    of graphs between differents authoring and browsing tools for graphs.
  requires_registration: false
  url: http://xml.coverpages.org/xgmml.html
- id: B2AI_STANDARD:328
  category: B2AI_STANDARD:DataStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FAIR Principles for Research Software
  has_relevant_organization:
  - B2AI_ORG:83
  is_open: true
  name: FAIR4RS
  publication: doi:10.1038/s41597-022-01710-x
  purpose_detail: Simple and research software appropriate goalposts to inform those
    who publish and/or preserve research software.
  requires_registration: false
  url: https://www.rd-alliance.org/group/fair-research-software-fair4rs-wg/outcomes/fair-principles-research-software-fair4rs
- id: B2AI_STANDARD:329
  category: B2AI_STANDARD:DataStandard
  collection:
  - codesystem
  - deprecated
  concerns_data_topic:
  - B2AI_TOPIC:6
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Federal Information Processing System Codes for States and Counties
  formal_specification: https://transition.fcc.gov/oet/info/maps/census/fips/fips.txt
  is_open: true
  name: FIPS
  purpose_detail: FIPS codes are numbers which uniquely identify geographic areas.
    As of database version 55, FIPS has been merged with the Geographic Names Information
    System (GNIS).
  requires_registration: false
  url: https://transition.fcc.gov/oet/info/maps/census/fips/fips.txt
- id: B2AI_STANDARD:330
  category: B2AI_STANDARD:DataStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FieldML
  is_open: true
  name: FieldML
  publication: doi:10.1007/s11517-013-1097-7
  purpose_detail: FieldML is a declarative language for building hierarchical models
    represented by generalized mathematical fields. FieldML is developed as a data
    model and accompanying API. Find out more about the FieldML API, where to get
    the latest release and how to contribute to its development. FieldML is a declarative
    language for representing hierarchical models using generalized mathematical fields.
    FieldML can be used to represent the dynamic 3D geometry and solution fields from
    computational models of cells, tissues and organs. It enables model interchange
    for the bioengineering and general engineering analysis communities. Example uses
    are models of tissue structure, the distribution of proteins and other biochemical
    compounds, anatomical annotation, and other biological annotation.
  requires_registration: false
  url: https://doi.org/10.1007/s11517-013-1097-7
- id: B2AI_STANDARD:331
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Free Lossless Audio Codec
  formal_specification: https://xiph.org/flac/format.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:49
  is_open: true
  name: FLAC
  purpose_detail: FLAC (Free Lossless Audio Codec) is an audio coding format for lossless
    compression of digital audio.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/FLAC
- id: B2AI_STANDARD:332
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Frictionless Data Package
  formal_specification: https://github.com/frictionlessdata/specs
  is_open: true
  name: Frictionless
  purpose_detail: 'Frictionless Data Package is a specification for packaging, describing, and
    sharing datasets in a lightweight, standardized container format developed by
    Open Knowledge Foundation. A Data Package consists of a descriptor (datapackage.json)
    containing metadata about the package and its resources, along with the data files
    themselves (local or remote). The specification defines comprehensive metadata
    including: package title, description, licenses, version, sources, contributors,
    and keywords; resource information for each data file (path/URL, format, schema,
    encoding); Table Schema for tabular data defining field names, types, constraints,
    and relationships; and optional goodtables validation rules. Frictionless supports
    FAIR data principles by ensuring datasets are Findable (rich metadata), Accessible
    (standard formats, clear paths), Interoperable (JSON-based descriptor, Table Schema),
    and Reusable (license, provenance, structure documentation). The ecosystem includes
    libraries for Python, JavaScript, R, Ruby, PHP, and command-line tools for validation,
    conversion, and publishing. Frictionless integrates with data portals (CKAN, Dataverse),
    notebooks (Jupyter), and analysis tools, enabling seamless dataset exchange and
    processing. Extensions support specialized data types (geospatial, time series,
    budget data) and validation frameworks. In AI/ML workflows, Frictionless Data
    Packages standardize dataset distribution for reproducible research, provide machine-readable
    schemas for automated data validation and ingestion, enable data versioning and
    provenance tracking for ML pipelines, and facilitate dataset documentation complementing
    model training by ensuring data quality, consistency, and interpretability across
    collaborative ML projects.'
  requires_registration: false
  url: https://specs.frictionlessdata.io/data-package/
- id: B2AI_STANDARD:333
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: GA4GH serviceinfo
  has_relevant_organization:
  - B2AI_ORG:34
  is_open: true
  name: serviceinfo
  purpose_detail: Provides a way for an API to expose a set of metadata to help discovery
    and aggregation of services via computational methods.
  requires_registration: false
  url: https://github.com/ga4gh-discovery/ga4gh-service-info
- id: B2AI_STANDARD:334
  category: B2AI_STANDARD:DataStandard
  collection:
  - workflowlanguage
  concerns_data_topic:
  - B2AI_TOPIC:20
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Galaxy workflow Format 2
  formal_specification: https://github.com/galaxyproject/gxformat2
  is_open: true
  name: gxformat2
  purpose_detail: A schema moving Galaxy's workflow description language toward standards
    such as the Common Workflow Language.
  related_to:
  - B2AI_STANDARD:766
  requires_registration: false
  url: https://github.com/galaxyproject/gxformat2
- id: B2AI_STANDARD:335
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:14
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Geographic Names Information System Feature IDs
  formal_specification: https://www.usgs.gov/u.s.-board-on-geographic-names/download-gnis-data
  has_relevant_organization:
  - B2AI_ORG:98
  is_open: true
  name: GNIS ID
  purpose_detail: The Geographic Names Information System (GNIS) is the Federal and
    national standard for geographic nomenclature. The U.S. Geological Survey's National
    Geospatial Program developed the GNIS in support of the U.S. Board on Geographic
    Names as the official repository of domestic geographic names data, the official
    vehicle for geographic names use by all departments of the Federal Government,
    and the source for applying geographic names to Federal electronic and printed
    products.
  requires_registration: false
  url: https://www.usgs.gov/us-board-on-geographic-names/domestic-names
- id: B2AI_STANDARD:336
  category: B2AI_STANDARD:DataStandard
  collection:
  - codesystem
  concerns_data_topic:
  - B2AI_TOPIC:6
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Geopolitical Entities, Names, and Codes
  formal_specification: https://evs.nci.nih.gov/ftp1/GENC/
  is_open: true
  name: GENC
  purpose_detail: The GENC Standard specifies a profile of ISO 3166 codes for the
    representation of names of countries and their subdivisions.
  requires_registration: false
  url: https://www.dni.gov/index.php/who-we-are/organizations/ic-cio/ic-cio-related-menus/ic-cio-related-links/ic-technical-specifications/geopolitical-entities-names-and-codes
- id: B2AI_STANDARD:337
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Graph Markup Language
  is_open: true
  name: GraphML
  purpose_detail: An XML file format and language for describing graphs.
  related_to:
  - B2AI_STANDARD:829
  requires_registration: false
  url: http://graphml.graphdrawing.org/specification.html
- id: B2AI_STANDARD:338
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Graphics Interchange Format
  is_open: true
  name: GIF
  purpose_detail: 'Graphics Interchange Format (GIF) is a bitmap image format developed by CompuServe in 1987 that supports lossless LZW compression, up to 256 colors from a 24-bit RGB palette, transparency, and multi-frame animation with per-frame delay controls. GIF became widely adopted on the early web for its efficient compression and cross-platform compatibility, and remains ubiquitous for short animated loops and simple graphics with well-defined edges. The format supports two versions (GIF87a and GIF89a), with the latter adding animation delays, transparent backgrounds, and application-specific metadata. While the LZW patent controversy drove development of PNG as an alternative, GIF''s animation capabilities and "hot" data accessibility (frames stored sequentially without random access penalties) maintain its relevance for social media reactions, educational demonstrations, and web UI elements. Modern applications leverage GIF''s deterministic looping (via Netscape Application Block extension) and universal browser support, though video formats like WebP and MP4 increasingly replace GIF for better compression ratios. The format''s 256-color limitation makes it suitable for logos, diagrams, and pixel art but less appropriate for photographs or gradients, where dithering techniques are often applied. GIF files consist of a logical screen descriptor, optional global color table, image descriptors with optional local color tables, and LZW-encoded pixel data stored in sub-blocks. For AI/ML workflows, GIF serves as a compact format for visualizing model predictions over time series, displaying attention mechanisms frame-by-frame, and sharing animated training/validation metrics without requiring video codec dependencies.'
  requires_registration: false
  url: https://en.wikipedia.org/wiki/GIF
- id: B2AI_STANDARD:339
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HDF5 format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:16
  has_relevant_organization:
  - B2AI_ORG:116
  is_open: true
  name: HDF5
  purpose_detail: HDF5 is a data model, library, and file format for storing and managing
    data. It supports an unlimited variety of datatypes, and is designed for flexible
    and efficient I/O and for high volume and complex data. HDF5 is portable and is
    extensible, allowing applications to evolve in their use of HDF5. The HDF5 Technology
    suite includes tools and applications for managing, manipulating, viewing, and
    analyzing data in the HDF5 format.
  requires_registration: false
  url: https://www.hdfgroup.org/solutions/hdf5/
  used_in_bridge2ai: true
- id: B2AI_STANDARD:340
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  - B2AI_TOPIC:48
  contribution_date: '2025-05-29'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Hierarchical Data Modeling Framework for Modern Science Data Standards
  formal_specification: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8500680/
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:16
  is_open: true
  name: HDMF
  publication: doi:10.1109/bigdata47090.2019.9005648
  purpose_detail: 'HDMF is a hierarchical data modeling framework for modern science
    data standards. It separates data standardization into three main components:
    (1) data modeling and specification, (2) data I/O and storage, and (3) data interaction
    and data APIs. HDMF provides object mapping infrastructure to insulate and integrate
    these components, supporting flexible development of data standards and extensions,
    optimized storage backends, and data APIs. It offers advanced data I/O functionality
    for iterative data write, lazy data load, parallel I/O, and modular data storage.
    HDMF is particularly used to design NWB 2.0, a data standard for neurophysiology
    data.'
  related_to:
  - B2AI_STANDARD:339
  - B2AI_STANDARD:379
  requires_registration: false
  url: https://hdmf-common-schema.readthedocs.io/en/latest/format.html
- id: B2AI_STANDARD:341
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ISA-Tab format
  formal_specification: https://github.com/ISA-tools/ISAdatasets
  has_relevant_organization:
  - B2AI_ORG:47
  is_open: true
  name: ISA-Tab
  purpose_detail: General-purpose ISA-Tab file format - an extensible, hierarchical
    structure that focuses on the description of the experimental metadata (i.e. sample
    characteristics, technology and measurement types, sample-to-data relationships).
  requires_registration: false
  url: https://isa-specs.readthedocs.io/en/latest/isatab.html
- id: B2AI_STANDARD:342
  category: B2AI_STANDARD:DataStandard
  collection:
  - codesystem
  concerns_data_topic:
  - B2AI_TOPIC:6
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ISO 3166 Country Codes
  formal_specification: https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes
  has_relevant_organization:
  - B2AI_ORG:49
  is_open: true
  name: ISO 3166
  purpose_detail: The purpose of ISO 3166 is to define internationally recognized
    codes of letters and/or numbers that we can use when we refer to countries and
    their subdivisions. However, it does not define the names of countries  this
    information comes from United Nations sources (Terminology Bulletin Country Names
    and the Country and Region Codes for Statistical Use maintained by the United
    Nations Statistics Divisions).
  requires_registration: false
  url: https://www.iso.org/iso-3166-country-codes.html
- id: B2AI_STANDARD:343
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ISO 8601 Date and time format
  has_relevant_organization:
  - B2AI_ORG:49
  is_open: true
  name: ISO 8601
  purpose_detail: A way of presenting dates and times that is clearly defined and
    understandable to both people and machines.
  requires_registration: false
  url: https://www.iso.org/iso-8601-date-and-time-format.html
- id: B2AI_STANDARD:344
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Joint Photographic Experts Group Format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:19
  has_relevant_organization:
  - B2AI_ORG:116
  is_open: true
  name: JPEG
  purpose_detail: A method of lossy compression for digital images.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/JPEG
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:53
    category: B2AI:Application
    name: Medical Image Compression and Transfer Learning
    description: JPEG format is used in AI applications for efficient storage and
      transmission of medical images, particularly in telepathology, dermatology AI,
      and mobile diagnostic applications where bandwidth and storage constraints are
      critical. Deep learning models are trained on JPEG-compressed whole slide images,
      dermoscopic images, and fundus photographs to perform tasks such as cancer detection,
      skin lesion classification, and diabetic retinopathy screening. While DICOM
      remains the standard for radiology, JPEG enables AI deployment in resource-constrained
      settings and supports transfer learning from natural image datasets (ImageNet)
      to medical imaging domains. AI researchers must account for JPEG compression
      artifacts when training models, and recent work explores AI-optimized compression
      techniques that preserve diagnostically relevant features.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:345
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: JSON-schema
  formal_specification: https://github.com/json-schema-org/json-schema-spec
  is_open: true
  name: JSON-schema
  purpose_detail: 'JSON Schema is a vocabulary for annotating and validating JSON documents,
    providing a declarative format for describing the structure, constraints, and
    semantics of JSON data. As an IETF standard (draft specifications progressing
    toward RFC status), JSON Schema defines a JSON-based format for specifying the
    expected shape of JSON data, including data types, required properties, value
    ranges, string patterns, array constraints, and object structures. The schema
    itself is expressed in JSON, enabling meta-schema validation and recursive definitions.
    JSON Schema supports multiple vocabularies including Core, Validation, Hyper-Schema
    for hypermedia, and Format for semantic validation. Widely adopted across industries
    with 60+ million weekly downloads, it powers API documentation (OpenAPI/Swagger),
    configuration validation, code generation, form generation in UIs, and data interchange
    contracts. Major implementations exist in JavaScript, Python, Java, Go, and 40+
    other languages, with extensive tooling ecosystem including validators, generators,
    linters, and editors. JSON Schema enables confident JSON data handling through
    streamlined testing, seamless data exchange via shared understanding, and clear
    documentation for developer collaboration. In AI/ML contexts, JSON Schema validates
    training data structures, API request/response payloads, configuration files for
    ML pipelines, and ensures data quality for machine learning workflows by catching
    inconsistencies and schema violations at ingestion time.'
  related_to:
  - B2AI_STANDARD:761
  requires_registration: false
  url: https://json-schema.org/
- id: B2AI_STANDARD:346
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Knowledge Graph Exchange
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:21
  is_open: true
  name: KGX
  purpose_detail: 'Knowledge Graph Exchange (KGX) is a standardized graph-oriented data format
    for exchanging knowledge graphs, developed by the Biolink community to facilitate
    interoperability between biomedical knowledge bases. KGX provides both a formal
    specification and a Python toolkit (kgx library) for transforming, validating,
    and exchanging knowledge graphs conforming to the Biolink Model. The format represents
    graphs as nodes (entities with identifiers, categories, and properties) and edges
    (relationships with predicates, subject/object references, and provenance), serialized
    in JSON, TSV, or RDF formats. KGX ensures semantic alignment by enforcing Biolink
    Model categories (e.g., Gene, Disease, Pathway) and predicates (e.g., causes,
    treats, interacts_with), enabling consistent cross-database integration. The toolkit
    supports graph transformations (merging, filtering, mapping), format conversions
    (Neo4j, RDF, GraphML), and validation against Biolink Model constraints. KGX is
    foundational for Translator knowledge graphs, integrating data from 150+ biomedical
    sources including Monarch Initiative, NCATS Biomedical Data Translator, and Clinical
    Data Commons. In AI/ML applications, KGX-formatted knowledge graphs power link
    prediction for drug repurposing, knowledge graph embeddings for biomedical entity
    representation learning, reasoning algorithms for hypothesis generation, and multi-modal
    knowledge integration combining genomics, phenotypes, pathways, and clinical data
    to support precision medicine and systems biology research.'
  related_to:
  - B2AI_STANDARD:783
  requires_registration: false
  url: https://github.com/biolink/kgx/blob/master/specification/kgx-format.md
- id: B2AI_STANDARD:347
  category: B2AI_STANDARD:DataStandard
  collection:
  - markuplanguage
  - standards_process_maturity_development
  - implementation_maturity_pilot
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: LinkML
  formal_specification: https://github.com/linkml/linkml
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:41
  - B2AI_SUBSTRATE:6
  has_relevant_organization:
  - B2AI_ORG:116
  is_open: true
  name: LinkML
  purpose_detail: LinkML is a flexible modeling language that allows you to author
    schemas in YAML that describe the structure of your data. Additionally, it is
    a framework for working with and validating data in a variety of formats (JSON,
    RDF, TSV), with generators for compiling LinkML schemas to other frameworks.
  requires_registration: false
  url: https://linkml.io/linkml
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:54
    category: B2AI:Application
    name: AI-Ready Data Schema Design and Validation
    description: LinkML is used in AI applications to define machine-readable data
      schemas that enable automated data validation, transformation, and integration
      for machine learning pipelines. AI systems leverage LinkML schemas to ensure
      data quality and consistency across heterogeneous biomedical datasets, automatically
      generate data loaders and validators for ML frameworks, and create semantic
      mappings that allow AI models to understand relationships between data elements.
      LinkML's ability to compile to multiple formats (JSON Schema, SHACL, SQL DDL)
      makes it particularly valuable for building reproducible AI/ML workflows where
      data provenance and validation are critical.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:348
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MathML
  has_relevant_organization:
  - B2AI_ORG:99
  is_open: true
  name: MathML
  purpose_detail: A product of the W3C Math Working Group, MathML is a low-level specification
    for describing mathematics as a basis for machine to machine communication which
    provides a much needed foundation for the inclusion of mathematical expressions
    in Web pages. It is also important in publishing workflows for science and technology
    and wherever mathematics has to be handled by software.
  requires_registration: false
  url: https://www.w3.org/Math/whatIsMathML.html
- id: B2AI_STANDARD:349
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Microsoft Excel spreadsheet container file
  formal_specification: https://www.iso.org/standard/71691.html
  has_relevant_organization:
  - B2AI_ORG:56
  is_open: false
  name: xlsx
  purpose_detail: Format used by Microsoft Excel spreadsheet software.
  requires_registration: false
  url: https://www.iso.org/standard/71691.html
- id: B2AI_STANDARD:350
  category: B2AI_STANDARD:DataStandard
  collection:
  - modelcards
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Model Cards
  formal_specification: https://github.com/tensorflow/model-card-toolkit/blob/master/model_card_toolkit/schema/v0.0.2/model_card.schema.json
  has_relevant_organization:
  - B2AI_ORG:37
  is_open: true
  name: Model Cards
  publication: https://arxiv.org/abs/1810.03993
  purpose_detail: Structured documentation detailing performance characteristics of
    machine learning models.
  requires_registration: false
  url: https://modelcards.withgoogle.com/about
- id: B2AI_STANDARD:351
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MPEG-1 Audio Layer 3 | MPEG-2 Audio Layer 3
  formal_specification: https://www.iso.org/standard/26797.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:49
  is_open: true
  name: MP3
  purpose_detail: MP3 (formally MPEG-1 Audio Layer III or MPEG-2 Audio Layer III)
    is a coding format for digital audio.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/MP3
- id: B2AI_STANDARD:352
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:15
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MPEG-4 Part 14 digital multimedia container format
  formal_specification: https://www.loc.gov/preservation/digital/formats/fdd/fdd000155.shtml
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:49
  has_relevant_organization:
  - B2AI_ORG:49
  is_open: true
  name: MPEG-4
  purpose_detail: A digital multimedia container format most commonly used to store
    video and audio, but it can also be used to store other data such as subtitles
    and still images. Like most modern container formats, it allows streaming over
    the Internet.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/MP4_file_format
- id: B2AI_STANDARD:353
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:3
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Network Common Data Form
  formal_specification: https://www.astm.org/e1947-98r14.html
  has_relevant_organization:
  - B2AI_ORG:8
  is_open: true
  name: netCDF
  purpose_detail: A standardized format for chromatographic data representation.
  requires_registration: false
  url: https://www.unidata.ucar.edu/software/netcdf/
- id: B2AI_STANDARD:354
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Neural Network Exchange Format
  formal_specification: https://github.com/KhronosGroup/NNEF-Tools
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:27
  - B2AI_SUBSTRATE:33
  - B2AI_SUBSTRATE:42
  is_open: true
  name: NNEF
  purpose_detail: An exchange format for neural network models produced using Torch,
    Caffe, TensorFlow, Theano, Chainer, Caffe2, PyTorch, or MXNet.
  related_to:
  - B2AI_STANDARD:816
  - B2AI_STANDARD:831
  - B2AI_STANDARD:834
  requires_registration: false
  url: https://www.khronos.org/nnef
  has_application:
  - id: B2AI_APP:55
    category: B2AI:Application
    name: Neural Network Exchange for Medical Device AI
    description: NNEF (Neural Network Exchange Format) is used in biomedical AI for
      deploying models on medical devices and embedded systems where hardware-specific
      optimization and standardized model representation are critical. Medical device
      manufacturers leverage NNEF to ensure neural network models can be optimized
      for diverse hardware accelerators (DSPs, NPUs, custom ASICs) commonly used in
      portable medical equipment, bedside monitors, and point-of-care devices. The
      format enables vendor-independent model deployment, facilitates regulatory approval
      by providing clear model specifications, and supports hardware efficiency optimizations
      necessary for real-time inference in resource-constrained medical devices. NNEF
      is particularly valuable for AI-enabled medical devices where power consumption,
      latency, and deterministic behavior are critical requirements.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:355
  category: B2AI_STANDARD:DataStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: OGC and ISO Observations and Measurements standard in XML
  has_relevant_organization:
  - B2AI_ORG:49
  is_open: true
  name: OMXML
  purpose_detail: This standard specifies an XML implementation for the OGC and ISO
    Observations and Measurements (O&M) conceptual model (OGC Observations and Measurements
    v2.0 also published as ISO/DIS 19156), including a schema for Sampling Features.
    This encoding is an essential dependency for the OGC Sensor Observation Service
    (SOS) Interface Standard. More specifically, this standard defines XML schemas
    for observations, and for features involved in sampling when making observations.
    These provide document models for the exchange of information describing observation
    acts and their results, both within and between different scientific and technical
    communities.
  requires_registration: false
  url: https://www.ogc.org/standards/om
- id: B2AI_STANDARD:356
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ogg Speex Audio Format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:49
  is_open: true
  name: OGG Speex
  purpose_detail: File format and bitstream encoding for for spoken content, targeted
    at a wide range of devices other than mobile phones.
  requires_registration: false
  url: https://speex.org/docs/manual/speex-manual/node8.html
- id: B2AI_STANDARD:357
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Open Neural Network Exchange
  formal_specification: https://github.com/onnx/onnx/blob/main/docs/IR.md
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:28
  is_open: true
  name: ONNX
  purpose_detail: ONNX (Open Neural Network Exchange) is an open, cross-platform
    intermediate representation for machine learning and deep learning models, developed
    collaboratively by Microsoft, Facebook/Meta, and the Linux Foundation AI & Data
    initiative. It defines a standardized, extensible computation graph format that
    enables models trained in one frameworksuch as PyTorch, TensorFlow, Keras, scikit-learn,
    or MXNetto be exported, optimized, and deployed in another environment without
    framework-specific runtime dependencies. ONNX represents both the model architecture
    (nodes as operators, edges as tensors) and learned parameters (weights, biases)
    in a protocol-buffer-based binary format (`.onnx` files), with a human-readable
    protobuf text representation for debugging and versioning. The core specification
    includes an operator set (ONNX opset) versioned incrementally to add new operators
    and preserve backward compatibility, covering a wide range of operations including
    convolutional layers, recurrent units, attention mechanisms, normalization layers,
    activation functions, and custom operators. ONNX's intermediate representation
    decouples the training environment from the inference environment, allowing data
    scientists to train models using high-level APIs and researchers to experiment
    with novel architectures, while deployment engineers optimize and integrate those
    models into production systems using ONNX Runtime or hardware-specific runtimes
    (NVIDIA TensorRT, Intel OpenVINO, Apple Core ML, Qualcomm SNPE, AWS SageMaker
    Neo, Google Edge TPU) that perform graph-level optimizations, quantization, pruning,
    and kernel fusion for target hardware. ONNX Runtime, the reference cross-platform
    inference engine, provides optimized execution on CPUs, GPUs, and specialized
    accelerators, with extensive support for quantization (INT8, INT4, mixed-precision),
    model compression, and dynamic shape inference, making it a cornerstone of MLOps
    pipelines and production AI systems. In biomedical and clinical AI, ONNX is particularly
    valuable for deploying models across heterogeneous hospital IT infrastructures,
    medical imaging workstations, portable diagnostic devices, and federated learning
    networks where framework dependencies and model portability are critical concerns.
    Research groups use ONNX to share pretrained models (e.g., medical image segmentation,
    clinical NLP, drug discovery) via model zoos and repositories (ONNX Model Zoo,
    Hugging Face Hub, BioImage Model Zoo), ensuring reproducibility and facilitating
    transfer learning without requiring the original training codebase. Regulatory
    submissions (FDA, CE mark) benefit from ONNX's stable, versioned model representation
    that can be validated and audited independently of the training framework, with
    explicit documentation of operator semantics and numerical behavior. ONNX supports
    advanced model architectures including transformers, graph neural networks, and
    recurrent networks, as well as pre- and post-processing operations (image resizing,
    normalization, tokenization) embedded directly in the computation graph, reducing
    preprocessing discrepancies between training and inference. The ONNX ecosystem
    includes converters for major frameworks (PyTorch's `torch.onnx.export`, TensorFlow's
    `tf2onnx`, scikit-learn's `sklearn-onnx`), visualization tools (Netron for graph
    inspection), optimization tools (ONNX Simplifier, ONNX Optimizer), and validation
    utilities (ONNX Checker) for ensuring model correctness and compliance with the
    specification. ONNX's adoption in clinical AI products enables vendor-neutral
    deployment, allowing models to run on diverse medical device platforms and hospital
    PACS systems, supporting interoperability in multi-site clinical trials and federated
    learning initiatives where different institutions may use different hardware and
    software stacks.
  requires_registration: false
  url: https://onnx.ai/
  has_application:
  - id: B2AI_APP:56
    category: B2AI:Application
    name: Cross-Platform Medical AI Model Interoperability
    description: ONNX (Open Neural Network Exchange) is used in biomedical AI for
      creating framework-agnostic models that can be trained in PyTorch, TensorFlow,
      or other frameworks and deployed across diverse clinical platforms and hardware
      accelerators. Healthcare AI developers leverage ONNX to ensure vendor independence,
      enabling models to run on different hospital IT systems, edge devices, and specialized
      medical hardware regardless of training framework. The standard facilitates
      regulatory submissions by providing a stable model representation, supports
      hardware optimization through ONNX Runtime's performance tuning for CPUs, GPUs,
      and custom accelerators, and enables model sharing across research institutions
      without requiring framework dependencies. ONNX is particularly valuable for
      clinical AI products that must support multiple deployment environments.
    used_in_bridge2ai: false
    references:
    - https://github.com/microsoft/onnxruntime
  - id: B2AI_APP:193
    category: B2AI:Application
    name: Cross-Framework Model Interoperability and Export-Once-Run-Anywhere Workflows
    description: ONNX serves as the standard cross-framework interchange format enabling
      "export once, run anywhere" workflows where models trained in PyTorch, TensorFlow,
      Keras, or scikit-learn are converted to a common intermediate representation
      with a standardized operator set and extensible custom operator domains. The
      format encodes computation graphs, learned weights, and input/output specifications
      in a portable .onnx file that decouples training frameworks from inference runtimes,
      allowing data scientists to experiment with diverse architectures while deployment
      engineers optimize for production targets. PyTorch models are exported via torch.onnx.export
      with configurable options (input/output names, dynamic_axes for variable batch
      sizes, opset_version for compatibility), and the resulting graphs can be visualized
      using Netron for inspection of layer connectivity, operator types, and tensor
      shapes. ONNX's extensibility through custom operator domains enables migration
      among frameworks, compilers (Apache TVM), and hardware-specific runtimes (TensorRT,
      OpenVINO, MiGraphX) while preserving model portability and avoiding vendor lock-in.
      In biomedical AI, cross-framework interoperability enables research groups to
      share pretrained models (medical image segmentation, clinical NLP, genomics
      variant calling) via model zoos (ONNX Model Zoo, Hugging Face Hub, BioImage
      Model Zoo) without requiring recipients to install the original training framework,
      facilitating reproducibility and transfer learning across institutions. Evaluation
      toolchains like kenning integrate ONNX as the standard export format, with
      dedicated onnxconverters modules and ModelWrapper implementations requiring
      ONNX serialization support, enabling systematic benchmarking of quality and
      inference performance across frameworks. The common file format simplifies regulatory
      submissions (FDA 510(k), EU MDR) by providing stable, versioned model representations
      that can be validated independently of training code, with explicit operator
      semantics and numerical behavior documentation.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.3390/electronics14152977
    - https://lirias.kuleuven.be/retrieve/c1369a18-92c9-4ba2-9752-7001dac30e46
    - https://udspace.udel.edu/bitstreams/92c9aa1c-0e9e-456d-bd9d-9cc913fed657/download
  - id: B2AI_APP:194
    category: B2AI:Application
    name: Edge and IoT Deployment with ONNX Runtime for Low-Latency Inference
    description: ONNX Runtime is an embeddable, multi-language inference engine designed
      for efficient deployment on edge devices, mobile applications, embedded systems,
      and server APIs, providing optimized execution across CPUs, GPUs, and specialized
      accelerators without framework-specific dependencies. Benchmarking studies on
      NVIDIA Jetson AGX Orin edge platforms include ONNX Runtime (v1.17.1) alongside
      TensorRT, TVM, PyTorch, and JAX, evaluating realistic deployment metrics including
      inference time, throughput, memory usage, power consumption, and accuracy (Top-1/Top-5)
      on ImageNet datasets with both convolutional networks and transformers, demonstrating
      ONNX Runtime's suitability for production edge AI. The runtime applies automatic
      graph-level optimizations including operator fusion (Conv+BatchNorm+ReLU patterns),
      constant folding, reshape/transpose elimination, and removal of identity operations
      to reduce inference latency and memory overhead. For IoT architectures, ONNX
      models provisioned to run locally on devices (Intel Atom, ARM-based embedded
      boards) with ONNX Runtime enable decentralized serving patterns that avoid central
      HTTP endpoints, reducing cloud dependency, network latency, and data privacy
      concernscritical for medical devices performing on-device diagnostics or wearable
      health monitors processing physiological signals. ONNX Runtime supports multiple
      programming languages (Python, C++, Java, C#, JavaScript) enabling integration
      into diverse application stacks, from Python-based research prototypes to C++
      embedded firmware for real-time clinical decision support systems. The runtime's
      predictable latency, efficient memory usage, and cross-platform consistency
      make it suitable for production-scale deployments in healthcare settings where
      AI models must execute reliably on heterogeneous hardware ranging from hospital
      workstations to portable ultrasound devices and point-of-care diagnostic instruments.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.3390/electronics14152977
    - https://lirias.kuleuven.be/retrieve/c1369a18-92c9-4ba2-9752-7001dac30e46
    - https://aaltodoc.aalto.fi/bitstreams/a68b52e2-5581-43ee-89a6-0358e2963df4/download
  - id: B2AI_APP:195
    category: B2AI:Application
    name: Hardware Acceleration via Pluggable Execution Providers and Backend Optimization
    description: ONNX Runtime provides a flexible execution provider architecture
      enabling hardware-appropriate acceleration through pluggable backends spanning
      NVIDIA CUDA GPUs, TensorRT for optimized NVIDIA inference, Intel OpenVINO for
      CPU/GPU/VPU, ARM XNNPACK for mobile/edge, AMD MiGraphX (beta status), and DirectML
      for Windows devices, allowing deployment engineers to target diverse hardware
      from desktop workstations to edge SoCs without modifying model code. Execution
      providers apply vendor-specific graph optimizations, kernel fusion, and memory
      layout transformations tailored to target hardware characteristicsfor example,
      TensorRT compiles ONNX graphs into fully optimized inference engines with serialization/deserialization
      support that eliminates rebuild overhead during application startup, while OpenVINO
      optimizes for Intel architectures including CPU SIMD instructions, integrated
      GPUs, and Vision Processing Units in medical imaging workstations. Studies on
      exascale supercomputers demonstrate ONNX Runtime's support for heterogeneous
      HPC hardware including CPU clusters, NVIDIA GPUs via CUDA/TensorRT, and AMD
      GPUs via MiGraphX, with containerization recommended to manage runtime dependencies
      and backend variations when migrating between hardware vendors (NVIDIAAMD)
      or acceleration strategies (CUDATensorRT). The pluggable backend architecture
      enables biomedical AI applications to maintain portable ONNX models while leveraging
      specialized acceleratorsradiotherapy planning systems can deploy on NVIDIA
      GPUs with TensorRT optimization, pathology whole-slide image analysis can utilize
      Intel integrated GPUs via OpenVINO on diagnostic workstations, and portable
      ultrasound devices can execute models on ARM processors with XNNPACK acceleration.
      For performance-critical applications, ONNX Runtime performs operator fusion,
      constant folding, and graph pruning at the IR level, with experiments reporting
      inference speedups when models are optimized specifically for ONNX Runtime execution.
      Integration with compiler frameworks like Apache TVM enables further low-level
      optimization through operator fusion and ML-based schedule search, combining
      ONNX portability with hardware-specific kernel generation for maximum throughput.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.3390/electronics14152977
    - https://udspace.udel.edu/bitstreams/92c9aa1c-0e9e-456d-bd9d-9cc913fed657/download
    - https://lirias.kuleuven.be/retrieve/c1369a18-92c9-4ba2-9752-7001dac30e46
  - id: B2AI_APP:196
    category: B2AI:Application
    name: Post-Training Quantization and Reduced Precision for Model Compression
    description: ONNX Runtime supports Post-Training Quantization (PTQ) to INT8 precision
      for reducing model size and inference latency without requiring retraining,
      with both dynamic quantization (weights converted to INT8, activations quantized
      on-the-fly at runtime in FP32) and static quantization (both weights and activations
      quantized using calibration datasets to measure activation ranges) available
      for CPU and accelerator deployments. Quantization guidance recommends FP16 execution
      on NVIDIA GPUs including Jetson edge platforms, industrial PCs, and datacenter
      GPUs to achieve 2x throughput improvements over FP32 with minimal accuracy degradation,
      particularly valuable for real-time medical imaging applications processing
      high-resolution scans (CT, MRI, whole-slide pathology) where inference latency
      directly impacts clinical workflow efficiency. While ONNX Runtime does not support
      Quantization-Aware Training (QAT) natively, models can undergo QAT in PyTorch
      or TensorFlow training frameworks and then be exported to ONNX format for optimized
      inference, enabling workflows where simulated quantization during training produces
      models robust to reduced precision at deployment. Integration with hardware-specific
      quantization tools (Intel Neural Compressor for CPU/OpenVINO targets, NVIDIA
      TensorRT for GPU INT8 calibration) extends quantization capabilities to vendor-optimized
      implementations. For biomedical AI deployed on resource-constrained devicesportable
      diagnostic instruments, wearable health monitors, point-of-care ultrasoundPTQ
      enables execution of sophisticated deep learning models (segmentation networks,
      object detectors, classification CNNs) within power and memory budgets while
      maintaining clinical-grade accuracy. Quantization also reduces cloud inference
      costs in large-scale screening programs and population health studies where
      millions of scans are processed, with INT8 models providing 4x memory reduction
      and proportional decreases in storage and bandwidth requirements for model distribution
      to federated clinical sites. The combination of ONNX's portable model representation
      and ONNX Runtime's PTQ support enables "quantize once, deploy anywhere" workflows
      where a single quantized model artifact runs efficiently across diverse edge
      and server hardware.
    used_in_bridge2ai: false
    references:
    - https://lirias.kuleuven.be/retrieve/c1369a18-92c9-4ba2-9752-7001dac30e46
  - id: B2AI_APP:197
    category: B2AI:Application
    name: Classical Machine Learning Support via ai.onnx.ml Operator Set
    description: ONNX's ai.onnx.ml operator set extends the standard ai.onnx deep
      learning operators to represent classical machine learning models and pipelines,
      including linear regression, support vector machines (SVM), decision trees,
      random forests, gradient boosting, bagging ensembles, and dimensionality reduction
      techniques (PCA mapped to matrix operations), enabling portable inference for
      traditional ML algorithms alongside deep neural networks. ONNX Runtime was the
      first inference engine to support both ai.onnx (deep learning) and ai.onnx.ml
      (classical ML) operator sets for unified execution, allowing deployment of hybrid
      pipelines that combine classical feature engineering, ensemble models, and deep
      learning components in a single ONNX graph. Conversion tools like sklearn-onnx
      enable exporting scikit-learn pipelines (preprocessing transformers, feature
      selectors, estimators) to ONNX format for deployment on diverse hardware (CPUs,
      GPUs, edge devices) without Python/scikit-learn dependencies, valuable for embedding
      ML models in mobile applications, microservices, or embedded systems. In biomedical
      applications, classical ML remains relevant for structured clinical data (EHR-derived
      features, lab values, vital signs) where tree-based models and linear models
      often achieve performance comparable to deep learning with greater interpretability
      and lower computational requirements. ONNX's support for classical ML enables
      clinical decision support systems to deploy interpretable risk prediction models
      (logistic regression for mortality prediction, random forests for readmission
      risk) alongside deep learning models for imaging or unstructured text analysis,
      with unified ONNX Runtime inference simplifying production deployment and monitoring.
      Genomics applications leverage classical ML for variant effect prediction, gene
      expression classification, and phenotype association where gradient boosting
      and ensemble methods excel, and ONNX export enables these models to be integrated
      into automated diagnostic pipelines or shared across research institutions as
      portable artifacts. The ai.onnx.ml operator set also supports feature engineering
      operations (scaling, normalization, one-hot encoding, binning) directly within
      the ONNX graph, reducing deployment complexity by embedding preprocessing logic
      that would otherwise require separate data pipeline code.
    used_in_bridge2ai: false
    references:
    - https://aaltodoc.aalto.fi/bitstreams/a68b52e2-5581-43ee-89a6-0358e2963df4/download
    - https://doi.org/10.3390/electronics14152977
  - id: B2AI_APP:198
    category: B2AI:Application
    name: HPC and Compiler-Level Optimization for Scientific ML Workflows
    description: ONNX provides the portability layer for deploying machine learning
      models on heterogeneous supercomputer and HPC infrastructure, enabling scientific
      AI workflows to target diverse CPU clusters, NVIDIA GPU nodes (via CUDA/TensorRT),
      and AMD GPU nodes (via MiGraphX) with a single model artifact, critical for
      exascale computing environments where code must efficiently utilize mixed vendor
      hardware across compute partitions. Studies on scientific applications in exascale
      supercomputing demonstrate ONNX Runtime's inference capabilities across HPC
      backends, with experiments reporting speedups when models are optimized for
      ONNX Runtime execution and recommendations for containerization (Docker, Singularity)
      to manage runtime dependencies and backend variations when migrating between
      hardware configurations (e.g., CUDA to TensorRT transitions or NVIDIA to AMD
      vendor switches). For compiler-level optimization, ONNX models serve as input
      to Apache TVM's compilation framework, which applies operator fusion, automated
      schedule search using ML-based cost models, and target-specific kernel generation
      to produce highly optimized executables for specific hardware; the combination
      of ONNX+ONNX Runtime+TVM provides a pathway from framework-agnostic model representation
      through portable inference to hardware-specialized performance. In biomedical
      research, HPC deployments of ONNX models enable large-scale molecular dynamics
      simulations coupled with ML-based force field prediction, cryo-EM image reconstruction
      with deep learning denoising on GPU clusters, genomic variant calling pipelines
      processing population-scale whole-genome sequencing datasets, and drug discovery
      virtual screening campaigns evaluating millions of compounds using structure-based
      binding affinity prediction models. The ONNX ecosystem's support for batching,
      dynamic shape inference, and efficient memory management makes it suitable for
      throughput-oriented HPC workloads where thousands of inference requests are
      processed in parallel across distributed compute nodes. ONNX's standardized
      operator semantics and versioned opsets ensure reproducibility in scientific
      ML experiments, allowing researchers to document exact model architectures and
      weights in publications while enabling independent verification by other groups
      using different hardware or software stacks. The integration of ONNX with HPC
      resource managers (Slurm, PBS) and workflow engines (Nextflow, Snakemake) supports
      complex multi-stage scientific pipelines that alternate between data preprocessing,
      simulation, ML inference, and analysis steps.
    used_in_bridge2ai: false
    references:
    - https://udspace.udel.edu/bitstreams/92c9aa1c-0e9e-456d-bd9d-9cc913fed657/download
- id: B2AI_STANDARD:358
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: OpenAPI Specification
  formal_specification: https://github.com/OAI/OpenAPI-Specification/
  is_open: true
  name: OpenAPI
  purpose_detail: Standard for describing program interfaces.
  requires_registration: false
  url: https://spec.openapis.org/oas/latest.html
- id: B2AI_STANDARD:359
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Parquet
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:30
  has_relevant_organization:
  - B2AI_ORG:114
  is_open: true
  name: parquet
  purpose_detail: Apache Parquet is an open-source, column-oriented data file format
    designed for efficient storage and retrieval of large datasets in analytical and
    machine learning workloads, providing superior compression and encoding schemes
    compared to row-based formats while enabling selective column reading (predicate
    pushdown) that dramatically accelerates queries filtering on specific attributes
    without scanning entire datasets. Developed as part of the Apache Hadoop ecosystem
    with contributions from Cloudera, Twitter, and the Apache Arrow community, Parquet
    addresses the performance limitations of traditional row-oriented formats (CSV,
    JSON) for analytical queries that aggregate or filter on subsets of columns across
    billions of rows, which is the dominant access pattern in data science, business
    intelligence, and machine learning feature engineering. The columnar layout stores
    all values from each column together, enabling column-specific compression algorithms
    (dictionary encoding, run-length encoding, bit-packing) that achieve compression
    ratios of 10x-100x compared to uncompressed data, while maintaining compatibility
    with schema evolution as datasets grow and change over time. Parquet files embed
    rich metadata including column statistics (min/max values, null counts, distinct
    counts per row group) that enable query engines to skip irrelevant data chunks
    entirely, supporting efficient filtering and aggregation without reading unnecessary
    columns or row groups from disk or cloud storage. The format is designed for
    cross-platform, language-agnostic data interchange with implementations in Java,
    C++, Python, Go, Rust, and other languages through Apache Arrow's unified memory
    model, enabling zero-copy data sharing between processes and efficient integration
    with distributed computing frameworks (Spark, Dask, Ray) and cloud data warehouses
    (Snowflake, BigQuery, Redshift Spectrum, Athena). For machine learning workflows,
    Parquet serves as the standard serialization format for feature stores (Feast,
    Tecton, Hopsworks), data lakehouse architectures (Delta Lake, Apache Iceberg),
    and ML dataset versioning systems (DVC, Pachyderm), providing ACID-like semantics
    for time-travel queries and reproducible model training. The format's partitioning
    capabilities (by date, category, or other dimensions) enable efficient incremental
    processing and support for streaming pipelines that continuously append new data
    while maintaining queryable history. Parquet integrates seamlessly with popular
    data processing libraries including pandas, Polars, DuckDB, PyArrow, Dask, and
    Vaex, and is the preferred format for AI/ML frameworks (TensorFlow Data API,
    PyTorch DataLoader, JAX) that require fast sequential or random access to training
    examples with selective feature loading. The format supports nested and complex
    data types (structs, arrays, maps) through its Dremel-inspired encoding, allowing
    representation of hierarchical biomedical data (nested clinical observations,
    multi-level ontology annotations, structured EHR records) without flattening into
    denormalized tables. In scientific computing and biomedical research, Parquet
    is adopted for genomics datasets (variant call tables, expression matrices), clinical
    data warehouses (harmonized EHR extracts, multi-site cohort studies), imaging
    metadata catalogs (DICOM header databases, radiomics feature stores), and multi-omics
    integration where columnar access patterns (e.g., loading gene expression for
    specific genes across millions of cells) match analytical needs. Cloud platforms
    provide native Parquet support with optimized readers/writers (AWS S3 Select,
    Google Cloud Storage, Azure Blob Storage with Synapse Analytics), enabling serverless
    queries over petabyte-scale archives without ETL overhead. The format's metadata
    layer supports Bloom filters for existence checks, dictionary-encoded categorical
    columns for efficient grouping operations, and column-level encryption for compliance
    with data privacy regulations (HIPAA, GDPR) in healthcare and genomics applications,
    making Parquet the de facto standard for storing structured and semi-structured
    analytical data in modern data platforms.
  requires_registration: false
  url: https://parquet.apache.org/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:57
    category: B2AI:Application
    name: Efficient Large-Scale ML Data Storage and Processing
    description: Apache Parquet is widely adopted in AI/ML pipelines for efficient
      storage and processing of large-scale biomedical datasets, particularly for
      training deep learning models on tabular clinical data, multi-omics datasets,
      and imaging metadata. The columnar storage format enables high-performance data
      loading during model training, reduces storage costs through efficient compression,
      and supports predicate pushdown for selective feature reading. AI frameworks
      like TensorFlow, PyTorch, and scikit-learn leverage Parquet's integration with
      Apache Arrow for zero-copy data transfer, enabling faster iteration during hyperparameter
      tuning and model development. Parquet is particularly valuable for storing processed
      features from EHR data, genomic variant annotations, and large-scale biobank
      datasets where query performance and storage efficiency are critical.
    used_in_bridge2ai: false
  - id: B2AI_APP:188
    category: B2AI:Application
    name: Direct Training from Parquet via Petastorm for Healthcare ML
    description: Apache Parquet serves as a direct training data format for machine
      learning pipelines using Petastorm, a Python library enabling TensorFlow, PyTorch,
      and PySpark to stream training data directly from Parquet files without intermediate
      format conversion. In healthcare data engineering workflows with Flat FHIR datasets,
      transforming bulk FHIR data to Parquet format enables training pipelines to
      read directly from columnar storage with measured read latency of ~0.3 s/record,
      significantly faster than row-oriented JSON. Petastorm's integration allows
      ML frameworks to load and stream untransformed Parquet data into training algorithms,
      reducing preprocessing overhead and I/O bottlenecks during model training. Healthcare
      organizations transform large-scale clinical datasets (immunization records,
      patient observations, medication histories) into Parquet format to achieve superior
      read/query performance while maintaining compatibility with cloud AI platforms
      (Google Cloud AI) and major ML frameworks. Parquet's columnar layout with compression
      (1.2 bytes/record for synthetic Flat-FHIR immunization data) and efficient
      SQL query execution makes it particularly suitable for analytic and ML workloads
      where training datasets are repeatedly accessed during hyperparameter tuning,
      cross-validation, and model development. The approach enables cost savings (potentially
      millions of dollars) on cloud-hosted ML training through improved storage efficiency
      and query performance, while supporting direct data exchange between clinical
      data warehouses and ML training environments without format conversion overhead.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1093/jamia/ocz207
    - https://pmc.ncbi.nlm.nih.gov/articles/PMC7153160/pdf/3203152.pdf
  - id: B2AI_APP:189
    category: B2AI:Application
    name: Delta Lake/Delta Parquet for ML Reproducibility and Feature Stores
    description: Apache Parquet serves as the foundational columnar file format for
      Delta Lake and Delta Parquet, adding ACID transactions, time-travel capabilities,
      and versioning on top of Parquet's efficient storage and read performance. In
      Spark and Databricks-centered ML workflows, Delta Parquet enables reproducible
      machine learning operations through sophisticated time-travel features that
      allow rollback to previous dataset versions, critical for experiment reproducibility,
      model lineage tracking, and regulatory compliance in automotive and healthcare
      applications. The format excels in feature store creation where training datasets
      require frequent updates with transactional guarantees, supporting both micro-batch
      and streaming ingestion patterns with ACID consistency. Delta's integration
      with Apache Spark provides faster access to versioned data, schema enforcement,
      and streamlined data governance for large training datasets and historical analytics.
      ML practitioners leverage Delta Parquet for model training reproducibility,
      enabling controlled experimentation where specific dataset versions can be referenced
      for training, validation split repeatability across research teams. The time-travel
      capabilities support continuous model training scenarios such as autonomous
      vehicle perception systems where annotation datasets evolve with new labeled
      images, allowing teams to train on specific annotation versions and compare
      model performance across dataset evolution. Delta Parquet simplifies debugging
      ML pipelines by providing snapshot isolation for training runs, supports regulatory-compliant
      governance through immutable audit trails of dataset changes, and enables efficient
      storage management through compaction and data skipping optimizations while
      maintaining Parquet's columnar efficiency for read-intensive ML workloads.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.48550/arxiv.2508.13396
    - https://doi.org/10.47363/jaicc/2023(2)e241
  - id: B2AI_APP:190
    category: B2AI:Application
    name: Columnar Feature Filtering for Multimodal AI and NLP Pipelines
    description: Apache Parquet's columnar storage architecture enables efficient
      filtering and selection of specific features from large multimodal datasets,
      significantly accelerating preprocessing pipelines for AI models that process
      text, images, and sensor data. In sentiment analysis and NLP applications, Parquet
      has been used to store and process millions of text samples with reported significant
      reductions in data-retrieval time compared to CSV formats, enabling quicker
      iterations during model training. The columnar layout allows AI pipelines to
      load only relevant features needed for specific model architectures, lowering
      memory consumption and speeding processing by avoiding full-record deserialization.
      For multimodal healthcare AI applications handling diverse data types (clinical
      text narratives, medical images, wearable sensor streams), standardizing data
      into unified schemas using Parquet enables models to seamlessly access and process
      heterogeneous data sources with consistent preprocessing. Parquet's standardized
      metadata embedded in column headers supports reproducible preprocessing steps
      (tokenization, lemmatization, normalization) across research teams, improving
      collaboration in multi-institution NLP projects where consistent data handling
      is critical. The format's schema evolution capabilities allow models to adapt
      as feature sets expand with new clinical variables, imaging biomarkers, or sensor
      modalities without requiring complete dataset reprocessing. In production ML
      pipelines, Parquet's efficient column pruning enables real-time feature serving
      where inference requests selectively load feature subsets for specific model
      endpoints, reducing latency and compute costs compared to formats requiring
      full-record reads. The combination of compression, selective column access,
      and standardized metadata makes Parquet particularly effective for iterative
      feature engineering workflows where data scientists experiment with different
      feature combinations during model development.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.47363/jaicc/2023(2)e241
  - id: B2AI_APP:191
    category: B2AI:Application
    name: Lakehouse Bronze Layer for Cost-Efficient ML Training Data Storage
    description: Apache Parquet is recommended as the storage format for raw, immutable
      "bronze" layers in modern data lakehouse architectures, serving as the cost-efficient,
      read-optimized foundation for feature engineering and model training pipelines.
      In hybrid lakehouse designs combining Parquet and Delta Lake, Parquet handles
      stable, read-intensive analytical workloads and serves as "the backbone for
      cross-engine compatibilityideal for lightweight analytics and ML model training."
      The format's unmatched cost efficiency for read-only analytics makes it particularly
      suitable for archival training datasets that are written once and read repeatedly
      during model development, hyperparameter tuning, and ensemble training. Parquet's
      marginally higher write throughput compared to transactional formats enables
      efficient bulk loading of large-scale training corpora from batch ETL processes,
      though it requires manual version directories for maintaining multiple dataset
      snapshots, adding complexity compared to native versioning in Delta/Iceberg.
      For ML workflows, the bronze Parquet layer stores raw or minimally processed
      data (e.g., deduplicated sensor readings, parsed clinical notes, normalized
      imaging metadata) that feeds feature engineering pipelines producing curated
      "silver" and "gold" datasets. The approach optimizes cloud storage costs by
      using Parquet for high-volume, infrequently modified training data while reserving
      more expensive transactional formats for actively updated feature tables and
      serving layers. Parquet's cross-engine compatibility ensures training datasets
      remain accessible to diverse ML frameworks (TensorFlow, PyTorch, scikit-learn,
      XGBoost) and compute engines (Spark, Dask, Ray, Polars) without vendor lock-in,
      enabling organizations to experiment with different ML stacks without data migration
      overhead. The bronze layer pattern supports data lineage and reproducibility
      by preserving immutable source datasets while allowing downstream curated datasets
      to evolve independently, critical for regulated ML applications in healthcare
      and finance requiring audit trails of training data provenance.
    used_in_bridge2ai: false
    references:
    - https://irjernet.com/index.php/fecsit/article/view/237/196
  - id: B2AI_APP:192
    category: B2AI:Application
    name: Cross-Framework ML Training Data Exchange and Cloud Cost Optimization
    description: Apache Parquet serves as the standard interchange format for machine
      learning training datasets across heterogeneous frameworks (TensorFlow, PyTorch,
      PySpark) and cloud platforms (AWS, Google Cloud, Azure), enabling vendor-neutral
      data pipelines and substantial cloud cost reductions through efficient storage
      and query performance. Healthcare AI projects transforming Flat FHIR bulk data
      to Parquet achieve storage efficiency (1.2 bytes/record for immunization data)
      and fast query execution, with authors recommending Parquet for large-scale
      analytic and ML projects where read-optimized access patterns dominate and cloud
      storage costs significantly impact total cost of ownership. Parquet's compatibility
      matrix with major ML platforms (TensorFlow Yes, PyTorch Yes, PySpark Yes, Google
      Cloud AI Yes) ensures training datasets can be shared across research institutions,
      clinical sites, and cloud environments without format conversion, reducing data
      engineering overhead and enabling reproducible multi-site ML studies. The format's
      columnar compression and predicate pushdown capabilities dramatically lower
      data transfer costs in cloud environments where egress charges apply, as queries
      retrieve only necessary columns rather than full records. For federated learning
      initiatives spanning multiple hospitals or research centers, Parquet's cross-platform
      implementations in Java, C++, Python, Go, and Rust through Apache Arrow enable
      consistent data representation across diverse IT infrastructures without framework-specific
      dependencies. In production ML systems, Parquet's integration with cloud-native
      services (AWS S3 Select, BigQuery, Redshift Spectrum, Azure Synapse) enables
      serverless SQL queries over training datasets for feature exploration, data
      quality monitoring, and model performance analysis without provisioning compute
      resources. The combination of high compression ratios (10x-100x), fast selective
      reads, and broad ecosystem support positions Parquet as the de facto standard
      for cost-efficient storage and exchange of large-scale ML training corpora,
      particularly in biomedical domains where multi-modal datasets (genomics, imaging,
      clinical records) require harmonization across institutions and platforms.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1093/jamia/ocz207
    - https://pmc.ncbi.nlm.nih.gov/articles/PMC7153160/pdf/3203152.pdf
    - https://doi.org/10.47363/jaicc/2023(2)e241
- id: B2AI_STANDARD:360
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Persistent Uniform Resource Locator
  formal_specification: https://code.google.com/archive/p/persistenturls/
  has_relevant_organization:
  - B2AI_ORG:43
  is_open: true
  name: PURL
  purpose_detail: PURLs are Web addresses or Uniform Resource Locators (URLs) that
    act as permanent identifiers in the face of a dynamic and changing Web infrastructure.
    Instead of resolving directly to Web resources (documents, data, services, people,
    etc.) PURLs provide a level of indirection that allows the underlying Web addresses
    of resources to change over time without negatively affecting systems that depend
    on them. This capability provides continuity of references to network resources
    that may migrate from machine to machine for business, social or technical reasons.
  requires_registration: false
  url: https://sites.google.com/site/persistenturls/
- id: B2AI_STANDARD:361
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:15
  - B2AI_TOPIC:32
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Portable Document Format
  is_open: true
  name: PDF
  purpose_detail: A file format to present documents, including text formatting and
    images, in a manner independent of application software, hardware, and operating
    systems.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/PDF
- id: B2AI_STANDARD:362
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Portable Network Graphics
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:19
  is_open: true
  name: PNG
  purpose_detail: A raster-graphics file format that supports lossless data compression.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/Portable_Network_Graphics
- id: B2AI_STANDARD:363
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:15
  - B2AI_TOPIC:32
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Postscript Format
  is_open: true
  name: PS
  purpose_detail: "PostScript (PS) is a Turing-complete, stack-based page description
    language and programming language developed by Adobe Systems (John Warnock, Chuck
    Geschke, Doug Brotz, Ed Taft, Bill Paxton) and released in 1984, revolutionizing
    desktop publishing by providing device-independent representation of documents
    combining text, vector graphics, and raster images. PostScript uses reverse Polish
    notation and an interpreted execution model where documents are programs that,
    when executed by a PostScript interpreter (Raster Image Processor or RIP), render
    pages at the target device's resolution. The language describes graphics using
    vector primitives (straight lines, cubic Bzier curves) enabling arbitrary scaling,
    rotation, and transformation without quality loss, crucial for professional typography
    and technical illustrations. PostScript's sophisticated font system uses outline
    fonts with font hinting to maintain glyph quality at low resolutions, standardized
    through Type 1, Type 2, and Type 3 font formats that influenced modern font technologies
    like TrueType and OpenType. Three major versions exist: PostScript Level 1 (1984)
    introducing basic page description capabilities, PostScript Level 2 (1991) adding
    improved speed, image decompression (JPEG support), composite fonts, color separation,
    and form caching, and PostScript 3 (1997) providing enhanced color handling with
    up to 4096 gray levels, smooth shading operations, DeviceN color space for spot
    colors, and better filtering. PostScript powered the Apple LaserWriter (1985),
    triggering the desktop publishing revolution by enabling WYSIWYG document creation
    on Macintosh with PageMaker software. The language became the de facto standard
    for electronic prepress systems, high-end typesetters (Linotronic), and professional
    printing workflows throughout the 1980s-1990s. PostScript's imaging model directly
    influenced PDF (Portable Document Format), Adobe's 1993 successor that simplified
    PostScript for document distribution by removing general-purpose programming features
    while retaining the imaging model, making PDF documents static data structures
    rather than executable programs. PostScript remains common in high-end printers,
    professional publishing, and scientific visualization where precise vector graphics
    control is required. Open-source implementations like Ghostscript enable PostScript
    rendering on devices lacking native PostScript support. Scientific applications
    include generation of publication-quality figures from computational analysis
    software, precise technical diagrams, and device-independent archival documents."
  requires_registration: false
  url: https://en.wikipedia.org/wiki/PostScript
- id: B2AI_STANDARD:364
  category: B2AI_STANDARD:DataStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Predictive Model Markup Language
  formal_specification: https://dmg.org/pmml/v4-4-1/GeneralStructure.html
  is_open: true
  name: PMML
  purpose_detail: PMML (Predictive Model Markup Language) uses XML to represent mining
    models. The structure of the models is described by an XML Schema. One or more
    mining models can be contained in a PMML document. A PMML document is an XML document
    with a root element of type PMML
  requires_registration: false
  url: https://dmg.org/pmml/v4-4-1/GeneralStructure.html
- id: B2AI_STANDARD:365
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Protocol Buffers
  formal_specification: https://github.com/protocolbuffers/protobuf
  has_relevant_organization:
  - B2AI_ORG:37
  is_open: true
  name: protobuf
  purpose_detail: Protocol Buffers (a.k.a., protobuf) are Google's language-neutral,
    platform-neutral, extensible mechanism for serializing structured data.
  requires_registration: false
  url: https://developers.google.com/protocol-buffers/
- id: B2AI_STANDARD:366
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Provenance
  has_relevant_organization:
  - B2AI_ORG:99
  is_open: true
  name: PROV
  purpose_detail: The PROV Family of Documents defines a model, corresponding serializations
    and other supporting definitions to enable the inter-operable interchange of provenance
    information in heterogeneous environments such as the Web.
  requires_registration: false
  url: https://www.w3.org/TR/prov-overview/
- id: B2AI_STANDARD:367
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Python pickle format
  formal_specification: https://github.com/python/cpython/blob/3.8/Lib/pickle.py
  is_open: true
  name: pickle
  purpose_detail: Serialization format for a Python object structure. Pickling is
    the process whereby a Python object hierarchy is converted into a byte stream,
    and unpickling is the inverse operation, whereby a byte stream (from a binary
    file or bytes-like object) is converted back into an object hierarchy.
  requires_registration: false
  url: https://docs.python.org/3.8/library/pickle.html
- id: B2AI_STANDARD:368
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Qualitative Data Exchange Schema
  formal_specification: https://dam.data-archive.ac.uk/standards/qudex_v03_01.xsd
  has_relevant_organization:
  - B2AI_ORG:97
  is_open: true
  name: QuDEx
  purpose_detail: The Qualitative Data Exchange Schema (QuDEx) allows users to discover,
    find, retrieve and cite complex qualitative data collections in context.
  requires_registration: false
  url: https://www.data-archive.ac.uk/managing-data/standards-and-procedures/metadata-standards/qudex/
- id: B2AI_STANDARD:369
  category: B2AI_STANDARD:DataStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: RDA CURE-FAIR 10 Things for Curating Reproducible and FAIR Research
  formal_specification: https://curating4reproducibility.org/10things/
  has_relevant_organization:
  - B2AI_ORG:83
  is_open: true
  name: RDA 10 Things
  purpose_detail: A framework for implementing effective curation workflows for achieving
    greater FAIR-ness and long-term usability of research data and code.
  requires_registration: false
  url: https://www.rd-alliance.org/group/cure-fair-wg/outcomes/10-things-curating-reproducible-and-fair-research
- id: B2AI_STANDARD:370
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: RDF Schema
  has_relevant_organization:
  - B2AI_ORG:99
  is_open: true
  name: RDFS
  purpose_detail: RDF Schema (RDFS) is the foundational vocabulary description language
    that extends the basic RDF vocabulary to provide essential data modeling capabilities
    for the Semantic Web and linked data applications. As a W3C Recommendation and
    semantic extension of RDF, RDFS enables the description of groups of related resources
    and relationships between them by defining classes, properties, and their hierarchical
    structures. Unlike traditional object-oriented programming models that define
    classes in terms of their instance properties, RDFS takes a property-centric approach
    where properties are described in terms of the classes they apply to through domain
    and range mechanisms. This design philosophy promotes the extensibility principle
    of the Web, allowing anyone to define additional properties for existing resources
    without requiring modification of original class definitions. RDFS provides core
    vocabulary elements including rdfs:Class, rdfs:Resource, rdfs:Property, rdfs:subClassOf,
    rdfs:subPropertyOf, rdfs:domain, rdfs:range, rdfs:label, and rdfs:comment, which
    form the basis for more sophisticated ontology languages like OWL. The schema
    supports the development of machine-readable vocabularies that can be processed
    automatically, enabling applications to discover and reason about resource relationships,
    making it an essential component of the Semantic Web infrastructure.
  requires_registration: false
  url: https://www.w3.org/TR/rdf-schema/
- id: B2AI_STANDARD:371
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Registry Interchange Format - Collections and Services schema
  has_relevant_organization:
  - B2AI_ORG:6
  is_open: true
  name: RIF-CS
  purpose_detail: The Registry Interchange Format - Collections and Services (RIF-CS)
    is an XML-based metadata schema developed by the Australian National Data Service
    (ANDS) for describing and exchanging information about research collections, services,
    parties (people and organizations), and activities. RIF-CS serves as the foundational
    data interchange format for Research Data Australia and enables institutions to
    contribute metadata about their research assets to national and international
    discovery services. The schema organizes metadata into four core entity types
    with rich relationship modeling capabilities - collections (datasets, databases,
    repositories), services (software tools, web services, facilities), parties (researchers,
    institutions, funders), and activities (projects, programs, events). Each entity
    supports comprehensive descriptive metadata including identifiers, names, descriptions,
    locations, dates, subjects, and crucially, relationships to other entities that
    create a connected graph of research infrastructure. RIF-CS enables automated
    harvesting and aggregation of research metadata across institutions, supporting
    research discovery, collaboration, and compliance with research data management
    policies.
  requires_registration: false
  url: https://services.ands.org.au/documentation/rifcs/1.2.0/guidelines/rif-cs.html
- id: B2AI_STANDARD:372
  category: B2AI_STANDARD:DataStandard
  collection:
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Research Object Crate
  formal_specification: https://www.researchobject.org/ro-crate/specification.html
  has_relevant_organization:
  - B2AI_ORG:116
  is_open: true
  name: RO-CRATE
  publication: doi:10.3233/DS-210053
  purpose_detail: A community effort to establish a lightweight approach to packaging
    research data with their metadata. It is based on schema.org annotations in JSON-LD,
    and aims to make best-practice in formal metadata description accessible and practical
    for use in a wider variety of situations, from an individual researcher working
    with a folder of data, to large data-intensive computational research environments.
  requires_registration: false
  url: https://www.researchobject.org/ro-crate/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:58
    category: B2AI:Application
    name: ML Model Packaging and Research Object Preservation
    description: RO-CRATE is used in AI applications for packaging machine learning
      models, training data, workflows, and associated metadata into portable, FAIR-compliant
      research objects. AI researchers leverage RO-CRATE to create self-describing
      archives that bundle trained models with their training datasets, preprocessing
      scripts, validation results, and computational environment specifications, ensuring
      reproducibility and reusability of AI research. The standard enables automated
      model repositories, facilitates model sharing across institutions, and supports
      provenance tracking for regulatory compliance in clinical AI applications. RO-CRATE's
      lightweight JSON-LD format makes it ideal for describing complex AI workflows
      while maintaining human readability.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:373
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Resource Description Framework
  has_relevant_organization:
  - B2AI_ORG:99
  is_open: true
  name: RDF
  purpose_detail: 'Resource Description Framework (RDF) is a W3C standard framework for representing
    information about resources on the Web using subject-predicate-object triples.
    RDF provides a graph-based data model where resources are identified by IRIs (Internationalized
    Resource Identifiers) and relationships form a directed, labeled graph. The framework
    enables decentralized knowledge representation, allowing anyone to make statements
    about any resource and merge distributed data seamlessly. RDF Schema (RDFS) extends
    RDF with vocabulary for describing classes, properties, domain, range, and hierarchical
    relationships, enabling semantic reasoning and inference. RDF supports multiple
    serialization formats including Turtle (human-readable), RDF/XML (verbose XML),
    JSON-LD (JSON-based), and N-Triples (line-oriented). The semantic web stack builds
    upon RDF: OWL for rich ontologies, SPARQL for querying RDF graphs, SHACL for
    shape validation. RDF powers linked open data initiatives (DBpedia, Wikidata),
    biomedical ontologies (OBO Foundry, Bio2RDF), knowledge graphs (Google Knowledge
    Graph principles), and enterprise knowledge management. The framework enables
    data integration across heterogeneous sources by providing common vocabularies
    (Dublin Core, FOAF, Schema.org) and federated querying. In AI/ML contexts, RDF
    graphs serve as structured knowledge bases for knowledge graph embeddings (TransE,
    DistMult, ComplEx), semantic reasoning for inference rules, ontology-guided feature
    engineering, and multi-relational learning where symbolic knowledge augments statistical
    learning, enabling explainable AI and knowledge-driven machine learning systems.'
  requires_registration: false
  url: https://www.w3.org/TR/rdf-schema/
- id: B2AI_STANDARD:374
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Safetensors
  formal_specification: https://github.com/huggingface/safetensors
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:42
  is_open: true
  name: Safetensors
  purpose_detail: A new simple format for storing tensors safely (as opposed to pickle)
    and that is still fast (zero-copy).
  requires_registration: false
  url: https://github.com/huggingface/safetensors
- id: B2AI_STANDARD:375
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Scalable Vector Graphics Format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:19
  has_relevant_organization:
  - B2AI_ORG:99
  is_open: true
  name: SVG
  purpose_detail: Scalable Vector Graphics (SVG) Version 1.1, a modularized language
    for describing two-dimensional vector and mixed vector/raster graphics in XML.
  requires_registration: false
  url: https://www.w3.org/Graphics/SVG/About.html
- id: B2AI_STANDARD:376
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Shapes Constraint Language
  has_relevant_organization:
  - B2AI_ORG:99
  is_open: true
  name: SHACL
  purpose_detail: A language for validating RDF graphs against a set of conditions.
    These conditions are provided as shapes and other constructs expressed in the
    form of an RDF graph. RDF graphs that are used in this manner are called shapes
    graphs in SHACL and the RDF graphs that are validated against a shapes graph are
    called data graphs. As SHACL shape graphs are used to validate that data graphs
    satisfy a set of conditions they can also be viewed as a description of the data
    graphs that do satisfy these conditions. Such descriptions may be used for a variety
    of purposes beside validation, including user interface building, code generation
    and data integration.
  requires_registration: false
  url: https://www.w3.org/TR/shacl/
- id: B2AI_STANDARD:377
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Silicon Graphics image format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:19
  is_open: true
  name: SGI
  purpose_detail: Silicon Graphics Image (SGI or RGB) is a raster graphics file format
    developed by Silicon Graphics, Inc. for storing and displaying digital images
    on SGI workstations and UNIX systems. The format supports 8-bit, 16-bit, and 24-bit
    color depths with optional alpha channel for transparency, allowing representation
    of grayscale, RGB, and RGBA images with various bit depths per channel. SGI images
    can be stored uncompressed or with run-length encoding (RLE) compression for reduced
    file sizes while maintaining lossless quality. The format was widely used in professional
    computer graphics, visual effects, scientific visualization, and medical imaging
    during the 1980s-1990s when SGI workstations dominated high-end graphics computing.
    SGI files use .sgi, .rgb, .rgba, .bw, or .int file extensions depending on color
    configuration and bit depth. The format specifies image dimensions, number of
    channels (1 for grayscale, 3 for RGB, 4 for RGBA), compression method, and pixel
    data in a header-based structure readable by graphics software on big-endian systems.
    While largely superseded by more modern formats like PNG and TIFF for general
    use, SGI format remains relevant in legacy scientific visualization applications,
    particularly in medical imaging archives, computational fluid dynamics visualization,
    and legacy 3D rendering pipelines. ImageMagick, GIMP, and specialized scientific
    visualization software maintain SGI format support for backward compatibility with
    historical image datasets. The format's simplicity and direct pixel representation
    made it suitable for high-performance graphics rendering on SGI's proprietary hardware
    and OpenGL-based visualization systems.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/Silicon_Graphics_Image
- id: B2AI_STANDARD:378
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Simple Standard for Sharing Ontological Mappings
  formal_specification: https://github.com/mapping-commons/sssom
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:41
  - B2AI_SUBSTRATE:6
  is_open: true
  name: SSSOM
  publication: doi:10.1093/database/baac035
  purpose_detail: SSSOM is a Simple Standard for Sharing Ontological Mappings, providing
    a TSV-based representation for ontology term mappings, a comprehensive set of
    standard metadata elements to describe mappings, and a standard translation between
    the TSV and the Web Ontology Language (OWL).
  requires_registration: false
  url: https://mapping-commons.github.io/sssom/
- id: B2AI_STANDARD:379
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Statismo format
  formal_specification: https://github.com/statismo/statismo
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:16
  is_open: true
  name: statismo
  purpose_detail: Statismo defines a storage format (Statistical Image And Shape Models)
    based on HDF5, which includes all the information necessary to use the model,
    as well as meta-data about the model creation, which helps to make model building
    reproducible.
  related_to:
  - B2AI_STANDARD:339
  requires_registration: false
  url: https://github.com/statismo/statismo
- id: B2AI_STANDARD:380
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Statistical Data and Metadata eXchange standard
  has_relevant_organization:
  - B2AI_ORG:88
  is_open: true
  name: SDMX
  purpose_detail: SDMX is an initiative to foster standards for the exchange of statistical
    information.
  requires_registration: false
  url: https://sdmx.org/
- id: B2AI_STANDARD:381
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: STereoLithography File Format Family
  is_open: true
  name: STL
  purpose_detail: STL files describe only the surface geometry of a three-dimensional
    object without any representation of color, texture or other common CAD model
    attributes. The STL format specifies both ASCII and binary representations.
  requires_registration: false
  url: https://www.loc.gov/preservation/digital/formats/fdd/fdd000504.shtml
- id: B2AI_STANDARD:382
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Structured Descriptive Data
  formal_specification: https://github.com/tdwg/sdd
  has_relevant_organization:
  - B2AI_ORG:93
  is_open: true
  name: SDD
  purpose_detail: The goal of the Structured Descriptive Data (SDD) standard is to
    allow capture, transport, caching and archiving of descriptive data in all the
    forms shown above, using a platform- and application-independent, international
    standard. Such a standard is crucial to enabling lossless porting of data between
    existing and future software platforms including identification, data-mining and
    analysis tools, and federated databases.
  requires_registration: false
  url: https://www.tdwg.org/standards/sdd/
- id: B2AI_STANDARD:383
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Tagged Image File Format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:19
  has_relevant_organization:
  - B2AI_ORG:116
  is_open: true
  name: TIFF
  purpose_detail: An image file format for storing raster graphics images.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/TIFF
  used_in_bridge2ai: true
- id: B2AI_STANDARD:384
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: TAR archive file format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:52
  is_open: true
  name: TAR
  purpose_detail: A tar (tape archive) file format is an archive created by tar, a
    UNIX-based utility used to package files together for backup or distribution purposes.
    It contains multiple files (also known as a tarball) stored in an uncompressed
    format along with metadata about the archive. Tar files are not compressed archive
    files. They are often compressed with file compression utilities such as gzip
    or bzip2.
  requires_registration: false
  url: https://www.loc.gov/preservation/digital/formats/fdd/fdd000531.shtml
- id: B2AI_STANDARD:385
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: TDWG Access Protocol for Information Retrieval
  has_relevant_organization:
  - B2AI_ORG:93
  is_open: true
  name: TAPIR
  purpose_detail: The TDWG Access Protocol for Information Retrieval (TAPIR) is a
    Web Service protocol and XML schema to perform queries across distributed databases
    of varied physical and logical structure. It was originally designed to be used
    by federated networks. TAPIR is intended for communication between applications,
    using HTTP as the transport mechanism. TAPIR's flexibility makes it suitable to
    both very simple service implementations where the provider only responds to a
    set of pre-defined queries, or more advanced implementations where the provider
    software can dynamically parse complex queries referencing output models supplied
    by the client.
  requires_registration: false
  url: https://www.tdwg.org/standards/tapir/
- id: B2AI_STANDARD:386
  category: B2AI_STANDARD:DataStandard
  collection:
  - codesystem
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: The Unified Code for Units of Measure
  has_relevant_organization:
  - B2AI_ORG:84
  is_open: true
  name: UCUM
  purpose_detail: A common syntax for communication of quantities and their units.
  requires_registration: false
  url: https://unitsofmeasure.org/ucum
- id: B2AI_STANDARD:387
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Waveform Audio File Format
  formal_specification: https://sites.google.com/site/musicgapi/technical-documents/wav-file-format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:48
  - B2AI_SUBSTRATE:49
  is_open: true
  name: WAV
  purpose_detail: Waveform Audio File Format (WAVE or WAV due to its filename extension
    is an audio file format standard.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/WAV
- id: B2AI_STANDARD:388
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Web Ontology Language
  has_relevant_organization:
  - B2AI_ORG:99
  is_open: true
  name: OWL
  purpose_detail: The Web Ontology Language (OWL) is a family of knowledge representation
    languages or ontology languages for authoring ontologies or knowledge bases. The
    languages are characterized by formal semantics and RDF/XML-based serializations
    for the Semantic Web. OWL is endorsed by the World Wide Web Consortium (W3C) and
    has attracted academic, medical and commercial interest. The OWL 2 Web Ontology
    Language, informally OWL 2, is an ontology language for the Semantic Web with
    formally defined meaning. OWL 2 ontologies provide classes, properties, individuals,
    and data values and are stored as Semantic Web documents. OWL 2 ontologies can
    be used along with information written in RDF, and OWL 2 ontologies themselves
    are primarily exchanged as RDF documents.
  requires_registration: false
  url: https://www.w3.org/TR/owl-overview/
- id: B2AI_STANDARD:389
  category: B2AI_STANDARD:DataStandard
  collection:
  - workflowlanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Workflow Description Language
  formal_specification: https://github.com/openwdl/wdl
  is_open: true
  name: WDL
  purpose_detail: The Workflow Description Language (WDL) is a way to specify data
    processing workflows with a human-readable and -writeable syntax. WDL makes it
    straightforward to define analysis tasks, chain them together in workflows, and
    parallelize their execution. The language makes common patterns simple to express,
    while also admitting uncommon or complicated behavior; and strives to achieve
    portability not only across execution platforms, but also different types of users.
    Whether one is an analyst, a programmer, an operator of a production system, or
    any other sort of user, WDL should be accessible and understandable.
  requires_registration: false
  url: https://openwdl.org/
- id: B2AI_STANDARD:390
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - deprecated
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: X PixMap bitmap image format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:19
  is_open: true
  name: XBM
  purpose_detail: X PixMap (XBM) is an image file format used by the X Window System.
    Replaced by XPM.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/X_PixMap
- id: B2AI_STANDARD:391
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: X PixMap image format
  is_open: true
  name: XPM
  purpose_detail: X PixMap (XPM) is an image file format used by the X Window System.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/X_PixMap
- id: B2AI_STANDARD:392
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: xarray
  formal_specification: https://github.com/pydata/xarray
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:1
  - B2AI_SUBSTRATE:50
  is_open: true
  name: xarray
  purpose_detail: An open source project and Python package that introduces labels
    in the form of dimensions, coordinates, and attributes on top of raw NumPy-like
    arrays, which allows for more intuitive, more concise, and less error-prone user
    experience.
  requires_registration: false
  url: https://xarray.dev/
- id: B2AI_STANDARD:393
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: YAML Ain't Markup Language
  is_open: true
  name: YAML
  purpose_detail: "YAML (YAML Ain't Markup Language, originally Yet Another Markup
    Language) is a human-readable data serialization language designed for configuration
    files, data exchange between programming languages with different data structures,
    and representation of complex hierarchical data in a format that prioritizes readability
    and minimal syntax over machine parsing efficiency. YAML's design philosophy emphasizes
    whitespace-based indentation rather than brackets or braces, uses intuitive syntax
    for common data types (scalars, lists, dictionaries/mappings), supports multi-line
    strings without escape characters, allows inline comments prefixed with pound symbol, and
    provides advanced features including anchors and aliases for avoiding repetition,
    custom data type tags, and document streaming with multiple YAML documents in
    a single file separated by ---. The language is a superset of JSON, meaning any
    valid JSON document is also valid YAML, but offers more expressive syntax with
    less visual noise, making it the preferred format for human-authored configuration
    files in DevOps, continuous integration/deployment pipelines (GitHub Actions,
    GitLab CI, CircleCI, Travis CI), container orchestration (Kubernetes manifests,
    Docker Compose), infrastructure-as-code tools (Ansible playbooks, Helm charts,
    CloudFormation templates), and application configuration management. YAML parsers
    exist for virtually all programming languages (PyYAML and ruamel.yaml for Python,
    js-yaml for JavaScript, SnakeYAML for Java, yaml-cpp for C++, gopkg.in/yaml for
    Go), enabling cross-language data interchange in polyglot software systems and
    supporting serialization of native data structures from one language for consumption
    by another. In scientific computing and data science, YAML serves as a lightweight
    schema language for defining data models and metadata standards (LinkML, FAIR
    Genomes semantic model, OpenAPI specifications, JSON Schema), configuration format
    for machine learning experiment tracking (MLflow, Weights & Biases, Hydra), pipeline
    definition files for workflow engines (Snakemake, Nextflow, CWL), and package
    metadata (conda environment.yml, Python pyproject.toml's dynamic dependencies).
    YAML's human-friendliness makes it suitable for version control and collaborative
    editing, as diffs are readable and merge conflicts can be resolved without specialized
    tools, though this same flexibility introduces security risks (arbitrary code
    execution through deserialization of unsafe types) that require careful parser
    configuration and validation of untrusted input. The format is maintained by the
    YAML community with formal specification (YAML 1.2.2 as of 2021) and reference
    implementations, and has become ubiquitous in cloud-native software development,
    where entire application stacks are defined as YAML manifests that declaratively
    specify desired infrastructure state, enabling GitOps workflows where infrastructure
    changes are reviewed, version-controlled, and audited through standard software
    development practices applied to configuration files."
  requires_registration: false
  url: https://en.wikipedia.org/wiki/YAML
- id: B2AI_STANDARD:394
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Zarr
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:1
  - B2AI_SUBSTRATE:24
  - B2AI_SUBSTRATE:51
  is_open: true
  name: Zarr
  purpose_detail: A format for storage of large N-dimensional typed arrays. Has implementations
    in multiple programming languages.
  requires_registration: false
  url: https://zarr.dev/
- id: B2AI_STANDARD:395
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ZIP compressed file format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:52
  is_open: true
  name: ZIP
  purpose_detail: An archive file format that supports lossless data compression.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/ZIP_(file_format)
- id: B2AI_STANDARD:396
  category: B2AI_STANDARD:ModelRepository
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: AdapterHub
  formal_specification: https://github.com/adapter-hub/Hub
  is_open: true
  name: AdapterHub
  publication: doi:10.48550/arXiv.2007.07779
  purpose_detail: Adapter refers to a set of newly introduced weights, typically within
    the layers of a transformer model. Adapters provide an alternative to fully fine-tuning
    the model for each downstream task, while maintaining performance. AdapterHub
    builds on the HuggingFace transformers framework, requiring as little as two additional
    lines of code to train adapters for a downstream task.
  requires_registration: false
  url: https://adapterhub.ml/
- id: B2AI_STANDARD:397
  category: B2AI_STANDARD:ModelRepository
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Bioimage Model Zoo
  formal_specification: https://github.com/bioimage-io/bioimage.io
  is_open: true
  name: Bioimage
  publication: doi:10.1101/2022.06.07.495102
  purpose_detail: A community-driven, fully open resource where standardized pre-trained
    models can be shared, explored, tested, and downloaded for further adaptation
    or direct deployment in multiple end user-facing tools (e.g., ilastik, deepImageJ,
    QuPath, StarDist, ImJoy, ZeroCostDL4Mic, CSBDeep).
  requires_registration: false
  url: https://bioimage.io/#/
- id: B2AI_STANDARD:398
  category: B2AI_STANDARD:ModelRepository
  collection:
  - dataregistry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Hugging Face  Models
  formal_specification: https://github.com/huggingface/huggingface_hub
  is_open: true
  name: HF Models
  purpose_detail: machine learning models, with focus on language models and NLP
  requires_registration: false
  url: https://huggingface.co/models
- id: B2AI_STANDARD:399
  category: B2AI_STANDARD:ModelRepository
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Model Zoo
  is_open: true
  name: modelzoo.co
  purpose_detail: 'Model Zoo (modelzoo.co) is a curated discovery platform that aggregates pre-trained machine learning models from diverse frameworks (TensorFlow, PyTorch, Caffe, MXNet, ONNX), providing a centralized index for finding models across computer vision, natural language processing, speech recognition, and reinforcement learning domains. Originally launched as a community-driven resource, Model Zoo enables practitioners to search for state-of-the-art architectures by task (object detection, semantic segmentation, translation), dataset (ImageNet, COCO, WMT), or framework compatibility. The platform serves as a model marketplace connecting researchers publishing novel architectures with practitioners seeking production-ready implementations, reducing the barrier to adopting cutting-edge techniques. Model Zoo listings typically include architecture descriptions, training configurations, performance benchmarks (accuracy, inference speed), framework-specific code repositories, and pre-trained weight files for transfer learning. Unlike framework-specific repositories (PyTorch Hub, TensorFlow Hub), Model Zoo provides cross-framework search capabilities, enabling users to discover equivalent architectures implemented in multiple ecosystems and compare performance characteristics across frameworks. The platform supports both academic research models (recent conference publications) and industry-proven architectures (ResNet, BERT variants), with community ratings and download metrics indicating model popularity and reliability. Model Zoo facilitates reproducibility by linking to original papers, training datasets, and hyperparameter configurations, while model cards provide metadata on intended use cases, known limitations, and ethical considerations. For AI/ML practitioners, Model Zoo accelerates prototyping by providing a starting point for transfer learning, enabling rapid experimentation with pre-trained models fine-tuned on domain-specific data rather than training from scratch, particularly valuable when compute resources or labeled data are limited.'
  requires_registration: false
  url: https://modelzoo.co/
- id: B2AI_STANDARD:400
  category: B2AI_STANDARD:ModelRepository
  collection:
  - dataregistry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: OpenML
  formal_specification: https://github.com/openml/OpenML
  is_open: true
  name: OpenML
  purpose_detail: OpenML is an open platform for sharing datasets, algorithms, and
    experiments
  requires_registration: true
  url: https://www.openml.org/
- id: B2AI_STANDARD:401
  category: B2AI_STANDARD:ModelRepository
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: PyTorch Hub
  formal_specification: https://github.com/pytorch/hub
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:33
  is_open: true
  name: PyTorch Hub
  purpose_detail: 'PyTorch Hub is the official model repository and distribution system for the PyTorch deep learning framework, providing programmatic access to pre-trained models through a standardized torch.hub API that enables single-line model loading with automatic dependency resolution and weight downloading. Launched by Facebook AI Research (now Meta AI) and the PyTorch Foundation, Hub hosts curated models from leading research institutions including FAIR, NVIDIA, Hugging Face, Intel, and academic labs, covering computer vision (YOLOv5, ResNet, EfficientNet), NLP (Transformers, RoBERTa), speech (Silero models, Tacotron2), and video understanding (SlowFast, X3D) domains. Each Hub model is defined by a hubconf.py file in a GitHub repository that specifies entry points, dependencies, and preprocessing pipelines, ensuring reproducible model instantiation across environments. PyTorch Hub supports both feature extraction (frozen backbone models) and fine-tuning workflows (unfrozen weights), with models returning standard torch.nn.Module objects compatible with PyTorch training loops, data loaders, and distributed training frameworks (DDP, FSDP). The platform integrates with PyTorch''s TorchScript compilation for deployment optimization, ONNX export for cross-framework compatibility, and TorchServe for production serving. Hub models include metadata specifying input/output tensor shapes, preprocessing requirements (normalization statistics, resize dimensions), and performance benchmarks (latency, throughput) across hardware configurations (CPU, GPU, mobile). The repository supports version pinning via Git commit hashes or tags, enabling deterministic model loading and reproducible research results. For researchers, PyTorch Hub accelerates experimentation by providing battle-tested implementations of recent architectures (often released alongside conference publications) with pre-trained ImageNet, COCO, or Kinetics weights, reducing training time from weeks to hours through transfer learning. In AI/ML pipelines, Hub models serve as feature extractors for downstream tasks (medical imaging, satellite analysis), few-shot learning backbones, and initialization points for domain adaptation, with the torch.hub.load() API supporting custom repositories for internal enterprise model sharing.'
  related_to:
  - B2AI_STANDARD:816
  requires_registration: false
  url: https://pytorch.org/hub/
- id: B2AI_STANDARD:402
  category: B2AI_STANDARD:ModelRepository
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Tensorflow Hub
  has_relevant_organization:
  - B2AI_ORG:37
  is_open: true
  name: TFHub
  purpose_detail: 'TensorFlow Hub (tfhub.dev) is Google''s official repository for reusable machine learning modules, providing pre-trained model components as SavedModel or TF2 format assets that integrate seamlessly into TensorFlow and Keras workflows via the tensorflow-hub library. Launched in 2018, TF Hub pioneered the concept of "transfer learning building blocks" by packaging not just model weights but complete computational graphs including preprocessing layers, embedding extractors, and prediction heads that can be composed into larger architectures. Hub modules span text embeddings (Universal Sentence Encoder, BERT, LaBSE), image feature vectors (MobileNet, EfficientNet, ResNet), object detection (SSD, Faster R-CNN), image generation (BigGAN, StyleGAN), video understanding (I3D, MoViNet), and audio processing (YAMNet, TRILL). Each module provides a consistent interface via hub.KerasLayer or hub.load(), supporting both feature extraction (trainable=False) and fine-tuning (trainable=True) modes with automatic gradient flow through module internals. TF Hub emphasizes model cards with detailed documentation on training data, performance metrics, intended use cases, and ethical considerations (bias, fairness), promoting responsible AI deployment. The platform supports multiple serving formats including TensorFlow Lite for mobile/edge deployment, TensorFlow.js for in-browser inference, and TensorFlow Serving for production APIs, with modules optimized for quantization and pruning. Hub''s standardized interface enables model composition where text embeddings feed into classification heads, or image encoders combine with text encoders for multimodal learning. For researchers, TF Hub reduces training time and computational costs by providing pre-trained representations on large-scale datasets (Wikipedia, ImageNet, YouTube-8M) that transfer effectively to specialized domains with limited data. In AI/ML production systems, Hub modules accelerate deployment by providing battle-tested, versioned components with defined input/output signatures, enabling A/B testing of different encoders and rapid iteration on model architectures without retraining entire pipelines from scratch.'
  requires_registration: false
  url: https://tfhub.dev/
- id: B2AI_STANDARD:403
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Acoustical Society of America / American National Standards Institute
    S3.20
  has_relevant_organization:
  - B2AI_ORG:7
  - B2AI_ORG:4
  is_open: false
  name: ASA/ANSI S3.20
  purpose_detail: Definitions for terms used in human bioacoustics.
  requires_registration: true
  url: https://webstore.ansi.org/Standards/ASA/ANSIASAS3202015R2020
- id: B2AI_STANDARD:404
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Alzheimer's Disease Ontology
  formal_specification: https://github.com/Fraunhofer-SCAI-Applied-Semantics/ADO
  is_open: true
  name: ADO
  purpose_detail: concepts related to Alzheimer's Disease
  requires_registration: false
  url: https://github.com/Fraunhofer-SCAI-Applied-Semantics/ADO
- id: B2AI_STANDARD:405
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Antibiotic Resistance Ontology
  formal_specification: https://github.com/arpcard/aro
  is_open: true
  name: ARO
  publication: doi:10.1093/nar/gkz935
  purpose_detail: Antibiotic resistance genes and mutations
  requires_registration: false
  url: https://github.com/arpcard/aro
- id: B2AI_STANDARD:406
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Apollo Structured Vocabulary
  formal_specification: https://github.com/ApolloDev/apollo-sv
  is_open: true
  name: APOLLO_SV
  purpose_detail: Terms and relations for interoperation between epidemic models and
    public health application software.
  requires_registration: false
  url: https://github.com/ApolloDev/apollo-sv
- id: B2AI_STANDARD:407
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Artificial Intelligence Ontology
  formal_specification: https://github.com/berkeleybop/artificial-intelligence-ontology
  is_open: true
  name: AIO
  purpose_detail: The Artificial Intelligence Ontology (AIO) is a comprehensive formal
    ontology that provides standardized terminology and semantic relationships for
    describing artificial intelligence systems, methods, and concepts. Developed to
    address the need for consistent AI terminology across research and applications,
    AIO models classes and relationships describing deep learning networks, their
    component layers and activation functions, machine learning algorithms, data processing
    techniques, and potential algorithmic biases. The ontology contains 443 classes
    organized in a hierarchical structure with a maximum depth of 5 levels, covering
    fundamental AI concepts from basic computational methods to complex neural architectures.
    AIO serves as a critical resource for AI researchers, practitioners, and systems
    developers who need standardized vocabularies for annotating AI models, describing
    experimental procedures, ensuring reproducibility, and enabling semantic interoperability
    between AI systems and databases. The ontology facilitates automated reasoning
    about AI systems, supports metadata annotation for AI workflows, and contributes
    to the broader goal of making artificial intelligence research more findable,
    accessible, interoperable, and reusable.
  requires_registration: false
  url: https://bioportal.bioontology.org/ontologies/AIO
  has_application:
  - id: B2AI_APP:59
    category: B2AI:Application
    name: AI/ML Ontology for Model Metadata and Pipeline Documentation
    description: AIO (Artificial Intelligence Ontology) is used in biomedical AI for
      standardizing the description of machine learning models, algorithms, and workflows,
      enabling semantic search for AI methods, automated model selection, and reproducible
      research documentation. Researchers leverage AIO to annotate AI models with
      formal descriptions of their architecture, training data requirements, and applicable
      use cases, facilitating discovery of appropriate models for specific biomedical
      tasks. The ontology supports automated reasoning about AI pipeline compatibility,
      enables knowledge graphs that link models to publications and datasets, and
      provides structured vocabulary for documenting AI experiments in compliance
      with reproducibility standards. AIO enables large language models to better
      understand and recommend AI approaches for biomedical problems.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:408
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Basic Formal Ontology
  formal_specification: https://github.com/BFO-ontology/BFO
  is_open: true
  name: BFO
  publication: doi:10.3233/AO-160164
  purpose_detail: 'Basic Formal Ontology (BFO) is a small, upper-level ontology designed for supporting
    information retrieval, analysis, and integration in scientific and other domains.
    As a genuine upper ontology, BFO provides foundational categories that are domain-neutral
    and applicable across all areas of scientific investigation, rather than containing
    domain-specific terms. BFO distinguishes between continuants (entities that endure
    through time, such as objects, qualities, and spatial regions) and occurrents
    (entities that unfold over time, such as processes and temporal regions), providing
    a rigorous framework for representing the fundamental structure of reality. The
    ontology employs formal logic (first-order logic, OWL2) to define its 39 core
    classes and relations, ensuring precise semantics and enabling automated reasoning.
    BFO is used by over 550 ontology-driven projects worldwide as the top-level framework
    for domain ontologies in biomedicine (OBO Foundry ontologies like GO, CHEBI, Uberon),
    healthcare (OGMS for disease), environmental science, manufacturing, and military
    intelligence. The ontology promotes interoperability by providing consistent upper-level
    structure, enabling ontology integration and cross-domain data federation. BFO
    has been developed through extensive international collaboration, with contributions
    from philosophers, logicians, and domain scientists, and is continuously refined
    based on practical applications. In AI/ML contexts, BFO provides foundational
    structure for knowledge graphs, enabling ontology-guided reasoning, semantic data
    integration across heterogeneous sources, knowledge representation for explainable
    AI, and principled ontology alignment, supporting knowledge-driven machine learning
    where symbolic foundations enhance statistical learning with formal semantics.'
  requires_registration: false
  url: http://basic-formal-ontology.org/
- id: B2AI_STANDARD:409
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Biological Collections Ontology
  formal_specification: https://github.com/BiodiversityOntologies/bco
  is_open: true
  name: BCO
  purpose_detail: Biodiversity data, including data on museum collections and environmental/metagenomic
    samples.
  requires_registration: false
  url: https://github.com/BiodiversityOntologies/bco
- id: B2AI_STANDARD:410
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Biological Imaging Methods Ontology
  formal_specification: https://github.com/CRBS/Biological_Imaging_Methods_Ontology/
  is_open: true
  name: FBBI
  purpose_detail: Sample preparation, visualization and imaging methods used in biomedical
    research.
  requires_registration: false
  url: http://cellimagelibrary.org/
- id: B2AI_STANDARD:411
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Biological Spatial Ontology
  formal_specification: https://github.com/obophenotype/biological-spatial-ontology
  is_open: true
  name: BSPO
  publication: doi:10.1186/2041-1480-5-34
  purpose_detail: Spatial concepts, anatomical axes, gradients, regions, planes, sides,
    and surfaces.
  requires_registration: false
  url: https://github.com/obophenotype/biological-spatial-ontology
- id: B2AI_STANDARD:412
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: BRENDA tissue / enzyme source
  formal_specification: https://github.com/BRENDA-Enzymes/BTO
  is_open: true
  name: BTO
  publication: doi:10.1093/nar/gkq968
  purpose_detail: A structured controlled vocabulary for the source of an enzyme comprising
    tissues, cell lines, cell types and cell cultures.
  requires_registration: false
  url: http://www.brenda-enzymes.org/
- id: B2AI_STANDARD:413
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: C. elegans phenotype ontology
  formal_specification: https://github.com/obophenotype/c-elegans-phenotype-ontology
  is_open: true
  name: WBPHENOTYPE
  publication: doi:10.1186/1471-2105-12-32
  purpose_detail: A structured controlled vocabulary of Caenorhabditis elegans phenotypes.
  requires_registration: false
  url: https://github.com/obophenotype/c-elegans-phenotype-ontology
- id: B2AI_STANDARD:414
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Cardiovascular Disease Ontology
  formal_specification: https://github.com/OpenLHS/CVDO
  is_open: true
  name: CVDO
  purpose_detail: Entities related to cardiovascular diseases.
  requires_registration: false
  url: https://github.com/OpenLHS/CVDO
- id: B2AI_STANDARD:415
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Cell Line Ontology
  formal_specification: https://github.com/CLO-Ontology/CLO
  is_open: true
  name: CLO
  publication: doi:10.1186/2041-1480-5-3
  purpose_detail: Standardize and integrate cell line information and to support computer-assisted
    reasoning.
  requires_registration: false
  url: https://github.com/CLO-Ontology/CLO
- id: B2AI_STANDARD:416
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Cell Ontology
  formal_specification: https://github.com/obophenotype/cell-ontology
  is_open: true
  name: CL
  publication: doi:10.1186/s13326-016-0088-7
  purpose_detail: "The Cell Ontology (CL) is an OBO Foundry ontology covering biological
    cell types with curation focused on animal cell types and interoperability with
    specialized cell type ontologies in other biological domains. CL is tightly integrated
    with the Uberon multi-species anatomy ontology (for recording cell location) and
    the Gene Ontology (GO, which uses CL as its main cell type reference and provides
    cell function annotations). Built on FAIR principles, CL enables community-driven
    curation with active embedded editors from multiple projects responsive on the
    issue tracker. The ontology is released in multiple standard formats (OWL/RDF/XML,
    OBO, JSON obographs) with multiple variants: full (all imports merged, reasoner-classified),
    base (not pre-reasoned, only CL axioms including non-CL class references), and
    simple (pre-reasoned, CL classes only), all with resolvable version IRIs for persistent
    access. CL is integrated into standard tools including Ubergraph (for logical
    queries like finding cells by location), Ontology Access Kit (OAK), and major
    browsers (OLS, Ontobee). The ontology supports major initiatives including BICCN
    cell type knowledge explorer, HubMap Human Reference Atlas, ENCODE, FANTOM5, Single
    Cell Expression Atlas, Human Cell Atlas, and CellKB. CL enables AI/ML applications
    including OnClass for automatic cell type classification, Brain Data Standards
    Ontology (BDSO) for cell type navigation and search, and provides standardized
    cell type annotations essential for single-cell omics machine learning, cross-dataset
    integration, and cell type discovery algorithms."
  requires_registration: false
  url: https://cell-ontology.github.io/
- id: B2AI_STANDARD:417
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:3
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CHEBI Integrated Role Ontology
  formal_specification: https://github.com/obophenotype/chiro
  has_relevant_organization:
  - B2AI_ORG:29
  is_open: true
  name: CHIRO
  publication: doi:10.26434/chemrxiv.12591221.v1
  purpose_detail: A distinct role hierarchy for chemicals.
  requires_registration: false
  url: https://github.com/obophenotype/chiro
- id: B2AI_STANDARD:418
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:3
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Chemical Entities of Biological Interest
  formal_specification: https://github.com/ebi-chebi/ChEBI
  has_relevant_organization:
  - B2AI_ORG:29
  is_open: true
  name: CHEBI
  publication: doi:10.1093/nar/gkv1031
  purpose_detail: "ChEBI is an open-access OBO Foundry database and ontology of chemical
    entities covering constitutionally or isotopically distinct atoms, molecules,
    ions, ion pairs, radicals, complexes, conformers, groups, chemical substances,
    and classes of molecular entities. The database contains over 195,000 entries
    of naturally occurring molecules or synthetic compounds used to intervene in biological
    processes, with macromolecules directly encoded by genomes (nucleic acids, proteins,
    peptides) excluded. ChEBI incorporates ontological classification defining relationships
    between chemical entities and their parent/child classes, enabling queries based
    on chemical class and role. The database provides comprehensive information including
    nomenclature following IUPAC and NC-IUBMB standards, molecular formulas, InChI
    and SMILES identifiers, literature citations, cross-references to other databases,
    and species data. ChEBI supports text and structure searches and is released in
    multiple formats (SDF, OBO, OWL, flat file, SQL dumps). As an ELIXIR Core Data
    Resource and Global Core Biodata Resource, ChEBI enables AI/ML applications in
    cheminformatics, drug discovery, metabolomics, systems biology, and chemical-phenotype
    association studies."
  requires_registration: false
  url: https://www.ebi.ac.uk/chebi/
- id: B2AI_STANDARD:419
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:3
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Chemical Information Ontology
  formal_specification: https://github.com/semanticchemistry/semanticchemistry
  is_open: true
  name: CHEMINF
  publication: doi:10.1371/journal.pone.0025513
  purpose_detail: Descriptors commonly used in cheminformatics software applications
    and the algorithms which generate them.
  requires_registration: false
  url: https://github.com/semanticchemistry/semanticchemistry
- id: B2AI_STANDARD:420
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:3
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Chemical Methods Ontology
  formal_specification: https://github.com/rsc-ontologies/rsc-cmo
  is_open: true
  name: CHMO
  purpose_detail: "The Chemical Methods Ontology (CHMO) provides a comprehensive,
    standardized vocabulary for describing experimental methods, techniques, and instruments
    used in chemical analysis, material synthesis, and sample preparation, maintained
    by the Royal Society of Chemistry (RSC) and aligned with OBO Foundry principles.
    CHMO encompasses three primary domains: analytical methods for data collection
    (mass spectrometry including ESI-MS, MALDI-TOF, GC-MS, LC-MS; spectroscopic techniques
    including NMR, IR, UV-Vis, Raman, X-ray spectroscopy; electron microscopy including
    SEM, TEM, STEM; and diffraction methods), separation and sample preparation techniques
    (chromatography including HPLC, GC, TLC, size-exclusion, affinity, ion-exchange;
    electrophoresis including SDS-PAGE, capillary electrophoresis, isoelectric focusing;
    sample ionization methods including electrospray, MALDI, electron impact; and
    extraction procedures), and material synthesis methods (chemical vapor deposition,
    epitaxy, sol-gel processes, crystallization, polymerization, and nanoparticle
    synthesis). The ontology also describes instruments and equipment (mass spectrometers,
    chromatography columns, detectors, vacuum systems, heating/cooling apparatus)
    and their components, operational parameters (temperature, pressure, flow rate,
    voltage, resolution), and measurement outputs (spectra, chromatograms, diffraction
    patterns, images). CHMO integrates with OBI (Ontology for Biomedical Investigations)
    for process and measurement concepts, CHEBI (Chemical Entities of Biological Interest)
    for chemical substances, and other OBO ontologies for cross-domain applications
    in metabolomics, proteomics, and materials science. The ontology provides both
    textual and formal OWL definitions enabling automated reasoning, classification
    of methods by input material types, conditions of application, and output data
    formats. CHMO supports reproducibility in chemical research by standardizing method
    descriptions in electronic laboratory notebooks, method sections of publications,
    protocols repositories, and analytical chemistry databases. Applications include
    semantic search for analytical protocols, automated method selection based on
    sample properties, FAIRification of chemical data workflows, text mining of chemical
    literature for method extraction, and quality control in analytical laboratories.
    The ontology is developed using the Ontology Development Kit (ODK) and distributed
    under CC-BY-4.0 license, with releases available in OBO, OWL, and JSON formats
    through http://purl.obolibrary.org/obo/chmo.owl."
  requires_registration: false
  url: https://github.com/rsc-ontologies/rsc-cmo
- id: B2AI_STANDARD:421
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: clinical LABoratory Ontology
  formal_specification: https://github.com/OpenLHS/LABO
  is_open: true
  name: LABO
  publication: doi:10.5281/zenodo.6522019
  purpose_detail: An ontology of informational entities formalizing clinical laboratory
    tests prescriptions and reporting documents.
  requires_registration: false
  url: https://github.com/OpenLHS/LABO
- id: B2AI_STANDARD:422
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Clinical measurement ontology
  formal_specification: https://github.com/rat-genome-database/CMO-Clinical-Measurement-Ontology
  is_open: true
  name: CMO
  publication: doi:10.1186/2041-1480-4-26
  purpose_detail: Morphological and physiological measurement records generated from
    clinical and model organism research and health programs.
  requires_registration: false
  url: https://github.com/rat-genome-database/CMO-Clinical-Measurement-Ontology
- id: B2AI_STANDARD:423
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Clinical Vaccines Administered
  has_relevant_organization:
  - B2AI_ORG:13
  is_open: true
  name: CVX
  purpose_detail: The Clinical Vaccines Administered (CVX) code set is a comprehensive
    standardized vocabulary developed and maintained by the CDC's National Center
    for Immunization and Respiratory Diseases (NCIRD) that provides unique numeric
    identifiers for all vaccines available in the United States healthcare system.
    This essential code set includes both active vaccines currently available for
    patient administration and inactive vaccines that may appear in historical immunization
    records, enabling complete tracking of vaccination history across a patient's
    lifetime. CVX codes are specifically designed for use in HL7 Version 2.3.1 and
    2.5.1 immunization messages and electronic health record systems, facilitating
    standardized communication between healthcare providers, immunization information
    systems (IIS), and public health agencies. Each CVX code entry includes detailed
    information about vaccine status (active, inactive, pending, non-US, or never
    active), last update timestamp, and clinical notes providing context about usage
    and availability. When paired with MVX (manufacturer) codes, CVX codes can precisely
    identify specific trade-named vaccine products, supporting accurate inventory
    management, adverse event reporting, vaccine safety monitoring, and public health
    surveillance activities essential for maintaining population immunity and preventing
    vaccine-preventable diseases.
  requires_registration: false
  url: https://www2a.cdc.gov/vaccines/iis/iisstandards/vaccines.asp?rpt=cvx
- id: B2AI_STANDARD:424
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Common Anatomy Reference Ontology
  formal_specification: https://github.com/obophenotype/caro/
  is_open: true
  name: CARO
  purpose_detail: An upper level ontology to facilitate interoperability between existing
    anatomy ontologies for different species.
  requires_registration: false
  url: https://github.com/obophenotype/caro/
- id: B2AI_STANDARD:425
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:4
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Common Terminology Criteria for Adverse Events
  formal_specification: https://ctep.cancer.gov/protocoldevelopment/electronic_applications/ctc.htm
  is_open: true
  name: CTCAE
  purpose_detail: Common Terminology Criteria for Adverse Events (CTCAE) is widely
    accepted throughout the oncology community as the standard classification and
    severity grading scale for adverse events in cancer therapy clinical trials and
    other oncology settings.
  requires_registration: false
  url: https://bioportal.bioontology.org/ontologies/CTCAE
- id: B2AI_STANDARD:426
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Comparative Data Analysis Ontology
  formal_specification: https://github.com/evoinfo/cdao
  is_open: true
  name: CDAO
  publication: doi:10.4137/EBO.S2320
  purpose_detail: A formalization of concepts and relations relevant to evolutionary
    comparative analysis.
  requires_registration: false
  url: https://github.com/evoinfo/cdao
- id: B2AI_STANDARD:427
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Compositional Dietary Nutrition Ontology
  formal_specification: https://github.com/Southern-Cross-Plant-Science/cdno
  is_open: true
  name: CDNO
  publication: doi:10.3389/fnut.2022.928837
  purpose_detail: Nutritional attributes of material entities that contribute to human
    diet.
  requires_registration: false
  url: https://cdno.info/
- id: B2AI_STANDARD:428
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Confidence Information Ontology
  formal_specification: https://github.com/BgeeDB/confidence-information-ontology
  is_open: true
  name: CIO
  publication: doi:10.1093/database/bav043
  purpose_detail: The Confidence Information Ontology (CIO) is a specialized OBO
    Foundry ontology that provides standardized terms for expressing the confidence
    or certainty associated with biological annotations, computational predictions,
    and experimental assertions, enabling researchers and bioinformatics resources
    to capture nuanced distinctions between high-confidence findings backed by multiple
    lines of evidence and tentative conclusions requiring further validation. Developed
    by the Bgee gene expression database team and published in Database (2015), CIO
    addresses the critical need to represent not just what is annotated (e.g., a
    gene-disease association, protein-protein interaction, or gene expression pattern)
    but also the strength of evidence supporting that annotation, distinguishing between
    statements derived from manual expert curation, automated computational inference,
    high-throughput screening, or text mining with varying reliability. The ontology
    defines hierarchical terms describing confidence levels (high confidence, medium
    confidence, low confidence), evidence types contributing to confidence assessments
    (experimental evidence, computational prediction, literature-based inference,
    author statement, curator inference), and sources of uncertainty (incomplete data,
    conflicting evidence, limited reproducibility, indirect evidence). CIO terms are
    structured to integrate with evidence codes from the Evidence and Conclusion Ontology
    (ECO), allowing biocuration teams to specify both the type of evidence (e.g.,
    "inferred from mutant phenotype") and the confidence in that evidence (e.g., "supported
    by single experiment" vs. "confirmed by independent studies"). The ontology supports
    practical use cases including quality assessment of automated annotations in large-scale
    databases (UniProt, Gene Ontology, model organism databases), prioritization
    of experimental targets by filtering high-confidence predictions from computational
    pipelines, meta-analysis weighting schemes that adjust contribution of individual
    studies based on confidence scores, and transparent reporting of AI/ML model predictions
    with associated uncertainty estimates. For machine learning applications in biology,
    CIO enables construction of training datasets with explicit confidence labels
    that inform loss functions and class weighting strategies, supports active learning
    systems that prioritize low-confidence predictions for human review, and facilitates
    ensemble methods that combine multiple predictors with confidence-weighted voting.
    The ontology includes terms for time-dependent confidence (e.g., annotations marked
    as "requires periodic review" due to rapidly evolving evidence), context-specific
    confidence (e.g., "applicable to specific tissue type" vs. "generalizable across
    species"), and mechanisms for tracking provenance of confidence assessments (curator
    identity, date of assessment, version of evidence used). CIO is maintained on
    GitHub with versioned releases, integrates with OBO Foundry best practices (unique
    persistent identifiers, human-readable labels, logical definitions in OWL), and
    is used by gene expression databases (Bgee), orthology resources, pathway databases,
    and systems biology platforms that require principled representation of data quality
    and reliability to support evidence-based research decision-making and computational
    workflows that propagate uncertainty through analysis pipelines.
  requires_registration: false
  url: https://github.com/BgeeDB/confidence-information-ontology
- id: B2AI_STANDARD:429
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:16
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Contributor Role Ontology
  formal_specification: https://github.com/data2health/contributor-role-ontology
  is_open: true
  name: CRO
  purpose_detail: The Contributor Role Ontology (CRO) is an OBO Foundry-aligned ontology
    that provides standardized, machine-readable terms for describing the diverse
    roles and contributions performed by individuals in scientific research projects,
    addressing the limitations of traditional author lists that obscure the specific
    intellectual and practical contributions each person made to a published work.
    Developed by the CD2H (Center for Data to Health) consortium and aligned with
    the CRediT (Contributor Roles Taxonomy) initiative, CRO enables granular attribution
    of research activities including conceptualization, methodology development, software
    implementation, formal analysis, investigation, data curation, writing (original
    draft and review/editing), visualization, supervision, project administration,
    and funding acquisition. The ontology hierarchically organizes contributor roles
    from high-level categories (intellectual contributions, practical work, administrative
    support) to specific activities (e.g., distinguishing between "statistical analysis,"
    "computational modeling," and "experimental validation"), allowing research outputs
    to accurately represent the multi-disciplinary nature of modern science where
    teams include wet-lab scientists, computational researchers, statisticians, data
    managers, software engineers, project coordinators, and community engagement specialists.
    CRO supports interoperability with metadata standards for scholarly communication
    including ORCID profiles, DataCite metadata schema, JATS XML for journal articles,
    and research information management systems (RIMS), enabling automated population
    of contributor metadata in institutional repositories, grant management systems,
    and researcher profile pages. By providing persistent identifiers and formal definitions
    for contribution types, the ontology facilitates impact assessment that goes beyond
    simple publication counts, allowing funders and institutions to recognize diverse
    forms of scientific contribution including data stewardship, software development,
    community building, and mentorship activities that are undervalued in traditional
    citation-based metrics. CRO addresses equity issues in scientific attribution
    by making visible the contributions of early-career researchers, core facility
    staff, data scientists, and other professionals whose work is essential to research
    success but often goes unrecognized in conventional authorship models. The ontology
    includes terms for temporal aspects of contributions (e.g., "conceived original
    idea" vs. "contributed to later revisions"), degree of contribution (lead, equal,
    supporting), and collaborative dynamics (independent work, collaborative effort,
    supervisory role), enabling nuanced representation of team science. For research
    reproducibility and transparency, CRO supports documentation of who performed
    critical methodological steps, who validated results, and who has authority to
    answer questions about specific aspects of published work, addressing challenges
    when corresponding authors cannot respond to inquiries about methods they did
    not personally execute. The ontology integrates with emerging practices in open
    science including preprint servers (bioRxiv, medRxiv) that capture contributor
    roles, data repositories (Zenodo, Dryad, Figshare) that attribute dataset creators,
    and software citation initiatives (CFF, CITATION.cff files) that recognize code
    contributors. CRO is maintained on GitHub with community input, follows OBO Foundry
    principles (unique identifiers, open licensing, semantic versioning), and is being
    adopted by scholarly publishers, academic institutions, and research consortia
    to modernize attribution practices for the era of large-scale, interdisciplinary,
    data-intensive collaborative science.
  requires_registration: false
  url: https://github.com/data2health/contributor-role-ontology
- id: B2AI_STANDARD:430
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Core Ontology for Biology and Biomedicine
  formal_specification: https://github.com/OBOFoundry/COB
  has_relevant_organization:
  - B2AI_ORG:75
  is_open: true
  name: COB
  purpose_detail: The Core Ontology for Biology and Biomedicine (COB) is a foundational
    upper-level ontology developed by the OBO Foundry that provides a minimal set
    of high-level, broadly applicable terms representing fundamental biological and
    biomedical entities, serving as a unifying framework to improve interoperability
    and semantic consistency across the diverse landscape of domain-specific OBO ontologies
    spanning anatomy, development, phenotypes, diseases, chemicals, and experimental
    methods. COB addresses the challenge that different OBO ontologies historically
    defined overlapping or conflicting representations of core concepts (organism,
    cell, anatomical structure, biological process, material entity, information content
    entity), leading to integration difficulties when combining data annotated with
    multiple ontologies or reasoning across ontology boundaries. By establishing authoritative,
    consensus definitions for approximately 50 core terms that appear throughout biological
    knowledge representation, COB enables domain ontologies to import and reuse these
    shared definitions rather than creating redundant or incompatible local versions,
    facilitating automated reasoning, cross-ontology mappings, and federated queries
    across biomedical data resources. The ontology is organized as an OWL (Web Ontology
    Language) hierarchy with formal logical definitions grounded in Basic Formal Ontology
    (BFO) and Relations Ontology (RO), ensuring philosophical rigor and computational
    tractability for description logic reasoners used in semantic web applications
    and knowledge graph construction. COB terms cover fundamental categories including
    material entities (organism, cell, anatomical entity, molecule), processes (biological
    process, molecular function, behavior), qualities and attributes (phenotypic quality,
    measurement datum), and information entities (data item, objective specification,
    model representation), with careful axiomatization to support inference of implicit
    relationships and detection of logical inconsistencies in downstream ontologies.
    The development process involves extensive community consultation through OBO
    Foundry working groups, GitHub issue discussions, and workshops to achieve consensus
    on definitions that balance precision for computational applications with accessibility
    for domain scientists, addressing edge cases and philosophical debates about entity
    boundaries, parthood relations, and temporal aspects of biological entities. COB
    is designed for lightweight adoption, providing a "core" that domain ontologies
    can extend with specialized terms while maintaining interoperabilityfor example,
    Uberon (anatomy), GO (Gene Ontology), ChEBI (Chemical Entities of Biological
    Interest), and MONDO (disease ontology) can all reference COB's definitions of
    "organism" or "biological process" as shared anchor points. For data integration
    and AI/ML applications, COB enables construction of unified knowledge graphs that
    span multiple biological domains by providing consistent semantics for entity
    types and relations, supports transfer learning across datasets annotated with
    different ontologies by mapping domain-specific terms to shared COB concepts,
    and facilitates development of foundation models for biomedical NLP by establishing
    ground-truth semantic categories for entity recognition and relation extraction.
    The ontology includes provenance metadata documenting the source ontologies contributing
    each term, editorial notes explaining design decisions, and mappings to external
    resources (MeSH, SNOMED CT, NCIT) to bridge OBO Foundry ontologies with clinical
    terminologies and legacy systems. COB is maintained on GitHub with public development,
    follows OBO Foundry principles (open licensing, unique persistent identifiers,
    semantic versioning), and undergoes periodic releases coordinated with major OBO
    ontology updates to ensure ecosystem-wide compatibility as biological knowledge
    and ontology best practices evolve.
  requires_registration: false
  url: https://github.com/OBOFoundry/COB
- id: B2AI_STANDARD:431
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Coronavirus Infectious Disease Ontology
  formal_specification: https://github.com/cido-ontology/cido
  is_open: true
  name: CIDO
  publication: doi:10.1038/s41597-020-0523-6
  purpose_detail: Ontologically represent and standardize various aspects of coronavirus
    infectious.
  requires_registration: false
  url: https://github.com/cido-ontology/cido
- id: B2AI_STANDARD:432
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CTO Core Ontology of Clinical Trials
  formal_specification: https://github.com/ClinicalTrialOntology/CTO/
  has_relevant_organization:
  - B2AI_ORG:33
  is_open: true
  name: CTO
  purpose_detail: A structured resource integrating basic terms and concepts in the
    context of clinical trials.
  requires_registration: false
  url: https://github.com/ClinicalTrialOntology/CTO/
- id: B2AI_STANDARD:433
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Data Use Ontology
  formal_specification: https://github.com/EBISPOT/DUO
  has_relevant_organization:
  - B2AI_ORG:34
  is_open: true
  name: DUO
  purpose_detail: 'The Data Use Ontology (DUO) provides standardized vocabulary for describing
    data use conditions, restrictions, and requirements in biomedical and genomics
    research. Developed by the Global Alliance for Genomics and Health (GA4GH), DUO
    enables semantic tagging of datasets with consent-derived restrictions (e.g.,
    health/medical/biomedical research only, disease-specific research, non-commercial
    use, research ethics approval). The ontology supports automated data access matching,
    where algorithms determine whether a researcher''s purpose is compatible with dataset
    restrictions, enabling services like DUOS (Data Use Oversight System) and EGA
    (European Genome-phenome Archive) to streamline data sharing while respecting
    participant consent. DUO extends NIH dbGaP data use categories with hierarchical
    structure for logical inference, includes consent codes for international data
    sharing, and implements ADA-M (Automated Data Access Matrix) for granular permissions.
    Used by 60+ million weekly downloads, DUO facilitates GDPR-aware data governance,
    phenotype-driven differential diagnostics, and translational research. The ontology
    ensures that informed consent language translates into machine-readable terms,
    accelerating responsible data reuse for AI/ML training datasets, clinical phenotyping,
    and genomic diagnostics while maintaining participant privacy and ethical oversight.'
  requires_registration: false
  url: https://github.com/EBISPOT/DUO
- id: B2AI_STANDARD:434
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Devices, Experimental scaffolds and Biomaterials Ontology
  formal_specification: https://github.com/ProjectDebbie/Ontology_DEB
  is_open: true
  name: DEB
  publication: doi:10.1002/adfm.201909910
  purpose_detail: An ontology developed to facilitate information curation in the
    area of medical devices, experimental scaffolds and biomaterials.
  requires_registration: false
  url: https://bioportal.bioontology.org/ontologies/DEB
- id: B2AI_STANDARD:435
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Devops Infrastructure Ontology
  formal_specification: https://github.com/oeg-upm/devops-infra
  is_open: true
  name: devops-infra
  purpose_detail: This ontology network aims at representing the main sets of entities
    and relationships used in the context of DevOps infrastructure. It is the result
    of a collaboration between Huawei Research Ireland and the Ontology Engineering
    Group at Universidad Politcnica de Madrid. It originally started from an analysis
    of the Configuration Management Databases used by Huawei Research Ireland for
    the management of a large part of its DevOps infrastructure, and has evolved into
    an ontology that may be used as a starting point for the standardisation of the
    representation of CMDB-related data across vendors.
  requires_registration: false
  url: https://oeg-upm.github.io/devops-infra/index.html
- id: B2AI_STANDARD:436
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Disease Drivers Ontology
  formal_specification: https://github.com/DiseaseOntology/DiseaseDriversOntology
  is_open: true
  name: DISDRIV
  purpose_detail: Ontology for drivers and triggers of human diseases, built to classify
    ExO ontology exposure stressors. An application ontology.
  requires_registration: false
  url: https://github.com/DiseaseOntology/DiseaseDriversOntology
- id: B2AI_STANDARD:437
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:25
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Drosophila Phenotype Ontology
  formal_specification: https://github.com/FlyBase/drosophila-phenotype-ontology
  is_open: true
  name: DPO
  publication: doi:10.1186/2041-1480-4-30
  purpose_detail: Commonly encountered and/or high level Drosophila phenotypes.
  requires_registration: false
  url: https://github.com/FlyBase/flybase-controlled-vocabulary/wiki
- id: B2AI_STANDARD:438
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:8
  - B2AI_TOPIC:26
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Drug Target Ontology
  formal_specification: http://drugtargetontology.org/
  is_open: true
  name: DTO
  publication: doi:10.1186/s13326-017-0161-x
  purpose_detail: Drug Target Ontology (DTO) is being developed at the University
    of Miami in the research group of Stephan Schrer. DTO was developed as part of
    the Illuminating the Druggable Genome (IDG) project (https://commonfund.nih.gov/idg/overview),
    is supported by grant (IDG Knowledge Management Center, (U54CA189205). DTO is
    a novel semantic framework to formalize knowledge about drug targets and is developed
    as a reference for drug targets with the longer-term goal to create a community
    standard that will facilitate the integration of diverse drug discovery information
    from numerous heterogeneous resources.
  requires_registration: false
  url: https://bioportal.bioontology.org/ontologies/DTO
- id: B2AI_STANDARD:439
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:8
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Drug-drug Interaction and Drug-drug Interaction Evidence Ontology
  formal_specification: https://github.com/DIDEO/DIDEO
  is_open: true
  name: DIDEO
  purpose_detail: The Potential Drug-drug Interaction and Potential Drug-drug Interaction
    Evidence Ontology
  requires_registration: false
  url: https://github.com/DIDEO/DIDEO
- id: B2AI_STANDARD:440
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: EMBRACE Data And Methods Ontology
  formal_specification: https://github.com/edamontology/edamontology
  has_relevant_organization:
  - B2AI_ORG:29
  is_open: true
  name: EDAM
  publication: doi:10.1093/bioinformatics/btt113
  purpose_detail: Data types, identifiers, and formats
  requires_registration: false
  url: https://github.com/edamontology/edamontology
- id: B2AI_STANDARD:441
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Emotion Ontology
  formal_specification: https://github.com/jannahastings/emotion-ontology
  is_open: true
  name: MFOEM
  purpose_detail: Affective phenomena such as emotions, moods, appraisals and subjective
    feelings.
  requires_registration: false
  url: https://github.com/jannahastings/emotion-ontology
- id: B2AI_STANDARD:442
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:11
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Environment Ontology
  formal_specification: https://github.com/EnvironmentOntology/envo
  is_open: true
  name: ENVO
  purpose_detail: Environmental systems, components, and processes.
  requires_registration: false
  url: http://environmentontology.org/
- id: B2AI_STANDARD:443
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:11
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Environmental conditions, treatments and exposures ontology
  formal_specification: https://github.com/EnvironmentOntology/environmental-exposure-ontology
  is_open: true
  name: ECTO
  purpose_detail: Exposures to experimental treatments of plants and model organisms.
  requires_registration: false
  url: https://github.com/EnvironmentOntology/environmental-exposure-ontology
- id: B2AI_STANDARD:444
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - standards_process_maturity_draft
  - implementation_maturity_pilot
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Evidence Graph Ontology
  formal_specification: https://github.com/EvidenceGraph/EVI
  has_relevant_organization:
  - B2AI_ORG:116
  is_open: true
  name: EVI
  purpose_detail: The Evidence Graph Ontology (EVI) extends core concepts from the
    W3C Provenance Ontology [PROV-O] to describe evidence for correctness of findings
    in biomedical publications. The semantic data model in EVI is expressed using
    OWL2 Web Ontology Language.
  requires_registration: false
  url: https://evidencegraph.github.io/EVI/index.html
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:60
    category: B2AI:Application
    name: Evidence-Based Biomedical Inference and Literature Mining
    description: EVI (Evidence Graph) ontology is used in AI applications for representing
      and reasoning over biomedical evidence from scientific literature, enabling
      automated hypothesis generation, evidence synthesis, and knowledge graph construction.
      Machine learning models leverage EVI's formal representation of claims, evidence
      types, and epistemic confidence to train systems that extract evidence-based
      relationships from publications, assess the quality and reliability of scientific
      findings, and generate evidence-weighted knowledge graphs. AI applications include
      automated systematic review, drug repurposing through evidence integration,
      and training of large language models that can cite and reason about scientific
      evidence with appropriate confidence levels. EVI enables AI systems to distinguish
      between different types of evidence and perform meta-analytic reasoning.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:445
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Evidence ontology
  formal_specification: https://github.com/evidenceontology/evidenceontology
  is_open: true
  name: ECO
  publication: doi:10.1093/nar/gkab1025
  purpose_detail: An ontology for experimental and other evidence statements.
  requires_registration: false
  url: https://www.evidenceontology.org/
- id: B2AI_STANDARD:446
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Experimental condition ontology
  formal_specification: https://github.com/rat-genome-database/XCO-experimental-condition-ontology
  is_open: true
  name: XCO
  publication: doi:10.1186/2041-1480-4-26
  purpose_detail: Conditions under which physiological and morphological measurements
    are made both in the clinic and in studies involving humans or model organisms.
  requires_registration: false
  url: https://rgd.mcw.edu/rgdweb/ontology/view.html?acc_id=XCO:0000000
- id: B2AI_STANDARD:447
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:11
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Exposure ontology
  formal_specification: https://github.com/CTDbase/exposure-ontology
  is_open: true
  name: EXO
  purpose_detail: Vocabularies for describing exposure data to inform understanding
    of environmental health.
  requires_registration: false
  url: https://github.com/CTDbase/exposure-ontology
- id: B2AI_STANDARD:448
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:25
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Fission Yeast Phenotype Ontology
  formal_specification: https://github.com/pombase/fypo
  is_open: true
  name: FYPO
  publication: doi:10.1093/bioinformatics/btt266
  purpose_detail: Phenotypes observed in fission yeast.
  requires_registration: false
  url: https://github.com/pombase/fypo
- id: B2AI_STANDARD:449
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:8
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Food Interactions with Drugs Evidence Ontology
  formal_specification: https://github.com/getbordea/fideo/
  is_open: true
  name: FIDEO
  purpose_detail: Food-Drug interactions automatically extracted from scientific literature.
  requires_registration: false
  url: https://gitub.u-bordeaux.fr/erias/fideo
- id: B2AI_STANDARD:450
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Food Ontology
  formal_specification: https://github.com/FoodOntology/foodon/
  is_open: true
  name: FOODON
  publication: doi:10.1038/s41538-018-0032-6
  purpose_detail: A broadly scoped ontology representing entities which bear a food
    role. It encompasses materials in natural ecosystems and agriculture tha...
  requires_registration: false
  url: https://foodon.org/
- id: B2AI_STANDARD:451
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Food-Biomarker Ontology
  formal_specification: https://github.com/pcastellanoescuder/FoodBiomarkerOntology
  is_open: true
  name: FOBI
  publication: doi:10.1093/bioinformatics/btab626
  purpose_detail: Represent food intake data and associate it with metabolomic data
  requires_registration: false
  url: https://github.com/pcastellanoescuder/FoodBiomarkerOntology
- id: B2AI_STANDARD:452
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:25
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FuTRES Ontology of Vertebrate Traits
  formal_specification: https://github.com/futres/fovt
  is_open: true
  name: FOVT
  purpose_detail: Application ontology used to convert vertebrate trait data in spreadsheets
    to triples.
  requires_registration: false
  url: https://futres.org/
- id: B2AI_STANDARD:453
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:6
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Gender, Sex, and Sexual Orientation Ontology
  formal_specification: https://github.com/Superraptor/GSSO
  is_open: true
  name: GSSO
  purpose_detail: Terms for annotating interdisciplinary information concerning gender,
    sex, and sexual orientation.
  requires_registration: false
  url: https://gsso.research.cchmc.org/
- id: B2AI_STANDARD:454
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:12
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Gene Ontology
  formal_specification: https://github.com/geneontology/go-ontology
  has_relevant_organization:
  - B2AI_ORG:36
  is_open: true
  name: GO
  publication: doi:10.1093/nar/gkaa1113
  purpose_detail: Function of genes and gene products.
  requires_registration: false
  url: http://geneontology.org/
- id: B2AI_STANDARD:455
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Genomic Epidemiology Ontology
  formal_specification: https://github.com/GenEpiO/genepio
  is_open: true
  name: GENEPIO
  purpose_detail: Vocabulary necessary to identify, document and research foodborne
    pathogens.
  requires_registration: false
  url: http://genepio.org/
- id: B2AI_STANDARD:456
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:4
  - B2AI_TOPIC:6
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Genomics Cohorts Knowledge Ontology
  formal_specification: https://github.com/IHCC-cohorts/GECKO
  is_open: true
  name: GECKO
  purpose_detail: "The Genomics Cohorts Knowledge Ontology (GECKO) provides a standardized
    vocabulary for describing attributes of genomics cohorts and individual-level
    data in population-based research studies. Developed by the CINECA (Common Infrastructure
    for National Cohorts in Europe, Canada, and Africa) project and maintained for
    the International HundredK+ Cohorts Consortium (IHCC), GECKO enables harmonized
    representation of cohort metadata across diverse genomics studies, biobanks, and
    research consortia. The ontology encompasses five major categories: cohort design
    characteristics (prospective/retrospective, longitudinal/cross-sectional, case-control,
    family-based), participant demographics and socioeconomic attributes, phenotypic
    data collection methods (questionnaires, clinical assessments, biospecimen types),
    genomics data types (whole genome sequencing, exome sequencing, genome-wide association
    studies, RNA-seq, methylation arrays), and data access policies with consent frameworks.
    GECKO standardizes terminology for cohort recruitment strategies, inclusion/exclusion
    criteria, sample sizes, age ranges, ancestry populations, geographic locations,
    and follow-up durations critical for cohort discovery and meta-analysis planning.
    The ontology integrates with other OBO Foundry ontologies including BFO (Basic
    Formal Ontology) as top-level and OBI (Ontology for Biomedical Investigations)
    as mid-level, ensuring semantic interoperability. GECKO provides two products:
    the OBO-compliant gecko.owl for formal ontology applications, and ihcc-gecko.owl
    tailored for IHCC cohort cataloging with specialized browser labels and categorization.
    Automated tools based on JSON schema mapping files enable generation of harmonized
    data dictionaries and FAIRified metadata for cohort studies. Applications include
    cohort discovery portals enabling researchers to identify suitable cohorts for
    collaborative studies, standardized phenotype harmonization across studies for
    meta-GWAS, ethical data sharing frameworks through consent ontology terms, and
    FAIR data principles implementation in population genomics repositories. GECKO
    facilitates interoperability between cohort catalogs like BBMRI-ERIC, Maelstrom
    Research, dbGaP, and EGA by providing common terminology for cohort characteristics,
    enabling federated queries across international biobanks. The ontology is distributed
    under Creative Commons Attribution 4.0 International License, ensuring open access
    for academic and commercial research."
  requires_registration: false
  url: https://github.com/IHCC-cohorts/GECKO
- id: B2AI_STANDARD:457
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Genotype Ontology
  formal_specification: https://github.com/monarch-initiative/GENO-ontology
  has_relevant_organization:
  - B2AI_ORG:58
  is_open: true
  name: GENO
  purpose_detail: GENO is an OWL2 OBO Foundry ontology representing levels of genetic
    variation specified in genotypes to support genotype-to-phenotype (G2P) data aggregation
    and analysis across diverse research communities and sources. The core model is
    a graph decomposing genotypes into smaller components of variation, from complete
    genotypes specifying sequence variation across entire genomes down to specific
    allelic variants and sequence alterations. This partonomy structure enables integrated
    analysis of G2P data where phenotype annotations are made at different granularity
    levels. GENO describes genotype attributes including zygosity, genomic position,
    expression, dominance, and functional dependencies or consequences of variants.
    Beyond heritable genomic sequence variation, GENO represents transient variation
    in gene expression from knockdown reagents or overexpression constructs, representing
    this variation in terms of targeted genes to parallel sequence variation representation.
    GENO models G2P associations focusing on genotype-phenotype-environment interplay
    and uses the Scientific Evidence and Provenance Information Ontology (SEPIO) for
    provenance and experimental evidence. The ontology is orthogonal to but integrates
    with the Sequence Ontology (SO), Human Phenotype Ontology (HPO), Feature Annotation
    Location Description Ontology (FALDO), and Variation Ontology (VariO), supporting
    AI/ML applications in variant effect prediction, phenotype association analysis,
    and precision medicine.
  requires_registration: false
  url: https://github.com/monarch-initiative/GENO-ontology
- id: B2AI_STANDARD:458
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:14
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Geographical Entity Ontology
  formal_specification: https://github.com/ufbmi/geographical-entity-ontology/
  has_relevant_organization:
  - B2AI_ORG:96
  is_open: true
  name: GEO
  purpose_detail: The Geographical Entity Ontology (GEO) is a domain ontology developed
    by the University of Florida Biomedical Informatics group and aligned with OBO
    Foundry principles to provide standardized, computationally tractable representations
    of geographical entities, spatial relationships, and location-based concepts essential
    for biomedical research, epidemiology, environmental health studies, and clinical
    data integration where geographic context influences disease patterns, healthcare
    access, and population health outcomes. GEO defines a hierarchical classification
    of geographical entities ranging from large-scale political and administrative
    units (countries, states, provinces, counties, municipalities) through natural
    geographic features (mountains, rivers, lakes, coastlines, watersheds) to human-constructed
    spatial divisions (postal codes, census tracts, health service areas, urban/rural
    classifications) and institutional locations (hospitals, clinics, research facilities,
    universities). The ontology encodes spatial relationships using formal logic and
    topological predicates (part_of, located_in, adjacent_to, overlaps_with, contains)
    that enable computational reasoning about geographic containment hierarchies,
    proximity patterns, and spatial aggregations necessary for multi-level epidemiological
    analyses and geospatial data integration. GEO integrates with the Basic Formal
    Ontology (BFO) upper-level ontology and coordinates with complementary domain
    ontologies including the Environment Ontology (ENVO) for environmental features,
    the Population and Community Ontology (PCO) for population groups defined by
    geography, and the Units of Measurement Ontology (UO) for representing spatial
    dimensions, areas, and distances with standardized units. For public health surveillance
    and disease tracking, GEO provides the semantic foundation for georeferencing
    clinical observations, outbreak investigations, and environmental exposure assessments,
    enabling queries that aggregate cases by administrative boundaries while accounting
    for population denominators and adjusting for geographic mobility patterns. The
    ontology supports One Health applications by linking human disease surveillance
    with animal health monitoring, vector ecology, and environmental conditions across
    shared geographic spaces, facilitating detection of zoonotic disease emergence,
    vectorborne disease risk mapping, and foodborne outbreak source attribution that
    require integration of human, veterinary, and environmental data streams referenced
    to common geographic entities. In health disparities research and social determinants
    of health studies, GEO enables standardized representation of neighborhood-level
    contextual variables (socioeconomic status, built environment, food access, healthcare
    facility distribution) linked to census geographies, supporting multilevel regression
    models that quantify how place-based factors influence individual health outcomes
    while properly accounting for geographic clustering and spatial autocorrelation.
    Clinical trial site selection and patient recruitment benefit from GEO's representation
    of healthcare facility catchment areas, travel time isochrones, and residential
    locations, enabling optimization of site networks to ensure diverse geographic
    representation and minimize patient travel burden while meeting enrollment targets.
    For environmental health research examining air quality, water contamination,
    climate exposures, or built environment features, GEO provides the semantic framework
    for linking environmental measurements (monitoring station data, remote sensing
    observations, land use classifications) to population exposures through spatial
    joins and proximity analyses that account for geographic boundaries and mobility
    patterns. GEO's computational tractability supports automated geocoding pipelines
    that standardize free-text location descriptions into ontology terms, quality
    control workflows that validate geographic coordinates fall within appropriate
    administrative boundaries, and data harmonization processes that reconcile geographic
    identifiers across datasets using different geographic classification schemes
    (ZIP codes versus census tracts, health districts versus counties). The ontology
    facilitates multi-site research collaborations and federated data networks by
    providing common geographic reference terms that enable privacy-preserving spatial
    analyses where exact addresses are obscured but coarse geographic regions (e.g.,
    county, metropolitan area) are shared for population-level inference. Machine
    learning applications in spatial epidemiology and disease prediction models leverage
    GEO terms as features encoding geographic context, enabling models to learn associations
    between location characteristics and health outcomes while generalizing across
    geographic scales through the ontology's hierarchical structure. GEO is maintained
    through community curation with updates reflecting changes in political boundaries,
    new administrative divisions, evolving urban/rural classifications, and refinements
    to spatial relationship definitions informed by use cases in biomedical research
    and public health practice.
  requires_registration: false
  url: https://github.com/ufbmi/geographical-entity-ontology/
- id: B2AI_STANDARD:459
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Global Medical Device Nomenclature
  has_relevant_organization:
  - B2AI_ORG:35
  is_open: false
  name: GMDN
  purpose_detail: The Global Medical Device Nomenclature (GMDN) is a comprehensive
    set of terms, within a structured category hierarchy, which name and group ALL
    medical device products including implantables, medical equipment, consumables,
    and diagnostic devices.
  requires_registration: true
  url: https://www.gmdnagency.org/
- id: B2AI_STANDARD:460
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Glycan Naming and Subsumption Ontology
  formal_specification: https://github.com/glygen-glycan-data/GNOme
  is_open: true
  name: GNO
  publication: doi:10.5281/zenodo.6678279
  purpose_detail: The Glycan Naming and Subsumption Ontology (GNO) is a comprehensive,
    computationally tractable framework for representing glycan structures at multiple
    levels of compositional and structural detail, developed by the GlyGen project
    and aligned with OBO Foundry principles to support standardized glycan data integration,
    querying, and semantic reasoning across glycobiology databases, mass spectrometry
    workflows, and computational glycoscience applications. GNO addresses the challenge
    that glycans can be characterized at varying degrees of structural resolutionfrom
    monosaccharide composition (no linkage or topology information) through partially
    defined topologies (some linkages known, others ambiguous) to fully defined structures
    (complete linkage, anomericity, and stereochemistry)by providing a hierarchical
    ontology where each glycan structure subsumes all less-specific representations,
    enabling queries that retrieve glycans matching a composition or partial structure
    regardless of how completely they were characterized in the original data source.
    The ontology encodes glycan structures using GlycoCT condensed format as the canonical
    representation, with systematic translation to IUPAC condensed nomenclature, WURCS
    (Web3 Unique Representation of Carbohydrate Structures), and GlyTouCan accession
    identifiers for cross-referencing with international glycan repositories. GNO's
    subsumption hierarchy allows compositional queries (e.g., "all glycans containing
    2 N-acetylglucosamine, 3 mannose, 3 galactose") to return both the composition
    node and all fully characterized structures matching that composition, supporting
    mass spectrometry data interpretation where structural details may be ambiguous
    but composition is reliably determined. The ontology integrates with GlycoRDF
    to provide RDF/OWL representations suitable for SPARQL querying and semantic web
    applications, enabling federated queries across GlyGen, UniCarbKB, GlyTouCan,
    and other glycomics resources. GNO includes structural classifications based on
    biosynthetic pathways (N-glycans, O-glycans, glycosphingolipids, glycosaminoglycans),
    functional roles (blood group antigens, selectin ligands, pathogen recognition
    motifs), and taxonomic distributions (mammalian-specific structures, plant polysaccharides,
    bacterial capsular antigens), facilitating biological interpretation of glycan
    datasets. For machine learning applications in glycoscience, GNO provides standardized
    feature representations where glycan structures are encoded as ontology terms
    with subsumption relationships, enabling training of models that predict glycosyltransferase
    activity, binding specificity of glycan-binding proteins (lectins, antibodies,
    viral hemagglutinins), or disease-associated glycosylation changes from glycan
    structural features and compositional patterns. The ontology supports integration
    of glycomics data with proteomics by linking glycan structures to glycosylation
    sites on proteins, enabling systems-level analysis of glycoprotein heterogeneity,
    site occupancy, and glycoform distributions relevant to therapeutic protein development
    (monoclonal antibody glycosylation quality control, biosimilar characterization).
    GNO's computational tractability stems from its explicit encoding of structural
    rules (e.g., allowable linkage positions for each monosaccharide, constraints
    on branching patterns) that enable automated reasoning about glycan biosynthesis
    pathways and structural feasibility, supporting quality control in glycan structure
    databases and automated error detection in manually curated entries. The ontology
    facilitates meta-analysis of glycomics studies by providing consistent terminology
    for glycan structural motifs (high-mannose, complex-type, hybrid N-glycans; core
    1/2/3/4 O-glycans; sulfated/sialylated epitopes), enabling aggregation of findings
    across datasets generated by different analytical techniques (MALDI-TOF MS, LC-MS/MS,
    lectin arrays, NMR spectroscopy). GNO is maintained through collaborative curation
    by the glycobiology community with continuous updates to incorporate newly discovered
    glycan structures, refined biosynthetic pathway knowledge, and emerging structural
    motifs associated with disease states or developmental stages, ensuring the ontology
    remains current with advances in glycan analysis technologies and functional glycomics
    research.
  requires_registration: false
  url: https://gnome.glyomics.org/
- id: B2AI_STANDARD:461
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Health Surveillance Ontology
  formal_specification: https://github.com/SVA-SE/HSO
  has_relevant_organization:
  - B2AI_ORG:92
  is_open: true
  name: HSO
  purpose_detail: The Health Surveillance Ontology (HSO) provides a comprehensive
    framework for standardizing terminology and concepts related to health surveillance
    systems, disease monitoring programs, and public health data collection across
    human and veterinary medicine. Developed by the Swedish Veterinary Agency (SVA)
    and aligned with OBO Foundry principles, HSO enables interoperability between
    surveillance databases, epidemiological studies, and public health information
    systems by providing consistent vocabulary for surveillance activities, case definitions,
    reporting requirements, and data quality metrics. The ontology encompasses surveillance
    system architectures (passive surveillance, active surveillance, sentinel surveillance,
    syndromic surveillance), data collection methodologies (laboratory-based surveillance,
    clinical reporting, population surveys, environmental monitoring), case classification
    criteria (confirmed cases, probable cases, suspect cases based on laboratory/clinical/epidemiological
    evidence), temporal and spatial granularity specifications (reporting periods,
    geographic resolution, population denominators), and data quality indicators (completeness,
    timeliness, representativeness, sensitivity, specificity). HSO supports One Health
    approaches by bridging human, animal, and environmental health surveillance domains,
    facilitating detection of zoonotic disease emergence, antimicrobial resistance
    tracking, and foodborne outbreak investigations. Applications include standardization
    of surveillance system metadata for interoperability between national and international
    health agencies (WHO, ECDC, OIE), automated validation of surveillance data submissions,
    harmonization of case definitions across jurisdictions, and machine-readable representation
    of surveillance protocols for reproducibility. HSO integrates with disease ontologies
    (DO, DOID), pathogen ontologies (IDO, NCBITaxon), and geographic ontologies (GAZ)
    to provide comprehensive semantic framework for epidemiological data. The ontology
    enables FAIR principles implementation in public health surveillance by making
    surveillance system documentation findable, accessible, interoperable, and reusable.
  requires_registration: false
  url: https://w3id.org/hso
- id: B2AI_STANDARD:462
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HL7 Vocabulary
  has_relevant_organization:
  - B2AI_ORG:40
  is_open: true
  name: HL7 Vocabulary
  purpose_detail: An index to the HL7-supported Code Systems.
  requires_registration: false
  url: https://www.hl7.org/documentcenter/public/standards/vocabulary/vocabulary_tables/infrastructure/vocabulary/vocabulary.html#voc-systems
- id: B2AI_STANDARD:463
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Homology Ontology
  formal_specification: https://github.com/BgeeDB/homology-ontology
  is_open: true
  name: HOM
  publication: doi:10.1016/j.tig.2009.12.012
  purpose_detail: Concepts related to homology, as well as other concepts used to
    describe similarity and non-homology.
  requires_registration: false
  url: https://github.com/BgeeDB/homology-ontology
- id: B2AI_STANDARD:464
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:25
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HPO - ORDO Ontological Module
  formal_specification: https://www.orphadata.com/ontologies/
  has_relevant_organization:
  - B2AI_ORG:80
  is_open: true
  name: HOOM
  purpose_detail: Orphanet provides phenotypic annotations of the rare diseases in
    the Orphanet nomenclature using the Human Phenotype Ontology (HPO). HOOM is a
    module that qualifies the annotation between a clinical entity and phenotypic
    abnormalities according to a frequency and by integrating the notion of diagnostic
    criterion.
  requires_registration: false
  url: https://bioportal.bioontology.org/ontologies/HOOM
- id: B2AI_STANDARD:465
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Human Ancestry Ontology
  formal_specification: https://github.com/EBISPOT/ancestro
  is_open: true
  name: HANCESTRO
  publication: doi:10.1186/s13059-018-1396-2
  purpose_detail: A systematic description of the ancestry concepts used in the NHGRI-EBI
    Catalog
  requires_registration: false
  url: https://github.com/EBISPOT/ancestro
- id: B2AI_STANDARD:466
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Human Developmental Stages
  formal_specification: https://github.com/obophenotype/developmental-stage-ontologies
  is_open: true
  name: HSAPDV
  purpose_detail: The Human Developmental Stages ontology (HsapDv) provides standardized
    terminology for describing human lifecycle stages from conception through senescence,
    developed by the Bgee group in collaboration with Uberon and EHDAA2 (Human Developmental
    Anatomy Ontology) developers to enable precise temporal annotation of biological
    data across developmental biology, clinical research, and population studies.
    HsapDv encompasses both embryonic and postnatal stages, utilizing Carnegie staging
    system for prenatal development (Carnegie stages 1-23 covering days 1-56 post-fertilization,
    approximately embryonic weeks 1-8) which provides morphology-based developmental
    milestones independent of gestational age variations. Embryonic stages capture
    critical developmental events including fertilization, cleavage, blastocyst formation,
    gastrulation, neurulation, organogenesis, and fetal development through birth.
    Postnatal stages include neonatal period (birth to 28 days), infancy (1 month
    to 2 years), early childhood (2-6 years), middle childhood (6-12 years), adolescence
    (12-18 years encompassing puberty), young adulthood (18-40 years), middle adulthood
    (40-65 years), and late adulthood/senescence (65+ years) with subdivisions for
    geriatric populations. Each stage is formally defined with temporal boundaries,
    morphological characteristics, physiological milestones (motor skills, cognitive
    development, hormonal changes), and relationships to other developmental stages
    through "immediately_preceded_by" and "part_of" relations. HsapDv integrates
    with Uberon for anatomical structure development timing, enabling queries like
    "when does the cerebral cortex develop" or "which genes are expressed in neural
    tube during neurulation." Applications span developmental biology research (temporal
    annotation of gene expression atlases, single-cell RNA-seq developmental trajectories,
    epigenetic modification timelines), clinical medicine (prenatal diagnosis, developmental
    delay assessment, age-appropriate clinical reference ranges), teratology studies
    (critical periods for teratogen exposure), pharmacology (age-specific drug metabolism
    and dosing), and epidemiology (age-stratified disease incidence). HsapDv enables
    cross-species developmental comparisons through alignment with other species-specific
    developmental ontologies (mouse MmusDv, zebrafish ZFS), facilitating translational
    research and comparative embryology. The ontology is distributed in OBO and OWL
    formats through http://purl.obolibrary.org/obo/hsapdv.owl and browsable via OBO
    Foundry portals, supporting reproducible temporal annotation in biomedical databases,
    developmental atlases, and clinical decision support systems.
  requires_registration: false
  url: https://github.com/obophenotype/developmental-stage-ontologies/wiki/HsapDv
- id: B2AI_STANDARD:467
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Human Disease Ontology
  formal_specification: https://github.com/DiseaseOntology/HumanDiseaseOntology
  is_open: true
  name: DOID
  purpose_detail: The Human Disease Ontology (DOID) is a comprehensive, standardized
    ontology that provides a hierarchical classification system for human diseases
    organized primarily by etiology (underlying cause). Developed as part of the Open
    Biomedical Ontologies (OBO) Foundry, DOID serves as a cornerstone reference for
    disease terminology in biomedical research, clinical informatics, and healthcare
    applications. The ontology integrates multiple disease classification systems
    including ICD, SNOMED CT, UMLS, and MeSH, providing extensive cross-references
    that enable interoperability between different medical coding systems. DOID structures
    diseases into logical hierarchies based on disease mechanisms, affected anatomical
    systems, and causal agents, enabling both broad categorical searches and precise
    disease identification. Each disease concept includes standardized names, definitions,
    synonyms, and relationships to parent and child terms, creating a rich semantic
    network that supports computational analysis of disease data. The ontology is
    extensively used in genomics databases, electronic health records, biomedical
    literature annotation, drug discovery pipelines, and epidemiological studies where
    consistent disease terminology is essential for data integration and comparative
    analysis.
  requires_registration: false
  url: http://www.disease-ontology.org
- id: B2AI_STANDARD:468
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:25
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Human Phenotype Ontology
  formal_specification: https://github.com/obophenotype/human-phenotype-ontology
  is_open: true
  name: HPO
  purpose_detail: 'The Human Phenotype Ontology (HPO) provides standardized vocabulary for phenotypic
    abnormalities encountered in human disease, containing over 18,000 terms and 156,000+
    annotations to hereditary diseases. Each HPO term describes a specific phenotypic
    feature (e.g., "Atrial septal defect" HP:0001631) organized in a hierarchical
    structure from general to specific findings. Developed using medical literature,
    Orphanet, DECIPHER, and OMIM, HPO enables precise phenotype-driven differential
    diagnostics, genomic variant interpretation, and translational research. The ontology
    integrates with major biomedical resources and powers phenotype matching algorithms
    that rank diseases by clinical feature similarity. HPO is foundational for rare
    disease diagnosis tools (Exomiser, Phenomizer, PhenoTips), electronic health record
    phenotyping, and clinical decision support systems. As a Monarch Initiative flagship
    product and GA4GH driver project, HPO enables semantic integration across species,
    connecting human phenotypes to model organism phenotypes for translational research.
    The ontology supports deep phenotyping in genomics studies, electronic health record
    phenotype extraction, natural language processing for clinical notes, and phenotype-driven
    gene prioritization. HPO annotations link phenotypes to genes, diseases, and publications,
    facilitating genotype-phenotype correlation studies. In AI/ML applications, HPO
    powers phenotype-based similarity learning for rare disease diagnosis, automated
    phenotype extraction from clinical narratives using NLP, ontology-guided feature
    engineering for predictive models, knowledge graph embeddings for disease gene
    discovery, and multi-modal patient representation learning combining genomics,
    phenotypes, and clinical data to support precision medicine and clinical genomics.'
  related_to:
  - B2AI_STANDARD:784
  requires_registration: false
  url: https://hpo.jax.org/
- id: B2AI_STANDARD:469
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HUPO-PSI cross-linking and derivatization reagents controlled vocabulary
  has_relevant_organization:
  - B2AI_ORG:41
  is_open: true
  name: XLMOD
  purpose_detail: A structured controlled vocabulary for cross-linking reagents used
    with proteomics mass spectrometry.
  requires_registration: false
  url: https://www.psidev.info/groups/controlled-vocabularies
- id: B2AI_STANDARD:470
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Hypertension Ontology
  formal_specification: https://github.com/aellenhicks/htn_owl
  is_open: true
  name: HTN
  purpose_detail: An ontology for representing clinical data about hypertension.
  requires_registration: false
  url: https://github.com/aellenhicks/htn_owl
- id: B2AI_STANDARD:471
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Infectious Disease Ontology
  formal_specification: https://github.com/infectious-disease-ontology/infectious-disease-ontology
  is_open: true
  name: IDO
  purpose_detail: A set of interoperable ontologies that will together provide coverage
    of the infectious disease domain. IDO core is the upper-level ontology...
  requires_registration: false
  url: https://github.com/infectious-disease-ontology/infectious-disease-ontology
- id: B2AI_STANDARD:472
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Information Artifact Ontology
  formal_specification: https://github.com/information-artifact-ontology/IAO
  is_open: true
  name: IAO
  purpose_detail: An ontology of information entities.
  requires_registration: false
  url: https://github.com/information-artifact-ontology/IAO
- id: B2AI_STANDARD:473
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Informed Consent Ontology
  formal_specification: https://github.com/ICO-ontology/ICO
  is_open: true
  name: ICO
  purpose_detail: An ontology of clinical informed consents
  requires_registration: false
  url: https://github.com/ICO-ontology/ICO
- id: B2AI_STANDARD:474
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Integrative and Conjugative Element Ontology
  formal_specification: https://github.com/ontoice/ICEO
  is_open: true
  name: ICEO
  publication: doi:10.1038/s41597-021-01112-5
  purpose_detail: An integrated biological ontology for the description of bacterial
    integrative and conjugative elements (ICEs).
  requires_registration: false
  url: http://db-mml.sjtu.edu.cn/ICEberg/
- id: B2AI_STANDARD:475
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Intelligence Task Ontology
  formal_specification: https://github.com/OpenBioLink/ITO
  has_relevant_organization:
  - B2AI_ORG:87
  is_open: true
  name: ITO
  purpose_detail: Comprehensive, curated and interlinked data of artificial intelligence
    tasks, benchmarks, AI performance metrics, benchmark results and research papers.
  requires_registration: false
  url: https://openbiolink.github.io/ITOExplorer/
- id: B2AI_STANDARD:476
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Interaction Network Ontology
  formal_specification: https://github.com/INO-ontology/ino
  is_open: true
  name: INO
  publication: doi:10.1186/2041-1480-6-2
  purpose_detail: An ontology of interactions and interaction networks.
  requires_registration: false
  url: https://github.com/INO-ontology/ino
- id: B2AI_STANDARD:477
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Interlinking Ontology for Biological Concepts
  formal_specification: https://github.com/kushidat/IOBC
  is_open: true
  name: IOBC
  publication: doi:10.1007/s00354-019-00074-y
  purpose_detail: biological, biomedical, and related concepts
  requires_registration: false
  url: https://github.com/kushidat/IOBC
- id: B2AI_STANDARD:478
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Kinetic Simulation Algorithm Ontology
  formal_specification: https://github.com/SED-ML/KiSAO
  is_open: true
  name: KISAO
  purpose_detail: Algorithms for simulating biology, their parameters, and their outputs.
  requires_registration: false
  url: http://co.mbine.org/standards/kisao
- id: B2AI_STANDARD:479
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:25
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Mammalian Phenotype Ontology
  formal_specification: https://github.com/mgijax/mammalian-phenotype-ontology
  is_open: true
  name: MP
  publication: doi:10.1007/s00335-012-9421-3
  purpose_detail: Standard terms for annotating mammalian phenotypic data.
  requires_registration: false
  url: https://github.com/mgijax/mammalian-phenotype-ontology
- id: B2AI_STANDARD:480
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Manufacturers of Vaccines
  has_relevant_organization:
  - B2AI_ORG:13
  is_open: true
  name: MVX
  purpose_detail: Code set for active and inactive manufacturers of vaccines in the
    US.
  requires_registration: false
  url: https://www2a.cdc.gov/vaccines/iis/iisstandards/vaccines.asp?rpt=mvx
- id: B2AI_STANDARD:481
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Mass spectrometry ontology
  formal_specification: https://github.com/HUPO-PSI/psi-ms-CV
  has_relevant_organization:
  - B2AI_ORG:41
  is_open: true
  name: MS
  publication: doi:10.1093/database/bat009
  purpose_detail: A structured controlled vocabulary for the annotation of experiments
    concerned with proteomics mass spectrometry.
  requires_registration: false
  url: http://www.psidev.info/groups/controlled-vocabularies
- id: B2AI_STANDARD:482
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Measurement method ontology
  formal_specification: https://github.com/rat-genome-database/MMO-Measurement-Method-Ontology/
  is_open: true
  name: MMO
  publication: doi:10.1186/2041-1480-4-26
  purpose_detail: A representation of the variety of methods used to make clinical
    and phenotype measurements.
  requires_registration: false
  url: https://rgd.mcw.edu/rgdweb/ontology/view.html?acc_id=MMO:0000000
- id: B2AI_STANDARD:483
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Medaka Developmental Stages
  formal_specification: https://github.com/obophenotype/developmental-stage-ontologies
  is_open: true
  name: OLATDV
  purpose_detail: The Medaka Developmental Stages ontology (OlatDv) provides standardized
    terminology for describing developmental stages of medaka (Oryzias latipes, Japanese
    rice fish), a key teleost model organism in developmental biology, genetics, toxicology,
    and comparative vertebrate research, based on Iwamatsu staging system and developed
    from the original Medaka Fish Ontology (MFO) by Thorsten Henrich. Medaka serves
    as a powerful model organism complementing zebrafish and mouse due to its transparent
    embryos enabling live imaging, short generation time (2-3 months), small size
    suitable for laboratory culture, fully sequenced genome with conserved vertebrate
    gene organization, and established genetic tools including transgenesis, CRISPR/Cas9
    editing, and mutant libraries. OlatDv encompasses embryonic and larval stages
    from fertilization through sexual maturity, currently focusing on pre-adult development.
    Embryonic stages follow Iwamatsu's morphological staging system (stages 1-40)
    covering fertilization (stage 1), cleavage (stages 2-7), blastula (stages 8-10),
    gastrulation (stages 11-17), neurulation (stages 18-20), organogenesis (stages
    21-30), and pre-hatching development (stages 31-40 leading to hatching at approximately
    day 7-10 post-fertilization at 26C). Each stage is defined by specific morphological
    landmarks including somite numbers, heart development, pigmentation patterns, fin
    bud appearance, eye development, and gill filament formation. Post-hatching larval
    stages capture metamorphosis, scale formation, sex differentiation, and juvenile
    maturation through first reproduction. OlatDv provides temporal annotations crucial
    for comparative developmental biology studies examining vertebrate evolution, particularly
    teleost-specific genome duplication events and developmental innovations. Applications
    include temporal annotation of gene expression databases (Medaka Expression Database),
    developmental toxicology studies (OECD fish embryo toxicity tests using medaka
    as alternative to zebrafish), endocrine disruption research (sex determination
    mechanisms), carcinogenesis studies (medaka exhibits spontaneous tumor formation),
    and aging research (short lifespan enables longitudinal studies). Integration
    with Uberon anatomical ontology enables queries linking developmental stage to
    organ system maturation, essential for understanding tissue-specific gene expression
    changes during development. OlatDv facilitates cross-species developmental comparisons
    through alignment with zebrafish ZFS, frog XAO, and mammalian developmental ontologies,
    supporting evolutionary developmental biology (evo-devo) research and identification
    of conserved versus lineage-specific developmental programs across vertebrates.
    The ontology is distributed through OBO Foundry as olatdv.obo and olatdv.owl formats.
  requires_registration: false
  url: https://github.com/obophenotype/developmental-stage-ontologies/wiki/OlatDv
- id: B2AI_STANDARD:484
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Medical Action Ontology
  formal_specification: https://github.com/monarch-initiative/MAxO
  has_relevant_organization:
  - B2AI_ORG:58
  is_open: true
  name: MAXO
  purpose_detail: Terms for medical procedures, interventions, therapies, treatments,
    and recommendations.
  requires_registration: false
  url: https://github.com/monarch-initiative/MAxO
- id: B2AI_STANDARD:485
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - codesystem
  concerns_data_topic:
  - B2AI_TOPIC:8
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Medication Reference Terminology
  has_relevant_organization:
  - B2AI_ORG:74
  is_open: true
  name: MED-RT
  purpose_detail: Formal ontological representations of medication terminology, pharmacologic
    classifications, and asserted authoritative relationships between them. Replaces
    NDF-RT. Provided through UMLS.
  requires_registration: true
  url: https://evs.nci.nih.gov/ftp1/NDF-RT/Introduction%20to%20MED-RT.pdf
- id: B2AI_STANDARD:486
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Mental Disease Ontology
  formal_specification: https://github.com/jannahastings/mental-functioning-ontology
  is_open: true
  name: MFOMD
  purpose_detail: Mental diseases such as schizophrenia, annotated with DSM-IV and
    ICD codes where applicable.
  requires_registration: false
  url: https://github.com/jannahastings/mental-functioning-ontology
- id: B2AI_STANDARD:487
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Mental Functioning Ontology
  formal_specification: https://github.com/jannahastings/mental-functioning-ontology
  is_open: true
  name: MF
  purpose_detail: The Mental Functioning Ontology (MF) is an OBO Foundry ontology
    providing structured terminology for mental processes, cognitive functions, emotional
    states, and behavioral phenomena to support standardized annotation of neuroscience
    data, psychiatric research, psychological assessments, and mental health informatics.
    Developed by Janna Hastings and collaborators, MF extends the Basic Formal Ontology
    (BFO) upper-level framework and integrates with domain ontologies including the
    Mental Disease Ontology (MD) for psychiatric disorders, the Cognitive Atlas for
    cognitive processes, the Emotion Ontology for affective states, and the Gene
    Ontology (GO) for molecular underpinnings of neural function. The ontology comprehensively
    represents cognitive domains including perception (visual, auditory, somatosensory,
    chemosensory), attention (selective, divided, sustained), memory (working, episodic,
    semantic, procedural), executive functions (planning, inhibition, cognitive flexibility,
    decision-making), language processing, and reasoning, alongside emotional and
    motivational constructs such as valence, arousal, mood states, personality traits,
    and social cognition (theory of mind, empathy, social perception). MF captures
    temporal dynamics of mental processes (onset, duration, termination), intensity
    dimensions, and contextual dependencies, enabling nuanced representation of psychological
    phenomena. Integration with clinical terminologies like DSM-5, ICD-11, and RDoC
    (Research Domain Criteria) facilitates translational psychiatry linking basic
    neuroscience to clinical phenotypes. Applications include standardized annotation
    of neuroimaging studies identifying brain regions associated with specific mental
    functions, computational psychiatry modeling mental disorders as disruptions in
    cognitive and emotional processes, natural language processing extracting mental
    state descriptions from clinical notes, AI-based mental health assessment systems,
    meta-analysis of psychological experiments through unified terminology, and personalized
    medicine approaches tailoring psychiatric treatments to individual cognitive profiles.
    MF follows OBO Foundry principles with open-source development, logical consistency
    checking, and community-driven refinement, essential for computational neuroscience,
    digital mental health platforms, cognitive science research, and integrative brain
    databases.
  requires_registration: false
  url: https://github.com/jannahastings/mental-functioning-ontology
- id: B2AI_STANDARD:488
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Metadata vocabulary for Ontology Description and Publication
  formal_specification: https://github.com/sifrproject/MOD-Ontology
  is_open: true
  name: MOD
  publication: doi:10.1007/978-3-319-70863-8_17
  purpose_detail: An OWL ontology and application profile to capture metadata information
    for ontologies, vocabularies or semantic resources/artefacts in general.
  requires_registration: false
  url: https://github.com/sifrproject/MOD-Ontology
- id: B2AI_STANDARD:489
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MHC Restriction Ontology
  formal_specification: https://github.com/IEDB/MRO
  is_open: true
  name: MRO
  purpose_detail: The Major Histocompatibility Complex (MHC) Restriction Ontology
    (MRO) is a specialized ontology developed by the Immune Epitope Database (IEDB)
    to provide standardized terminology for describing MHC restriction phenomena in
    immunological experiments and data. MHC restriction refers to the biological process
    by which T-cell recognition of antigens is limited to peptides presented by specific
    MHC molecules that are compatible with the T-cell's own MHC background. MRO systematically
    organizes the complex relationships between MHC alleles, T-cell responses, and
    antigen presentation contexts that are crucial for understanding adaptive immune
    responses, vaccine development, and transplantation immunology. The ontology enables
    precise annotation of immunological experiments by providing controlled vocabulary
    terms for MHC class I and class II molecules, their allelic variants, restriction
    patterns, and associated experimental conditions. This standardization is essential
    for comparative immunology studies, epitope mapping projects, and the development
    of personalized immunotherapies where accurate description of MHC-restricted immune
    responses is critical for data interpretation and clinical translation.
  requires_registration: false
  url: https://github.com/IEDB/MRO
- id: B2AI_STANDARD:490
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MIAPA Ontology
  formal_specification: https://github.com/evoinfo/miapa/
  is_open: true
  name: MIAPA
  publication: doi:10.1089/omi.2006.10.231
  purpose_detail: An application ontology to formalize annotation of phylogenetic
    data.
  requires_registration: false
  url: https://www.evoio.org/wiki/MIAPA
- id: B2AI_STANDARD:491
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Minimum PDDI Information Ontology
  formal_specification: https://github.com/MPIO-Developers/MPIO
  is_open: true
  name: MPIO
  purpose_detail: Minimum information regarding potential drug-drug interaction information.
  requires_registration: false
  url: https://github.com/MPIO-Developers/MPIO
- id: B2AI_STANDARD:492
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - modelcards
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Model Card Report Ontology
  formal_specification: https://github.com/UTHealth-Ontology/MCRO
  is_open: true
  name: MCRO
  publication: doi:10.1186/s12859-022-04797-6
  purpose_detail: An OWL2-based artifact that represents and formalizes model card
    report information. The current release of this ontology utilizes standard concepts
    and properties from OBO Foundry ontologies.
  requires_registration: false
  url: https://github.com/UTHealth-Ontology/MCRO
- id: B2AI_STANDARD:493
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:20
  - B2AI_TOPIC:26
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Molecular Interactions Controlled Vocabulary
  formal_specification: https://github.com/HUPO-PSI/psi-mi-CV
  has_relevant_organization:
  - B2AI_ORG:41
  is_open: true
  name: MI
  purpose_detail: Vocabulary for the annotation of experiments concerned with protein-protein
    interactions.
  requires_registration: false
  url: https://github.com/HUPO-PSI/psi-mi-CV
- id: B2AI_STANDARD:494
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Molecular Process Ontology
  formal_specification: https://github.com/rsc-ontologies/rxno
  has_relevant_organization:
  - B2AI_ORG:29
  is_open: true
  name: MOP
  purpose_detail: The Molecular Process Ontology (MOP) provides standardized vocabulary
    for describing molecular-level processes, transformations, and mechanisms that
    occur in chemical and biochemical systems, maintained by the Royal Society of
    Chemistry as part of the RSC ontology ecosystem. MOP encompasses reaction mechanisms
    (nucleophilic substitution, electrophilic addition, radical reactions, pericyclic
    reactions), molecular interactions (hydrogen bonding, van der Waals forces, -
    stacking, hydrophobic interactions, electrostatic interactions), conformational
    changes (protein folding, ligand-induced conformational shifts, allosteric transitions),
    energy transfer processes (fluorescence resonance energy transfer FRET, photoinduced
    electron transfer, vibrational energy relaxation), and transport phenomena (diffusion,
    membrane permeation, active transport, facilitated diffusion). The ontology provides
    detailed mechanistic descriptions including activation energies, transition states,
    reaction intermediates, rate-determining steps, and catalytic cycles essential
    for understanding chemical reactivity and biological function at the molecular
    scale. MOP integrates with the Chemical Methods Ontology (CHMO) for experimental
    techniques, CHEBI for chemical entities, and the Gene Ontology (GO) for biological
    processes, enabling comprehensive annotation of molecular transformations from
    pure chemistry through biochemistry to systems biology. Applications include annotation
    of reaction databases for synthetic chemistry planning, mechanistic modeling of
    enzymatic catalysis, drug-target interaction mechanisms for rational drug design,
    metabolic pathway analysis with detailed reaction mechanisms, and computational
    chemistry workflow documentation. MOP supports reproducibility in mechanistic studies
    by standardizing descriptions of reaction conditions, stereochemical outcomes,
    regioselectivity, and stereoselectivity patterns. The ontology enables semantic
    searches for reactions by mechanism type, facilitating discovery of analogous
    transformations across different chemical contexts and supporting retrosynthetic
    analysis in computer-aided synthesis planning tools.
  requires_registration: false
  url: https://www.ebi.ac.uk/ols/ontologies/mop
- id: B2AI_STANDARD:495
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Mondo Disease Ontology
  formal_specification: https://github.com/monarch-initiative/mondo
  has_relevant_organization:
  - B2AI_ORG:58
  is_open: true
  name: MONDO
  publication: doi:10.1093/nar/gkw1128
  purpose_detail: "MONDO is an OBO Foundry ontology providing a unified disease terminology
    that harmonizes disease definitions across multiple resources including HPO,
    OMIM, SNOMED CT, ICD, ORDO, DO, MedGen, GARD, and others. It addresses the proliferation
    of inconsistent disease mappings by providing logic-based structure with precise
    1:1 equivalence axioms connecting to other resources, validated by OWL reasoning.
    MONDO contains over 25,880 diseases including 22,919 human diseases (4,727 cancers,
    1,074 infectious diseases, 11,601 Mendelian diseases, 15,857 rare diseases) and
    2,960 non-human diseases, with 129,785 database cross-references and 108,076
    synonyms (exact, narrow, broad, and related). The ontology provides hierarchical
    classification for disease grouping and rolling up and uses precise semantic
    annotations for each mapping rather than loose cross-references. MONDO is released
    in three formats: mondo-with-equivalents.owl (with OWL equivalence axioms and
    inter-ontology axiomatization using CL, Uberon, GO, HP, RO, NCBITaxon), mondo.obo
    (simplified with xrefs), and mondo-with-equivalents.json. Coordinated with the
    Human Phenotype Ontology (HPO) which describes phenotypic features, MONDO supports
    AI/ML applications in disease classification, phenotype-disease association, rare
    disease diagnosis, and cross-resource knowledge integration."
  requires_registration: false
  url: https://mondo.monarchinitiative.org/
- id: B2AI_STANDARD:496
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Mouse Developmental Anatomy Ontology
  formal_specification: https://github.com/obophenotype/mouse-anatomy-ontology
  is_open: true
  name: EMAPA
  purpose_detail: Mouse anatomy covering embryonic development and postnatal stages.
  requires_registration: false
  url: http://www.informatics.jax.org/expression.shtml
- id: B2AI_STANDARD:497
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Mouse pathology ontology
  formal_specification: https://github.com/PaulNSchofield/mpath
  is_open: true
  name: MPATH
  purpose_detail: A structured controlled vocabulary of mutant and transgenic mouse
    pathology phenotypes.
  requires_registration: false
  url: http://www.pathbase.net/
- id: B2AI_STANDARD:498
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:3
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Name Reaction Ontology
  formal_specification: https://github.com/rsc-ontologies/rxno
  is_open: true
  name: RXNO
  purpose_detail: Connects organic name reactions to their roles in an organic synthesis
    and to processes in MOP
  requires_registration: false
  url: https://github.com/rsc-ontologies/rxno
- id: B2AI_STANDARD:499
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:8
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: National Drug Code
  has_relevant_organization:
  - B2AI_ORG:31
  is_open: true
  name: NDC
  purpose_detail: Information about finished drug products, unfinished drugs and compounded
    drug products
  requires_registration: false
  url: https://www.accessdata.fda.gov/scripts/cder/ndc/index.cfm
- id: B2AI_STANDARD:500
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: NCBI organismal classification
  formal_specification: https://github.com/obophenotype/ncbitaxon
  has_relevant_organization:
  - B2AI_ORG:74
  is_open: true
  name: NCBITAXON
  purpose_detail: An ontology representation of the NCBI organismal taxonomy.
  requires_registration: false
  url: http://www.ncbi.nlm.nih.gov/taxonomy
- id: B2AI_STANDARD:501
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: NCI Thesaurus OBO Edition
  formal_specification: https://github.com/NCI-Thesaurus/thesaurus-obo-edition
  has_relevant_organization:
  - B2AI_ORG:74
  is_open: true
  name: NCIT
  purpose_detail: A reference terminology that includes broad coverage of the cancer
    domain, including cancer related diseases.
  requires_registration: false
  url: https://github.com/NCI-Thesaurus/thesaurus-obo-edition
- id: B2AI_STANDARD:502
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Neuro Behavior Ontology
  formal_specification: https://github.com/obo-behavior/behavior-ontology/
  is_open: true
  name: NBO
  purpose_detail: Human and animal behaviours and behavioural phenotypes
  requires_registration: false
  url: https://github.com/obo-behavior/behavior-ontology/
- id: B2AI_STANDARD:503
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:22
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Neuronames Brain Hierarchy
  is_open: true
  name: NeuroNames
  publication: doi:10.1007/s12021-011-9128-8
  purpose_detail: A Comprehensive Hierarchical Nomenclature for Structures of the
    Primate Brain (human and macaque)
  requires_registration: false
  url: http://braininfo.rprc.washington.edu/aboutBrainInfo.aspx#NeuroNames
- id: B2AI_STANDARD:504
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Nomenclatural ontology for biological names
  formal_specification: https://github.com/SpeciesFileGroup/nomen
  is_open: true
  name: NOMEN
  purpose_detail: A nomenclatural ontology for biological names (not concepts). It
    encodes the goverened rules of nomenclature.
  requires_registration: false
  url: https://github.com/SpeciesFileGroup/nomen
- id: B2AI_STANDARD:505
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:33
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Non-Coding RNA Ontology
  formal_specification: https://github.com/OmniSearch/ncro
  is_open: true
  name: NCRO
  purpose_detail: An ontology for non-coding RNA, both of biological origin, and engineered.
  requires_registration: false
  url: https://github.com/OmniSearch/ncro
- id: B2AI_STANDARD:506
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Nutrition Care Process Terminology
  is_open: false
  name: NCPT
  purpose_detail: Terms for nutrition assessment, diagnosis, intervention, and monitoring/evaluation.
  requires_registration: true
  url: https://www.ncpro.org/
- id: B2AI_STANDARD:507
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: OBO Metadata Ontology
  formal_specification: https://github.com/information-artifact-ontology/ontology-metadata
  is_open: true
  name: OMO
  purpose_detail: Terms that are used to annotate ontology terms for all OBO ontologies.
  requires_registration: false
  url: https://github.com/information-artifact-ontology/ontology-metadata
- id: B2AI_STANDARD:508
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Obstetric and Neonatal Ontology
  formal_specification: https://github.com/ontoneo-project/Ontoneo
  is_open: true
  name: ONTONEO
  purpose_detail: A structured controlled vocabulary to provide a representation of
    the data from electronic health records involved in the care of pregnancy.
  requires_registration: false
  url: https://github.com/ontoneo-project/Ontoneo
- id: B2AI_STANDARD:509
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: OntoAvida ontology for Avida digital evolution platform
  formal_specification: https://gitlab.com/fortunalab/ontoavida
  is_open: true
  name: ONTOAVIDA
  purpose_detail: Vocabulary for the description of the most widely-used computational
    approach for studying digital evolution.
  requires_registration: false
  url: https://gitlab.com/fortunalab/ontoavida
- id: B2AI_STANDARD:510
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology for Biobanking
  formal_specification: https://github.com/biobanking/biobanking
  is_open: true
  name: OBIB
  purpose_detail: Annotation and modeling of biobank repository and biobanking administration.
  requires_registration: false
  url: https://github.com/biobanking/biobanking
- id: B2AI_STANDARD:511
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology for Biomedical Investigations
  formal_specification: https://github.com/obi-ontology/obi
  is_open: true
  name: OBI
  publication: doi:10.1371/journal.pone.0154556
  purpose_detail: Description of life-science and clinical investigations.
  requires_registration: false
  url: http://obi-ontology.org
- id: B2AI_STANDARD:512
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:4
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology for General Medical Science
  formal_specification: https://github.com/OGMS/ogms
  is_open: true
  name: OGMS
  purpose_detail: Treatment of disease and diagnosis and on carcinomas and other pathological
    entities.
  requires_registration: false
  url: https://github.com/OGMS/ogms
- id: B2AI_STANDARD:513
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology for MIRNA Target
  formal_specification: https://github.com/OmniSearch/omit
  is_open: true
  name: OMIT
  purpose_detail: Data exchange standards and common data elements in the microRNA
    (miR) domain.
  requires_registration: false
  url: https://github.com/OmniSearch/omit
- id: B2AI_STANDARD:514
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology for Nutritional Epidemiology
  formal_specification: https://github.com/cyang0128/Nutritional-epidemiologic-ontologies
  is_open: true
  name: ONE
  publication: doi:10.3390/nu11061300
  purpose_detail: Research output of nutritional epidemiologic studies.
  requires_registration: false
  url: https://github.com/cyang0128/Nutritional-epidemiologic-ontologies
- id: B2AI_STANDARD:515
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology for Nutritional Studies
  formal_specification: https://github.com/enpadasi/Ontology-for-Nutritional-Studies
  is_open: true
  name: ONS
  publication: doi:10.1186/s12263-018-0601-y
  purpose_detail: Description of concepts in the nutritional studies domain.
  requires_registration: false
  url: https://github.com/enpadasi/Ontology-for-Nutritional-Studies
- id: B2AI_STANDARD:516
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:4
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology of Adverse Events
  formal_specification: https://github.com/OAE-ontology/OAE/
  is_open: true
  name: OAE
  purpose_detail: 'Ontology of Adverse Events (OAE) is a community-developed biomedical ontology following OBO Foundry principles that provides standardized representation and classification of adverse events resulting from medical interventions including drug administration, vaccination, medical devices, procedures, and dietary supplements, enabling systematic analysis of safety data across clinical trials, pharmacovigilance systems, and electronic health records. OAE structures adverse events hierarchically under upper-level classes from the Basic Formal Ontology (BFO), integrating with domain ontologies including OGMS (disease processes), VO (vaccine components), DRON (drug products), and UBERON (anatomical structures) to capture mechanistic relationships between interventions, biological processes, and observed adverse outcomes. The ontology distinguishes between adverse events (any untoward medical occurrence temporally associated with intervention use) and adverse drug reactions (events with causal relationship to intervention), incorporating causality assessment frameworks (Naranjo scale, WHO-UMC criteria) as logical axioms that infer ADR status based on evidence patterns. OAE represents clinical manifestations (symptoms, signs, laboratory abnormalities), severity grades (CTCAE scales from mild to life-threatening), temporal patterns (immediate hypersensitivity, delayed reactions, cumulative toxicity), and anatomical localizations, supporting detailed phenotyping of safety profiles. The ontology enables cross-study aggregation of adverse event data by providing standardized terms that harmonize heterogeneous reporting formats from FDA FAERS, EMA EudraVigilance, WHO VigiBase, and clinical trial databases, facilitating meta-analyses of intervention safety and identification of rare adverse events invisible in individual studies. OAE supports pharmacovigilance signal detection by structuring adverse event hierarchies that enable mining of parent-child term relationships, discovering drug-event associations through disproportionality analysis (ROR, IC025), and prioritizing signals for regulatory review. In vaccine safety surveillance, OAE terms annotate Brighton Collaboration case definitions for standardized adverse event reporting post-vaccination (AEFI), supporting global safety monitoring networks coordinated by WHO. For AI/ML applications, OAE provides structured labels for training natural language processing models to extract adverse event mentions from clinical notes, social media posts, and regulatory documents, enables knowledge graph construction linking drugs, vaccines, adverse events, and patient characteristics for predictive safety modeling, and supports explainable AI systems that generate human-interpretable safety signals by reasoning over ontology-encoded mechanistic pathways connecting interventions to adverse outcomes, ultimately enhancing patient safety through earlier detection and characterization of intervention-related harms.'
  requires_registration: false
  url: https://github.com/OAE-ontology/OAE/
- id: B2AI_STANDARD:517
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology of Biological and Clinical Statistics
  formal_specification: https://github.com/obcs/obcs
  is_open: true
  name: OBCS
  purpose_detail: Biological and clinical statistics.
  requires_registration: false
  url: https://github.com/obcs/obcs
- id: B2AI_STANDARD:518
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology of Biological Attributes
  formal_specification: https://github.com/obophenotype/bio-attribute-ontology
  is_open: true
  name: OBA
  purpose_detail: A collection of biological attributes (traits) covering all kingdoms
    of life.
  requires_registration: false
  url: https://wiki.geneontology.org/index.php/Extensions/x-attribute
- id: B2AI_STANDARD:519
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology of Genetic Susceptibility Factor
  formal_specification: https://github.com/linikujp/OGSF
  is_open: true
  name: OGSF
  purpose_detail: An application ontology to represent genetic susceptibility to a
    specific disease, adverse event, or a pathological process.
  requires_registration: false
  url: https://github.com/linikujp/OGSF
- id: B2AI_STANDARD:520
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology of Host Pathogen Interactions
  formal_specification: https://github.com/OHPI/ohpi
  is_open: true
  name: OHPI
  publication: doi:10.1093/nar/gky999
  purpose_detail: Host-pathogen interactions and virulence factors.
  requires_registration: false
  url: https://github.com/OHPI/ohpi
- id: B2AI_STANDARD:521
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology of Host-Microbiome Interactions
  formal_specification: https://github.com/ohmi-ontology/ohmi
  is_open: true
  name: OHMI
  purpose_detail: Entities and relations related to microbiomes, microbiome host organisms
    (e.g., human and mouse), and the interactions between the hosts and microbiomes
    at different conditions.
  requires_registration: false
  url: https://github.com/ohmi-ontology/ohmi
- id: B2AI_STANDARD:522
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology of Medically Related Social Entities
  formal_specification: https://github.com/ufbmi/OMRSE
  is_open: true
  name: OMRSE
  publication: doi:10.1186/s13326-016-0087-8
  purpose_detail: This ontology covers the domain of social entities that are related
    to health care, such as demographic information and the roles of various...
  requires_registration: false
  url: https://github.com/ufbmi/OMRSE/wiki/OMRSE-Overview
- id: B2AI_STANDARD:523
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology of Organizational Structures of Trauma centers and Trauma
    systems
  formal_specification: https://github.com/OOSTT/OOSTT
  has_relevant_organization:
  - B2AI_ORG:96
  is_open: true
  name: OOSTT
  purpose_detail: Organizational components of trauma centers and trauma systems.
  requires_registration: false
  url: https://github.com/OOSTT/OOSTT
- id: B2AI_STANDARD:524
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology of Precision Medicine and Investigation
  formal_specification: https://github.com/OPMI/opmi
  is_open: true
  name: OPMI
  purpose_detail: Entities and relations associated with precision medicine and related
    investigations at different conditions.
  requires_registration: false
  url: https://github.com/OPMI/opmi
- id: B2AI_STANDARD:525
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:33
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology of RNA Sequencing
  formal_specification: https://github.com/safisher/ornaseq
  is_open: true
  name: ORNASEQ
  purpose_detail: An application ontology designed to annotate next-generation sequencing
    experiments performed on RNA.
  requires_registration: false
  url: https://github.com/safisher/ornaseq
- id: B2AI_STANDARD:526
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:4
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology of Vaccine Adverse Events
  formal_specification: https://github.com/OVAE-Ontology/ovae
  is_open: true
  name: OVAE
  purpose_detail: A biomedical ontology in the domain of vaccine adverse events.
  requires_registration: false
  url: https://github.com/OVAE-Ontology/ovae
- id: B2AI_STANDARD:527
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Oral Health and Disease Ontology
  formal_specification: https://github.com/oral-health-and-disease-ontologies/ohd-ontology
  is_open: true
  name: OHD
  publication: doi:10.1186/s13326-020-00222-0
  purpose_detail: Content of dental practice health records.
  requires_registration: false
  url: https://github.com/oral-health-and-disease-ontologies/ohd-ontology
- id: B2AI_STANDARD:528
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Orphanet Rare Disease Ontology
  formal_specification: https://www.orphadata.com/ontologies/
  has_relevant_organization:
  - B2AI_ORG:80
  is_open: true
  name: ORDO
  purpose_detail: The Orphanet Rare Disease ontology (ORDO) is jointly developed by
    Orphanet and the EBI to provide a structured vocabulary for rare diseases capturing
    relationships between diseases, genes and other relevant features which will form
    a useful resource for the computational analysis of rare diseases.
  requires_registration: false
  url: https://bioportal.bioontology.org/ontologies/ORDO
- id: B2AI_STANDARD:529
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Pathogen Host Interaction Phenotype Ontology
  formal_specification: https://github.com/PHI-base/phipo
  is_open: true
  name: PHIPO
  publication: doi:10.1093/nar/gkab1037
  purpose_detail: Species-neutral phenotypes observed in pathogen-host interactions.
  requires_registration: false
  url: https://github.com/PHI-base/phipo
- id: B2AI_STANDARD:530
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Pathogen Transmission Ontology
  formal_specification: https://github.com/DiseaseOntology/PathogenTransmissionOntology
  is_open: true
  name: TRANS
  publication: doi:10.1093/nar/gkp832
  purpose_detail: An ontology representing the disease transmission process during
    which the pathogen is transmitted directly or indirectly.
  requires_registration: false
  url: https://github.com/DiseaseOntology/PathogenTransmissionOntology
- id: B2AI_STANDARD:531
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Pathway ontology
  formal_specification: https://github.com/rat-genome-database/PW-Pathway-Ontology
  is_open: true
  name: PW
  publication: doi:10.1186/2041-1480-5-7
  purpose_detail: A controlled vocabulary for annotating gene products to pathways.
  requires_registration: false
  url: http://rgd.mcw.edu/rgdweb/ontology/search.html
- id: B2AI_STANDARD:532
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Performance Summary Display Ontology
  formal_specification: https://github.com/Display-Lab/psdo
  is_open: true
  name: PSDO
  purpose_detail: Ontology to reproducibly study visualizations of clinical performance
  requires_registration: false
  url: https://github.com/Display-Lab/psdo
- id: B2AI_STANDARD:533
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:3
  - B2AI_TOPIC:7
  - B2AI_TOPIC:12
  - B2AI_TOPIC:21
  - B2AI_TOPIC:25
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Phenomics Integrated Ontology
  formal_specification: https://github.com/monarch-initiative/phenio
  has_relevant_organization:
  - B2AI_ORG:58
  is_open: true
  name: PHENIO
  purpose_detail: An application ontology for accessing and comparing knowledge concerning
    phenotypes across species and genetic backgrounds.
  requires_registration: false
  url: https://github.com/monarch-initiative/phenio
- id: B2AI_STANDARD:534
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:25
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Phenotype And Trait Ontology
  formal_specification: https://github.com/pato-ontology/pato
  is_open: true
  name: PATO
  purpose_detail: Phenotypic qualities (properties, attributes or characteristics).
  requires_registration: false
  url: https://github.com/pato-ontology/pato
- id: B2AI_STANDARD:535
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Population and Community Ontology
  formal_specification: https://github.com/PopulationAndCommunityOntology/pco
  is_open: true
  name: PCO
  purpose_detail: Groups of interacting organisms such as populations and communities.
  requires_registration: false
  url: https://github.com/PopulationAndCommunityOntology/pco
- id: B2AI_STANDARD:536
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:3
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Process Chemistry Ontology
  formal_specification: https://github.com/proco-ontology/PROCO
  is_open: true
  name: PROCO
  purpose_detail: Process chemistry, the chemical field concerned with scaling up
    laboratory syntheses to commercially viable processes.
  requires_registration: false
  url: https://github.com/proco-ontology/PROCO
- id: B2AI_STANDARD:537
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:26
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Protein modification
  formal_specification: https://github.com/HUPO-PSI/psi-mod-CV
  has_relevant_organization:
  - B2AI_ORG:41
  is_open: true
  name: PSI-MOD
  publication: doi:10.1038/nbt0808-864
  purpose_detail: PSI-MOD is an ontology consisting of terms that describe protein
    chemical modifications
  requires_registration: false
  url: https://www.psidev.info/groups/protein-modifications
- id: B2AI_STANDARD:538
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:26
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: PRotein Ontology
  formal_specification: https://github.com/PROconsortium/PRoteinOntology/
  is_open: true
  name: PR
  purpose_detail: The Protein Ontology (PR) is a comprehensive formal ontology that
    provides standardized terminology and semantic relationships for describing protein-related
    entities across the complete spectrum of protein science. Developed by the PRotein
    Ontology consortium, PR serves as a unifying framework that integrates protein
    sequence, structure, and functional information into a coherent knowledge representation
    system. The ontology encompasses multiple levels of protein organization including
    protein families and complexes, individual protein molecules, protein domains
    and regions, post-translational modifications, and protein isoforms generated
    through alternative splicing or processing. PR maintains extensive cross-references
    to major protein databases including UniProt, NCBI, and Ensembl, enabling seamless
    integration with existing protein annotation resources. The ontology supports
    advanced protein function annotation by providing precise vocabulary for describing
    enzymatic activities, binding sites, regulatory mechanisms, and cellular localization
    patterns. PR is essential for proteomics data standardization, comparative protein
    analysis, functional genomics research, and systems biology applications where
    consistent protein terminology facilitates data integration, automated reasoning,
    and knowledge discovery across diverse experimental platforms.
  requires_registration: false
  url: https://github.com/PROconsortium/PRoteinOntology/
- id: B2AI_STANDARD:539
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:2
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Provisional Cell Ontology
  formal_specification: https://github.com/obophenotype/provisional_cell_ontology
  is_open: true
  name: PCL
  purpose_detail: Cell types that are provisionally defined by experimental techniques
    such as single cell or single nucleus transcriptomics.
  requires_registration: false
  url: https://github.com/obophenotype/provisional_cell_ontology
- id: B2AI_STANDARD:540
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:1
  - B2AI_TOPIC:11
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Radiation Biology Ontology
  formal_specification: https://github.com/Radiobiology-Informatics-Consortium/RBO
  is_open: true
  name: RBO
  purpose_detail: Effects of radiation on biota in terrestrial and space environments.
  requires_registration: false
  url: https://github.com/Radiobiology-Informatics-Consortium/RBO
- id: B2AI_STANDARD:541
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: RadLex radiology lexicon
  has_relevant_organization:
  - B2AI_ORG:85
  is_open: true
  name: RadLex
  purpose_detail: A comprehensive set of radiology terms for use in radiology reporting,
    decision support, data mining, data registries, education and research.
  requires_registration: true
  url: https://www.rsna.org/practice-tools/data-tools-and-standards/radlex-radiology-lexicon
- id: B2AI_STANDARD:542
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Rat Strain Ontology
  formal_specification: https://github.com/rat-genome-database/RS-Rat-Strain-Ontology
  is_open: true
  name: RS
  publication: doi:10.1186/2041-1480-4-36
  purpose_detail: 'Rat Strain Ontology (RS) is a structured vocabulary maintained by the Rat Genome Database (RGD) at the Medical College of Wisconsin, providing standardized nomenclature and hierarchical classification for laboratory rat strains, wild-derived strains, mutant lines, consomic/congenic strains, and transgenic/knockout rat models used in biomedical research, with integration into the OBO Foundry ecosystem supporting cross-species comparative genomics and phenotype studies. RS catalogs 5,000+ rat strains spanning inbred strains with defined genetic backgrounds (Wistar, Sprague-Dawley, Fischer 344, Brown Norway), outbred stocks with genetic heterogeneity, recombinant inbred lines for QTL mapping, consomic strains where complete chromosomes are substituted between strains, and gene-edited models (CRISPR knockouts, transgenic insertions) targeting specific disease mechanisms. The ontology structures strain relationships through "derived from" properties linking parent-progeny strains, "has genetic background" properties specifying founding strains for congenic lines, and "model of" properties connecting strains to disease phenotypes (hypertension, diabetes, cancer susceptibility), enabling navigation of complex breeding schemes and genetic derivations. RS terms include strain-specific metadata on phenotypic characteristics (coat color, obesity, behavioral traits), genetic markers (microsatellites, SNPs), tissue/cell sources, husbandry requirements, and availability from repositories (Rat Resource and Research Center, Charles River), facilitating experimental planning and reproducibility. The ontology integrates with RGD''s comprehensive annotations linking strains to quantitative trait loci (QTL), genes, pathways, diseases, and phenotypes (Mammalian Phenotype Ontology terms), supporting genome-wide association studies (GWAS), eQTL mapping, and systems genetics analyses that connect genetic variation to physiological endpoints. RS enables translational research by mapping rat models to human disease orthologs, facilitating target validation for drug discovery where rat pharmacology and toxicology data inform human clinical trial design, particularly for cardiovascular disease, neurodegeneration, and metabolic disorders where rat models recapitulate human pathophysiology better than mouse models. In AI/ML applications for precision medicine, RS provides structured metadata for training genomic prediction models that associate strain genotypes with phenotypic outcomes, supports knowledge graph construction linking rat genetic data to human disease mechanisms for cross-species inference, and enables meta-analyses aggregating phenotypic data across studies by standardizing strain identifiers, ultimately accelerating translational discoveries by leveraging the rat''s role as a premier mammalian model for human disease research.'
  requires_registration: false
  url: http://rgd.mcw.edu/rgdweb/search/strains.html
- id: B2AI_STANDARD:543
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Relation Ontology
  formal_specification: https://github.com/oborel/obo-relations
  is_open: true
  name: RO
  purpose_detail: Relationship types shared across multiple ontologies.
  requires_registration: false
  url: https://oborel.github.io/
- id: B2AI_STANDARD:544
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - codesystem
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:8
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: RxNorm
  has_relevant_organization:
  - B2AI_ORG:74
  is_open: true
  name: RxNorm
  purpose_detail: Medication terminology. Provided through UMLS.
  requires_registration: true
  url: https://www.nlm.nih.gov/research/umls/rxnorm/index.html
- id: B2AI_STANDARD:545
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Scientific Evidence and Provenance Information Ontology
  formal_specification: https://sepio-framework.github.io/sepio-linkml/
  is_open: true
  name: SEPIO
  publication: doi:10.5281/zenodo.5214269
  purpose_detail: The Scientific Evidence and Provenance Information Ontology (SEPIO)
    provides a structured framework for representing scientific evidence and provenance
    information supporting knowledge claims. It supports rich, computable representations
    of the evidence and provenance behind scientific assertions, particularly for
    genetic variants and their clinical interpretations.
  related_to:
  - B2AI_STANDARD:256
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:58
  url: https://github.com/monarch-initiative/SEPIO-ontology
- id: B2AI_STANDARD:546
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Semantic Sensor Network Ontology
  formal_specification: https://github.com/w3c/sdw
  has_relevant_organization:
  - B2AI_ORG:99
  is_open: true
  name: SSN
  purpose_detail: An ontology for describing sensors and their observations, the involved
    procedures, the studied features of interest, the samples used to do so, and the
    observed properties, as well as actuators.
  requires_registration: false
  url: https://www.w3.org/TR/vocab-ssn/
- id: B2AI_STANDARD:547
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Sequence types and features ontology
  formal_specification: https://github.com/The-Sequence-Ontology/SO-Ontologies
  is_open: true
  name: SO
  publication: doi:10.1016/j.jbi.2010.03.002
  purpose_detail: A structured controlled vocabulary for sequence annotation, for
    the exchange of annotation data and for the description of sequence objects.
  requires_registration: false
  url: http://www.sequenceontology.org/
- id: B2AI_STANDARD:548
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Sickle Cell Disease Ontology
  formal_specification: https://github.com/scdodev/scdo-ontology
  is_open: true
  name: SCDO
  purpose_detail: The Sickle Cell Disease Ontology (SCDO) provides comprehensive
    standardized terminology for describing all aspects of sickle cell disease (SCD),
    a group of inherited hemoglobin disorders affecting millions globally, particularly
    populations of African, Mediterranean, Middle Eastern, and Indian ancestry. Developed
    by the H3ABioNet consortium and aligned with OBO Foundry principles, SCDO encompasses
    genetic variants (HbS, HbC, HbE, beta-thalassemia mutations), disease phenotypes
    (sickle cell anemia HbSS, HbSC disease, HbS-beta thalassemia, sickle cell trait),
    clinical manifestations (vaso-occlusive crises, acute chest syndrome, stroke,
    priapism, splenic sequestration, chronic organ damage), laboratory findings (hemoglobin
    electrophoresis patterns, reticulocyte counts, bilirubin levels, fetal hemoglobin
    percentages), complications (pulmonary hypertension, renal dysfunction, avascular
    necrosis, leg ulcers, retinopathy), treatment modalities (hydroxyurea, blood transfusions,
    hematopoietic stem cell transplantation, gene therapy, pain management), and patient
    outcomes (quality of life measures, hospitalization rates, mortality). The ontology
    integrates genetic, clinical, laboratory, and treatment concepts to support comprehensive
    SCD patient data management and research. SCDO enables standardized phenotyping
    for genotype-phenotype correlation studies identifying disease modifiers (fetal
    hemoglobin levels, alpha-thalassemia co-inheritance, genetic polymorphisms affecting
    disease severity), facilitates clinical trial recruitment through precise inclusion/exclusion
    criteria specification, and supports electronic health record integration for
    automated SCD surveillance and quality improvement initiatives. Applications include
    natural history studies characterizing disease progression patterns, pharmacogenomics
    research examining hydroxyurea response variability, health disparities research
    documenting access to disease-modifying therapies, and global SCD registries enabling
    cross-population comparisons. SCDO facilitates data harmonization across international
    SCD cohorts, enabling meta-analyses and collaborative research essential for rare
    disease studies where no single center has sufficient patient numbers.
  requires_registration: false
  url: https://scdontology.h3abionet.org/
- id: B2AI_STANDARD:549
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Software ontology
  formal_specification: https://github.com/allysonlister/swo
  is_open: true
  name: SWO
  publication: doi:10.1186/2041-1480-5-25
  purpose_detail: Software tools, their types, tasks, versions, provenance and associated
    data.
  requires_registration: false
  url: https://github.com/allysonlister/swo
- id: B2AI_STANDARD:550
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Standard Current Procedural Terminology Consumer Friendly Descriptors
  is_open: false
  name: CFDs
  purpose_detail: Translate each code descriptor from the official CPT code set into
    language that is easily understood by the average patient and/or his or her caregiver.
    The objective is to simplify the highly technical CPT code descriptors into something
    more patient-focused and patient-friendly.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:3
  url: https://commerce.ama-assn.org/catalog/media/Consumer-and-Clinician-Descriptors-in-CPT-Data-Files.pdf
- id: B2AI_STANDARD:551
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Statistics Ontology
  formal_specification: https://github.com/ISA-tools/stato
  has_relevant_organization:
  - B2AI_ORG:47
  is_open: true
  name: STATO
  purpose_detail: "The Statistics Ontology (STATO) provides comprehensive standardized
    vocabulary for statistical methods, experimental design concepts, hypothesis testing
    procedures, and statistical measures used across life sciences, biomedical research,
    and data-intensive scientific domains, developed by the ISA-tools community to
    support reproducible research and transparent reporting of statistical analyses.
    STATO encompasses five major categories: statistical tests (parametric tests including
    t-tests, ANOVA, ANCOVA, linear regression, mixed models; non-parametric tests
    including Mann-Whitney U, Kruskal-Wallis, Wilcoxon signed-rank, Spearman correlation;
    and specialized methods like survival analysis, multivariate analysis, time series
    analysis), probability distributions (normal, binomial, Poisson, exponential,
    chi-square, t-distribution, F-distribution) essential for understanding test assumptions,
    descriptive statistics (measures of central tendency including mean, median, mode;
    measures of dispersion including standard deviation, variance, interquartile range;
    and measures of shape including skewness, kurtosis), data types and variables
    (categorical/nominal, ordinal, continuous, discrete, dependent/independent variables,
    covariates, confounding variables), and experimental design concepts (randomization,
    blocking, replication, control groups, factorial designs, crossover designs, longitudinal
    studies). STATO provides formal OWL definitions enabling automated reasoning about
    test conditions of application, linking test selection to data characteristics
    (e.g., use Mann-Whitney U when comparing two independent groups with non-normal
    distributions), and capturing assumptions (normality, homoscedasticity, independence)
    that must be verified before test application. Each statistical method includes
    textual definitions for human understanding, formal logical definitions for machine
    reasoning, associated R code snippets via 'R-command' annotations enabling direct
    implementation, and documentation of appropriate use cases and interpretation guidelines.
    STATO integrates with BFO (Basic Formal Ontology) as upper-level ontology and
    OBI (Ontology for Biomedical Investigations) for process definitions, ensuring
    interoperability across biomedical ontologies. Applications include annotation
    of analysis methods in ISA-Tab metadata files for omics studies, standardized
    reporting of statistical procedures in publications to meet journal guidelines
    (CONSORT, STROBE, ARRIVE), automated validation of statistical analysis workflows
    in computational notebooks, education and training through formal definitions
    of statistical concepts with R implementation examples, and text mining of scientific
    literature to extract statistical method usage patterns. STATO supports reproducibility
    by precisely specifying analysis procedures, capturing multiple testing correction
    methods (Bonferroni, Benjamini-Hochberg FDR, permutation tests), effect size measures
    (Cohen's d, odds ratios, hazard ratios), and confidence interval calculations.
    The ontology facilitates peer review by enabling reviewers to verify appropriate
    test selection and assists researchers in choosing correct statistical methods
    based on study design and data characteristics."
  requires_registration: false
  url: http://stato-ontology.org/
- id: B2AI_STANDARD:552
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Symptom Ontology
  formal_specification: https://github.com/DiseaseOntology/SymptomOntology
  is_open: true
  name: SYMP
  publication: doi:10.1093/nar/gkab1063
  purpose_detail: Disease symptoms, with symptoms encompasing perceived changes in
    function, sensations or appearance reported by a patient.
  requires_registration: false
  url: http://symptomontologywiki.igs.umaryland.edu/mediawiki/index.php/Main_Page
- id: B2AI_STANDARD:553
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - codesystem
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Systematized Nomenclature of Medicine - Clinical Terms
  has_relevant_organization:
  - B2AI_ORG:74
  is_open: true
  name: SNOMED CT
  purpose_detail: Standard for electronic exchange of clinical health information.
    Provided through UMLS.
  related_to:
  - B2AI_STANDARD:769
  requires_registration: true
  url: https://www.nlm.nih.gov/healthit/snomedct/index.html
- id: B2AI_STANDARD:554
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Systematized Nomenclature of Medicine, International Version
  has_relevant_organization:
  - B2AI_ORG:74
  is_open: true
  name: SNMI
  purpose_detail: 'Systematized Nomenclature of Medicine International (SNMI) is a historical medical
    terminology and classification system that preceded SNOMED CT. Developed in the
    1960s-1970s by the College of American Pathologists, SNMI represented one of the
    earliest attempts to create a comprehensive, multi-axial medical nomenclature covering
    topography (anatomy), morphology (structural changes), etiology (causes), and function
    (physiological processes). SNMI employed a systematic coding structure where concepts
    were represented by alphanumeric codes and could be combined using post-coordination
    to express complex clinical findings (e.g., combining topography codes with morphology
    codes to describe disease locations and characteristics). The terminology was designed
    primarily for pathology and clinical documentation, providing structured vocabulary
    for diagnoses, procedures, and laboratory findings. SNMI evolved through several
    versions including SNOMED (Systematized Nomenclature of Medicine), SNOMED II, and
    SNOMED III before being integrated into SNOMED CT (Clinical Terms) in 2002 through
    merger with UK''s Clinical Terms Version 3 (Read Codes). While SNMI itself is
    now deprecated and replaced by SNOMED CT for current clinical use, it is historically
    significant as a foundational medical terminology that pioneered multi-axial compositional
    approaches to medical concept representation. Legacy SNMI data may still exist
    in historical medical records and research databases, requiring mapping to modern
    terminologies for interoperability. Understanding SNMI''s structure provides context
    for SNOMED CT''s architecture and the evolution of standardized medical terminologies
    used in electronic health records, clinical research, and healthcare data analytics.'
  requires_registration: false
  url: https://bioportal.bioontology.org/ontologies/SNMI
- id: B2AI_STANDARD:555
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Systems Biology Ontology
  formal_specification: https://github.com/EBI-BioModels/SBO
  has_relevant_organization:
  - B2AI_ORG:29
  is_open: true
  name: SBO
  purpose_detail: Terms commonly used in Systems Biology and computational modeling.
  requires_registration: false
  url: https://github.com/EBI-BioModels/SBO
- id: B2AI_STANDARD:556
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Taxonomic rank vocabulary
  formal_specification: https://github.com/phenoscape/taxrank
  is_open: true
  name: TAXRANK
  publication: doi:10.1186/2041-1480-4-34
  purpose_detail: A vocabulary of taxonomic ranks (species, family, phylum, etc).
  requires_registration: false
  url: https://github.com/phenoscape/taxrank
- id: B2AI_STANDARD:557
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: terms4FAIRskills
  formal_specification: https://github.com/terms4fairskills/FAIRterminology
  is_open: true
  name: T4FS
  publication: doi:10.5281/zenodo.4772741
  purpose_detail: A terminology for the skills necessary to make data FAIR and to
    keep it FAIR.
  requires_registration: false
  url: https://obofoundry.org/ontology/t4fs.html
- id: B2AI_STANDARD:558
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:8
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: The Drug Ontology
  formal_specification: https://github.com/ufbmi/dron
  has_relevant_organization:
  - B2AI_ORG:96
  is_open: true
  name: DRON
  publication: doi:10.1186/s13326-017-0121-5
  purpose_detail: An ontology to support comparative effectiveness researchers studying
    claims data.
  requires_registration: false
  url: https://github.com/ufbmi/dron
- id: B2AI_STANDARD:559
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: The Extensible Observation Ontology
  formal_specification: https://github.com/NCEAS/oboe/
  has_relevant_organization:
  - B2AI_ORG:62
  is_open: true
  name: OBOE
  purpose_detail: The Extensible Observation Ontology (OBOE) is a formal ontology
    for capturing the semantics of scientific observation and measurement.
  requires_registration: false
  url: https://bioportal.bioontology.org/ontologies/OBOE
- id: B2AI_STANDARD:560
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: The Ontology of Genes and Genomes
  formal_specification: https://bitbucket.org/hegroup/ogg/src/master/
  is_open: true
  name: OGG
  purpose_detail: A formal ontology of genes and genomes of biological organisms.
  requires_registration: false
  url: https://bitbucket.org/hegroup/ogg/src/master/
- id: B2AI_STANDARD:561
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:8
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: The Prescription of Drugs Ontology
  formal_specification: https://github.com/OpenLHS/PDRO
  is_open: true
  name: PDRO
  publication: doi:10.3390/ijerph182212025
  purpose_detail: An ontology to describe entities related to prescription of drugs
  requires_registration: false
  url: https://github.com/OpenLHS/PDRO
- id: B2AI_STANDARD:562
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Toxic Process Ontology
  formal_specification: https://github.com/txpo-ontology/TXPO/
  is_open: true
  name: TXPO
  purpose_detail: Terms involving toxicity courses and processes.
  requires_registration: false
  url: https://toxpilot.nibiohn.go.jp/
- id: B2AI_STANDARD:563
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Uberon
  formal_specification: https://github.com/obophenotype/uberon
  is_open: true
  name: UBERON
  publication: doi:10.1186/gb-2012-13-1-r5
  purpose_detail: UBERON is an OBO Foundry ontology providing an integrated cross-species
    anatomy ontology covering anatomical structures in animals, with a focus on multi-species
    interoperability. The ontology enables semantic annotation of anatomical entities
    across diverse species, supporting comparative anatomy research, phenotype studies,
    and integration with specialized anatomies. UBERON is tightly integrated with
    the Gene Ontology (GO) and the Cell Ontology (CL), using formal ontology design
    patterns to represent anatomical locations and relationships. It follows FAIR
    principles and is released in standard formats (OWL, OBO, JSON obographs) with
    resolvable version IRIs. UBERON is integrated into standard tools including Ubergraph
    for logical queries (e.g., finding cell types by location), the Ontology Access
    Kit (OAK), and major browsers (OLS, Ontobee, BioPortal). The ontology supports
    AI/ML applications by providing standardized anatomical annotations for training
    data across species and enabling cross-species knowledge transfer in biomedical
    machine learning models.
  requires_registration: false
  url: https://obophenotype.github.io/uberon/
- id: B2AI_STANDARD:564
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: UMLS Metathesaurus
  has_relevant_organization:
  - B2AI_ORG:74
  - B2AI_ORG:115
  is_open: true
  name: Metathesaurus
  purpose_detail: Biomedical terminology and hierarchical relationships between concepts.
  requires_registration: true
  url: https://www.nlm.nih.gov/research/umls/knowledge_sources/metathesaurus/index.html
  used_in_bridge2ai: true
- id: B2AI_STANDARD:565
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:25
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Unified phenotype ontology
  formal_specification: https://github.com/obophenotype/upheno
  is_open: true
  name: UPHENO
  purpose_detail: Integrates multiple phenotype ontologies into a unified cross-species
    phenotype ontology.
  requires_registration: false
  url: https://github.com/obophenotype/upheno
- id: B2AI_STANDARD:566
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Units of measure ontology
  is_open: true
  name: UO
  publication: doi:10.1093/database/bas033
  purpose_detail: The Units Ontology - a tool for integrating units of measurement
    in science
  requires_registration: false
  url: https://obofoundry.org/ontology/uo.html
- id: B2AI_STANDARD:567
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Units of measurement ontology
  formal_specification: https://github.com/bio-ontology-research-group/unit-ontology
  is_open: true
  name: UO
  purpose_detail: Metrical units for use in conjunction with PATO.
  requires_registration: false
  url: https://github.com/bio-ontology-research-group/unit-ontology
- id: B2AI_STANDARD:568
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Universal Medical Device Nomenclature System
  has_relevant_organization:
  - B2AI_ORG:27
  is_open: false
  name: UMDNS
  purpose_detail: Universal Medical Device Nomenclature System (UMDNS) is a nomenclature
    that has been officially adopted by many nations. UMDNS facilitates identifying,
    processing, filing, storing, retrieving, transferring, and communicating data
    about medical devices. The nomenclature is used in applications ranging from hospital
    inventory and work-order controls to national agency medical device regulatory
    systems.
  requires_registration: true
  url: https://www.ecri.org/solutions/umdns
- id: B2AI_STANDARD:569
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Vaccine Ontology
  formal_specification: https://github.com/vaccineontology/VO
  is_open: true
  name: VO
  publication: doi:10.1186/2041-1480-3-17
  purpose_detail: The Vaccine Ontology (VO) is an OBO Foundry community-based biomedical
    ontology that systematically represents vaccine components, vaccine types, vaccination
    procedures, vaccine-induced immune responses, and vaccine-preventable infectious
    diseases in a standardized machine-readable format to support vaccine research,
    immunization program management, and vaccinomics data integration. Developed through
    international collaboration coordinated by the University of Michigan Medical
    School, VO encompasses comprehensive coverage of licensed human and veterinary
    vaccines including live-attenuated vaccines, inactivated vaccines, subunit vaccines,
    toxoid vaccines, mRNA vaccines, viral vector vaccines, and conjugate vaccines,
    representing their antigenic components, adjuvants, preservatives, manufacturing
    processes, and delivery routes. The ontology integrates with multiple biomedical
    ontologies including the Infectious Disease Ontology (IDO) for pathogen-host
    interactions, the Ontology for Biomedical Investigations (OBI) for vaccination
    protocols, the Protein Ontology (PRO) for vaccine antigens, and SNOMED CT and
    ICD codes for clinical documentation, enabling semantic interoperability across
    vaccine databases and immunization information systems. VO formally represents
    immunization schedules (primary series, booster doses, catch-up schedules), adverse
    events following immunization (AEFI) including local reactions and systemic effects,
    contraindications and precautions for specific populations, vaccine efficacy and
    effectiveness measures, and herd immunity thresholds. Applications include standardized
    annotation of vaccine clinical trials data, integration of vaccine safety surveillance
    systems (VAERS, Vaccine Safety Datalink), comparative vaccine effectiveness studies
    across populations and time periods, semantic queries of vaccine literature through
    PubMed and clinical databases, machine learning models predicting vaccine immunogenicity
    or reactogenicity from vaccine composition, and global immunization data harmonization
    supporting WHO vaccination coverage monitoring and pandemic preparedness. VO follows
    OBO Foundry principles with open-source development, versioned releases, and persistent
    URIs, making it essential infrastructure for computational vaccinology and evidence-based
    immunization policy.
  requires_registration: false
  url: https://github.com/vaccineontology/VO
- id: B2AI_STANDARD:570
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Vertebrate Breed Ontology
  formal_specification: https://github.com/monarch-initiative/vertebrate-breed-ontology
  has_relevant_organization:
  - B2AI_ORG:58
  is_open: true
  name: VBO
  purpose_detail: 'Vertebrate Breed Ontology (VBO) is a comprehensive ontology providing standardized nomenclature and hierarchical classification for vertebrate animal breeds across 38 species including cattle, sheep, goats, pigs, horses, chickens, dogs, and cats, developed collaboratively by the Monarch Initiative, Online Mendelian Inheritance in Animals (OMIA), and FAO''s Domestic Animal Diversity Information System (DAD-IS). VBO structures breed information using OBO Foundry principles, capturing breed names, synonyms (alternate spellings, historical names, local language variants), geographical origins, breed characteristics (size, coat color, production traits), and relationships to parent breeds or breed groups. The ontology integrates with DAD-IS''s 15,000+ national breed populations representing 8,800+ breeds maintained by 182 countries'' National Coordinators, ensuring global coverage and continuous updates reflecting new breed registrations and naming conventions. VBO supports cross-species breed queries by organizing breeds under species-specific classes (bovine breeds, equine breeds) while maintaining inter-breed relationships such as "derived from" for composite breeds and "related to" for breeds sharing genetic heritage. The ontology enables precise phenotype-genotype associations in animal genetics research by providing stable breed identifiers (VBO IDs) that link to genomic datasets, genetic variant databases, and phenotype repositories, facilitating genome-wide association studies (GWAS) and quantitative trait locus (QTL) mapping in livestock. VBO integrates with other biomedical ontologies including the Livestock Breed Ontology (LBO), Mammalian Phenotype Ontology (MP), and UBERON anatomy ontology to support complex queries across breed characteristics, anatomical features, and disease susceptibilities. For agricultural genomics, VBO enables tracking of breed-specific genetic diversity, conservation status of rare breeds (endangered, critical, vulnerable), and lineage documentation for breeding programs aimed at preserving genetic resources. In AI/ML applications, VBO provides structured metadata for training computer vision models on breed classification, natural language processing systems for extracting breed information from veterinary records, and knowledge graphs connecting breed genetics to production traits (milk yield, growth rate, disease resistance) and animal welfare indicators, supporting precision livestock farming and genomic selection programs.'
  requires_registration: false
  url: https://github.com/monarch-initiative/vertebrate-breed-ontology
- id: B2AI_STANDARD:571
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Vertebrate Taxonomy Ontology
  formal_specification: https://github.com/phenoscape/vertebrate-taxonomy-ontology
  is_open: true
  name: VTO
  purpose_detail: Extinct and extant vertebrate taxa.
  requires_registration: false
  url: https://github.com/phenoscape/vertebrate-taxonomy-ontology
- id: B2AI_STANDARD:572
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:25
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Vertebrate trait ontology
  formal_specification: https://github.com/AnimalGenome/vertebrate-trait-ontology
  is_open: true
  name: VT
  purpose_detail: The Vertebrate Trait Ontology (VT) provides comprehensive standardized
    terminology for describing measurable or observable phenotypic characteristics
    across vertebrate species, maintained by the AnimalGenome organization to support
    agricultural genomics, comparative biology, quantitative genetics, and translational
    research linking animal models to human health. VT encompasses broad trait categories
    including morphological traits (body size, organ dimensions, skeletal measurements,
    integument characteristics), physiological traits (metabolic parameters, cardiovascular
    function, respiratory capacity, immune response, reproductive performance), behavioral
    traits (temperament, learning ability, social interactions, feeding behavior),
    production traits critical for livestock industries (growth rate, feed efficiency,
    milk yield, egg production, meat quality, wool production), disease resistance
    traits (pathogen susceptibility, vaccine response, parasite load), and life history
    traits (longevity, fertility, developmental timing). Each trait is formally defined
    with measurement methodologies, units, biological context, and relationships to
    anatomical structures (via Uberon) and biological processes (via Gene Ontology).
    VT serves as the primary phenotype ontology for agricultural animal genomics databases
    including AnimalQTLdb (quantitative trait loci database for cattle, pig, chicken,
    sheep, horse, rainbow trout), supporting annotation of QTL mapping studies that
    identify genomic regions influencing economically important traits. The ontology
    enables cross-species phenotype comparisons essential for translational research,
    allowing researchers to leverage livestock as biomedical models (e.g., pig cardiovascular
    traits for human heart disease research, sheep bone traits for osteoporosis studies).
    Applications span livestock breeding programs through genomic selection where VT
    standardizes trait definitions for estimated breeding values (EBVs), wildlife
    conservation genetics for monitoring population health indicators, aquaculture
    for trait improvement in fish and shellfish, and comparative physiology studies
    examining adaptive evolution of traits across vertebrate lineages. VT facilitates
    GWAS (genome-wide association studies) meta-analyses by harmonizing trait definitions
    across studies, enables phenotype-driven queries in genomics databases ("find
    all QTL affecting milk fat percentage in dairy cattle"), and supports machine
    learning models predicting complex traits from genotype data by providing structured
    phenotype representations. Integration with other ontologies including Mammalian
    Phenotype Ontology (MP), Human Phenotype Ontology (HPO), and Clinical Measurement
    Ontology (CMO) enables bidirectional translation between agricultural animal research
    and human biomedicine. VT is distributed under CC-BY-NC 4.0 license through http://purl.obolibrary.org/obo/vt.owl
    with releases synchronized to OBO Foundry and AgroPortal.
  requires_registration: false
  url: https://github.com/AnimalGenome/vertebrate-trait-ontology
- id: B2AI_STANDARD:573
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: VEuPathDB ontology
  formal_specification: https://github.com/VEuPathDB-ontology/VEuPathDB-ontology
  is_open: true
  name: EUPATH
  publication: doi:10.5281/zenodo.6685957
  purpose_detail: Support ontology for the Eukaryotic Pathogen, Host & Vector Genomics
    Resource (VEuPathDB; https://veupathdb.org).
  requires_registration: false
  url: https://github.com/VEuPathDB-ontology/VEuPathDB-ontology
- id: B2AI_STANDARD:574
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:25
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Xenopus Phenotype Ontology
  formal_specification: https://github.com/obophenotype/xenopus-phenotype-ontology
  is_open: true
  name: XPO
  publication: doi:10.1186/s12859-022-04636-8
  purpose_detail: The Xenopus Phenotype Ontology (XPO) is a formal ontology for representing
    anatomical, cellular, and gene function phenotypes observed throughout the development
    of the African frogs Xenopus laevis and Xenopus tropicalis. XPO enables standardized
    annotation of phenotypes in Xenopus research, supporting integration with genotype,
    phenotype, and disease data across species. The ontology is used by Xenbase and
    other resources to facilitate cross-species comparisons, data sharing, and computational
    analysis of developmental and functional phenotypes in model organism research.
  requires_registration: false
  url: https://github.com/obophenotype/xenopus-phenotype-ontology
- id: B2AI_STANDARD:575
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:25
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Zebrafish Phenotype Ontology
  formal_specification: https://github.com/obophenotype/zebrafish-phenotype-ontology
  is_open: true
  name: ZP
  purpose_detail: All phenotypes of the Zebrafish model organism.
  requires_registration: false
  url: https://github.com/obophenotype/zebrafish-phenotype-ontology
- id: B2AI_STANDARD:576
  category: B2AI_STANDARD:DataStandardOrTool
  concerns_data_topic:
  - B2AI_TOPIC:6
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDC Race and Ethnicity Code Set
  formal_specification: https://phinvads.cdc.gov/vads/ViewValueSet.action?id=B246B692-6DF8-E111-B875-001A4BE7FA90
  has_relevant_organization:
  - B2AI_ORG:12
  - B2AI_ORG:40
  is_open: true
  name: PHVS_Race_HL7_2x
  purpose_detail: A code set for use in coding race and ethnicity data.
  requires_registration: false
  url: https://www.cdc.gov/phin/resources/vocabulary/documents/cdc-race--ethnicity-background-and-purpose.pdf
- id: B2AI_STANDARD:577
  category: B2AI_STANDARD:DataStandardOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  - B2AI_TOPIC:6
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: IETF Request for Comment 5646 Tags for Identifying Languages
  has_relevant_organization:
  - B2AI_ORG:45
  is_open: true
  name: RFC 5646
  purpose_detail: Best practices for the structure, content, construction, and semantics
    of language tags for use in cases where it is desirable to indicate the language
    used in an information object.
  requires_registration: false
  url: https://www.rfc-editor.org/rfc/rfc5646
- id: B2AI_STANDARD:578
  category: B2AI_STANDARD:DataStandardOrTool
  concerns_data_topic:
  - B2AI_TOPIC:31
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: International Telecommunication Union E.123 Notation for national and
    international telephone numbers, e-mail addresses and web addresses
  has_relevant_organization:
  - B2AI_ORG:50
  is_open: true
  name: ITUT E.123
  purpose_detail: Standard notation for printing telephone numbers, E-mail addresses
    and Web addresses.
  requires_registration: false
  url: https://www.itu.int/rec/T-REC-E.123-200102-I/en
- id: B2AI_STANDARD:579
  category: B2AI_STANDARD:DataStandardOrTool
  concerns_data_topic:
  - B2AI_TOPIC:31
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: International Telecommunication Union E.164 The international public
    telecommunication numbering plan
  has_relevant_organization:
  - B2AI_ORG:50
  is_open: true
  name: ITUT E.164
  purpose_detail: Number structure and functionality for the five categories of numbers
    used for international public telecommunication - geographic areas, global services,
    Networks, groups of countries (GoC) and resources for trials.
  requires_registration: false
  url: https://www.itu.int/rec/T-REC-E.164-201011-I/en
- id: B2AI_STANDARD:704
  category: B2AI_STANDARD:Registry
  collection:
  - dataregistry
  - softwareregistry
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: bio.tools
  formal_specification: https://github.com/bio-tools/biotoolsregistry/
  is_open: true
  name: bio.tools
  purpose_detail: Registry of software tools, databases and services for bioinformatics
    and the life sciences.
  requires_registration: false
  url: https://bio.tools/
- id: B2AI_STANDARD:705
  category: B2AI_STANDARD:Registry
  collection:
  - softwareregistry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Bioconductor
  formal_specification: https://github.com/Bioconductor/BiocManager
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:40
  is_open: true
  name: Bioconductor
  purpose_detail: The mission of the Bioconductor project is to develop, support,
    and disseminate free open source software that facilitates rigorous and reproducible
    analysis of data from current and emerging biological assays. We are dedicated
    to building a diverse, collaborative, and welcoming community of developers and
    data scientists.
  requires_registration: false
  url: https://www.bioconductor.org/
- id: B2AI_STANDARD:706
  category: B2AI_STANDARD:Registry
  collection:
  - ontologyregistry
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: BioPortal
  formal_specification: https://github.com/ncbo
  is_open: true
  name: BioPortal
  purpose_detail: 'BioPortal is a comprehensive, open-access ontology repository and collaborative
    platform developed by the National Center for Biomedical Ontology (NCBO) at Stanford,
    hosting over 1,000 biomedical ontologies, terminologies, and vocabularies. The
    platform provides centralized access to diverse biomedical knowledge organization
    systems including OBO Foundry ontologies (GO, Uberon, CHEBI, HPO), medical terminologies
    (SNOMED CT, ICD, RxNorm, LOINC), domain-specific vocabularies, and community-developed
    ontologies. BioPortal offers rich functionality including: ontology browsing with
    hierarchical visualization and concept lookup; powerful search across all ontologies
    with autocomplete and recommendations; mapping services for finding correspondences
    between ontologies; annotator services for identifying ontology concepts in free
    text; versioning and change tracking for ontology evolution; widgets and web services
    (REST APIs) for programmatic access; and community features for commenting, discussions,
    and ontology reviews. The platform serves as infrastructure for ontology developers
    (submission, hosting, versioning), data curators (annotation, mapping, validation),
    and application developers (APIs, widgets, SPARQL endpoints). BioPortal enables
    semantic annotation of biomedical data, cross-resource data integration through
    ontology mappings, and standardized vocabularies for clinical research, genomics,
    drug discovery, and translational medicine. In AI/ML applications, BioPortal supports
    ontology-based feature engineering for predictive models, semantic search for literature
    and dataset discovery, text mining and NLP by providing concept recognizers, knowledge
    graph construction by linking data to standardized ontologies, and explainable
    AI through ontological reasoning and structured domain knowledge integration.'
  requires_registration: false
  url: https://bioportal.bioontology.org/
- id: B2AI_STANDARD:707
  category: B2AI_STANDARD:Registry
  collection:
  - standardsregistry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Bioregistry
  formal_specification: https://github.com/biopragmatics/bioregistry
  is_open: true
  name: Bioregistry
  publication: doi:10.1101/2022.07.08.499378
  purpose_detail: The Bioregistry is an open-source, community-curated registry and
    meta-registry that catalogs prefixes, identifier formats, and metadata for biomedical
    ontologies, databases, and controlled vocabularies. It integrates and harmonizes
    information from multiple registries (e.g., OBO Foundry, Identifiers.org, OLS),
    providing a unified resource for resolving compact URIs (CURIEs) and supporting
    semantic interoperability. The Bioregistry also functions as a resolver, mapping
    CURIEs to web resources, and is governed by transparent contribution and review
    processes. It is widely used for data integration, annotation, and knowledge graph
    construction in the life sciences.
  requires_registration: false
  url: https://bioregistry.io/
- id: B2AI_STANDARD:708
  category: B2AI_STANDARD:Registry
  collection:
  - standardsregistry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Bridge to Artificial Intelligence Registry
  formal_specification: https://github.com/bridge2ai/b2ai-standards-registry
  is_open: true
  name: Bridge2AI registry
  purpose_detail: Standards, tools, reference implementations, and related resources.
  requires_registration: false
  url: https://github.com/bridge2ai/b2ai-standards-registry
- id: B2AI_STANDARD:709
  category: B2AI_STANDARD:Registry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDISC Shared Health And Research Electronic library
  has_relevant_organization:
  - B2AI_ORG:15
  is_open: true
  name: CDISC SHARE
  publication: PUBMED:29888049
  purpose_detail: CDISC launched the CDISC Shared Health And Research Electronic library
    (SHARE) to provide the standards metadata in machine-readable formats to facilitate
    the automated management and implementation of the standards.
  requires_registration: true
  url: https://www.cdisc.org/faq/share/what-cdisc-share
- id: B2AI_STANDARD:710
  category: B2AI_STANDARD:Registry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Database Commons
  is_open: true
  name: Database Commons
  publication: doi:10.1016/j.gpb.2022.12.004
  purpose_detail: A catalog of worldwide biological databases maintained by the China
    National Center for Bioinformation,
  requires_registration: false
  url: https://ngdc.cncb.ac.cn/databasecommons/
- id: B2AI_STANDARD:711
  category: B2AI_STANDARD:Registry
  collection:
  - softwareregistry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Dockstore
  formal_specification: https://github.com/dockstore/dockstore
  has_relevant_organization:
  - B2AI_ORG:34
  is_open: true
  name: Dockstore
  purpose_detail: Dockstore is a comprehensive, open-source platform developed by
    the Ontario Institute for Cancer Research (OICR) and other collaborators that
    serves as a central registry for sharing, discovering, and executing containerized
    bioinformatics tools and computational workflows. Built on modern container technologies
    including Docker and Singularity, Dockstore enables researchers to package their
    analytical pipelines with all dependencies and configurations, ensuring reproducibility
    across different computing environments. The platform supports multiple workflow
    languages including Common Workflow Language (CWL), Workflow Description Language
    (WDL), and Nextflow, providing flexibility for diverse computational approaches
    in genomics, proteomics, and systems biology. Dockstore integrates with popular
    code repositories like GitHub, GitLab, and Bitbucket, enabling version-controlled
    development and automated testing of computational tools. The platform facilitates
    scientific collaboration by allowing researchers to discover validated, ready-to-use
    analytical workflows, reducing duplication of effort and accelerating research
    discovery. With built-in execution engines and cloud integration capabilities,
    Dockstore supports scalable workflow execution on local clusters, cloud platforms,
    and high-performance computing systems, making advanced bioinformatics accessible
    to researchers regardless of their computational expertise.
  requires_registration: false
  url: https://dockstore.org/
- id: B2AI_STANDARD:712
  category: B2AI_STANDARD:Registry
  collection:
  - standardsregistry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Fairsharing
  formal_specification: https://github.com/FAIRsharing
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:9
  is_open: true
  name: Fairsharing
  purpose_detail: 'FAIRsharing (fairsharing.org) is a comprehensive, curated registry of research data standards, databases, repositories, and policies that promotes Findable, Accessible, Interoperable, and Reusable (FAIR) data practices across life sciences, environmental sciences, social sciences, and humanities domains. Established by the UK BBSRC, ELIXIR, and Oxford e-Research Centre, FAIRsharing catalogs 2,000+ data standards (ontologies, terminologies, formats, models), 1,800+ databases and repositories (discipline-specific archives, institutional repositories, generalist repositories), and 800+ data policies from funders, journals, and institutions, providing a one-stop resource for discovering community-endorsed resources that facilitate data reuse and reproducibility. Each FAIRsharing record includes structured metadata describing the resource''s scope, governance (community-driven, institutional, commercial), licenses, interoperability capabilities, and relationships to other standards/databases, with persistent identifiers (FAIRsharing DOIs) enabling stable citations in data management plans and publications. The registry employs a domain-tagging system covering biomedical sciences (genomics, proteomics, metabolomics), environmental sciences (climate, biodiversity, geosciences), social sciences, and interdisciplinary fields, with cross-links to related resources illustrating standard-database-policy connections within research ecosystems. FAIRsharing supports researchers in selecting appropriate standards for data annotation (MIAME for microarrays, STROBE for epidemiology) and repositories for data deposition (GEO for gene expression, PDB for protein structures), facilitating compliance with journal and funder mandates for open data sharing. The platform integrates with research infrastructure projects (ELIXIR, RDA, CODATA) and provides APIs for embedding FAIRsharing recommendations into data management planning tools, electronic lab notebooks, and repository submission interfaces. For data stewards, FAIRsharing enables discovery of domain-specific controlled vocabularies (ontologies, taxonomies) that enhance dataset interoperability, semantic search capabilities, and cross-study integration in meta-analyses. In AI/ML research, FAIRsharing guides selection of training data repositories with rich metadata, standardized formats that minimize preprocessing overhead, and permissive licenses enabling model training, while the registry''s coverage of ML-specific resources (model registries, benchmark datasets, evaluation metrics) supports reproducible AI research aligned with FAIR principles for algorithms and models (FAIR4ML).'
  requires_registration: false
  url: https://fairsharing.org/
- id: B2AI_STANDARD:713
  category: B2AI_STANDARD:DataStandardOrTool
  collection:
  - standardsregistry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: LinkML schema registry
  formal_specification: https://github.com/linkml/linkml-registry/
  is_open: true
  name: LinkML registry
  purpose_detail: schemas and machine-actionable standards
  requires_registration: false
  url: https://linkml.io/linkml-registry/registry/
- id: B2AI_STANDARD:714
  category: B2AI_STANDARD:Registry
  collection:
  - standardsregistry
  concerns_data_topic:
  - B2AI_TOPIC:4
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: National Cancer Institute Cancer Data Standards Repository
  formal_specification: https://cdebrowser.nci.nih.gov/cdebrowserClient/cdeBrowser.html#/search
  has_relevant_organization:
  - B2AI_ORG:71
  is_open: true
  name: NCI caDSR
  purpose_detail: Registry and repository for oncology research common data elements
    and forms.
  requires_registration: false
  url: https://datascience.cancer.gov/resources/metadata
- id: B2AI_STANDARD:715
  category: B2AI_STANDARD:Registry
  collection:
  - dataregistry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: National Radiology Data Registry
  is_open: false
  name: NRDR
  publication: doi:10.1016/j.jacr.2011.05.014
  purpose_detail: The primary purpose of NRDR is to aid facilities with their quality
    improvement programs and efforts to improve patient care by comparing facility
    data to that of their region and the nation. A practice or facility may choose
    to participate in any or all registries as appropriate for their practice. When
    a facility joins more than one registry, the warehouse allows information to be
    shared across registries within the facility.
  requires_registration: true
  url: https://nrdr.acr.org/Portal/Nrdr/Main/page.aspx
- id: B2AI_STANDARD:716
  category: B2AI_STANDARD:Registry
  collection:
  - ontologyregistry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Open Biological and Biomedical Ontology Foundry
  has_relevant_organization:
  - B2AI_ORG:75
  is_open: true
  name: OBO Foundry
  publication: doi:10.1093/database/baab069
  purpose_detail: The OBO Foundry is a collaborative community initiative for developing
    interoperable ontologies for the biological and biomedical sciences. It establishes
    and maintains a set of principles for ontology development to ensure quality,
    consistency, and interoperability across its library of domain ontologies. The
    OBO Foundry principles address key requirements including open availability, common
    syntax and semantics, clearly defined scope and content, use of well-documented
    collaborative procedures, orthogonality with other ontologies, provision of unique
    identifiers, and adherence to established naming conventions. The Foundry provides
    comprehensive community resources including the OBO tutorial, ontology browsers
    and tools, operations committees and working groups, and communication channels
    (mailing list, Slack workspace). The OBO Library registry provides standardized
    access to member ontologies in multiple formats (YAML, JSON-LD, RDF/Turtle) and
    includes domain-specific ontologies covering anatomy (UBERON), cell types (CL),
    diseases (MONDO), chemicals (ChEBI), genotypes (GENO), phenotypes (HPO), and many
    others. The OBO Foundry infrastructure includes consistent use of permanent URLs
    (PURLs), version control via GitHub, automated quality checks, and standardized
    release workflows. By ensuring semantic interoperability through shared upper-level
    ontologies and design patterns, OBO Foundry enables cross-domain data integration
    essential for AI/ML applications in biomedical knowledge graphs, automated reasoning,
    data harmonization, and multi-modal machine learning across diverse biological
    datasets.
  requires_registration: false
  url: https://obofoundry.org/
- id: B2AI_STANDARD:717
  category: B2AI_STANDARD:Registry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Papers With Code
  formal_specification: https://github.com/paperswithcode
  is_open: true
  name: PwC
  purpose_detail: A free and open resource with Machine Learning papers, code, datasets,
    methods and evaluation tables.
  requires_registration: false
  url: https://paperswithcode.com/
- id: B2AI_STANDARD:718
  category: B2AI_STANDARD:DataStandardOrTool
  collection:
  - dataregistry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Registry of Research Data Repositories
  is_open: true
  name: r3data
  publication: doi:10.5281/zenodo.6697943
  purpose_detail: re3data is a global registry of research data repositories. The
    registry covers research data repositories from different academic disciplines.
    re3data presents repositories for the permanent storage and access of data sets
    to researchers, funding bodies, publishers and scholarly institutions. re3data
    aims to promote a culture of sharing, increased access and better visibility of
    research data.
  requires_registration: false
  url: https://www.re3data.org/
- id: B2AI_STANDARD:719
  category: B2AI_STANDARD:Registry
  collection:
  - dataregistry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: RSNA-ACR 3D Printing Registry
  has_relevant_organization:
  - B2AI_ORG:85
  is_open: false
  name: 3DP Registry
  purpose_detail: The joint RSNA and American College of Radiology (ACR) 3D printing
    clinical data registry collects 3D printing data at the point of clinical care.
    With the goal of improving both patient care and characterizing resource utilization,
    the brand-new registry collects anonymized 3D printing case information, clinical
    indications and intended uses for printed models, source imaging, model construction
    techniques and effort, 3D printing techniques and effort, and the clinical impact
    of the models.
  requires_registration: true
  url: https://www.rsna.org/practice-tools/RSNA-ACR-3D-printing-registry
- id: B2AI_STANDARD:720
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: A corpus-driven standardization framework for encoding clinical problems
    with HL7 FHIR
  formal_specification: https://github.com/OHNLP/clinical-problem-standardization
  has_relevant_organization:
  - B2AI_ORG:40
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: clinical-problem-standardization
  publication: doi:10.1016/j.jbi.2020.103541
  purpose_detail: A framework for transforming free-text problem descriptions into
    standardized Health Level 7 (HL7) Fast Healthcare Interoperability Resources (FHIR)
    models.
  related_to:
  - B2AI_STANDARD:109
  requires_registration: false
  url: https://github.com/OHNLP/clinical-problem-standardization
- id: B2AI_STANDARD:721
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: A Research Exploration System
  formal_specification: https://github.com/OHDSI/Ares
  has_relevant_organization:
  - B2AI_ORG:76
  is_open: true
  name: ARES
  purpose_detail: A Research Exploration System designed to improved the transparency
    of observational data research. ARES is an opinionated framework that delineates
    three levels of observational data assessment.
  requires_registration: false
  url: https://github.com/OHDSI/Ares
- id: B2AI_STANDARD:722
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - machinelearningframework
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Aesara
  formal_specification: https://github.com/aesara-devs/aesara
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:1
  is_open: true
  name: Aesara
  purpose_detail: Aesara is a Python library that allows one to define, optimize,
    and efficiently evaluate mathematical expressions involving multi-dimensional
    arrays.
  requires_registration: false
  url: https://github.com/aesara-devs/aesara
  has_application:
  - id: B2AI_APP:63
    category: B2AI:Application
    name: Probabilistic Modeling and Bayesian Inference for Biomedicine
    description: Aesara (successor to Theano) is used in biomedical AI for implementing
      probabilistic graphical models, Bayesian neural networks, and statistical inference
      algorithms that quantify uncertainty in clinical predictions. Researchers leverage
      Aesara's symbolic math capabilities and automatic differentiation to build sophisticated
      probabilistic models for tasks like uncertainty-aware disease risk prediction,
      Bayesian optimization of drug dosing regimens, and hierarchical models for multi-center
      clinical studies. The library enables AI applications that provide calibrated
      confidence intervals for predictions, perform approximate Bayesian inference
      through variational methods, and integrate domain knowledge through informative
      priors. Aesara is particularly valuable when prediction uncertainty quantification
      is critical for clinical decision-making.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:723
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - cloudservice
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Amazon Web Services
  is_open: false
  name: AWS
  purpose_detail: 'Amazon Web Services (AWS) is the world''s most comprehensive and broadly adopted cloud computing platform, providing 200+ fully managed services spanning compute (EC2, Lambda serverless), storage (S3 object storage, EBS block storage, EFS file systems), databases (RDS, DynamoDB, Redshift), networking (VPC, CloudFront CDN), machine learning (SageMaker, Bedrock, Rekognition), analytics (EMR, Athena, Kinesis), security (IAM, KMS, CloudHSM), and application integration (SQS, SNS, EventBridge) across 38 geographic regions with 120 availability zones globally. Launched in 2006, AWS pioneered the Infrastructure-as-a-Service (IaaS) model, transforming IT economics by offering pay-as-you-go pricing, elastic scalability, and eliminating upfront capital expenditures for hardware, enabling organizations from startups to enterprises to access enterprise-grade infrastructure previously available only to large corporations. AWS''s compute offerings range from general-purpose EC2 instances for traditional workloads to specialized instance types optimized for memory-intensive applications (R-series), compute-intensive HPC (C-series, Hpc7a), GPU-accelerated deep learning (P5, G5, Trn1 with AWS Trainium/Inferentia chips), and serverless Lambda functions billed per millisecond execution time. The platform''s storage hierarchy includes S3 for durable object storage with 99.999999999% durability, S3 Glacier for archival with retrieval SLAs from minutes to hours, EBS for low-latency block storage attached to EC2 instances, and EFS for scalable shared file systems supporting concurrent access across thousands of compute nodes. AWS''s managed database services eliminate operational overhead of patching, backups, and replication, offering relational databases (Amazon RDS for MySQL, PostgreSQL, Oracle, SQL Server; Amazon Aurora with MySQL/PostgreSQL compatibility), NoSQL databases (DynamoDB for key-value, DocumentDB for MongoDB workloads), graph databases (Neptune), time-series databases (Timestream), and data warehouses (Redshift) with columnar storage for OLAP queries on petabyte-scale datasets. For AI/ML workloads, AWS SageMaker provides end-to-end ML lifecycle management with built-in algorithms, Jupyter notebooks, distributed training across GPU clusters, hyperparameter tuning, model deployment to real-time/batch inference endpoints, and MLOps capabilities (model registry, monitoring, CI/CD pipelines). AWS Bedrock offers access to foundation models from AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon Titan via unified API with retrieval-augmented generation (RAG) capabilities connecting models to enterprise data in S3 or knowledge bases. The platform''s security model includes IAM for fine-grained access control with role-based policies, KMS for encryption key management, CloudHSM for FIPS 140-2 Level 3 hardware security modules, AWS Shield for DDoS protection, GuardDuty for threat detection, and comprehensive compliance certifications (SOC 1/2/3, PCI-DSS, HIPAA, FedRAMP, ISO 27001) enabling deployment of regulated workloads. AWS supports hybrid cloud architectures through AWS Outposts (on-premises hardware running AWS services), AWS Direct Connect (dedicated network connections bypassing public internet), and Storage Gateway (seamless cloud-extension of on-premises storage), enabling gradual cloud migrations and latency-sensitive edge computing scenarios. For AI/ML infrastructure in Bridge2AI and biomedical research, AWS provides scalable compute for genome sequencing analysis (AWS Batch, EC2 Spot instances), secure storage for HIPAA-compliant patient data (S3 with server-side encryption, VPC endpoints preventing internet exposure), managed databases for clinical trial data (RDS with automated backups, multi-AZ high availability), federated learning frameworks (AWS HealthLake for FHIR-based interoperability), and specialized AI services for medical imaging analysis (AWS HealthImaging, Amazon Rekognition Custom Labels), accelerating research workflows while maintaining data privacy, auditability, and compliance with healthcare regulations.'
  requires_registration: true
  url: https://aws.amazon.com/
  used_in_bridge2ai: true
- id: B2AI_STANDARD:724
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Amundsen
  formal_specification: https://github.com/amundsen-io/amundsen
  is_open: true
  name: Amundsen
  purpose_detail: Amundsen is a data discovery and metadata engine for improving the
    productivity of data analysts, data scientists and engineers when interacting
    with data. It does that today by indexing data resources (tables, dashboards,
    streams, etc.) and powering a page-rank style search based on usage patterns (e.g.
    highly queried tables show up earlier than less queried tables).
  requires_registration: false
  url: https://www.amundsen.io/
- id: B2AI_STANDARD:725
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - toolkit
  concerns_data_topic:
  - B2AI_TOPIC:20
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Anduril
  formal_specification: https://bitbucket.org/anduril-dev/anduril/src/stable/
  is_open: true
  name: Anduril
  publication: doi:10.1093/bioinformatics/btz133
  purpose_detail: Anduril is a workflow platform for analyzing large data sets. Anduril
    provides facilities for analyzing high-thoughput data in biomedical research,
    and the platform is fully extensible by third parties.
  requires_registration: false
  url: https://anduril.org/site/
- id: B2AI_STANDARD:726
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Apache Atlas
  formal_specification: https://github.com/apache/atlas
  has_relevant_organization:
  - B2AI_ORG:5
  is_open: true
  name: Apache Atlas
  purpose_detail: Apache Atlas provides open metadata management and governance capabilities
    for organizations to build a catalog of their data assets, classify and govern
    these assets and provide collaboration capabilities around these data assets for
    data scientists, analysts and the data governance team.
  requires_registration: false
  url: https://atlas.apache.org/
- id: B2AI_STANDARD:727
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Apache Spark
  formal_specification: https://github.com/apache/spark
  has_relevant_organization:
  - B2AI_ORG:5
  is_open: true
  name: Spark
  purpose_detail: Apache Spark is a unified analytics engine for large-scale data
    processing. It provides high-level APIs in Java, Scala, Python and R, and an optimized
    engine that supports general execution graphs.
  requires_registration: false
  url: https://spark.apache.org/docs/latest/index.html
- id: B2AI_STANDARD:728
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - deprecated
  - workflowlanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Apache Taverna
  is_open: true
  name: Taverna
  purpose_detail: Taverna is a domain-independent suite of tools used to design and
    execute data-driven workflows.
  requires_registration: false
  url: https://incubator.apache.org/projects/taverna.html
- id: B2AI_STANDARD:729
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - datavisualization
  - notebookplatform
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Appyters
  formal_specification: https://github.com/MaayanLab/appyter-catalog
  is_open: true
  name: Appyters
  publication: doi:10.1016/j.patter.2021.100213
  purpose_detail: Appyters extend the Jupyter Notebook language to support external,
    end-user configurable variables. Appyters can be considered a meta Jupyter Notebook
    language that is compatible with standard Jupyter Notebook execution.
  requires_registration: false
  url: https://appyters.maayanlab.cloud/
- id: B2AI_STANDARD:730
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ATHENA
  formal_specification: https://github.com/OHDSI/Athena
  has_relevant_organization:
  - B2AI_ORG:76
  has_training_resource:
  - B2AI_STANDARD:844
  is_open: true
  name: ATHENA
  purpose_detail: A resource of searchable and loadable standardized vocabularies.
  requires_registration: false
  url: https://athena.ohdsi.org/search-terms/terms
  used_in_bridge2ai: true
- id: B2AI_STANDARD:731
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ATLAS
  formal_specification: https://github.com/OHDSI/Atlas
  has_relevant_organization:
  - B2AI_ORG:76
  has_training_resource:
  - B2AI_STANDARD:844
  is_open: true
  name: ATLAS
  purpose_detail: An open source software tool for researchers to conduct scientific
    analyses on standardized observational data converted to the OMOP Common Data
    Model V5. Researchers can create cohorts by defining groups of people based on
    an exposure to a drug or diagnosis of a particular condition using healthcare
    claims data.
  requires_registration: false
  url: https://atlas-demo.ohdsi.org/#/home
- id: B2AI_STANDARD:732
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:4
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: AtriumDB
  has_relevant_organization:
  - B2AI_ORG:117
  is_open: false
  name: AtriumDB
  publication: doi:10.1088/1361-6579/ab7cb5
  purpose_detail: A database of continuously-recorded physiological waveform data
    and other associated clinical and medical device data. Also the platform for storage
    and retrieval of clinical waveform data.
  requires_registration: true
  url: https://laussenlabs.ca/atriumdb/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:64
    category: B2AI:Application
    name: High-Frequency Physiological Waveform Analysis
    description: AtriumDB is used in AI applications for managing and analyzing high-frequency
      physiological waveform data at scale, enabling deep learning models for intensive
      care monitoring, perioperative risk prediction, and continuous vital sign analysis.
      The platform's efficient storage and query capabilities support training of
      neural networks on bedside monitor data including ECG, arterial blood pressure,
      and other high-resolution physiological signals collected over extended periods.
      AI systems leverage AtriumDB to access synchronized multi-parameter waveforms
      for developing early warning systems, detecting subtle physiological deterioration,
      and predicting adverse events in critically ill patients. The database's time-series
      optimization enables real-time AI inference on streaming waveform data.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:733
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Automated Characterization of Health Information at Large-scale Longitudinal
    Evidence System
  formal_specification: https://github.com/OHDSI/Achilles
  has_relevant_organization:
  - B2AI_ORG:76
  has_training_resource:
  - B2AI_STANDARD:844
  is_open: true
  name: ACHILLES
  purpose_detail: Provides descriptive statistics on an OMOP CDM database.
  related_to:
  - B2AI_STANDARD:243
  requires_registration: false
  url: https://ohdsi.github.io/TheBookOfOhdsi/DataQuality.html#data-quality-checks
- id: B2AI_STANDARD:734
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: balance package
  formal_specification: https://github.com/facebookresearch/balance
  has_relevant_organization:
  - B2AI_ORG:55
  is_open: true
  name: balance
  purpose_detail: A Python package for balancing biased data samples.
  requires_registration: false
  url: https://import-balance.org/
- id: B2AI_STANDARD:735
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: BigQuery
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:4
  has_relevant_organization:
  - B2AI_ORG:37
  is_open: false
  name: BigQuery
  purpose_detail: A fully managed, serverless data warehouse that enables scalable
    analysis over petabytes of data. It is a Platform as a Service (PaaS) that supports
    querying using ANSI SQL.
  requires_registration: true
  url: https://cloud.google.com/bigquery
- id: B2AI_STANDARD:736
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - toolkit
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Biopython
  formal_specification: https://github.com/biopython/biopython
  is_open: true
  name: Biopython
  purpose_detail: Biopython is a set of freely available tools for biological computation
    written in Python by an international team of developers. It is a distributed
    collaborative effort to develop Python libraries and applications which address
    the needs of current and future work in bioinformatics. The source code is made
    available under the Biopython License, which is extremely liberal and compatible
    with almost every license in the world.
  requires_registration: false
  url: https://biopython.org/
- id: B2AI_STANDARD:737
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - toolkit
  concerns_data_topic:
  - B2AI_TOPIC:20
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Biotite
  formal_specification: https://github.com/biotite-dev/biotite
  is_open: true
  name: Biotite
  purpose_detail: This package bundles popular tasks in computational molecular biology
    into a uniform Python library.
  requires_registration: false
  url: https://www.biotite-python.org/
- id: B2AI_STANDARD:738
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:22
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: BrainStat
  formal_specification: https://github.com/MICA-MNI/BrainStat
  is_open: true
  name: BrainStat
  publication: doi:10.1016/j.neuroimage.2022.119807
  purpose_detail: A toolbox for the statistical analysis and context decoding of neuroimaging
    data. It implements both univariate and multivariate linear models and interfaces
    with the BigBrain Atlas, Allen Human Brain Atlas and Nimare databases.
  requires_registration: false
  url: https://brainstat.readthedocs.io/
- id: B2AI_STANDARD:739
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Cavatica data analysis platform
  has_relevant_organization:
  - B2AI_ORG:1
  is_open: false
  name: Cavatica
  purpose_detail: A data analysis and sharing platform designed to accelerate discovery
    in a scalable, cloud-based compute environment where data, results, and workflows
    are shared among the world's research community. Developed by Seven Bridges and
    funded in-part by a grant from the National Institutes of Health (NIH) Common
    Fund, CAVATICA is continuously updated with new tools and datasets.
  requires_registration: true
  url: https://www.cavatica.org/
- id: B2AI_STANDARD:740
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Celery - Distributed Task Queue
  formal_specification: https://github.com/celery/celery
  is_open: true
  name: Celery
  purpose_detail: Celery is a simple, flexible, and reliable distributed system to
    process vast amounts of messages, while providing operations with the tools required
    to maintain such a system. Its a task queue with focus on real-time processing,
    while also supporting task scheduling.
  requires_registration: false
  url: https://docs.celeryq.dev/en/latest/
- id: B2AI_STANDARD:741
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Clique eXtracted Ontology
  formal_specification: https://github.com/fanzheng10/CliXO-1.0
  is_open: true
  name: CLiXO
  publication: doi:10.1126/science.abf3067
  purpose_detail: An updated version of the CliXO (Clique eXtracted Ontology) algorithm
    for inferring gene ontology terms from pairwise gene similarity data.
  requires_registration: false
  url: https://github.com/fanzheng10/CliXO-1.0
- id: B2AI_STANDARD:742
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Comet
  formal_specification: https://github.com/comet-ml
  is_open: false
  name: Comet
  purpose_detail: Platform for managing machine learning models.
  requires_registration: true
  url: https://www.comet.com/
- id: B2AI_STANDARD:743
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Community Detection APplication and Service
  formal_specification: https://github.com/cytoscape/cy-community-detection
  is_open: true
  name: CDAPS
  publication: doi:10.1371/journal.pcbi.1008239
  purpose_detail: Performs multiscale community detection and functional enrichment
    for network analysis through a service-oriented architecture. These features are
    provided by integrating popular community detection algorithms and enrichment
    tools. All the algorithms and tools run remotely on a dedicated server.
  requires_registration: false
  url: https://cdaps.readthedocs.io/
- id: B2AI_STANDARD:744
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Community Detection CliXO
  formal_specification: https://github.com/idekerlab/cdclixo
  is_open: true
  name: CD-CLiXO
  purpose_detail: Builds a CDAPS compatible community detection Docker image using
    CliXO.
  requires_registration: false
  url: https://github.com/idekerlab/cdclixo
- id: B2AI_STANDARD:745
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Contextual Ontology-based Repository Analysis Library
  formal_specification: https://github.com/jmchandonia/CORAL
  is_open: true
  name: CORAL
  publication: doi:10.1093/gigascience/giac089
  purpose_detail: A framework for rigorous self-validated data modeling and integrative,
    reproducible data analysis
  requires_registration: false
  url: https://github.com/jmchandonia/CORAL
- id: B2AI_STANDARD:746
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Continuous Machine Learning
  formal_specification: https://github.com/iterative/cml
  is_open: true
  name: CML library
  purpose_detail: Continuous Machine Learning (CML) is an open-source library for
    implementing continuous integration & delivery (CI/CD) in machine learning projects.
    Use it to automate parts of your development workflow, including model training
    and evaluation, comparing ML experiments across your project history, and monitoring
    changing datasets.
  requires_registration: false
  url: https://cml.dev/
- id: B2AI_STANDARD:747
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Cromwell Workflow Management System
  formal_specification: https://github.com/broadinstitute/cromwell
  is_open: true
  name: Cromwell
  publication: doi:10.7490/f1000research.1114634.1
  purpose_detail: Cromwell is an open-source Workflow Management System for bioinformatics.
  requires_registration: false
  url: https://cromwell.readthedocs.io/en/stable/
- id: B2AI_STANDARD:748
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Cytoscape software
  formal_specification: https://github.com/cytoscape/cytoscape
  is_open: true
  name: Cytoscape
  purpose_detail: An open source software platform for visualizing complex networks
    and integrating these with any type of attribute data.
  requires_registration: false
  url: https://cytoscape.org/
- id: B2AI_STANDARD:749
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Dagster
  formal_specification: https://github.com/dagster-io/dagster
  is_open: false
  name: Dagster
  purpose_detail: Open source orchestration platform for the development, production,
    and observation of data assets.
  requires_registration: true
  url: https://dagster.io/
- id: B2AI_STANDARD:750
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - scrnaseqanalysis
  concerns_data_topic:
  - B2AI_TOPIC:34
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DANCE platform
  formal_specification: https://github.com/OmicsML/dance
  is_open: true
  name: DANCE
  purpose_detail: A Python toolkit to support deep learning models for analyzing single-cell
    gene expression at scale.
  requires_registration: false
  url: https://omicsml.ai/
- id: B2AI_STANDARD:751
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Data Repository Service
  has_relevant_organization:
  - B2AI_ORG:34
  is_open: true
  name: DRS
  purpose_detail: The Data Repository Service (DRS) is a standard for describing and
    accessing data objects in a data repository. The DRS provides a RESTful API for
    accessing data objects, as well as a set of metadata fields for describing the
    data objects. The DRS is designed to be used in conjunction with the GA4GH Data
    Repository Service API, which provides a standard way to access data objects in
    a data repository.
  requires_registration: false
  url: https://ga4gh.github.io/data-repository-service-schemas/preview/release/drs-1.2.0/docs/
- id: B2AI_STANDARD:752
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Data Version Control
  formal_specification: https://github.com/iterative/dvc
  is_open: true
  name: DVC
  purpose_detail: Data Version Control (DVC) is an open-source version control system
    for data science and machine learning projects. It is designed to help data scientists
    and machine learning engineers manage their data, models, and experiments in a
    reproducible and collaborative way.
  requires_registration: false
  url: https://dvc.org/
- id: B2AI_STANDARD:753
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DataHub
  formal_specification: https://github.com/linkedin/datahub
  is_open: true
  name: DataHub
  purpose_detail: DataHub is a metadata platform. It provides a central place to manage
    and discover data assets, including datasets, dashboards, and machine learning
    models. DataHub is designed to be extensible and customizable.
  requires_registration: false
  url: https://datahubproject.io/
- id: B2AI_STANDARD:754
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Datasette
  formal_specification: https://github.com/simonw/datasette
  is_open: true
  name: Datasette
  purpose_detail: 'Datasette is a comprehensive open-source tool ecosystem designed
    for exploring, analyzing, and publishing data through an intuitive web interface
    and accompanying JSON API. Created by Simon Willison, Datasette transforms data
    of any shape into interactive websites that enable users to perform exploratory
    data analysis, create visualizations, and share findings with colleagues without
    requiring extensive technical expertise. The platform excels in three primary
    use cases: exploratory data analysis (automatically detecting patterns in CSV,
    JSON, and database data), instant data publishing (using the `datasette publish`
    command to deploy data to cloud hosting providers like Google Cloud Run, Heroku,
    or Vercel), and rapid prototyping (spinning up JSON APIs in minutes for proof-of-concept
    development). Datasette is part of a broader ecosystem of 44 related tools and
    154 plugins that enhance productivity when working with structured data. The tool
    supports multiple data import formats, provides built-in security features for
    data access control, and offers extensive customization options through its plugin
    architecture. Datasette serves data journalists, museum curators, archivists,
    local governments, scientists, researchers, and anyone seeking to make their data
    more accessible and discoverable through web-based interfaces.'
  requires_registration: false
  url: https://datasette.io/
- id: B2AI_STANDARD:755
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - scrnaseqanalysis
  concerns_data_topic:
  - B2AI_TOPIC:34
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DENDRO
  formal_specification: https://github.com/zhouzilu/DENDRO
  is_open: true
  name: DENDRO
  publication: doi:10.1186/s13059-019-1922-x
  purpose_detail: An analysis method for scRNA-seq data that clusters single cells
    into genetically distinct subclones and reconstructs the phylogenetic tree relating
    the subclones.
  requires_registration: false
  url: https://github.com/zhouzilu/DENDRO
- id: B2AI_STANDARD:756
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - machinelearningframework
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Determined
  formal_specification: https://github.com/determined-ai/determined
  is_open: true
  name: Determined
  purpose_detail: Determined is an open-source deep learning training platform.
  requires_registration: false
  url: https://www.determined.ai/
  has_application:
  - id: B2AI_APP:66
    category: B2AI:Application
    name: Scalable ML Training Platform for Healthcare Research
    description: Determined is used in biomedical AI for managing large-scale deep
      learning experiments, hyperparameter tuning, and distributed training across
      GPU clusters in research and healthcare organizations. AI teams leverage Determined's
      automated experiment tracking, resource scheduling, and hyperparameter optimization
      to efficiently develop medical imaging models, genomic prediction algorithms,
      and clinical NLP systems. The platform provides reproducible experiment management
      with automatic checkpointing, fault tolerance for long-running medical imaging
      training jobs, and collaborative features for multi-institutional research teams.
      Determined's integration with healthcare security requirements and support for
      diverse model frameworks makes it valuable for organizations scaling from research
      to production clinical AI.
    used_in_bridge2ai: false
    references:
    - https://docs.determined.ai/
- id: B2AI_STANDARD:757
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Dicoogle Picture Archiving and Communications System
  formal_specification: https://github.com/bioinformatics-ua/dicoogle
  is_open: true
  name: Dicoogle
  publication: doi:10.1109/ISCC50000.2020.9219545
  purpose_detail: Dicoogle is an open source Picture Archiving and Communications
    System (PACS) archive. Its modular architecture allows the quick development of
    new functionalities, due the availability of a Software Development Kit (SDK).
  requires_registration: false
  url: https://dicoogle.com/
- id: B2AI_STANDARD:758
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - cloudservice
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DigitalOcean
  is_open: false
  name: DigitalOcean
  purpose_detail: DigitalOcean is a comprehensive cloud infrastructure platform that
    provides scalable computing resources designed to simplify cloud deployment for
    developers, startups, and enterprises. The platform offers a complete suite of
    cloud services including Droplets (virtual machines), Kubernetes clusters, App
    Platform for application deployment, managed databases, object storage (Spaces),
    load balancers, and specialized AI/ML infrastructure including GPU-powered Droplets
    with NVIDIA H100, H200, and AMD MI325X accelerators. DigitalOcean distinguishes
    itself through developer-friendly interfaces, predictable pricing without bandwidth
    overage charges, and extensive community-contributed tutorials and documentation.
    The platform serves over 600,000 customers ranging from individual developers
    to large enterprises, offering both self-service infrastructure management and
    managed services with premium support options. DigitalOcean's global data center
    network provides 99.99% uptime SLAs and supports diverse use cases including web
    hosting, application development, AI inference workloads, container orchestration,
    and data processing pipelines, making it particularly attractive for cost-conscious
    organizations seeking reliable cloud infrastructure without the complexity of
    larger cloud providers.
  requires_registration: true
  url: https://www.digitalocean.com/
- id: B2AI_STANDARD:759
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - scrnaseqanalysis
  concerns_data_topic:
  - B2AI_TOPIC:34
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DoubletDecon
  formal_specification: https://github.com/EDePasquale/DoubletDecon
  is_open: true
  name: DoubletDecon
  publication: doi:10.1016/j.celrep.2019.09.082
  purpose_detail: An approach that detects doublet cell capture artifacts in scRNA-seq
    data with a combination of deconvolution analyses and the identification of unique
    cell-state gene expression.
  requires_registration: false
  url: https://github.com/EDePasquale/DoubletDecon
- id: B2AI_STANDARD:760
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Egeria
  formal_specification: https://github.com/odpi/egeria
  is_open: true
  name: Egeria
  purpose_detail: Open metadata and governance for enterprises - automatically capturing,
    managing and exchanging metadata between tools and platforms, no matter the vendor.
  requires_registration: false
  url: https://egeria-project.org/
- id: B2AI_STANDARD:761
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Eido
  formal_specification: https://github.com/pepkit/eido
  is_open: true
  name: Eido
  publication: doi:10.1093/gigascience/giab077
  purpose_detail: Eido is used to 1) validate or 2) convert format of sample metadata.
    Sample metadata is stored according to the standard PEP specification. For validation,
    eido is based on JSON Schema and extends it with new features, like required input
    files.
  related_to:
  - B2AI_STANDARD:260
  - B2AI_STANDARD:345
  requires_registration: false
  url: http://eido.databio.org/
- id: B2AI_STANDARD:762
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - scrnaseqanalysis
  concerns_data_topic:
  - B2AI_TOPIC:34
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Expression denoising heuristic using aggregation of neighbors and principal
    component extraction
  formal_specification: https://github.com/yanailab/enhance
  is_open: true
  name: ENHANCE
  publication: doi:10.1101/655365
  purpose_detail: ENHANCE is a computational method for accurate denoising of single-cell
    RNA-sequencing data that addresses technical noise while preserving biological
    signal. The algorithm uses k-nearest neighbor aggregation to identify similar
    cells, followed by principal component analysis (PCA) to remove noise components
    while retaining significant biological variation. The method automatically determines
    the optimal number of neighbors (k) based on target transcript counts and identifies
    significant principal components using variance fold-thresholding against simulated
    noise datasets. ENHANCE processes UMI-count matrices in tab-separated format,
    supports gzip compression, and provides configurable parameters for neighbor selection,
    PC threshold determination, and precision control. The tool enhances downstream
    analyses such as clustering, trajectory inference, and differential expression
    by reducing technical artifacts while maintaining cell type distinctions and biological
    relationships in single-cell transcriptomic data.
  requires_registration: false
  url: https://github.com/yanailab/enhance
- id: B2AI_STANDARD:763
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FAIR Data Point
  formal_specification: https://github.com/FAIRDataTeam/FAIRDataPoint
  is_open: true
  name: FDP
  publication: doi:10.1162/dint_a_00160
  purpose_detail: FAIR Data Point (FDP) is a REST API for creating, storing, and serving
    FAIR metadata. This FDP implementation also presents a Web-based graphical user
    interface (GUI). The metadata contents are generated semi-automatically according
    to the FAIR Data Point software specification document.
  requires_registration: false
  url: https://github.com/FAIRDataTeam/FAIRDataPoint
- id: B2AI_STANDARD:764
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - implementation_maturity_pilot
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FAIRSCAPE digital commons framework
  formal_specification: https://github.com/fairscape/fairscape
  has_relevant_organization:
  - B2AI_ORG:116
  is_open: true
  name: FAIRSCAPE
  publication: doi:10.1007/s12021-021-09529-4
  purpose_detail: A reusable computational framework, enabling simplified access to
    modern scalable cloud-based components. FAIRSCAPE fully implements the FAIR data
    principles and extends them to provide fully FAIR Evidence, including machine-interpretable
    provenance of datasets, software and computations, as metadata for all computed
    results.
  requires_registration: false
  url: https://fairscape.github.io/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:67
    category: B2AI:Application
    name: FAIR ML Workflows and Computational Reproducibility
    description: FAIRSCAPE is used in AI applications to create Findable, Accessible,
      Interoperable, and Reusable machine learning workflows with comprehensive provenance
      tracking and metadata management. AI researchers leverage FAIRSCAPE to package
      complete ML pipelines including data preprocessing, model training, validation,
      and deployment with detailed computational provenance, enabling reproducible
      AI research and regulatory compliance. The framework supports automated metadata
      capture during model development, versioning of datasets and models, and generation
      of computational notebooks that document the entire ML lifecycle. FAIRSCAPE
      is particularly valuable for multi-institutional AI studies where workflow transparency,
      data lineage, and reproducibility are essential for scientific validity and
      clinical translation.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:765
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - machinelearningframework
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FastAI
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:33
  is_open: true
  name: FastAI
  purpose_detail: fastai is a deep learning library which provides practitioners with
    high-level components that can quickly and easily provide state-of-the-art results
    in standard deep learning domains, and provides researchers with low-level components
    that can be mixed and matched to build new approaches. It aims to do both things
    without substantial compromises in ease of use, flexibility, or performance. This
    is possible thanks to a carefully layered architecture, which expresses common
    underlying patterns of many deep learning and data processing techniques in terms
    of decoupled abstractions. These abstractions can be expressed concisely and clearly
    by leveraging the dynamism of the underlying Python language and the flexibility
    of the PyTorch library.
  related_to:
  - B2AI_STANDARD:816
  requires_registration: false
  url: https://github.com/fastai/fastai
  has_application:
  - id: B2AI_APP:68
    category: B2AI:Application
    name: Rapid Medical AI Prototyping and Transfer Learning
    description: FastAI is used in biomedical research for rapid prototyping and training
      of deep learning models with minimal code, particularly valuable for researchers
      without extensive ML engineering backgrounds. Healthcare researchers leverage
      FastAI's high-level APIs, best-practice defaults, and powerful transfer learning
      capabilities to quickly develop models for medical image classification, clinical
      text analysis, and tabular clinical data prediction. The library's sophisticated
      learning rate scheduling, progressive resizing, and mixed precision training
      enable efficient use of computational resources, while its extensive documentation
      and educational materials democratize AI development in medicine. FastAI is
      particularly popular for exploratory research, kaggle-style medical AI competitions,
      and educational settings teaching clinical AI.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:766
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - toolkit
  concerns_data_topic:
  - B2AI_TOPIC:20
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Galaxy
  formal_specification: https://github.com/galaxyproject/galaxy
  is_open: true
  name: Galaxy
  publication: doi:10.1093/nar/gky379
  purpose_detail: Galaxy is an open source, web-based platform for data intensive
    biomedical research.
  related_to:
  - B2AI_STANDARD:334
  requires_registration: false
  url: https://usegalaxy.org/
- id: B2AI_STANDARD:767
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - cloudservice
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Google Cloud Platform
  has_relevant_organization:
  - B2AI_ORG:37
  - B2AI_ORG:115
  is_open: false
  name: GCP
  purpose_detail: Google Cloud Platform (GCP) is a comprehensive cloud computing
    platform offering over 200+ services spanning compute, storage, databases, networking,
    AI/machine learning, data analytics, and developer tools across Google's global
    infrastructure. Core compute offerings include Compute Engine VMs, App Engine
    for platform-as-a-service deployment, Google Kubernetes Engine for container
    orchestration, and Cloud Functions for serverless computing. For AI and machine
    learning, GCP provides Vertex AI as a unified platform for building, deploying,
    and scaling ML models, including access to Google's Gemini large language models,
    AutoML capabilities, and specialized hardware like Tensor Processing Units (TPUs)
    for accelerated training. Storage solutions include Cloud Storage for object storage,
    persistent disks for block storage, and Filestore for managed file storage. Database
    services encompass Cloud SQL for managed relational databases (MySQL, PostgreSQL,
    SQL Server), Cloud Spanner for globally distributed relational databases, BigQuery
    for serverless data warehousing and analytics, Firestore for NoSQL document databases,
    and Bigtable for wide-column NoSQL. Healthcare and life sciences researchers benefit
    from specialized services like Healthcare API for FHIR-based health data exchange,
    Life Sciences API for genomics pipeline execution, and Cloud Healthcare Console
    for managing sensitive patient data with HIPAA and HITRUST compliance. BigQuery
    ML enables researchers to create and execute machine learning models using SQL
    queries directly on large biomedical datasets. GCP is explicitly used in Bridge2AI
    projects for scalable biomedical data processing, AI model training and deployment,
    and collaborative research infrastructure. The platform emphasizes security with
    encryption at rest and in transit, IAM for access control, VPC for network isolation,
    and compliance certifications including HIPAA, FedRAMP, ISO 27001, and SOC 2.
    GCP's global network spans 40+ regions and 120+ edge locations, providing low-latency
    access and data residency options for international research collaborations. Integration
    with Google Workspace, Looker for business intelligence, and open-source frameworks
    like TensorFlow and PyTorch facilitates comprehensive workflows from data ingestion
    through analysis, visualization, and publication.
  requires_registration: true
  url: https://cloud.google.com/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:69
    category: B2AI:Application
    name: Cloud-Based Biomedical AI Infrastructure and Model Deployment
    description: Google Cloud Platform is extensively used in AI applications for
      scalable biomedical data processing, large-scale model training, and deployment
      of clinical AI systems with healthcare-compliant infrastructure. AI researchers
      leverage GCP's specialized services including Vertex AI for managed ML pipelines,
      BigQuery for analyzing massive clinical datasets, and Cloud Healthcare API for
      FHIR-based data integration. The platform enables distributed training of deep
      learning models on genomic sequences, medical images, and electronic health
      records at population scale, while providing HIPAA-compliant infrastructure
      for clinical AI deployment. GCP's TPU infrastructure accelerates training of
      large biomedical language models and computer vision systems, and its AutoML
      capabilities democratize AI development for healthcare institutions.
    used_in_bridge2ai: false
    references:
    - https://cloud.google.com/healthcare-api
    - https://cloud.google.com/vertex-ai
- id: B2AI_STANDARD:768
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - graphdataplatform
  - machinelearningframework
  concerns_data_topic:
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Graph Representation leArning, Predictions and Evaluation library
  formal_specification: https://github.com/AnacletoLAB/grape
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:14
  - B2AI_SUBSTRATE:15
  is_open: true
  name: GrAPE
  publication: doi:10.48550/arXiv.2110.06196
  purpose_detail: A fast graph processing and embedding library, designed to scale
    with big graphs and to run on both off-the-shelf laptop and desktop computers
    and High Performance Computing clusters of workstations.
  requires_registration: false
  url: https://github.com/AnacletoLAB/grape
  has_application:
  - id: B2AI_APP:70
    category: B2AI:Application
    name: Graph Embedding and Network Medicine AI
    description: GrAPE (Graph Automatic Programming Environment) is used in biomedical
      AI for creating graph embeddings and training graph neural networks on biological
      networks, enabling applications in drug discovery, protein function prediction,
      and disease gene prioritization. Researchers leverage GrAPE's efficient graph
      processing capabilities to learn low-dimensional representations of large-scale
      biological networks including protein-protein interactions, gene regulatory
      networks, and knowledge graphs. The tool supports AI applications that predict
      novel drug-target interactions, identify disease modules, and perform link prediction
      in biomedical knowledge graphs. GrAPE's optimization for large graphs enables
      training on genome-scale networks and integration of multi-omics data through
      graph-based representations.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1093/gigascience/giab044
- id: B2AI_STANDARD:769
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:32
  - B2AI_TOPIC:16
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Half-Edge Semantic Measures Library
  formal_specification: https://github.com/jjlastra/HESML
  is_open: true
  name: HESML
  publication: doi:10.1186/S12859-021-04539-0
  purpose_detail: HESML is an efficient, scalable and large Java software library
    of ontology-based semantic similarity measures and Information Content (IC) models
    based on WordNet, SNOMED-CT, MeSH or any other OBO-based ontology.
  related_to:
  - B2AI_STANDARD:553
  requires_registration: false
  url: http://hesml.lsi.uned.es/
- id: B2AI_STANDARD:770
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Hangar
  formal_specification: https://github.com/tensorwerk/hangar-py
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:42
  is_open: true
  name: Hangar
  purpose_detail: Hangar is a version control system specifically designed for managing
    tensor data and numerical arrays in machine learning and scientific computing
    workflows, providing Git-like versioning capabilities for datasets that are too
    large or complex for traditional version control systems. Built with Python and
    optimized for performance, Hangar enables data scientists and ML engineers to
    track changes in training datasets, model weights, embeddings, and experimental
    results with full history, branching, merging, and collaboration features. The
    system uses content-addressable storage with automatic data deduplication, storing
    only changed array blocks rather than entire files, making it efficient for large
    multidimensional datasets common in deep learning (image datasets, genomic sequences,
    time series, medical imaging volumes). Hangar provides atomic commits ensuring
    data consistency, supports concurrent read access for distributed training, and
    enables reproducible machine learning by linking dataset versions to specific
    model training runs. Key features include automatic detection of array schema
    changes, efficient storage of sparse arrays, lazy loading for memory-efficient
    data access, and integration with NumPy, PyTorch, and TensorFlow workflows. Applications
    span ML experiment tracking where different dataset versions are tested with model
    architectures, collaborative dataset curation where multiple researchers contribute
    annotations or corrections, model debugging through inspection of training data
    that caused specific failures, and regulatory compliance in medical AI where dataset
    provenance and versioning history must be maintained. Hangar addresses critical
    gaps in ML operations by providing reproducibility (exact dataset version used
    for published results), collaboration (merge dataset contributions from multiple
    sources), and auditability (full history of dataset modifications with timestamps
    and contributors), essential for rigorous scientific research and production ML
    systems.
  requires_registration: false
  url: https://hangar-py.readthedocs.io/en/stable/
- id: B2AI_STANDARD:771
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Health Analytics Data-to-Evidence Suite
  formal_specification: https://github.com/OHDSI/Hades
  has_relevant_organization:
  - B2AI_ORG:76
  is_open: true
  name: HADES
  purpose_detail: HADES (formally known as the OHDSI Methods Library) is a set of
    open source R packages for large scale analytics, including population characterization,
    population-level causal effect estimation, and patient-level prediction.
  requires_registration: false
  url: https://ohdsi.github.io/Hades/
- id: B2AI_STANDARD:772
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - multimodal
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Holistic AI in Medicine framework
  formal_specification: https://github.com/lrsoenksen/HAIM
  is_open: true
  name: HAIM
  publication: doi:10.1038/s41746-022-00689-4
  purpose_detail: A unified Holistic AI in Medicine (HAIM) framework to facilitate
    the generation and testing of AI systems that leverage multimodal inputs.
  requires_registration: false
  url: https://github.com/lrsoenksen/HAIM
  has_application:
  - id: B2AI_APP:71
    category: B2AI:Application
    name: Holistic Multi-Modal Healthcare AI Integration
    description: HAIM (Holistic AI in Medicine) framework is used for developing and
      evaluating multi-modal AI systems that integrate diverse healthcare data types
      including imaging, time-series, tabular clinical data, and text for comprehensive
      patient assessment. The framework enables training of models that leverage complementary
      information across modalities to improve diagnostic accuracy, risk prediction,
      and clinical decision support beyond single-modality approaches. HAIM provides
      standardized methods for multi-modal data fusion, handles missing modalities
      gracefully, and enables systematic evaluation of how different data types contribute
      to model performance. Applications include integrated ICU monitoring systems,
      comprehensive cancer diagnosis combining radiology and pathology, and patient
      deterioration prediction using all available clinical data streams.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1038/s41746-022-00689-4
- id: B2AI_STANDARD:773
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HTSeq
  formal_specification: https://github.com/htseq/htseq
  is_open: true
  name: HTSeq
  publication: doi:10.1093/bioinformatics/btu638
  purpose_detail: A Python library to facilitate the rapid development of high throughput
    sequencing data analysis scripts. HTSeq offers parsers for many common data formats
    in HTS projects, as well as classes to represent data, such as genomic coordinates,
    sequences, sequencing reads, alignments, gene model information and variant calls,
    and provides data structures that allow for querying via genomic coordinates.
  requires_registration: false
  url: https://github.com/htseq/htseq
- id: B2AI_STANDARD:774
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ImJoy
  formal_specification: https://github.com/imjoy-team/ImJoy
  is_open: true
  name: ImJoy
  publication: doi:10.1038/s41592-019-0627-0
  purpose_detail: A plugin powered hybrid computing platform for deploying deep learning
    applications such as advanced image analysis tools. ImJoy runs on mobile and desktop
    environment cross different operating systems, plugins can run in the browser,
    localhost, remote and cloud servers.
  requires_registration: false
  url: https://imjoy.io/
- id: B2AI_STANDARD:775
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Informatics for Integrating Biology and the Bedside platform
  formal_specification: https://github.com/i2b2
  has_relevant_organization:
  - B2AI_ORG:42
  is_open: true
  name: i2b2
  publication: doi:10.1093/jamia/ocv188
  purpose_detail: A system for searching and exchanging clinical data.
  requires_registration: false
  url: https://www.i2b2.org/software/index.html
- id: B2AI_STANDARD:776
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Insight Segmentation and Registration Toolkit
  formal_specification: https://github.com/InsightSoftwareConsortium/ITK
  is_open: true
  name: ITK
  purpose_detail: The Insight Toolkit (ITK) is an open-source, cross-platform toolkit
    for N-dimensional scientific image processing, segmentation, and registration.
  requires_registration: false
  url: https://itk.org/
- id: B2AI_STANDARD:777
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:9
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Integrated digital Multimodal PATIENt daTa framework
  formal_specification: https://github.com/lambda-science/IMPatienT
  is_open: true
  name: IMPatienT
  purpose_detail: A free and open-source web application to digitize, process and
    explore multimodal patient data. IMPatienT has a modular architecture, including
    four components to (i) create a standard vocabulary for a domain, (ii) digitize
    and process free-text data by mapping it to a set of standard terms, (iii) annotate
    images and perform image segmentation, and (iv) generate an automatic visualization
    dashboard to provide insight on the data and perform automatic diagnosis suggestions.
  requires_registration: false
  url: https://impatient.lbgi.fr
- id: B2AI_STANDARD:778
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:27
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Integrative Modeling Platform
  formal_specification: https://github.com/salilab/imp
  is_open: true
  name: IMP
  purpose_detail: An open source C++ and Python toolbox for solving complex modeling
    problems, and a number of applications for tackling some common problems in a
    user-friendly way. IMP can also be used from the Chimera molecular modeling system,
    or via one of several web applications.
  requires_registration: false
  url: https://integrativemodeling.org/
- id: B2AI_STANDARD:779
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - notebookplatform
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Jupyter Notebook
  formal_specification: https://github.com/jupyter/notebook
  is_open: true
  name: Jupyter
  purpose_detail: A web-based notebook environment for interactive computing.
  requires_registration: false
  url: https://jupyter.org/
  used_in_bridge2ai: true
- id: B2AI_STANDARD:780
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Kaldi Speech Recognition Toolkit
  formal_specification: https://github.com/kaldi-asr/kaldi
  is_open: true
  name: Kaldi
  purpose_detail: Kaldi is an open-source toolkit for speech recognition research
    and development, written primarily in C++ with scripting interfaces, providing
    comprehensive implementations of state-of-the-art automatic speech recognition
    (ASR) algorithms including deep neural networks (DNNs), hidden Markov models (HMMs),
    Gaussian mixture models (GMMs), and modern end-to-end architectures. Developed
    by researchers at Johns Hopkins University and maintained by a global community,
    Kaldi emphasizes flexibility, efficiency, and reproducibility in speech recognition
    pipelines. The toolkit provides modular components for acoustic feature extraction
    (MFCCs, PLPs, filterbank energies, pitch features), acoustic modeling (DNN-HMM
    hybrid systems, time-delay neural networks TDNNs, chain models with lattice-free
    MMI training), language modeling (n-gram models, recurrent neural network language
    models), and decoding (weighted finite-state transducers WFSTs for efficient search).
    Kaldi supports both speaker-independent and speaker-adaptive recognition, with
    tools for vocal tract length normalization, feature-space maximum likelihood linear
    regression (fMLLR), and i-vector/x-vector speaker embeddings for robust recognition
    across diverse acoustic conditions. The toolkit includes recipes for training
    ASR systems on standard benchmarks (LibriSpeech, WSJ, Switchboard, Fisher, TED-LIUM,
    AMI, CHiME) with documented pipelines enabling researchers to replicate published
    results and adapt models to new datasets and languages. Applications span transcription
    of clinical conversations for medical documentation, analysis of patient-physician
    interactions, voice biomarker extraction for neurological disease monitoring (Parkinson's,
    ALS, dementia), mental health assessment through speech characteristics, and accessibility
    tools for hearing-impaired patients. Kaldi's extensive documentation, active mailing
    list, and well-tested recipes make it accessible for both research and production
    deployment despite its complexity, bridging traditional statistical ASR approaches
    with modern deep learning methods.
  requires_registration: false
  url: https://kaldi-asr.org/
- id: B2AI_STANDARD:781
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Keepsake
  formal_specification: https://github.com/replicate/keepsake
  is_open: true
  name: Keepsake
  purpose_detail: Version control for machine learning. A Python library that uploads
    files and metadata (like hyperparameters) to Amazon S3 or Google Cloud Storage.
  requires_registration: false
  url: https://keepsake.ai/
- id: B2AI_STANDARD:782
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: khmer
  formal_specification: https://github.com/dib-lab/khmer/
  is_open: true
  name: khmer
  publication: doi:10.12688/f1000research.6924.1
  purpose_detail: khmer is a free, open-source software library and suite of command-line
    tools for efficient analysis of high-throughput DNA sequencing data, including
    genomes, transcriptomes, metagenomes, and single-cell datasets. It implements
    advanced algorithms for probabilistic k-mer counting, digital normalization, compressible
    De Bruijn graph representation, and graph partitioning, enabling scalable de novo
    assembly, error correction, and data reduction. khmer is designed for use in UNIX
    environments and is supported by extensive documentation, community protocols,
    and integration with popular bioinformatics workflows. It is widely used for preprocessing
    and quality control in large-scale sequencing projects.
  related_to:
  - B2AI_STANDARD:183
  requires_registration: false
  url: https://khmer.readthedocs.io/
- id: B2AI_STANDARD:783
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Koza data transformation framework
  formal_specification: https://github.com/monarch-initiative/koza
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:6
  has_relevant_organization:
  - B2AI_ORG:58
  is_open: true
  name: Koza
  purpose_detail: Transform csv, json, yaml, jsonl, and xml and converting them to
    a target csv, json, or jsonl format based on your dataclass model. Koza also can
    output data in the KGX format. Write data transforms in semi-declarative Python.
    Configure source files, expected columns/json properties and path filters, field
    filters, and metadata in yaml. Create or import mapping files to be used in ingests
    (eg id mapping, type mappings). Create and use translation tables to map between
    source and target vocabularies.
  related_to:
  - B2AI_STANDARD:346
  requires_registration: false
  url: https://github.com/monarch-initiative/koza
- id: B2AI_STANDARD:784
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:7
  - B2AI_TOPIC:25
  - B2AI_TOPIC:35
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: LIkelihood Ratio Interpretation of Clinical AbnormaLities
  formal_specification: https://github.com/TheJacksonLaboratory/LIRICAL
  has_relevant_organization:
  - B2AI_ORG:58
  is_open: true
  name: LIRICAL
  publication: doi:10.1016/j.ajhg.2020.06.021
  purpose_detail: Performs phenotype-driven prioritization of candidate diseases and
    genes in the setting of genomic diagnostics (exome or genome) in which the phenotypic
    abnormalities are described as Human Phenotype Ontology (HPO) terms.
  related_to:
  - B2AI_STANDARD:468
  requires_registration: false
  url: https://lirical.readthedocs.io/
- id: B2AI_STANDARD:785
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - cloudservice
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Linode
  is_open: false
  name: Linode
  purpose_detail: Linode (now part of Akamai Connected Cloud) is a developer-focused
    cloud computing platform emphasizing simplicity, predictable pricing, and high
    performance for compute, storage, and networking services across a globally distributed
    infrastructure. Core compute offerings include Essential Compute instances (Shared
    CPU, Dedicated CPU, High Memory, and Premium CPU), GPU instances for parallel
    processing workloads including machine learning and scientific computing, and
    Accelerated Compute for specialized workloads. Container orchestration is provided
    through Linode Kubernetes Engine (LKE) for managed Kubernetes clusters, and App
    Platform for rapid cloud-native application deployment with automated infrastructure
    management. Storage solutions encompass Block Storage for scalable persistent
    volumes, Object Storage with S3-compatible API for unstructured data, and automated
    Backups for data protection. Managed Databases support MySQL and PostgreSQL with
    automated maintenance, backups, and high availability configurations. Networking
    features include NodeBalancers for load distribution, Cloud Firewall for security,
    DNS Manager, Private Networking (VLAN), and DDoS protection. Linode differentiates
    itself through transparent flat pricing bundling CPU, transfer, storage, and
    RAM without hidden egress fees or complex tier structures, making it particularly
    attractive for budget-conscious researchers and startups. The platform provides
    comprehensive API access, CLI tools, Terraform provider, and extensive documentation
    for programmatic infrastructure management. Following acquisition by Akamai in
    2022, Linode benefits from integration with Akamai's global CDN network and security
    capabilities while maintaining its developer-friendly approach and competitive
    pricing. Linode's global presence includes data centers across North America,
    Europe, Asia-Pacific, and emerging markets, providing edge computing capabilities
    and reduced latency for distributed applications. The platform offers free DDoS
    protection, 24/7/365 support, and a Cloud Computing Foundations Certification
    program for developer education. While more streamlined than hyperscale competitors,
    Linode provides essential cloud services suitable for web applications, data pipelines,
    containerized workloads, and computational research without vendor lock-in through
    standard open-source technologies.
  requires_registration: true
  url: https://www.linode.com/
- id: B2AI_STANDARD:786
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - scrnaseqanalysis
  concerns_data_topic:
  - B2AI_TOPIC:34
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Markov Affinity-based Graph Imputation of Cells
  formal_specification: https://github.com/KrishnaswamyLab/MAGIC
  is_open: true
  name: MAGIC
  publication: doi:10.1016/j.cell.2018.05.061
  purpose_detail: A method for imputing missing values in scRNA-seq data.
  requires_registration: false
  url: https://www.krishnaswamylab.org/projects/magic
- id: B2AI_STANDARD:787
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Marquez
  formal_specification: https://github.com/MarquezProject/marquez
  is_open: true
  name: Marquez
  purpose_detail: Marquez is an open source metadata service for the collection, aggregation,
    and visualization of a data ecosystem's metadata. It maintains the provenance
    of how datasets are consumed and produced, provides global visibility into job
    runtime and frequency of dataset access, centralization of dataset lifecycle management,
    and much more. Marquez was released and open sourced by WeWork.
  requires_registration: false
  url: https://lfaidata.foundation/projects/marquez/
- id: B2AI_STANDARD:788
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - machinelearningframework
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Medical Open Network for AI
  formal_specification: https://github.com/Project-MONAI/MONAI
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:33
  is_open: true
  name: MONAI
  purpose_detail: MONAI (Medical Open Network for AI) is a PyTorch-based, open-source
    framework specifically designed for deep learning in healthcare imaging, providing
    domain-optimized data loaders, transforms, network architectures, loss functions,
    and training utilities tailored to the unique challenges of medical image analysis
    including 3D volumetric data, high-resolution whole slide images, multi-modal
    imaging, and clinical deployment constraints. Developed through collaboration
    between NVIDIA, King's College London, and a global consortium of academic medical
    centers and industry partners, MONAI addresses the gap between general-purpose
    deep learning frameworks and the specialized requirements of medical imaging AI
    by offering production-ready implementations of state-of-the-art architectures
    (U-Net, SegResNet, DenseNet, EfficientNet, Vision Transformers adapted for medical
    imaging), pretrained model weights from large-scale medical imaging datasets,
    and composable building blocks that accelerate development while maintaining flexibility
    for research innovation. The framework provides comprehensive data preprocessing
    pipelines with medical imaging-specific transforms including intensity normalization
    (z-score, percentile clipping, histogram matching), spatial operations (resampling,
    cropping, padding with orientation-aware handling for DICOM/NIfTI), augmentation
    strategies proven effective for medical data (elastic deformations, MixUp, CutMix,
    RandAugment), and handling of metadata from clinical imaging standards (DICOM
    headers, NIfTI affines). MONAI's data loading infrastructure optimizes memory
    and compute efficiency for large 3D medical volumes through smart caching, deterministic
    transforms for reproducibility, distributed data parallel training, and ThreadDataLoader/ThreadBuffer
    for CPU-bound preprocessing during GPU training. The framework includes task-specific
    modules for organ and lesion segmentation (including cascaded approaches, nnU-Net-style
    architectures), image registration (affine and deformable, learning-based and
    classical), anomaly detection, image synthesis and super-resolution, classification
    and detection, and federated learning for privacy-preserving multi-institutional
    collaboration via NVIDIA FLARE integration. MONAI Core provides the foundational
    library, while the MONAI ecosystem extends to MONAI Label (interactive annotation
    with active learning), MONAI Deploy (containerized deployment for clinical PACS
    integration), MONAI Bundle (shareable pretrained models and workflows with standardized
    configuration), and Auto3DSeg (automated neural architecture search and ensemble
    learning for segmentation). The framework supports integration with clinical workflows
    through DICOM/NIfTI I/O via pydicom and nibabel, deployment tools for FDA/CE
    mark regulatory pathways, model explainability modules (Grad-CAM, occlusion sensitivity),
    and quantitative evaluation metrics specific to medical imaging (Dice score, Hausdorff
    distance, surface distance). MONAI has become the de facto standard framework
    for academic medical imaging AI research with adoption by hundreds of institutions
    worldwide, forms the foundation for international challenges (Medical Segmentation
    Decathlon, BraTS), and powers production clinical AI systems for automated organ
    segmentation, tumor detection, treatment response assessment, and pathology slide
    analysis deployed in hospital PACS environments.
  related_to:
  - B2AI_STANDARD:816
  requires_registration: false
  url: https://monai.io/
  has_application:
  - id: B2AI_APP:72
    category: B2AI:Application
    name: Medical Imaging Deep Learning Framework and Workflows
    description: MONAI (Medical Open Network for AI) is a specialized PyTorch-based
      framework extensively used for developing deep learning models on 3D medical
      imaging data across radiology, pathology, and microscopy. Researchers and clinical
      AI developers leverage MONAI's domain-specific transforms (intensity normalization,
      resampling, augmentation), pretrained models for medical imaging tasks, and
      optimized data loaders for large 3D volumes to accelerate development of segmentation,
      classification, and detection models. The framework provides standardized workflows
      for common medical imaging AI tasks, federated learning capabilities for multi-institutional
      collaboration, and Auto3DSeg for automated model development. MONAI's integration
      with clinical imaging standards and deployment tools makes it a bridge between
      research and clinical implementation.
    used_in_bridge2ai: false
  - id: B2AI_APP:179
    category: B2AI:Application
    name: Generative Medical Image Synthesis with MONAI Generative Models
    description: 'MONAI Generative Models extends the framework with latent diffusion
      and GAN-based architectures to synthesize multi-modality medical images for
      data augmentation, privacy-preserving dataset generation, and methodological
      research. The package provides implementations of "Latent Diffusion Model comprising
      an autoencoder (compression) and a diffusion generative model" with perceptual,
      spectral, and patch-based adversarial losses, plus a "2.5D perceptual-loss strategy
      for 3D" volumes. Concrete applications span chest X-ray synthesis on MIMIC-CXR
      (96,161 images, FID 8.8325, MS-SSIM 0.4276), mammography on CSAW-M (9,523 images),
      retinal OCT (84,483 images), 2D brain MRI slices (UK Biobank, 360,525 slices),
      and 3D T1 brain volumes (UK Biobank, 41,162 volumes, FID 0.0051, MS-SSIM 0.9217).
      MONAI components include pretrained networks (RadImageNet for 2D, MedicalNet
      for 3D) and standardized metrics (MS-SSIM for autoencoder reconstruction, FID
      and MS-SSIM for sample fidelity/diversity, CLIP Score for conditioning). The
      package releases "experiment code publicly available" with pretrained models
      for reproducible benchmarking, enabling researchers to generate synthetic medical
      images that preserve anatomical realism while avoiding patient identifiers,
      supporting data augmentation for rare diseases and methodological studies of
      generative model behavior in medical imaging domains.'
    references:
    - https://doi.org/10.48550/arxiv.2307.15208
    used_in_bridge2ai: false
  - id: B2AI_APP:180
    category: B2AI:Application
    name: Clinical Prostate MRI Segmentation Deployment with MONAI Deploy
    description: 'MONAI Deploy Express enables production clinical deployment of AI
      segmentation and lesion detection pipelines integrated with PACS and biopsy
      planning workflows. A research-based clinical deployment at a major medical
      center used MONAI Deploy to operationalize a prostate mpMRI pipeline running
      on "a dedicated inference server" that generates "binary lesion prediction and
      probability maps" presented in biopsy preparation platforms where clinicians
      approve or reject AI-identified targets. Prospective evaluation across Phase
      1 (58 patients) and Phase 2 (35 patients, 72 radiologist-accepted lesions with
      23 AI+/Rad- lesions) demonstrated that "concordant AI+/Rad+ lesions showed highest
      cancer detection rate (CDR)82.1% for PCa and 42.9% for csPCa" with added value
      from AI-only ROIs showing "47.8% CDR when AI-proposed ROIs were accepted" and
      clinically significant cancer found in 4 of 23 AI+/Rad- lesions. The deployment
      achieved "results similar to original model development" with "excellent concordance
      for PIRADS 5" while maintaining clinical integration through user approval workflows.
      This exemplifies MONAI Deploy''s capability to package trained models into containerized
      applications that integrate with radiology information systems, enable clinician
      oversight of AI predictions, and deliver actionable outputs within clinical
      decision-making timelines for interventional procedures.'
    references:
    - https://doi.org/10.1007/s00261-025-05014-7
    used_in_bridge2ai: false
  - id: B2AI_APP:181
    category: B2AI:Application
    name: Interactive Annotation and Active Learning with MONAI Label
    description: 'MONAI Label provides interactive annotation workflows with active
      learning that accelerate dataset curation and iteratively improve model performance
      through human-in-the-loop refinement. At Karolinska University Hospital within
      the MAIA platform, a CT vertebral metastasis segmentation project began with
      30 labeled subjects and used MONAI Label to enable "radiologists to load new
      batches of unlabeled data, visualize model predictions, and perform corrective
      annotations," scaling to 150 cases through iterative labeling cycles. Model
      performance improved dramatically: "segmentation model sensitivity improved
      from 55% (30 cases) to 82% (150 cases)" through MONAI Label-assisted expansion.
      The workflow was "containerized, enabling on-demand instantiation for prediction
      on new patient data" with end-to-end Kubeflow orchestration managing "DICOM
      loading and NIfTI conversion through preprocessing, segmentation, optional correction,
      and postprocessing, to the final DICOM output." MONAI Label integrates with
      clinical viewers (OHIF, 3D Slicer, XNAT) where radiologists review and correct
      predictions, with "updated annotations returned to the server" for model retraining.
      This active learning approach addresses the annotation bottleneck in medical
      AI by prioritizing uncertain predictions for expert review, progressively refining
      models with minimal labeling effort while maintaining clinical DICOM format
      compatibility throughout the annotation-training-deployment lifecycle.'
    references:
    - https://doi.org/10.1038/s44387-025-00042-6
    - https://openarchive.ki.se/articles/thesis/Design_and_integration_of_AI_solutions_in_oncology_and_healthcare_infrastructures_bridging_the_gap_between_AI_innovation_and_clinical_practice/30466691/1/files/59123939.pdf
    used_in_bridge2ai: false
  - id: B2AI_APP:182
    category: B2AI:Application
    name: Clinical Chest X-ray Device Detection with MONAI Frameworks
    description: 'MONAI-based frameworks operationalize clinical AI pipelines for
      safety-critical applications including detection and identification of MRI-hazardous
      implanted devices on chest radiographs. At Mayo Clinic, a production system
      uses "MONAI-based frameworks to operationalize the pipeline" with "two cascading
      modelsdetection with a Faster R-CNN followed by identification with a multi-class
      CNN" to identify leadless electronic devices (e.g., Bravo esophageal reflux
      pH capsules) that pose MRI safety risks. The system processes "frontal chest
      x-rays at Mayo Clinic" with continuous learning and reports exemplar performance
      such as devices "correctly detected and identified (with 10/10 certainty)."
      Clinical integration includes "integration into a robust viewer with back-end
      data recording and adjudication by radiologists," enabling continuous quality
      monitoring and iterative improvement from real-world deployment feedback. The
      application demonstrates MONAI''s role in building clinical decision support
      systems for patient safety, where automated detection flags potentially hazardous
      conditions for radiologist review before MRI procedures, combining computer
      vision with clinical workflow integration and human oversight to prevent adverse
      events while maintaining audit trails for quality assurance and model refinement.'
    references:
    - https://doi.org/10.2196/55833
    used_in_bridge2ai: false
  - id: B2AI_APP:183
    category: B2AI:Application
    name: MONAI Application Packages for Standards-Compliant Clinical Deployment
    description: 'MONAI Deploy App SDK structures clinical inference as MONAI Application
      Packages (MAPs) that are "DAGs of Operators" providing standardized components
      for DICOM-native medical AI deployment with EMR integration. Operators include
      StudyLoaderOperator, SeriesSelectorOperator, SeriesToVolumeOperator for DICOM
      parsing, preprocessing operators (Gaussian filter, threshold), and model prediction
      operators that "parse DICOM into numpy arrays for inference" with models retrievable
      from "MONAI Model Zoo." Clinical outputs are formatted as "DICOM Segmentation
      or DICOM Structured Reports" that integrate with PACS systems, with deep integration
      via "DICOM Router (Laurel Bridge)" enabling "routing rules to normalize and
      filter studies (e.g., thin vs thick slices)" and "parallel routing to reduce
      latency" for real-time clinical throughput. Advanced integration includes generation
      of "HL7 v2 messages from DICOM SR (via custom C# on the router) for EMR/worklist
      integration," enabling AI predictions to populate electronic health records
      and trigger clinical workflows. This architecture addresses the gap between
      research prototypes and clinical deployment by providing reusable, standards-compliant
      components that handle medical imaging complexities (modality-specific preprocessing,
      series selection, coordinate systems) while maintaining interoperability with
      radiology IT infrastructure through DICOM/HL7 messaging standards and supporting
      regulatory requirements for medical device software through documented, version-controlled
      deployment packages.'
    references:
    - https://doi.org/10.48550/arxiv.2311.10840
    used_in_bridge2ai: false
- id: B2AI_STANDARD:789
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Metacat
  formal_specification: https://github.com/Netflix/metacat
  has_relevant_organization:
  - B2AI_ORG:65
  is_open: true
  name: Metacat
  purpose_detail: Metacat is a unified metadata exploration API service. You can explore
    Hive, RDS, Teradata, Redshift, S3 and Cassandra. Metacat provides you information
    about what data you have, where it resides and how to process it.
  requires_registration: false
  url: https://github.com/Netflix/metacat
- id: B2AI_STANDARD:790
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - cloudservice
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Microsoft Azure
  has_relevant_organization:
  - B2AI_ORG:56
  is_open: false
  name: Azure
  purpose_detail: Microsoft Azure is a comprehensive enterprise-grade cloud computing
    platform offering 200+ services spanning compute, storage, databases, networking,
    AI/machine learning, analytics, IoT, and developer tools with deep integration
    into Microsoft's enterprise ecosystem. Core compute services include Azure Virtual
    Machines for Windows and Linux workloads, Azure App Service for web application
    hosting, Azure Kubernetes Service (AKS) for container orchestration, Azure Container
    Instances for serverless containers, and Azure Functions for event-driven serverless
    computing. For AI and machine learning, Azure provides Azure AI Studio (including
    Microsoft Foundry for AI agent development), Azure Machine Learning for end-to-end
    ML workflows, Azure Cognitive Services for pre-trained AI models (vision, speech,
    language), and Azure OpenAI Service for accessing GPT, Codex, and DALL-E models.
    Data and analytics capabilities include Azure Synapse Analytics for unified data
    warehousing and big data analytics, Azure Databricks for Apache Spark-based analytics,
    Azure Data Lake for scalable data storage, and Microsoft Fabric for unified data
    platform integration. Database services encompass Azure SQL Database for managed
    relational databases, Azure Cosmos DB for globally distributed NoSQL, Azure Database
    for PostgreSQL/MySQL/MariaDB, and Azure DocumentDB for MongoDB-compatible document
    storage. Healthcare and life sciences benefit from Azure Health Data Services
    for FHIR-based health data management, Azure Genomics for scalable genomics pipeline
    execution, Azure Healthcare APIs, and compliance with HIPAA, HITRUST, GxP, and
    FDA 21 CFR Part 11 regulations. Researchers leverage Azure Machine Learning for
    model training with GPU/FPGA acceleration, automated ML (AutoML) capabilities,
    MLOps for model lifecycle management, and integration with popular frameworks
    like TensorFlow, PyTorch, and scikit-learn. Azure's hybrid cloud capabilities
    through Azure Arc enable consistent management across on-premises, multi-cloud,
    and edge environments, crucial for institutions with existing infrastructure investments.
    Enterprise integration with Active Directory, Microsoft 365, Windows Server, SQL
    Server, and Visual Studio provides seamless authentication, collaboration, and
    development workflows. Azure's global infrastructure spans 60+ regions worldwide
    with data residency and compliance options for international regulations. Security
    features include Azure Security Center, Azure Sentinel for SIEM, Azure Active
    Directory for identity management, encryption at rest and in transit, and extensive
    compliance certifications across healthcare, government, and financial sectors.
  requires_registration: true
  url: https://azure.microsoft.com/
- id: B2AI_STANDARD:791
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - cloudservice
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MinIO
  formal_specification: https://github.com/minio/
  is_open: true
  name: MinIO
  purpose_detail: MinIO is a high-performance object storage system that implements
    the Amazon S3 API for cloud-native environments. The system is designed as software-defined
    storage and provides S3-compatible operations with reported latency under 10ms
    and high throughput capabilities. MinIO is designed to run on Kubernetes and can
    be deployed across public cloud platforms, private cloud infrastructure, and edge
    environments. The architecture uses a distributed design that supports scaling
    beyond traditional storage system limits, with deployment capabilities from single
    servers to exabyte-scale installations using a single namespace. The platform
    includes automated data management features such as self-healing, load balancing,
    and data protection with erasure coding. Security features include encryption
    at rest and in transit, identity and access management integration, and policy-based
    access controls. MinIO is used in AI and machine learning workflows, providing
    storage integration with frameworks like PyTorch and data lakehouse technologies
    including Apache Iceberg. The system supports data lakehouse analytics engines
    such as Apache Spark and Trino for structured and unstructured data processing.
    Client libraries are available for multiple programming languages including Go,
    Python, Java, .NET, Rust, and JavaScript. Associated tools include mc (command-line
    client for object operations), DirectPV (a Kubernetes CSI driver), and a Kubernetes
    operator for cluster deployment and management. The source code is released under
    the GNU AGPLv3 license. The project reports over 2 billion downloads and maintains
    an active development community with more than 50,000 stars on GitHub.
  requires_registration: false
  url: https://min.io/
- id: B2AI_STANDARD:792
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ML Metadata
  formal_specification: https://github.com/google/ml-metadata
  has_relevant_organization:
  - B2AI_ORG:37
  is_open: true
  name: MLMD
  purpose_detail: ML Metadata (MLMD) is a library for recording and retrieving metadata
    associated with ML developer and data scientist workflows.
  requires_registration: false
  url: https://www.tensorflow.org/tfx/guide/mlmd
- id: B2AI_STANDARD:793
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MLflow platform
  formal_specification: https://github.com/mlflow/mlflow/
  is_open: true
  name: MLflow
  purpose_detail: MLflow is an open source platform to manage the ML lifecycle, including
    experimentation, reproducibility, deployment, and a central model registry
  requires_registration: false
  url: https://mlflow.org/
- id: B2AI_STANDARD:794
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - machinelearningframework
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MLPro framework
  formal_specification: https://github.com/fhswf/MLPro
  is_open: true
  name: MLPro
  publication: doi:10.1016/j.simpa.2022.100421
  purpose_detail: 'MLPro (Machine Learning Professional) is a middleware framework
    for standardized machine learning development in Python. The framework provides
    an object-oriented architecture that enables the integration and standardization
    of multiple machine learning paradigms through reusable process models and templates.
    MLPro consists of several sub-frameworks organized as modules: MLPro-BF (Basic
    Functions) provides cross-sectional infrastructure including mathematics, data
    management, plotting capabilities, and logging; MLPro-SL supports supervised learning
    workflows; MLPro-RL implements reinforcement learning processes; MLPro-GT covers
    game theory applications; and MLPro-OA focuses on online adaptive machine learning.
    The architecture uses standardized interfaces that allow ML models to be embedded
    into training pipelines and operational workflows in a modular fashion. The framework
    integrates with external machine learning libraries including River (for online
    learning), scikit-learn (for supervised learning), and OpenML (for dataset and
    experiment management), providing wrappers that standardize their usage within
    MLPro processes. Development follows object-oriented design principles with test-driven
    development for quality assurance and continuous integration/deployment (CI/CD)
    practices. The framework includes an extension hub for third-party integrations,
    comprehensive documentation with API references, and an example pool demonstrating
    various use cases. MLPro supports hybrid ML applications that combine multiple
    learning paradigms and real-time adaptive systems. The project is developed and
    maintained by the Group for Automation Technology and Learning Systems at South
    Westphalia University of Applied Sciences in Germany. The source code is released
    under the Apache 2.0 license and is available through PyPI for installation.'
  requires_registration: false
  url: https://mlpro.readthedocs.io/
  has_application:
  - id: B2AI_APP:73
    category: B2AI:Application
    name: Reinforcement Learning Framework for Adaptive Healthcare Systems
    description: MLPro is used in biomedical AI for developing reinforcement learning
      agents that learn optimal treatment strategies, adaptive monitoring protocols,
      and personalized intervention policies through interaction with healthcare environments.
      Researchers leverage MLPro's modular framework to implement and compare different
      RL algorithms for applications like dynamic treatment regimen optimization,
      adaptive clinical trial design, and automated ICU management. The platform supports
      simulation-based learning where agents train on synthetic patients before clinical
      deployment, multi-agent systems for coordinated care, and safe exploration strategies
      essential for healthcare applications. MLPro enables systematic evaluation of
      RL approaches for sequential decision-making problems in medicine where actions
      have long-term consequences.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:795
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - scrnaseqanalysis
  concerns_data_topic:
  - B2AI_TOPIC:23
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MoClust
  is_open: true
  name: MoClust
  publication: doi:10.1093/bioinformatics/btac736
  purpose_detail: A novel joint clustering framework that can be applied to several
    types of single-cell multi-omics data. A selective automatic doublet detection
    module that can identify and filter out doublets is introduced in the pretraining
    stage to improve data quality. Omics-specific autoencoders are introduced to characterize
    the multi-omics data.
  requires_registration: false
  url: https://zenodo.org/record/7306504
- id: B2AI_STANDARD:796
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - modelcards
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Model Card Toolkit
  formal_specification: https://github.com/tensorflow/model-card-toolkit
  has_relevant_organization:
  - B2AI_ORG:37
  is_open: true
  name: MCT
  publication: doi:10.48550/arXiv.1810.03993
  purpose_detail: The Model Card Toolkit (MCT) streamlines and automates generation
    of Model Cards, machine learning documents that provide context and transparency
    into a model's development and performance.
  requires_registration: false
  url: https://github.com/tensorflow/model-card-toolkit
- id: B2AI_STANDARD:797
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MongoDB
  formal_specification: https://github.com/mongodb/mongo
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:13
  - B2AI_SUBSTRATE:22
  - B2AI_SUBSTRATE:9
  is_open: true
  name: MongoDB
  purpose_detail: A non-relational document database that provides support for JSON-like
    storage.
  requires_registration: false
  url: https://www.mongodb.com/
- id: B2AI_STANDARD:798
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Monocle 2
  formal_specification: https://github.com/cole-trapnell-lab/monocle-release
  is_open: true
  name: Monocle2
  publication: doi:10.1038/nmeth.4402
  purpose_detail: An algorithm that uses reversed graph embedding to describe multiple
    fate decisions in a fully unsupervised manner.
  requires_registration: false
  url: https://github.com/cole-trapnell-lab/monocle-release
- id: B2AI_STANDARD:799
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - multimodal
  concerns_data_topic:
  - B2AI_TOPIC:23
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Multi-Omic Integration by Machine Learning
  formal_specification: https://github.com/jessegmeyerlab/MIMaL
  is_open: true
  name: MIMaL
  publication: doi:10.1093/bioinformatics/btac631
  purpose_detail: MIMaL is a new method for integrating multiomic data using SHAP
    model explanations.
  requires_registration: false
  url: https://mimal.app/
- id: B2AI_STANDARD:800
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - multimodal
  concerns_data_topic:
  - B2AI_TOPIC:26
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Multi-Scale Integrated Cell pipeline
  formal_specification: https://github.com/idekerlab/MuSIC
  has_relevant_organization:
  - B2AI_ORG:116
  is_open: true
  name: MuSIC
  publication: doi:10.1038/s41586-021-04115-9
  purpose_detail: MuSIC is a hierarchical map of human cell architecture created from
    integrating immunofluorescence images in the Human Protein Atlas with affinity
    purification experiments from the BioPlex resource. Integration involves configuring
    each approach to produce a general measure of protein distance, then calibrating
    the two measures using machine learning.
  requires_registration: false
  url: https://nrnb.org/music/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:74
    category: B2AI:Application
    name: Multi-Scale Cell Type Annotation and Classification
    description: MuSIC (Multi-Scale Integrated Cell classification) is used in AI
      applications for automated cell type annotation in single-cell RNA-seq data
      by leveraging reference datasets and hierarchical classification strategies.
      The tool employs machine learning algorithms that integrate multiple sources
      of evidence including marker genes, reference atlases, and cross-dataset mapping
      to assign cell type labels with confidence scores. AI systems build upon MuSIC's
      probabilistic framework to develop more sophisticated deep learning models for
      cell state identification, rare cell type discovery, and cross-species cell
      type mapping. The multi-scale approach enables AI models to make predictions
      at varying levels of granularity, from broad cell classes to fine-grained subtypes,
      which is essential for biological interpretation and downstream analysis.
    used_in_bridge2ai: false
  - id: B2AI_APP:184
    category: B2AI:Application
    name: Multimodal Representation Learning and Contrastive Co-Embedding
    description: 'MuSIC applies deep learning-based representation learning to fuse
      immunofluorescence imaging and protein-protein interaction (PPI) network data
      for integrated mapping of subcellular architecture. The pipeline uses "a deep
      convolutional neural network" to embed each protein''s IF images as a 1024-dimension
      feature vector and "node2vec embeds AP-MS interaction neighborhoods as a second
      1024-dimension vector," computing protein-protein similarities as cosine distances
      in these high-dimensional embeddings. The CM4AI extension employs "contrastive
      deep learning to integrate those embeddings into a joint co-embedding per protein"
      and applies "graph representation learning (PPI networks processed with node2vec)"
      plus "image representation learning (IF images processed with a Human Protein
      Atlas deep learning model)" to produce integrated representations. MuSIC calibrates
      and combines the two distance measures "using machine learning, sampling known
      subcellular components of varying physical size to relate embedding distances
      to spatial scale," enabling cross-modal integration where "pairs close by one
      modality are enriched for closeness by the other." Downstream unsupervised ML
      tasks include "community detection and hierarchy creation from all-by-all similarities
      in co-embedding space" to produce hierarchical cell maps, with MuSIC 1.0 resolving
      69 subcellular systems. This demonstrates end-to-end application of deep learning
      for image feature extraction, graph embedding for network topology, metric learning
      for cross-modal calibration, and unsupervised clustering for biological system
      discovery.'
    references:
    - https://doi.org/10.1101/2020.06.21.163709
    - https://doi.org/10.1101/2024.05.21.589311
    used_in_bridge2ai: false
  - id: B2AI_APP:185
    category: B2AI:Application
    name: AI-Ready Dataset Packaging with FAIRSCAPE and RO-Crate
    description: 'CM4AI operationalizes MuSIC as an AI-ready data production pipeline
      that packages outputs with comprehensive provenance and metadata for reproducible
      machine learning benchmarking. MuSIC processes input datasets (Human Protein
      Atlas microscopy, BioPlex AP-MS interactions) "to RO-Crate outputs" with "FAIRSCAPE
      provenance graphs, persistent identifiers, and JSON-Schema characterization"
      that establish "FAIR metadata and persistent identifiers; computing machine-readable
      provenance graphs for inputs, computations, software, and outputs; and characterizing/validating
      datasets with JSON-Schema mini-data-dictionaries." The standardized outputs
      are "shared via NDEx/HiView/Cytoscape for downstream ML use and benchmarking,"
      enabling researchers to access hierarchical cell maps with "communities derived
      from embeddings that integrate protein image and affinity purification data"
      through interactive visualization that supports "zooming into nested communities,
      showing underlying interaction networks and providing lists of proteins per
      community." CM4AI frames AI-readiness around explicit criteria: "FAIR, Provenanced,
      Explainable, Ethical," emphasizing "interoperability, provenance metadata, statistical
      characterization, and ethical documentation." Released data packages include
      "software links, input datasets, dataset schemas, and deep provenance graphs"
      under CC-BY-NC-SA 4.0 licensing with Data Access Committee oversight. This standardization
      infrastructure "targets reproducible training/validation and structured benchmarking
      of multimodal cell-architecture models" and provides production-ready datasets
      for foundation model pretraining, graph neural network development, and multimodal
      integration model evaluation.'
    references:
    - https://doi.org/10.1101/2024.05.21.589311
    used_in_bridge2ai: false
  - id: B2AI_APP:186
    category: B2AI:Application
    name: Model Interpretability via Visible Neural Networks and LLM Annotation
    description: 'MuSIC provides interpretability mechanisms for AI models through
      biological grounding, structural validation, and natural language annotation
      of learned protein assemblies. The pipeline "align[s] cell-map communities to
      GO/Reactome for biological validation" and "use[s] an LLM to name protein assemblies
      with confidence scores, supporting interpretability and human-readable labels
      for ML evaluation," enabling researchers to understand what biological processes
      and structures correspond to learned representations. CM4AI "ranks communities
      by available structural data (PDB, AlphaFoldDB, crosslinking) and performs integrative
      structure modeling, which can provide structural restraints and validation data
      for model training and interpretation." The framework explicitly supports "visible
      machine learning" or "visible neural networks (VNNs)" that "link protein assemblies
      to cell-level phenotype predictions, supporting interpretability" by mapping
      learned neural network components directly to interpretable biological modules
      from MuSIC hierarchies. This approach enables "human-interpretable model inspection
      and hypothesis generation" where each layer or module of a neural architecture
      corresponds to a specific protein complex or cellular system with known functional
      annotations and structural evidence. The combination of ontology alignment,
      structural evidence ranking, and LLM-based natural language naming creates a
      multi-layered interpretability strategy that makes black-box models transparent
      to biologists, facilitates validation against orthogonal data sources (crystallographic
      structures, pathway databases), and supports explainability requirements for
      clinical or diagnostic AI applications.'
    references:
    - https://doi.org/10.1101/2024.05.21.589311
    used_in_bridge2ai: true
  - id: B2AI_APP:187
    category: B2AI:Application
    name: Foundation Models and Graph Neural Network Training on Cell Architecture
      Maps
    description: 'MuSIC outputs serve as training corpora and benchmarks for foundation
      models, graph neural networks, and multimodal generative models that require
      multi-scale, multi-modal, perturbation-aware cell architecture data. While not
      MuSIC-specific, AI guidance highlights that "MuSIC-style multi-modal, multi-scale,
      perturbation-aware datasets enable foundation-model pretraining, graph neural
      networks on protein assemblies, multimodal generative/integration models, and
      causal/perturbation-aware evaluation." MuSIC provides hierarchical protein assembly
      graphs with rich node features (image embeddings, interaction profiles, localization
      patterns) and edge attributes (co-membership scores, physical interactions)
      suitable for graph convolutional networks and attention-based architectures
      that learn from network topology. The availability of "co-measured dynamic/perturbation
      cohorts" from single-cell CRISPR screens integrated with MuSIC maps enables
      "action-response analyses (causal or intervention-aware training and validation)"
      where models can learn how genetic perturbations propagate through cellular
      systems. Foundation models (scGPT, transformers, Stofm) can be pretrained on
      MuSIC-derived representations to learn generalizable features of cellular organization
      transferable across cell types and experimental conditions. Multimodal integration
      models (totalVI, Multivi) benefit from MuSIC''s calibrated fusion of imaging
      and interaction modalities as reference implementations for cross-modal alignment
      strategies. The standardized, FAIR-compliant packaging of MuSIC outputs with
      "platform-aware alignment and leakage-aware partitions" supports "fair train/validation
      splits and generalization testing," enabling reproducible benchmarking of model
      architectures and rigorous evaluation of generalization to held-out cell lines,
      conditions, or perturbations.'
    references:
    - https://doi.org/10.1101/2024.05.21.589311
    - https://doi.org/10.48550/arxiv.2510.12498
    used_in_bridge2ai: false
- id: B2AI_STANDARD:801
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MySQL
  formal_specification: https://github.com/mysql/mysql-server
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:23
  - B2AI_SUBSTRATE:37
  - B2AI_SUBSTRATE:9
  is_open: true
  name: MySQL
  purpose_detail: MySQL is an open-source relational database management system (RDBMS)
    developed and maintained by Oracle Corporation that implements the SQL (Structured
    Query Language) standard for storing, organizing, and retrieving structured data
    in tables with defined schemas, supporting ACID (Atomicity, Consistency, Isolation,
    Durability) transactions for data integrity and concurrent access control essential
    to enterprise applications and web services. As one of the world's most popular
    databases, MySQL powers millions of websites, applications, and data-driven systems
    ranging from small personal projects to massive-scale platforms including Facebook,
    Twitter, YouTube, Wikipedia, and major financial institutions requiring reliable,
    high-performance data storage. The database engine supports multiple storage engines
    (InnoDB for transactional workloads with foreign key constraints and crash recovery,
    MyISAM for read-heavy scenarios, Memory for temporary in-memory tables) allowing
    administrators to optimize performance characteristics per table based on access
    patterns and consistency requirements. MySQL provides comprehensive indexing strategies
    (B-tree, hash, full-text, spatial indexes for GIS data), query optimization with
    EXPLAIN plan analysis, replication topologies (primary-replica, multi-source,
    group replication for high availability), partitioning for horizontal scaling
    of large tables, and connection pooling for efficient resource utilization under
    concurrent load. The system integrates with virtually all programming languages
    through native drivers and ODBC/JDBC interfaces (Python MySQLdb/PyMySQL, PHP
    mysqli, Java JDBC, Node.js mysql2, R RMySQL), supporting both embedded applications
    and client-server architectures across operating systems (Linux, Windows, macOS,
    BSD). For data science and analytics, MySQL serves as a backend for data warehousing
    solutions, ETL pipelines, reporting dashboards (Tableau, Power BI, Looker), and
    machine learning feature stores where structured transactional or reference data
    must be joined with model training datasets or queried during inference for entity
    lookups and business rule application. MySQL's role in AI/ML workflows includes
    storing labeled training data with annotations and metadata, maintaining experiment
    tracking databases for hyperparameter tuning and model versioning (MLflow, Weights
    & Biases backends), serving feature values in real-time inference pipelines through
    low-latency point queries, and hosting patient registries, clinical trial data,
    EHR-derived tables, and biomedical ontology mappings in translational research
    platforms. The database supports stored procedures and triggers for complex business
    logic, views for data abstraction and security through row-level access control,
    JSON data type for semi-structured data alongside relational schemas, and full-text
    search capabilities for unstructured text fields. MySQL's ecosystem includes management
    tools (MySQL Workbench for visual design and administration, phpMyAdmin for web-based
    management), monitoring solutions (MySQL Enterprise Monitor, Prometheus exporters),
    and cloud-managed services (Amazon RDS for MySQL, Google Cloud SQL, Azure Database
    for MySQL) that provide automated backups, patch management, and scaling without
    infrastructure overhead. The database's mature security model offers authentication
    mechanisms (native passwords, LDAP/Kerberos integration, SSL/TLS encrypted connections),
    privilege management at database/table/column granularity, and audit logging for
    compliance with healthcare (HIPAA) and financial regulations. MySQL continues
    active development with regular releases introducing performance improvements
    (query optimizer enhancements, parallel replication), modern features (CTEs, window
    functions, recursive queries), and compatibility with cloud-native architectures
    (Kubernetes operators, containerized deployments), maintaining relevance as a
    foundational data persistence layer for contemporary data-intensive applications
    requiring structured data storage, referential integrity, and transactional guarantees.
  requires_registration: false
  url: https://www.mysql.com/
- id: B2AI_STANDARD:802
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - graphdataplatform
  concerns_data_topic:
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Neo4j Graph Data Platform
  formal_specification: https://github.com/neo4j/neo4j
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:14
  - B2AI_SUBSTRATE:15
  - B2AI_SUBSTRATE:25
  - B2AI_SUBSTRATE:9
  is_open: true
  name: Neo4j
  purpose_detail: "Neo4j is a native graph database platform implementing property
    graph model with ACID-compliant transactions, designed for storing and querying
    highly connected data through nodes, relationships, and properties, enabling efficient
    traversal of complex relationship patterns at scale. Developed by Neo4j, Inc.
    and available in both open-source Community Edition and commercial Enterprise
    Edition, Neo4j uses the declarative Cypher query language allowing pattern-matching
    queries that naturally express graph relationships (e.g., `MATCH (person:Person)-[:FRIENDS_WITH]->(friend)
    RETURN person, friend`) without complex join operations required in relational
    databases. The architecture employs index-free adjacency where each node directly
    references its adjacent nodes enabling constant-time traversals regardless of
    graph size, native graph storage optimized for relationship-heavy queries, and
    clustered deployment supporting high availability, horizontal scaling, and causal
    consistency across distributed systems. Key features include rich graph algorithms
    library (Graph Data Science) implementing PageRank, community detection, shortest
    path, centrality measures, and similarity algorithms; APOC (Awesome Procedures
    on Cypher) extending functionality with graph refactoring, data integration, and
    advanced algorithms; support for temporal queries and multiple graph projections;
    and integration with analytics tools (Apache Spark, Python data science stack)
    and machine learning frameworks. Biomedical and AI applications span knowledge
    graphs integrating biomedical ontologies, drug databases, genomic data, and literature
    for drug repurposing and target discovery; clinical decision support modeling
    patient-symptom-disease-treatment relationships for diagnosis recommendation;
    biological network analysis representing protein-protein interactions, metabolic
    pathways, gene regulatory networks for systems biology research; healthcare interoperability
    mapping relationships between FHIR resources, HL7 messages, and clinical terminologies;
    and ML feature engineering extracting graph embeddings, network features, and
    relationship patterns as input for predictive models. Neo4j enables real-time
    recommendation systems, fraud detection networks, identity and access management,
    supply chain optimization, and social network analysis, essential for data scientists
    working with connected data, bioinformaticians building biological knowledge bases,
    clinical informaticians developing patient care pathways, and ML engineers requiring
    graph-structured features."
  requires_registration: false
  url: https://neo4j.com/
- id: B2AI_STANDARD:803
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:16
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: NETME
  is_open: true
  name: NETME
  publication: doi:10.1007/S41109-021-00435-X
  purpose_detail: Starting from a set of fulltext obtained from PubMed, through an
    easy-to-use web interface, interactively extracts a group of biological elements
    stored into a selected list of ontological databases and then synthesizes a network
    with inferred relations among such elements.
  requires_registration: false
  url: https://netme.click/#/
- id: B2AI_STANDARD:804
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - scrnaseqanalysis
  concerns_data_topic:
  - B2AI_TOPIC:34
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: NetSeekR network analysis R package
  formal_specification: https://github.com/igbb-popescu-lab/NetSeekR
  is_open: true
  name: NetSeekR
  publication: doi:10.1186/S12859-021-04554-1
  purpose_detail: NetSeekR is an R package that implements a network analysis pipeline
    specifically designed for RNA-Seq time series data, enabling researchers to identify
    dynamic gene regulatory networks and temporal patterns of gene expression changes
    by integrating differential expression analysis, gene co-expression network construction,
    and time-dependent module detection to reveal biological processes activated or
    repressed across experimental time courses. Published in BMC Bioinformatics (2021),
    NetSeekR addresses the challenge that traditional differential expression analysis
    identifies individual genes changing over time but fails to capture coordinated
    regulation among functionally related genes and the temporal dynamics of regulatory
    network rewiring during biological processes such as development, disease progression,
    drug response, or environmental adaptation. The package provides an end-to-end
    workflow that begins with RNA-Seq count matrices and sample metadata, performs
    temporal differential expression analysis using methods like DESeq2 or edgeR to
    identify significantly changing genes, constructs weighted gene co-expression
    networks (WGCN) based on pairwise correlation of expression profiles across time
    points, and applies graph clustering algorithms to identify gene modules representing
    co-regulated functional units. NetSeekR implements time-aware network inference
    that accounts for temporal ordering of samples, allowing detection of early-response
    and late-response gene programs, identification of hub genes (highly connected
    regulators likely controlling module behavior), and tracking of how module membership
    and connectivity evolve across time series stages. The pipeline integrates functional
    enrichment analysis to annotate identified modules with Gene Ontology terms, pathway
    databases (KEGG, Reactome), and disease associations, providing biological context
    for temporal network patterns and enabling hypothesis generation about regulatory
    mechanisms. NetSeekR supports visualization of time series expression profiles,
    network topology with module-level summaries, and temporal trajectories of module
    activity scores, facilitating interpretation of complex multi-gene dynamics through
    dimensionality reduction from thousands of genes to dozens of interpretable modules.
    The package is designed for common RNA-Seq time series experimental designs including
    longitudinal sampling in developmental biology (embryogenesis, organogenesis),
    disease progression studies (cancer evolution, viral infection time courses, neurodegeneration),
    drug treatment response profiling (pharmacodynamics, resistance emergence), and
    environmental perturbation experiments (stress response, circadian rhythms). For
    machine learning and systems biology applications, NetSeekR-derived network modules
    serve as biologically informed feature sets for predictive modeling (using module
    eigengenes as features instead of individual genes), enable transfer learning
    across related time series datasets by mapping conserved modules, and provide
    mechanistic interpretations for AI models trained on temporal omics data by linking
    predictions to specific regulatory programs. The package outputs can be integrated
    with causal inference methods to distinguish correlation from regulatory causation,
    combined with ChIP-seq or ATAC-seq data to validate predicted transcription factor-target
    relationships, and used to prioritize candidate drug targets by identifying hub
    genes in disease-associated temporal modules. NetSeekR is implemented in R with
    dependencies on Bioconductor packages (DESeq2, edgeR, WGCNA), provides example
    datasets and vignettes for reproducible analysis, and outputs network structures
    compatible with Cytoscape and other network visualization tools, supporting integrative
    analysis of temporal transcriptomics in developmental biology, immunology, cancer
    research, and pharmacology where understanding time-dependent gene regulation
    is essential for mechanistic insight and therapeutic intervention.
  requires_registration: false
  url: https://github.com/igbb-popescu-lab/NetSeekR
- id: B2AI_STANDARD:805
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - graphdataplatform
  concerns_data_topic:
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Network Data Exchange
  is_open: true
  name: NDEx
  purpose_detail: The NDEx Project provides an open-source framework where scientists
    and organizations can store, share, manipulate, and publish biological network
    knowledge.
  requires_registration: true
  url: https://www.ndexbio.org/
- id: B2AI_STANDARD:806
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - scrnaseqanalysis
  concerns_data_topic:
  - B2AI_TOPIC:34
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: NeuCA - Neural-network based Cell Annotation tool
  formal_specification: https://github.com/haoharryfeng/NeuCA
  is_open: true
  name: NeuCA
  publication: doi:10.1038/s41598-021-04473-4
  purpose_detail: A R/Bioconductor tool for cell type annotation using single-cell
    RNA-seq data. It is a supervised cell label assignment method that uses existing
    scRNA-seq data with known labels to train a neural network-based classifier, and
    then predict cell labels in single-cell RNA-seq data of interest.
  requires_registration: false
  url: https://github.com/haoharryfeng/NeuCA
  has_application:
  - id: B2AI_APP:75
    category: B2AI:Application
    name: Cell Type Annotation in Single-Cell Genomics
    description: NeuCA (Neural network-based Cell Annotation) is used in AI applications
      for automated cell type identification in single-cell RNA sequencing data, leveraging
      deep learning to achieve accurate, scalable annotation across diverse tissues
      and species. The tool employs neural network architectures optimized for single-cell
      expression profiles to classify cells based on marker gene expression patterns,
      enabling rapid analysis of large-scale atlas projects and clinical samples.
      NeuCA's transfer learning capabilities allow models trained on reference atlases
      to annotate new datasets with limited manual curation, supporting applications
      in cancer cell identification, immune profiling, and developmental biology.
      The method provides confidence scores for cell type assignments and can identify
      novel or transitional cell states.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:807
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:20
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Nextflow
  formal_specification: https://github.com/nextflow-io/nextflow
  has_relevant_organization:
  - B2AI_ORG:90
  is_open: true
  name: Nextflow
  purpose_detail: Enables scalable and reproducible scientific workflows using software
    containers. It allows the adaptation of pipelines written in the most common scripting
    languages.
  requires_registration: false
  url: https://www.nextflow.io/
- id: B2AI_STANDARD:808
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - cloudplatform
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: NHGRI Analysis Visualization and Informatics Lab-space
  formal_specification: https://github.com/anvilproject
  has_relevant_organization:
  - B2AI_ORG:73
  is_open: true
  name: AnVIL
  purpose_detail: AnVIL is NHGRI's Genomic Data Science Analysis, Visualization, and
    Informatics Lab-Space.
  requires_registration: true
  url: https://anvilproject.org/
- id: B2AI_STANDARD:809
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:23
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: OmicsEV
  formal_specification: https://github.com/bzhanglab/OmicsEV
  is_open: true
  name: OmicsEV
  publication: doi:10.1093/bioinformatics/btac698
  purpose_detail: An R package for quality evaluation of omics data tables. For each
    data table, OmicsEV uses a series of methods to evaluate data depth, data normalization,
    batch effect, biological signal, platform reproducibility, and multi-omics concordance,
    producing comprehensive visual and quantitative evaluation results that help assess
    data quality of individual data tables and facilitate the identification of the
    optimal data processing method and parameters for the omics study under investigation.
  requires_registration: false
  url: https://github.com/bzhanglab/OmicsEV
- id: B2AI_STANDARD:810
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology Access Kit
  formal_specification: https://github.com/INCATools/ontology-access-kit
  has_relevant_organization:
  - B2AI_ORG:58
  is_open: true
  name: OAK
  purpose_detail: OAK provides a collection of interfaces for various ontology operations.
  requires_registration: false
  url: https://incatools.github.io/ontology-access-kit/
- id: B2AI_STANDARD:811
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology Development Kit
  formal_specification: https://github.com/INCATools/ontology-development-kit
  has_relevant_organization:
  - B2AI_ORG:58
  is_open: true
  name: ODK
  publication: doi:10.1093/database/baac087
  purpose_detail: A toolkit and workflow system for managing the ontology life-cycle.
  requires_registration: false
  url: https://github.com/INCATools/ontology-development-kit
- id: B2AI_STANDARD:812
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:18
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: OpenHealth
  is_open: true
  name: OpenHealth
  publication: doi:10.1109/MDAT.2019.2906110
  purpose_detail: An open-source platform for wearable health monitoring. It aims
    to design a standard set of hardware/software and wearable devices that can enable
    autonomous collection of clinically relevant data.
  requires_registration: false
  url: https://sites.google.com/view/openhealth-wearable-health/home
- id: B2AI_STANDARD:813
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: pandas
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:29
  - B2AI_SUBSTRATE:8
  is_open: true
  name: pandas
  purpose_detail: An open source data analysis and manipulation tool built on top
    of the Python programming language.
  requires_registration: false
  url: https://pandas.pydata.org/
- id: B2AI_STANDARD:814
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - datavisualization
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Panel
  formal_specification: https://github.com/holoviz/panel
  is_open: true
  name: Panel
  purpose_detail: An open-source Python library that lets you create custom interactive
    web apps and dashboards by connecting user-defined widgets to plots, images, tables,
    or text.
  requires_registration: false
  url: https://panel.holoviz.org/
- id: B2AI_STANDARD:815
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: PostgreSQL
  formal_specification: https://github.com/postgres/postgres
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:31
  - B2AI_SUBSTRATE:37
  - B2AI_SUBSTRATE:9
  is_open: true
  name: PostgreSQL
  purpose_detail: An open source object-relational database system.
  requires_registration: false
  url: https://www.postgresql.org/
- id: B2AI_STANDARD:816
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - machinelearningframework
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: PyTorch
  formal_specification: https://github.com/pytorch/pytorch
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:33
  is_open: true
  name: PyTorch
  purpose_detail: A popular machine learning platform.
  related_to:
  - B2AI_STANDARD:354
  requires_registration: false
  url: https://pytorch.org/
  has_application:
  - id: B2AI_APP:76
    category: B2AI:Application
    name: Deep Learning for Biomedical Research and Clinical AI
    description: PyTorch is the dominant framework for biomedical AI research, used
      extensively for developing deep learning models across medical imaging, genomics,
      drug discovery, and clinical prediction. Researchers leverage PyTorch's dynamic
      computational graphs, extensive ecosystem (torchvision, torchaudio, TorchIO),
      and pretrained models to build custom neural architectures for tasks like cancer
      detection in pathology images, protein structure prediction, medical report
      generation, and patient outcome forecasting. PyTorch's flexibility enables rapid
      prototyping of novel architectures, its strong academic community supports reproducible
      research through shared model implementations, and its production deployment
      tools (TorchServe, TorchScript) facilitate clinical translation of research
      models.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:817
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - notebookplatform
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Quarto publishing system
  formal_specification: https://github.com/quarto-dev/quarto-cli
  is_open: true
  name: Quarto
  purpose_detail: An open-source scientific and technical publishing system built
    on Pandoc.
  requires_registration: false
  url: https://quarto.org/
- id: B2AI_STANDARD:818
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - cloudplatform
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Rare Disease Cures Accelerator-Data and Analytics Platform
  has_relevant_organization:
  - B2AI_ORG:31
  is_open: false
  name: RDCA-DAP
  purpose_detail: The Rare Disease Cures Accelerator-Data and Analytics Platform (RDCA-DAP)
    is an FDA-funded initiative that provides a centralized and standardized infrastructure
    to support and accelerate rare disease characterization, with the goal of accelerating
    therapy development across rare diseases.
  requires_registration: true
  url: https://c-path.org/programs/rdca-dap/
- id: B2AI_STANDARD:819
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: refget API
  formal_specification: https://samtools.github.io/hts-specs/refget.html
  has_relevant_organization:
  - B2AI_ORG:34
  is_open: true
  name: refget
  purpose_detail: Enables access to reference genomic sequences without ambiguity
    from different databases and servers using a checksum identifier based on the
    sequence content itself.
  requires_registration: false
  url: https://samtools.github.io/hts-specs/refget.html
- id: B2AI_STANDARD:820
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - machinelearningframework
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Relexi
  formal_specification: https://github.com/flexi-framework/relexi
  is_open: true
  name: Relexi
  publication: doi:10.1016/j.simpa.2022.100422
  purpose_detail: Relexi is an open source reinforcement learning (RL) framework written
    in Python and based on TensorFlow's RL library TF-Agents. Relexi allows to employ
    RL for environments that require computationally intensive simulations like applications
    in computational fluid dynamics. For this, Relexi couples legacy simulation codes
    with the RL library TF-Agents at scale on modern high-performance computing (HPC)
    hardware using the SmartSim library. Relexi thus provides an easy way to explore
    the potential of RL for HPC applications.
  requires_registration: false
  url: https://github.com/flexi-framework/relexi
  has_application:
  - id: B2AI_APP:77
    category: B2AI:Application
    name: Explainable Reinforcement Learning for Clinical Decision Support
    description: Relexi is used in biomedical AI for developing interpretable reinforcement
      learning systems that can explain their decision-making process, crucial for
      clinical applications where treatment recommendations must be understandable
      to physicians. The framework enables training of RL agents for sequential clinical
      decisions (medication dosing, ventilator management, treatment timing) while
      maintaining explainability through attention mechanisms and policy distillation.
      Relexi supports safe exploration in healthcare settings by incorporating domain
      constraints and enabling clinicians to understand why specific actions are recommended.
      Applications include explainable sepsis treatment protocols, interpretable insulin
      dosing algorithms, and transparent clinical trial enrollment strategies where
      understanding the agent's reasoning is essential for clinical trust and regulatory
      approval.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:821
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:31
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Research Electronic Data Capture
  has_relevant_organization:
  - B2AI_ORG:114
  - B2AI_ORG:117
  is_open: true
  name: REDCap
  publication: doi:10.1016/j.jbi.2008.08.010
  purpose_detail: REDCap (Research Electronic Data Capture) is a mature, metadata-driven
    software platform designed for building and managing online databases and surveys
    for clinical and translational research studies, developed at Vanderbilt University
    and now maintained by an international consortium of over 6,000 institutional
    partners across 150 countries. The platform provides a comprehensive web-based
    application with an intuitive interface for rapid development of electronic data
    capture instruments using a data dictionary that defines field types, validation
    rules, branching logic, and calculated fields without requiring programming expertise.
    Core features include secure user authentication with role-based access controls,
    audit trails tracking all data changes with timestamps and user attribution, automated
    data quality checks through validation rules and range constraints, repeating
    instruments and events for longitudinal data collection, survey distribution with
    public and private links, and mobile-responsive data entry forms accessible from
    any device. REDCap supports complex study designs through its project management
    capabilities including multi-site studies, randomization modules, scheduling and
    calendar features, file upload repositories, and automated survey invitations
    with customizable reminder logic. The platform emphasizes data security and regulatory
    compliance with HIPAA-compatible infrastructure, 21 CFR Part 11 validation documentation,
    encrypted data transmission and storage, automatic de-identification tools for
    exporting datasets, and comprehensive logging for audit purposes. REDCap's extensibility
    is enabled through a robust RESTful API supporting programmatic data import/export,
    project metadata retrieval, and integration with external systems, plus the REDCap
    External Modules framework allowing custom functionality development. The Clinical
    Data Interoperability Services (CDIS) extension provides FHIR-based integration
    enabling bidirectional data exchange with electronic health records, supporting
    real-time EHR data extraction and write-back capabilities. Data analysis features
    include built-in descriptive statistics, graphical data visualization, data export
    to multiple formats (CSV, SPSS, SAS, R, Stata, Excel), and the REDCap-ETL tool
    for transforming and loading data into relational databases for advanced analytics.
    The platform has been extensively adopted in AI and machine learning workflows
    as a standardized data capture layer feeding training datasets, with documented
    applications spanning multimodal ML combining clinical tabular data with imaging
    features, federated rare disease registries supporting deep learning on aggregated
    cohorts, in-app risk scoring and triage algorithms implemented via calculated
    fields, reproducible ETL pipelines to SQLite/Python/scikit-learn for nuclear
    medicine ML, FHIR-mediated interoperable AI data pipelines for oncology research,
    and prospective ML model monitoring frameworks with continuous retraining tied
    to REDCap-collected outcomes. REDCap's metadata-driven architecture, standardized
    common data elements support, API-driven ETL capabilities, and integration with
    research informatics ecosystems make it essential infrastructure for rigorous,
    reproducible clinical research and AI-ready biomedical data collection.
  requires_registration: true
  url: https://www.project-redcap.org/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:78
    category: B2AI:Application
    name: Clinical Research Data Capture for ML Model Development
    description: REDCap is widely used in AI applications as a platform for collecting
      high-quality, structured clinical research data that feeds machine learning
      model development and validation studies. AI researchers leverage REDCap's data
      dictionaries, validation rules, and standardized data collection instruments
      to create clean training datasets for predictive models in clinical trials,
      cohort studies, and patient registries. REDCap's API enables automated data
      extraction for ML pipelines, and its audit trails and data quality features
      ensure reproducibility in AI research. The platform is particularly valuable
      for multi-site AI studies where standardized data collection across institutions
      is essential for model generalizability.
    used_in_bridge2ai: false
  - id: B2AI_APP:159
    category: B2AI:Application
    name: Interoperable AI Data Pipelines via REDCap-to-FHIR ETL
    references:
    - https://doi.org/10.1101/2024.03.15.24303032
    description: The EuCanImage consortium uses REDCap as a standardized intermediary
      data store with Python ETL pipelines that map to FHIR endpoints and common
      data models to create AI-ready datasets for oncology research. REDCap Clinical
      Data Interoperability Services (CDIS) enables high-fidelity extraction of treatment
      data from EHRs via FHIR, positioning REDCap within AI data pipelines by unlocking
      structured, trustworthy inputs for downstream modeling. This approach enables
      harmonized multicenter data capture and transformation for AI model development
      and submission to permanent repositories, demonstrating how REDCap supports
      FAIR data principles and interoperability standards essential for collaborative
      AI research.
    used_in_bridge2ai: false
  - id: B2AI_APP:160
    category: B2AI:Application
    name: Multimodal Machine Learning Combining Imaging and Clinical Data
    references:
    - https://doi.org/10.48550/arxiv.2501.11535
    description: REDCap-derived tabular clinical variables are combined with radiomics
      features in multimodal machine learning models for cancer staging and prognosis.
      In hepatocellular carcinoma studies, XGBoost models trained on combined imaging
      and REDCap tabular data achieved superior performance compared to single-modality
      models, with feature importance analyses identifying critical REDCap-captured
      variables such as age and tumor characteristics. This application demonstrates
      REDCap's role in providing curated clinical features that complement imaging
      biomarkers, with preprocessing workflows that filter REDCap columns based on
      data quality thresholds and domain relevance before model training.
    used_in_bridge2ai: false
  - id: B2AI_APP:161
    category: B2AI:Application
    name: Federated Rare Disease Registries Supporting Deep Learning
    references:
    - https://doi.org/10.1093/jamia/ocad132
    description: The National Mesothelioma Virtual Bank uses REDCap with standardized
      Common Data Elements (CDEs) and an API-driven ETL pipeline (CSV export via REDCap
      API, processed through R and JavaScript, served via AWS Lambda) to power cohort
      discovery and deidentified data provisioning for advanced analytics and deep
      learning on rare-disease cohorts. This federated REDCap deployment aggregates
      over 2,000 cases across multiple institutions, enabling scalable multicenter
      datasets with faceted search capabilities and honest-broker data protection
      models. The standardized data capture and automated deidentification workflows
      make these registries valuable resources for training ML models on rare diseases
      where data scarcity is a major challenge.
    used_in_bridge2ai: false
  - id: B2AI_APP:162
    category: B2AI:Application
    name: REDCap-Embedded Risk Scoring and Triage Algorithms
    references:
    - https://doi.org/10.1017/cts.2024.1134
    description: Clinical decision support algorithms are implemented directly within
      REDCap using calculated fields and conditional logic to perform real-time risk
      prediction and automated triage. The Mayo Clinic deployed a Risk Prediction
      and Management (RPM) scoring tool in REDCap that evaluates operational risks
      for incoming clinical studies, automatically scoring approximately 200 projects
      and routing high-risk cases to physician leadership for review. This in-app
      decision support approach leverages REDCap's computational capabilities to operationalize
      algorithmic scoring without requiring external systems, demonstrating how REDCap
      can serve not only as a data capture platform but also as a deployment environment
      for prediction models in operational workflows.
    used_in_bridge2ai: false
  - id: B2AI_APP:163
    category: B2AI:Application
    name: Prospective ML Model Monitoring and Continuous Retraining
    references:
    - https://doi.org/10.1186/s13017-025-00594-7
    description: The MINERVA study protocol uses REDCap as the secure primary data
      repository for training a deep learning relapse-risk model with plans for continuous
      real-time monitoring, automated logging, and iterative retraining tied to REDCap-collected
      outcomes. The framework includes web-based monitoring of model performance metrics
      (sensitivity, specificity, PPV, NPV), automated capture of instances where predictions
      diverge from clinical outcomes, root-cause analysis workflows, and data validation
      checks that feed back into model refinement. This application demonstrates how
      REDCap can support the full ML lifecycle from initial training data collection
      through deployment monitoring and model maintenance, with role-based access
      controls and deidentification protocols ensuring data security throughout the
      process.
    used_in_bridge2ai: false
  - id: B2AI_APP:164
    category: B2AI:Application
    name: Reproducible ML Pipelines for Nuclear Medicine
    references:
    - https://iris.unito.it/bitstream/2318/2067667/2/01_PhD_thesis_Rovera_Guido.pdf
    description: REDCap-centered workflows in nuclear medicine use REDCap for standardized
      clinical data capture with ETL pipelines (REDCap-ETL or custom Python/R scripts)
      that export to SQLite databases for machine learning analyses with scikit-learn.
      These pipelines emphasize anonymization, reproducibility through version-controlled
      scripts, dynamic real-time data retrieval, and automated computation of derived
      variables to support ML workflows. The use of REDCap's repeating instruments
      feature enables capture of relational time-series events, while project XML
      templates support multi-center reproducibility. This approach demonstrates how
      REDCap integrates into end-to-end ML pipelines for medical imaging and clinical
      prediction tasks, with synthetic data generation capabilities for testing model
      scalability.
    used_in_bridge2ai: false
  - id: B2AI_APP:165
    category: B2AI:Application
    name: Clinical Decision Support for Post-Surgical Risk and STI Recommendations
    references:
    - https://doi.org/10.1017/cts.2021.529
    description: REDCap implementations have been documented generating ML-based risk
      predictions for post-surgical outcomes, producing sexually transmitted infection
      (STI) test recommendations and orders, and improving clinical handoffs in surgical
      oncology through decision support workflows. These applications leverage REDCap's
      data capture capabilities combined with statistical and machine learning methods
      to deliver actionable clinical recommendations, though published documentation
      of such novel uses remains limited. The integration patterns demonstrate REDCap's
      potential to bridge research data collection and clinical decision support,
      enabling evidence-based interventions derived from ML models trained on REDCap-captured
      data.
    used_in_bridge2ai: false
  - id: B2AI_APP:166
    category: B2AI:Application
    name: Translational Research Frameworks Integrating Multimodal Data for Predictive
      Modeling
    references:
    - https://iris.unito.it/bitstream/2318/2067667/2/01_PhD_thesis_Rovera_Guido.pdf
    description: Preclinical and translational research frameworks use REDCap instruments
      for single-measure clinical data capture with customized API and ETL pipelines
      to integrate multimodal datasets (clinical, imaging, molecular) for dashboards
      and predictive modeling applications. These frameworks demonstrate REDCap's
      role as a data backbone that feeds analytic workflows, supporting integrated
      datasets that combine REDCap-captured clinical assessments with laboratory results,
      imaging features, and genomic data. The metadata-driven structure of REDCap
      projects facilitates standardized variable definitions across data modalities,
      enabling machine learning models to leverage complementary information sources
      for improved prediction accuracy in translational medicine.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:822
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Sage Synapse
  formal_specification: https://github.com/Sage-Bionetworks/Synapse-Repository-Services
  has_relevant_organization:
  - B2AI_ORG:86
  is_open: true
  name: Synapse
  publication: doi:10.2139/ssrn.3502410
  purpose_detail: Synapse is a collaborative research platform developed by Sage Bionetworks
    that provides web services and tools for aggregating, organizing, analyzing, and
    sharing scientific data, code, and insights across biomedical research communities.
    The platform implements a comprehensive data management infrastructure through
    its Repository Services, offering RESTful JSON APIs for entity management, file
    storage, versioning, and access control. Core functionality includes hierarchical
    project organization with entities such as Projects, Folders, Files, Tables, and
    Views that support structured data organization and metadata annotation. Synapse
    provides advanced data access control through Access Requirements and Access Approvals,
    enabling researchers to implement controlled access policies including ACTAccessRequirement
    for managed access to sensitive data. The platform supports collaborative research
    through Teams, subscription services for change notifications, messaging capabilities,
    and wiki documentation integrated with research artifacts. Data governance features
    include a Qualified Research Program that balances broad researcher access with
    participant protections, research project management for data access requests,
    and submission workflows for access review by the Access and Compliance Team (ACT).
    Technical capabilities include multi-part file uploads, version tracking with
    provenance through Activity records, search indexing using OpenSearch for entity
    discovery, DOI minting for permanent identifiers, and integration with OAuth2
    authentication and OpenID Connect. Synapse implements the GA4GH DRS (Data Repository
    Service) API specification for standardized data access, supports tabular data
    through Table entities with SQL-like querying, and provides form-based data collection
    with review workflows. The platform serves as infrastructure for open science
    initiatives, enabling data sharing while encoding contractual protections for
    research participants, and has been deployed in numerous biomedical studies including
    mHealth research such as the mPower Parkinson disease observational study.
  requires_registration: true
  url: https://www.synapse.org/
  used_in_bridge2ai: true
- id: B2AI_STANDARD:823
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: SemEHR
  formal_specification: https://github.com/CogStack/CogStack-SemEHR
  is_open: true
  name: SemEHR
  publication: doi:10.1093/jamia/ocx160
  purpose_detail: An open source semantic search and analytics tool for EHRs.
  requires_registration: false
  url: https://github.com/CogStack/CogStack-SemEHR
- id: B2AI_STANDARD:824
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - scrnaseqanalysis
  concerns_data_topic:
  - B2AI_TOPIC:34
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Single-cell Analysis Via Expression Recovery
  formal_specification: https://github.com/mohuangx/SAVER
  is_open: true
  name: SAVER
  publication: doi:10.1038/s41592-018-0033-z
  purpose_detail: A regularized regression prediction and empirical Bayes method to
    recover the true gene expression profile in noisy and sparse scRNA-seq data.
  requires_registration: false
  url: https://mohuangx.github.io/SAVER/
- id: B2AI_STANDARD:825
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Snorkel
  formal_specification: https://github.com/snorkel-team/snorkel
  is_open: false
  name: Snorkel
  purpose_detail: Snorkel is a platform for automated data labeling. It has since
    been extended into a full machine learning platform (see https://snorkel.ai/).
  requires_registration: true
  url: https://www.snorkel.org/
  collection:
  - deprecated
- id: B2AI_STANDARD:826
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:25
  - B2AI_TOPIC:35
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: SnpEff
  formal_specification: https://github.com/pcingola/SnpEff
  is_open: true
  name: SnpEff
  publication: doi:10.4161/fly.19695
  purpose_detail: SnpEff is a variant annotation and effect prediction tool. It annotates
    and predicts the effects of genetic variants (such as amino acid changes).
  requires_registration: false
  url: https://pcingola.github.io/SnpEff/se_introduction/
- id: B2AI_STANDARD:827
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - scrnaseqanalysis
  concerns_data_topic:
  - B2AI_TOPIC:34
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Souporcell
  formal_specification: https://github.com/wheaton5/souporcell
  is_open: true
  name: Souporcell
  publication: doi:10.1038/s41592-020-0820-1
  purpose_detail: A method to cluster cells using the genetic variants detected within
    the scRNA-seq reads.
  requires_registration: false
  url: https://github.com/wheaton5/souporcell
- id: B2AI_STANDARD:828
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:33
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Spliced Transcripts Alignment to a Reference
  formal_specification: https://github.com/alexdobin/STAR
  is_open: true
  name: STAR
  publication: doi:10.1093/bioinformatics/bts635
  purpose_detail: Software based on an RNA-seq alignment algorithm that uses sequential
    maximum mappable seed search in uncompressed suffix arrays followed by seed clustering
    and stitching procedure.
  requires_registration: false
  url: https://github.com/alexdobin/STAR
- id: B2AI_STANDARD:829
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Systems Biology Layout and Rendering Service
  formal_specification: https://github.com/iVis-at-Bilkent/syblars
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:36
  is_open: true
  name: SyBLaRS
  publication: doi:10.1371/journal.pcbi.1010635
  purpose_detail: A web service for automatic layout of biological data in various
    standard formats as well as construction of customized images in both raster image
    and scalable vector formats of these maps. Some of the supported standards are
    more generic such as GraphML and JSON, whereas others are specialized to biology
    such as SBGNML (The Systems Biology Graphical Notation Markup Language) and SBML
    (The Systems Biology Markup Language).
  related_to:
  - B2AI_STANDARD:337
  - B2AI_STANDARD:289
  requires_registration: false
  url: http://syblars.cs.bilkent.edu.tr/
- id: B2AI_STANDARD:830
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Task Execution Service
  has_relevant_organization:
  - B2AI_ORG:34
  is_open: true
  name: TES
  purpose_detail: The Task Execution Service (TES) API is a proposed standard for
    describing and executing tasks in a platform-agnostic way. Includes a TES API
    validator and a Task Execution API specification.
  requires_registration: false
  url: https://ga4gh.github.io/task-execution-schemas/docs/
- id: B2AI_STANDARD:831
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - machinelearningframework
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Tensorflow
  formal_specification: https://github.com/tensorflow/tensorflow
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:42
  has_relevant_organization:
  - B2AI_ORG:37
  is_open: true
  name: TF
  purpose_detail: TensorFlow is an end-to-end open source platform for machine learning.
    It has an ecosystem of tools, libraries and community resources that lets researchers
    and developers easily build and deploy ML powered applications.
  related_to:
  - B2AI_STANDARD:354
  requires_registration: false
  url: https://www.tensorflow.org/
  has_application:
  - id: B2AI_APP:79
    category: B2AI:Application
    name: Production-Scale Healthcare AI Systems and Deployment
    description: TensorFlow is widely used for deploying production-grade AI systems
      in healthcare, particularly for applications requiring high-throughput inference,
      mobile deployment, and integration with enterprise IT infrastructure. Healthcare
      organizations leverage TensorFlow's mature ecosystem (TF Serving, TF Lite, TF.js)
      to deploy models for real-time clinical decision support, mobile diagnostic
      apps, and edge computing in medical devices. The framework's strong support
      for model optimization, quantization, and hardware acceleration (TPUs, GPUs)
      enables efficient deployment of complex models like retinal disease screening
      systems, ECG interpretation algorithms, and clinical NLP pipelines. TensorFlow
      Extended (TFX) provides production ML pipelines for managing data validation,
      model training, and continuous monitoring in regulated healthcare environments.
    used_in_bridge2ai: false
    references:
    - https://www.tensorflow.org/tfx
- id: B2AI_STANDARD:832
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - cloudplatform
  concerns_data_topic:
  - B2AI_TOPIC:20
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Terra Community Workbench
  formal_specification: https://github.com/DataBiosphere/terra-ui
  has_relevant_organization:
  - B2AI_ORG:71
  is_open: true
  name: Terra
  purpose_detail: Terra is a cloud-native platform for biomedical researchers to access
    data, run analysis tools, and collaborate.
  requires_registration: true
  url: https://app.terra.bio/
- id: B2AI_STANDARD:833
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: The R Project for Statistical Computing
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:34
  - B2AI_SUBSTRATE:35
  - B2AI_SUBSTRATE:40
  is_open: true
  name: R
  purpose_detail: A free software environment for statistical computing and graphics.
  requires_registration: false
  url: https://www.r-project.org/
  used_in_bridge2ai: true
- id: B2AI_STANDARD:834
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - deprecated
  - machinelearningframework
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Theano
  formal_specification: https://github.com/Theano/Theano
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:1
  is_open: true
  name: Theano
  purpose_detail: A Python library that allows you to define, optimize, and evaluate
    mathematical expressions involving multi-dimensional arrays efficiently. It is
    being continued as aesara.
  related_to:
  - B2AI_STANDARD:354
  requires_registration: false
  url: https://github.com/Theano/Theano
  has_application:
  - id: B2AI_APP:80
    category: B2AI:Application
    name: Legacy Deep Learning Models and Reproducible Research
    description: Theano was historically important in early biomedical deep learning
      research and continues to be relevant for reproducing published models and maintaining
      legacy clinical AI systems. Many influential early papers in medical AI used
      Theano, and researchers still need to run these models for comparison benchmarks,
      reproduce published results, and maintain systems deployed before modern frameworks
      emerged. The library's symbolic computation approach and automatic differentiation
      influenced the design of current frameworks, and understanding Theano remains
      valuable for historical context in AI research. While active development has
      ceased (succeeded by Aesara), Theano-based code remains in production in some
      clinical settings and research archives.
    used_in_bridge2ai: false
    references:
    - https://theano-pymc.readthedocs.io/
- id: B2AI_STANDARD:835
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Tool Registry Service
  has_relevant_organization:
  - B2AI_ORG:34
  is_open: true
  name: TRS
  purpose_detail: A proposed standard for sharing and discovering tools and workflows
    in a platform-agnostic way. Includes a Tool Registry Search validator and a Tool
    Discovery API specification.
  requires_registration: false
  url: https://ga4gh.github.io/tool-registry-service-schemas/
- id: B2AI_STANDARD:836
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:4
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: UAB Biomedical Research Information Technology Enhancement Commons
    Program
  has_relevant_organization:
  - B2AI_ORG:94
  is_open: false
  name: U-BRITE
  purpose_detail: U-BRITE (UAB Biomedical Research Information Technology Enhancement)
    assembles new and existing HIPAA-compliant, high-performance informatics tools
    to provide researchers with a means to better manage and analyze clinical and
    genomic data sets and implements a translational research commons to facilitate
    and enable interdisciplinary team science across geographical locations.
  requires_registration: true
  url: https://ubrite.org/
- id: B2AI_STANDARD:837
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Usagi
  formal_specification: https://github.com/OHDSI/Usagi
  has_relevant_organization:
  - B2AI_ORG:76
  has_training_resource:
  - B2AI_STANDARD:844
  is_open: true
  name: Usagi
  purpose_detail: An application to help create mappings between coding systems and
    the Vocabulary standard concepts.
  requires_registration: false
  url: https://github.com/OHDSI/Usagi
- id: B2AI_STANDARD:838
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:34
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Vireo
  formal_specification: https://github.com/single-cell-genetics/vireo
  is_open: true
  name: Vireo
  publication: doi:10.1186/s13059-019-1865-2
  purpose_detail: A computationally efficient Bayesian model to demultiplex single-cell
    data from pooled experimental designs.
  requires_registration: false
  url: https://github.com/single-cell-genetics/vireo
- id: B2AI_STANDARD:839
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - cloudservice
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Wasabi Cloud Storage
  is_open: false
  name: Wasabi
  purpose_detail: 'Wasabi Cloud Storage is a high-performance, S3-compatible object storage service providing hot cloud storage with predictable pricing, no egress fees, and 80% lower total cost of ownership compared to hyperscaler alternatives (AWS S3, Azure Blob, Google Cloud Storage), designed for data-intensive workloads requiring frequent access and large-scale data archival. Founded in 2017 by Carbonite co-founders, Wasabi operates 16 globally distributed storage regions across North America, Europe, Asia-Pacific, with SOC-2, ISO 27001, and PCI-DSS certified data centers providing enterprise-grade security, immutability features (WORM, Object Lock), and compliance certifications (HIPAA, GDPR, FERPA) suitable for regulated industries. Wasabi''s "always hot" architecture eliminates tiered storage complexity by storing all data on high-performance disk arrays with consistent millisecond latency for reads/writes, avoiding cold storage retrieval delays that plague glacier-tier alternatives while maintaining cost parity with archival storage services. The platform''s S3 API compatibility ensures drop-in replacement for existing AWS workflows, supporting standard S3 operations (PUT, GET, LIST, multipart uploads), bucket policies, IAM-style access controls, and seamless integration with S3-compatible tools (AWS CLI, SDKs, third-party backup software, media asset managers). Wasabi''s zero-fee model for egress, API requests, and reads eliminates the unpredictable costs that typically double hyperscaler storage bills, enabling cost-effective access for AI/ML training pipelines, video surveillance archives, genomic datasets, and medical imaging repositories where frequent data retrieval is essential. The service provides native integrations with major backup platforms (Veeam, Commvault, Rubrik), media workflows (Adobe Premiere, Frame.io), surveillance systems (Milestone, Hanwha), and object storage gateways, supporting hybrid cloud architectures where on-premises applications seamlessly tier to Wasabi for capacity expansion and disaster recovery. For AI/ML workloads, Wasabi''s high-throughput object storage (up to 10 Gbps per connection) supports rapid dataset ingestion for model training, low-latency access to training data during distributed training across GPU clusters, and cost-effective storage for model checkpoints, experiment artifacts, and inference result archives without egress penalties for iterative model development and hyperparameter tuning requiring repeated dataset access.'
  requires_registration: true
  url: https://wasabi.com/
- id: B2AI_STANDARD:840
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Weights and Balances platform
  is_open: true
  name: W&B
  purpose_detail: Platform for tracking, comparing, and visualizing machine learning
    experiments.
  requires_registration: true
  url: https://wandb.ai/
- id: B2AI_STANDARD:841
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Workflow Execution Service
  has_relevant_organization:
  - B2AI_ORG:34
  is_open: true
  name: WES
  purpose_detail: The Workflow Execution Service (WES) is a GA4GH standard API specification
    that provides a platform-agnostic approach for submitting and monitoring workflow
    execution across different computational environments. WES enables researchers
    to run standardized workflows, currently supporting Common Workflow Language (CWL)
    and Workflow Description Language (WDL) formats, on multiple platforms, clouds,
    and execution systems using a consistent interface. The API specification is written
    in OpenAPI and embodies RESTful service principles, using JSON for requests and
    responses with standard HTTP/HTTPS transport. Core functionality includes workflow
    submission with parameter passing, run status monitoring through detailed logs
    capturing stdout and stderr output, task-level execution tracking with timing
    and exit codes, and workflow cancellation capabilities. WES addresses critical
    use cases such as "bring your code to the data" scenarios where researchers submit
    custom analyses to run on externally-owned datasets without data transfer, and
    best-practices pipeline execution where researchers discover workflows from shared
    repositories like Dockstore and execute them over controlled data environments.
    The service implements OAuth2 bearer token authentication and authorization, with
    implementations responsible for verifying user credentials and enforcing submission
    policies. WES provides comprehensive service introspection through its service-info
    endpoint, reporting supported workflow types, versions, filesystem protocols,
    workflow engines, and system state information. The API supports paginated listing
    of workflow runs, detailed run logs with output file locations, and granular task-level
    monitoring. Run states include QUEUED, INITIALIZING, RUNNING, COMPLETE, EXECUTOR_ERROR,
    SYSTEM_ERROR, CANCELED, and PREEMPTED, enabling detailed tracking of workflow
    execution lifecycle. As a GA4GH standard, WES promotes interoperability across
    bioinformatics workflow execution platforms and supports integration with related
    GA4GH standards including the Data Object Service for credential management and
    the Task Execution Service for extended task definitions.
  requires_registration: false
  url: https://ga4gh.github.io/workflow-execution-service-schemas/docs/
- id: B2AI_STANDARD:842
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Xethub
  is_open: true
  name: Xethub
  purpose_detail: Git-based collaboration to large scale repositories of data, code,
    or any combination of files.
  requires_registration: true
  url: https://xethub.com/assets/docs/
- id: B2AI_STANDARD:843
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - machinelearningframework
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ZenML
  formal_specification: https://github.com/zenml-io/zenml
  is_open: true
  name: ZenML
  purpose_detail: ZenML is an extensible, open-source MLOps framework for creating
    portable, production-ready MLOps pipelines.
  requires_registration: false
  url: https://zenml.io/
  has_application:
  - id: B2AI_APP:81
    category: B2AI:Application
    name: MLOps Orchestration for Biomedical AI Pipelines
    description: ZenML is used in biomedical AI for building reproducible, production-grade
      machine learning pipelines with comprehensive experiment tracking, model versioning,
      and deployment orchestration. Healthcare AI teams leverage ZenML to standardize
      workflows from data ingestion through model deployment, ensuring compliance
      with regulatory requirements for traceability and reproducibility. The platform
      integrates with diverse healthcare data sources, model registries, and deployment
      targets while maintaining complete lineage tracking essential for clinical AI
      validation. ZenML enables teams to implement best practices for ML operations
      including automated testing, continuous training, and model monitoring in healthcare
      settings where reliability and auditability are critical.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:844
  category: B2AI_STANDARD:TrainingProgram
  concerns_data_topic:
  - B2AI_TOPIC:4
  - B2AI_TOPIC:52
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: 2019 OHDSI Tutorials - OMOP Common Data Model and Standardized Vocabularies
  formal_specification: https://github.com/OHDSI/Tutorial-CDM
  has_relevant_organization:
  - B2AI_ORG:76
  is_open: true
  name: OHDSI Tutorials
  purpose_detail: This workshop is for data holders who want to apply OHDSI's data
    standards to their own observational datasets and researchers who want to be aware
    of OHDSI's data standards, so they can leverage data in OMOP CDM format for their
    own research purposes.
  requires_registration: false
  url: https://www.ohdsi.org/2019-tutorials-omop-common-data-model-and-standardized-vocabularies/
- id: B2AI_STANDARD:845
  category: B2AI_STANDARD:TrainingProgram
  concerns_data_topic:
  - B2AI_TOPIC:52
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDC Introduction to FHIR - Training Recordings
  has_relevant_organization:
  - B2AI_ORG:40
  is_open: true
  name: CDC Introduction to FHIR
  purpose_detail: A series of HL7 FAIR training lecture recordings made available
    through YouTube.
  requires_registration: false
  url: https://www.cdc.gov/nchs/data/nvss/modernization/Introductory-Training-FHIR.pdf
- id: B2AI_STANDARD:846
  category: B2AI_STANDARD:TrainingProgram
  concerns_data_topic:
  - B2AI_TOPIC:52
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FHIR Drills
  has_relevant_organization:
  - B2AI_ORG:40
  is_open: true
  name: FHIR Drills
  purpose_detail: This set of pages contains a series of FHIR tutorials for those
    just beginning to learn the new specification. The tutorials require no prior
    knowledge of FHIR or REST. At present these tutorials are in their beta stage
    of development and we would appreciate any feedback you may have as we plan to
    build upon these in time to create a full set of tutorials from the very basic
    to the more complex.
  requires_registration: false
  url: https://fhir-drills.github.io/
- id: B2AI_STANDARD:847
  category: B2AI_STANDARD:TrainingProgram
  concerns_data_topic:
  - B2AI_TOPIC:52
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HL7 FHIR Fundamentals Course
  formal_specification: https://courses.hl7fundamentals.org/campus/
  has_relevant_organization:
  - B2AI_ORG:40
  is_open: false
  name: FHIR Fundamentals
  purpose_detail: This is an asynchronous, instructor-led online course that allows
    you to work at your own pace. Learning takes place through discussions with the
    instructor, tutors and peers. Assessments are in the form of weekly assignments,
    quizzes, exams and projects. Plan on spending 5 to 7 hours per week. There are
    no live lectures to attend.
  requires_registration: true
  url: https://www.hl7.org/training/fhir-fundamentals.cfm
- id: B2AI_STANDARD:848
  category: B2AI_STANDARD:TrainingProgram
  concerns_data_topic:
  - B2AI_TOPIC:52
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Learn LOINC
  has_relevant_organization:
  - B2AI_ORG:53
  is_open: true
  name: Learn LOINC
  purpose_detail: Welcome to the LOINC Library. This is our A to Z collection of resources
    that we've collected to help you learn about LOINC and get connected to the community.
  requires_registration: false
  url: https://loinc.org/learn/
- id: B2AI_STANDARD:849
  category: B2AI_STANDARD:TrainingProgram
  concerns_data_topic:
  - B2AI_TOPIC:15
  - B2AI_TOPIC:52
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Microsoft Learn - Work with medical imaging data and DICOM
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_relevant_organization:
  - B2AI_ORG:25
  - B2AI_ORG:56
  is_open: true
  name: Microsoft Medical Imaging
  purpose_detail: Learn why DICOM standards are important. Explore the DICOM standards
    and DICOM service. Review the use case for radiology data in cancer treatment
    with examples.
  requires_registration: false
  url: https://learn.microsoft.com/en-us/training/modules/medical-imaging-data/
- id: B2AI_STANDARD:850
  category: B2AI_STANDARD:TrainingProgram
  concerns_data_topic:
  - B2AI_TOPIC:52
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Udemy - Introduction to FHIR
  has_relevant_organization:
  - B2AI_ORG:40
  is_open: true
  name: Udemy FHIR
  purpose_detail: This course will help you understand the basics of FHIR. It is a
    FREE sample of a comprehensive hands-on introductory course (details inside).
    The full course includes direct access to the course creator via a private members-only
    Slack room.
  requires_registration: true
  url: https://www.udemy.com/course/introduction-to-fhir/
- id: B2AI_STANDARD:851
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Fast Healthcare Interoperability Resources - US Core
  has_relevant_organization:
  - B2AI_ORG:103
  - B2AI_ORG:117
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: FHIR US Core
  purpose_detail: This is subset of all FHIR profiles for the US Realm, i.e., those
    supporting the minimum requirements for clinical data exchange in the United States.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  subclass_of:
  - B2AI_STANDARD:109
  url: https://build.fhir.org/ig/HL7/US-Core/index.html
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:82
    category: B2AI:Application
    name: US-Specific Clinical AI and Regulatory Compliance
    description: FHIR US Core Implementation Guide is used in AI applications that
      require compliance with US healthcare regulations and interoperability requirements,
      enabling standardized data extraction from US-based EHR systems for training
      clinical prediction models. AI systems leverage US Core's profiles for patient
      demographics, vital signs, laboratory results, medications, and conditions to
      build models that meet ONC certification requirements and support CMS quality
      measures. The implementation guide ensures AI applications can reliably access
      structured clinical data across diverse US healthcare systems, supporting use
      cases in risk adjustment, quality metric prediction, social determinants of
      health analysis, and value-based care optimization. US Core's mandatory data
      elements provide a consistent feature set for federated learning across US hospitals.
    used_in_bridge2ai: false
    references:
    - https://www.healthit.gov/isa/united-states-core-data-interoperability-uscdi
- id: B2AI_STANDARD:852
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Fast Healthcare Interoperability Resources - US Core Data for Interoperability
  has_relevant_organization:
  - B2AI_ORG:103
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: FHIR USCDI
  purpose_detail: USCDI is the set of basic healthcare data types expected to supported
    by other systems. This is the FHIR US Core profile with all elements required
    by USCDI.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  subclass_of:
  - B2AI_STANDARD:851
  url: https://build.fhir.org/ig/HL7/US-Core/uscdi.html
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:83
    category: B2AI:Application
    name: Standardized Data Element Mapping for Healthcare AI
    description: FHIR USCDI (US Core Data for Interoperability) implementation is
      used in AI applications to ensure consistent access to core clinical data elements
      required by federal regulations, enabling interoperable AI systems across US
      healthcare. Machine learning models leverage USCDI-compliant data elements including
      allergies, procedures, immunizations, lab results, and clinical notes to train
      on standardized features that are guaranteed to be available across certified
      EHR systems. This standardization is critical for AI applications that need
      to be deployed broadly across healthcare organizations, ensuring model portability
      and consistent performance. USCDI compliance enables AI researchers to develop
      models that align with national interoperability goals and can participate in
      health information exchange networks.
    used_in_bridge2ai: false
    references:
    - https://www.healthit.gov/isa/united-states-core-data-interoperability-uscdi
- id: B2AI_STANDARD:853
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Fast Healthcare Interoperability Resources - US Core Data for Interoperability,
    version 1
  has_relevant_organization:
  - B2AI_ORG:103
  - B2AI_ORG:117
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: FHIR USCDI v1
  purpose_detail: USCDI is the set of basic healthcare data types expected to supported
    by other systems. This is the FHIR US Core profile with all elements required
    by USCDI v1.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  subclass_of:
  - B2AI_STANDARD:852
  url: https://build.fhir.org/ig/HL7/US-Core/uscdi.html
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:84
    category: B2AI:Application
    name: Foundation Models on Core Clinical Data Elements
    description: FHIR USCDI v1 provides the baseline data elements for AI applications
      requiring backward compatibility and stable data definitions for longitudinal
      model training. AI systems developed against USCDI v1 can reliably access essential
      clinical information including patient demographics, problems, medications,
      allergies, lab results, vital signs, and procedures across time, enabling training
      of models on historical data and ensuring consistency in production deployments.
      This version stability is crucial for validating AI models in clinical trials,
      meeting regulatory requirements for locked algorithms, and maintaining model
      performance monitoring over multi-year periods. USCDI v1 compliance ensures
      AI applications can function across healthcare systems at different stages of
      EHR modernization.
    used_in_bridge2ai: false
    references:
    - https://www.healthit.gov/isa/united-states-core-data-interoperability-uscdi-v1
- id: B2AI_STANDARD:854
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Fast Healthcare Interoperability Resources - US Core Data for Interoperability,
    version 4
  has_relevant_organization:
  - B2AI_ORG:103
  - B2AI_ORG:117
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: FHIR USCDI v4
  purpose_detail: USCDI is the set of basic healthcare data types expected to supported
    by other systems. This is the FHIR US Core profile with all elements required
    by USCDI v4.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  subclass_of:
  - B2AI_STANDARD:852
  url: https://build.fhir.org/ig/HL7/US-Core/uscdi.html
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:85
    category: B2AI:Application
    name: Advanced Clinical AI with Expanded Data Elements
    description: FHIR USCDI v4 enables next-generation AI applications with access
      to expanded data elements including social determinants of health, mental health
      assessments, substance use information, and care team details. Machine learning
      models leverage these additional data classes to develop more comprehensive
      prediction models that account for social, behavioral, and environmental factors
      affecting health outcomes. AI systems benefit from USCDI v4's enhanced data
      elements for health equity research, population health modeling, and holistic
      patient risk assessment that goes beyond traditional clinical variables. The
      expanded scope supports AI applications in addressing health disparities, improving
      care coordination, and developing interventions that consider the full spectrum
      of factors influencing patient health.
    used_in_bridge2ai: false
    references:
    - https://www.healthit.gov/isa/united-states-core-data-interoperability-uscdi-v4
- id: B2AI_STANDARD:855
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - multimodal
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-03-10'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Dedoose app
  is_open: false
  name: Dedoose
  purpose_detail: A cross-platform app for analyzing qualitative and mixed methods
    research
  requires_registration: true
  url: https://www.dedoose.com/
- id: B2AI_STANDARD:856
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-03-13'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FAIR Data Station
  is_open: true
  name: FAIR Data Station
  publication: doi:10.1093/gigascience/giad014
  purpose_detail: A lightweight application written in Java, that aims to support
    researchers in managing research metadata according to the FAIR principles.
  requires_registration: false
  url: https://fairbydesign.nl/
- id: B2AI_STANDARD:857
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-03-14'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Patient-Led Research Scorecards
  is_open: true
  name: Patient-Led Research Scorecards
  purpose_detail: The Council of Medical Specialty Societies (CMSS) and Patient-Led
    Research Collaborative (PLRC) have developed a sustainable collaborative model
    of CER based on information from and the expertise of patient communities, researchers,
    funders, and clinical research organizations. This model takes the form of scorecards
    which serve to evaluate how effective a patient group and research partner collaboration
    will be at conducting truly patient-led research.
  requires_registration: false
  url: https://patientresearchcovid19.com/storage/2023/02/Patient-Led-Research-Scorecards.pdf
- id: B2AI_STANDARD:858
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:32
  contribution_date: '2023-03-16'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Zshot
  formal_specification: https://github.com/IBM/zshot
  has_relevant_organization:
  - B2AI_ORG:104
  is_open: true
  name: Zshot
  purpose_detail: A framework for performing Zero and Few shot named entity recognition.
  requires_registration: false
  url: https://ibm.github.io/zshot/
- id: B2AI_STANDARD:859
  category: B2AI_STANDARD:Registry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-03-21'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: NIH Common Data Elements Repository
  formal_specification: https://cde.nlm.nih.gov/api
  has_relevant_organization:
  - B2AI_ORG:74
  is_open: true
  name: NIH CDE
  purpose_detail: A registry of standardized, precisely defined questions, paired
    with sets of allowable responses, used systematically across different sites,
    studies, or clinical trials to ensure consistent data collection.
  requires_registration: false
  url: https://cde.nlm.nih.gov/
- id: B2AI_STANDARD:860
  category: B2AI_STANDARD:Registry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-03-24'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Basic Register of Thesauri, Ontologies & Classifications
  is_open: true
  name: BARTOC
  purpose_detail: A database of Knowledge Organization Systems and KOS related registries.
  requires_registration: false
  url: https://bartoc.org/
- id: B2AI_STANDARD:861
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-03-24'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DuckDB
  formal_specification: https://github.com/duckdb/duckdb
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:9
  is_open: true
  name: DuckDB
  purpose_detail: A database platform designed for working with tabular data.
  requires_registration: false
  url: https://duckdb.org/
- id: B2AI_STANDARD:862
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-03-27'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Polars library
  formal_specification: https://github.com/pola-rs/polars/
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:8
  is_open: true
  name: Polars
  purpose_detail: Polars is a high-performance DataFrame library written in Rust with
    bindings for Python, Node.js, and R, designed as a fast alternative to pandas.
    The library features a multi-threaded query engine with lazy evaluation, query
    optimization, and streaming capabilities for processing larger-than-RAM datasets.
    Polars utilizes Apache Arrow columnar format for zero-copy data sharing and SIMD
    vectorization for cache-coherent algorithms. It supports both eager and lazy execution
    modes, with the lazy API enabling automatic query optimization and parallel execution
    across CPU cores. The library handles diverse data formats including CSV, JSON,
    Parquet, Delta Lake, AVRO, Excel, and direct database connections to MySQL, PostgreSQL,
    SQLite, and cloud storage systems. Polars provides an intuitive expression API
    for data manipulation operations while maintaining minimal dependencies and fast
    import times (70ms vs 520ms for pandas), making it suitable for data analysis,
    ETL pipelines, and analytical workloads requiring high performance.
  requires_registration: false
  url: https://www.pola.rs/
- id: B2AI_STANDARD:863
  category: B2AI_STANDARD:Registry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-03-27'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Linked Open Vocabularies
  formal_specification: https://github.com/pyvandenbussche/lov
  is_open: true
  name: LOV
  purpose_detail: A collection of searchable ontologies and vocabularies, spanning
    multiple fields.
  requires_registration: false
  url: https://lov.linkeddata.es/
- id: B2AI_STANDARD:864
  category: B2AI_STANDARD:Registry
  concerns_data_topic:
  - B2AI_TOPIC:4
  contribution_date: '2023-03-27'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: PhenX Toolkit
  is_open: true
  name: PhenX Toolkit
  purpose_detail: A catalog of recommended measurement protocols for biomedical research.
  requires_registration: false
  url: https://www.phenxtoolkit.org/
- id: B2AI_STANDARD:866
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:23
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Sequence Alignment Ontology
  is_open: true
  name: SALON
  publication: doi:10.1186/s12859-023-05190-7
  purpose_detail: An OWL2 ontology for representing and semantically annotating pairwise
    and multiple sequence alignments.
  requires_registration: false
  url: https://benhid.com/SALON/
- id: B2AI_STANDARD:867
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - diagnosticinstrument
  concerns_data_topic:
  - B2AI_TOPIC:4
  contribution_date: '2023-05-23'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Glasgow Coma Scale
  is_open: true
  name: GCS
  publication: doi:10.1016/s0140-6736(74)91639-0
  purpose_detail: 'A neurological assessment tool used to evaluate level of consciousness
    based on patient responses in three categories: eye-opening, verbal response,
    and motor response, with a higher score indicating a more favorable neurological
    status.'
  requires_registration: false
  url: https://www.glasgowcomascale.org/
- id: B2AI_STANDARD:868
  category: B2AI_STANDARD:DataStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-05-23'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Frictionless data standards
  formal_specification: https://github.com/frictionlessdata/specs
  is_open: true
  name: Frictionless
  purpose_detail: Frictionless data standards provide a comprehensive suite of lightweight,
    extensible specifications for describing datasets, data files, and tabular data
    to enhance FAIR (Findability, Accessibility, Interoperability, Reusability) principles.
    The core specifications include Data Package for dataset-level metadata and resource
    collections, Data Resource for individual file descriptions, and Table Schema
    for tabular data structure definition with field types, constraints, and relationships.
    These specifications combine to create specialized formats like Tabular Data Packages
    that integrate CSV/JSON data with JSON Schema-based metadata descriptors. The
    standards follow a "small pieces, loosely joined" philosophy, enabling individual
    components to be used independently or combined for complex data scenarios. They
    support cross-technology implementation with human-readable JSON metadata that
    facilitates machine processing, data validation, and automated discovery. The
    specifications enable data portability, version control, and collaborative data
    workflows while maintaining compatibility with existing data formats and tools
    in the data science ecosystem.
  requires_registration: false
  url: https://specs.frictionlessdata.io/
- id: B2AI_STANDARD:869
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:29
  contribution_date: '2023-06-20'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Epic Compass Rose module
  has_relevant_organization:
  - B2AI_ORG:105
  - B2AI_ORG:115
  is_open: false
  name: Compass Rose
  purpose_detail: Care coordination module focused on social determinant of health
    factors. Part of the Epic EHR platform.
  requires_registration: true
  url: https://www.epic.com/software/population-health
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:87
    category: B2AI:Application
    name: Population Health Analytics and Risk Stratification
    description: Compass Rose (Epic's population health tool) is used in AI applications
      for large-scale risk stratification, care gap identification, and population-level
      outcome prediction across Epic's extensive user base. Machine learning models
      leverage Compass Rose's aggregated clinical data, standardized quality measures,
      and longitudinal patient tracking to develop predictive algorithms for chronic
      disease management, preventive care optimization, and resource allocation. AI
      systems built on this platform can identify high-risk patient populations, predict
      hospital readmissions, and recommend targeted interventions at scale. The tool's
      integration with Epic's EHR ecosystem enables real-time AI inference during
      clinical encounters and automated outreach programs guided by ML-based risk
      scores.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:870
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-06-20'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Great Expectations platform
  formal_specification: https://github.com/great-expectations/great_expectations
  is_open: true
  name: GX
  purpose_detail: A platform for organizing, testing, and validating data.
  requires_registration: false
  url: https://greatexpectations.io/
- id: B2AI_STANDARD:871
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-06-20'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Pinecone vector database
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:54
  - B2AI_SUBSTRATE:55
  - B2AI_SUBSTRATE:9
  is_open: false
  name: Pinecone
  purpose_detail: A database platform built around creating vector representations
    of data. The basic implementation is a managed, cloud-native product, though there
    is a free tier.
  requires_registration: true
  url: https://www.pinecone.io/
- id: B2AI_STANDARD:872
  category: B2AI_STANDARD:DataStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-06-20'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Command Line Interface Guidelines
  is_open: true
  name: CLIG
  purpose_detail: An open-source guide to help with writing command-line programs,
    based on UNIX principles.
  requires_registration: false
  url: https://clig.dev/
- id: B2AI_STANDARD:873
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:13
  contribution_date: '2023-09-25'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: 0000-0001-5705-7831
  description: Minimum Information about a high-throughput nucleotide SEQuencing Experiment
  formal_specification: https://zenodo.org/record/5706412
  is_open: true
  name: MINSEQE
  purpose_detail: Five elements of experimental description considered essential when
    making sequencing data available.
  requires_registration: false
  url: https://www.fged.org/projects/minseqe/
- id: B2AI_STANDARD:874
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:18
  contribution_date: '2023-09-25'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: 0000-0001-5705-7831
  description: GGIR accelerometry package
  formal_specification: https://github.com/wadpac/GGIR
  is_open: true
  name: GGIR
  purpose_detail: An R package to process multi-day raw accelerometer data for physical
    activity and sleep research.
  requires_registration: false
  url: https://cran.r-project.org/web/packages/GGIR/vignettes/GGIR.html
- id: B2AI_STANDARD:875
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:18
  contribution_date: '2023-09-25'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: 0000-0001-5705-7831
  description: Metadata Concepts for Advancing the Use of Digital Health Technologies
    in Clinical Research
  is_open: true
  name: Badawy et al. 2019
  publication: doi:10.1159/000502951
  purpose_detail: A proposed metadata set for digital health studies.
  requires_registration: false
  url: https://figshare.com/articles/dataset/Supplementary_Material_for_Metadata_Concepts_for_Advancing_the_Use_of_Digital_Health_Technologies_in_Clinical_Research/9944303
- id: B2AI_STANDARD:876
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-09-25'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: 0000-0001-5705-7831
  description: GSCID/BRC Clinical Metadata Standard
  has_relevant_organization:
  - B2AI_ORG:118
  is_open: true
  name: GSCID/BRC CMS v1.5
  purpose_detail: A general standard for clinical metadata.
  requires_registration: false
  url: https://www.niaid.nih.gov/research/clinical-metadata-standard
- id: B2AI_STANDARD:877
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-09-25'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: 0000-0001-5705-7831
  description: Minimum information about clinical artificial intelligence modeling
  formal_specification: https://github.com/beaunorgeot/MI-CLAIM
  is_open: true
  name: MI-CLAIM
  publication: doi:10.1038/s41591-020-1041-y
  purpose_detail: MI-CLAIM (Minimum Information about CLinical AI Modeling) is a reporting
    standard and documentation checklist developed to address transparency and reproducibility
    challenges in clinical artificial intelligence research, published in Nature Medicine
    in 2020 by a multidisciplinary team of clinical and data scientists. MI-CLAIM
    serves two primary purposes - enabling direct assessment of clinical impact including
    fairness considerations, and allowing rapid replication of the technical design
    process for clinical AI studies. The standard provides a comprehensive checklist
    in MS Word table format covering essential reporting elements including study
    design and data characteristics (patient demographics, inclusion/exclusion criteria,
    data sources, temporal validation), model development details (feature engineering,
    architecture selection, hyperparameter tuning, training procedures), performance
    evaluation (metrics across demographic subgroups, confidence intervals, comparison
    to clinical standards), clinical implementation considerations (decision thresholds,
    interpretability mechanisms, failure modes), and ethical aspects (bias assessment,
    fairness metrics, regulatory status). The repository encourages community feedback
    through GitHub Issues to continuously improve the standard as the field evolves,
    promoting best practices for transparent, reproducible, and equitable clinical
    AI development and deployment across healthcare applications.
  requires_registration: false
  url: https://github.com/beaunorgeot/MI-CLAIM
  has_application:
  - id: B2AI_APP:88
    category: B2AI:Application
    name: Clinical AI Reporting Standards and Model Documentation
    description: MI-CLAIM (Minimum Information about Clinical Artificial Intelligence
      Modeling) checklist is used to standardize reporting of clinical AI studies,
      ensuring reproducibility, transparency, and appropriate evaluation of machine
      learning models in healthcare. Researchers leverage MI-CLAIM guidelines to document
      essential details about model development, validation approaches, and clinical
      context that enable others to assess reliability and reproduce findings. The
      standard supports automated model card generation, structured documentation
      for regulatory submissions, and systematic reviews of clinical AI literature
      by providing a consistent framework for reporting. MI-CLAIM compliance facilitates
      responsible AI development by ensuring key information about data provenance,
      model limitations, and intended use cases is explicitly documented.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:878
  category: B2AI_STANDARD:DataStandardOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-09-25'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: 0000-0001-5705-7831
  description: CSV on the Web
  formal_specification: https://w3c.github.io/csvw/syntax/
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:6
  is_open: true
  name: CSVW
  purpose_detail: A standard for describing and clarifying the content of CSV tables.
  requires_registration: false
  url: https://csvw.org/
- id: B2AI_STANDARD:879
  category: B2AI_STANDARD:DataStandardOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-09-25'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: 0000-0001-5705-7831
  description: BagIt file packaging format
  formal_specification: https://datatracker.ietf.org/doc/rfc8493/
  is_open: true
  name: BagIt
  publication: doi:10.17487/RFC8493
  purpose_detail: A set of hierarchical file layout conventions for storage and transfer
    of arbitrary digital content.
  requires_registration: false
  url: https://datatracker.ietf.org/doc/rfc8493/
- id: B2AI_STANDARD:880
  category: B2AI_STANDARD:DataStandardOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2024-11-02'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: 0000-0001-5705-7831
  description: Unity Catalog
  formal_specification: https://github.com/unitycatalog/unitycatalog
  is_open: true
  name: Unity Catalog
  purpose_detail: A universal catalog for data and AI. It includes a core set of APIs
    for tables, unstructured data, and AI assets.
  requires_registration: false
  url: https://www.unitycatalog.io/
  has_application:
  - id: B2AI_APP:89
    category: B2AI:Application
    name: Unified Data and AI Asset Governance
    description: Unity Catalog is used in biomedical AI for centralized governance
      of data assets, ML models, and AI artifacts across multi-cloud and hybrid healthcare
      IT environments. Healthcare organizations leverage Unity Catalog to implement
      fine-grained access controls for sensitive patient data used in model training,
      track lineage from raw clinical data through processed features to trained models,
      and ensure HIPAA compliance across distributed AI development teams. The catalog
      provides a single source of truth for data discovery, enables secure data sharing
      across research and clinical domains, and maintains comprehensive audit logs
      for regulatory compliance. Unity Catalog's integration with major ML platforms
      facilitates governed AI development where data scientists can access approved
      datasets while maintaining security and privacy requirements.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:881
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:36
  contribution_date: '2023-12-06'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Senselab package
  formal_specification: https://github.com/sensein/senselab
  is_open: true
  name: Senselab
  purpose_detail: A Python package for streamlining the processing and analysis of
    behavioral data, such as voice and speech patterns, with robust and reproducible
    methodologies.
  requires_registration: false
  url: https://sensein.group/senselab/
- id: B2AI_STANDARD:882
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:15
  contribution_date: '2023-12-06'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Checklist for Artificial Intelligence in Medical Imaging (CLAIM)
  is_open: true
  name: CLAIM
  publication: doi:10.1148/ryai.2020200029
  purpose_detail: CLAIM is modeled after the STARD guideline and has been extended
    to address applications of AI in medical imaging that include classification,
    image reconstruction, text analysis, and workflow optimization. It is intended
    to provide a framework for the development and validation of AI algorithms in
    medical imaging.
  requires_registration: false
  url: https://doi.org/10.1148/ryai.2020200029
- id: B2AI_STANDARD:883
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:15
  contribution_date: '2023-12-06'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Pydicom software package
  formal_specification: https://github.com/pydicom/pydicom
  is_open: true
  name: pydicom
  purpose_detail: Pydicom is a pure Python package that provides comprehensive tools
    for reading, modifying, and writing DICOM (Digital Imaging and Communications
    in Medicine) files, serving as the de facto standard library for medical imaging
    data manipulation in the Python ecosystem. As a foundational dependency for medical
    imaging AI research and clinical software development, pydicom implements the
    complex DICOM standard (PS3.5-PS3.10) without requiring external C libraries,
    making it portable across platforms and compatible with environments from laptops
    to high-performance computing clusters. The library provides both high-level abstractions
    for common workflows and low-level access to DICOM data elements, enabling researchers
    to extract pixel arrays, access metadata tags, handle diverse transfer syntaxes
    (including compressed formats like JPEG 2000, JPEG-LS, and RLE), and manipulate
    DICOM data structures programmatically. Pydicom's Dataset class represents DICOM
    information objects as Python dictionaries, allowing natural Pythonic access to
    attributes like patient demographics, imaging parameters, and clinical context
    using standard dictionary notation or attribute-style access with keyword lookups.
    The library handles multi-frame images, color spaces, overlay data, and private
    tags, while supporting both implicit and explicit value representations. For AI/ML
    workflows, pydicom seamlessly integrates with NumPy to convert pixel data into
    arrays suitable for deep learning frameworks, automatically applying rescale slopes,
    intercepts, and window/level transformations. It forms the data loading foundation
    for major medical imaging AI libraries including MONAI (Medical Open Network for
    AI), TorchIO, and Cornerstone.js, and is extensively used in research pipelines
    for tasks ranging from tumor segmentation to fracture detection to radiomics feature
    extraction. Pydicom supports batch processing of large DICOM datasets through
    memory-efficient streaming, enables conversion between DICOM and research formats
    like NIfTI, and provides utilities for anonymization and de-identification critical
    for privacy-compliant research. The library maintains compatibility with thousands
    of DICOM implementations from medical device manufacturers, handles institution-specific
    private tags, and supports DICOM-RT (radiotherapy) structures, DICOM-SR (structured
    reports), and DICOM-SEG (segmentation) objects increasingly used in clinical AI
    deployments. With over 1 million monthly downloads and adoption by virtually all
    Python-based medical imaging projects, pydicom has become indispensable infrastructure
    for translating medical images from clinical PACS systems into AI/ML pipelines
    and returning model predictions in formats compatible with clinical workflows
    and regulatory requirements.
  related_to:
  - B2AI_STANDARD:98
  requires_registration: false
  url: https://pydicom.github.io/
  has_application:
  - id: B2AI_APP:90
    category: B2AI:Application
    name: Medical Imaging Data Processing for Deep Learning
    description: pydicom is the essential Python library for AI researchers working
      with medical imaging data, enabling reading, writing, and manipulation of DICOM
      files in machine learning pipelines. Virtually all medical imaging AI research
      using Python leverages pydicom to extract pixel data and metadata from DICOM
      images, convert images to NumPy arrays for neural network input, and create
      DICOM-compliant outputs for clinical integration. The library handles diverse
      DICOM transfer syntaxes and encodings, enables efficient batch processing of
      large imaging datasets, and provides the foundation for medical imaging data
      loaders in PyTorch and TensorFlow. pydicom's ability to preserve clinical metadata
      during AI processing ensures that model outputs maintain appropriate associations
      with patient context and imaging parameters.
    used_in_bridge2ai: false
  - id: B2AI_APP:170
    category: B2AI:Application
    name: DICOM Loading and Model Output Encoding via Highdicom
    description: "Highdicom builds on pydicom to provide standardized encoding of
      image annotations and ML model outputs for pathology and radiology workflows.
      The library uses pydicom to read DICOM files and interpret image metadata and
      pixel data, passing pixel data as NumPy arrays to ML models for inference on
      CT and slide microscopy images. Model outputs (NumPy arrays) are encoded back
      into DICOM Structured Reporting (SR) and Segmentation (SEG) objects constructed
      from pydicom.Dataset instances, enabling storage through DICOMweb and visualization
      in clinical viewers (OHIF, Slim). Highdicom explicitly \"copies this metadata
      directly from the source images provided as evidence to the constructor\" using
      pydicom, ensuring proper propagation of patient, study, and specimen metadata
      to derived objects for dataset curation, model validation, and audit trails.
      The library exposes parsing methods that decode DICOM annotations into NumPy
      label maps for use as training targets, demonstrating bidirectional integration
      of pydicom-based DICOM I/O with PyTorch and TensorFlow ML frameworks."
    references:
    - https://doi.org/10.1007/s10278-022-00683-y
    used_in_bridge2ai: false
  - id: B2AI_APP:171
    category: B2AI:Application
    name: Real-time Radiology ML Pipelines with Niffler Metadata Extraction
    description: 'The Niffler DICOM framework for machine learning against real-time
      radiology images explicitly "uses the Pydicom library to extract metadata and
      process the DICOM images" at institutional scale (715 scanners, up to 350 GB/day).
      Niffler''s Python3 core runs a periodic extract_metadata thread that iterates
      series and extracts metadata (by default one image per series) into a metadata
      store indexed by PatientID/StudyInstanceUID/SeriesInstanceUID, enabling dataset
      curation, real-time feature collection, and operational analytics for downstream
      ML pipelines. The system converts received compressed DICOM to PNG using GDCM
      "for the ML pipelines to consume in a de-identified manner," with ML workloads
      running as Docker containers "on the images and metadata that it stores." Niffler
      demonstrates pydicom''s role in production-scale metadata extraction, format
      conversion, and continuous ingestion workflows supporting real-time ML applications
      such as IVC filter detection and segmentation pipelines.'
    references:
    - https://doi.org/10.1007/s10278-021-00491-w
    used_in_bridge2ai: false
  - id: B2AI_APP:172
    category: B2AI:Application
    name: DICOM Annotation Encoding and Decoding for ML Training
    description: 'Highdicom implements Python classes derived from pydicom.Dataset
      to encode ML model outputs (NumPy arrays) plus metadata into DICOM annotation
      objects and decode DICOM annotations (pydicom objects) into NumPy arrays used
      as training targets. The library provides validated constructors for DICOM Segmentation
      and Structured Reporting that handle "highly nested and interdependent structure"
      of annotation IODs, addressing limitations of pydicom''s low-level API which
      "has no concept of IODs" and requires manual attribute setting. Highdicom uses
      pydicom for coded vocabularies ("the SCT vocabulary built in to pydicom" via
      pydicom.sr.codedict.codes) and file loading (pydicom.dcmread), then constructs
      SR and SEG objects (hd.sr.Comprehensive3DSR.from_dataset, hd.seg.Segmentation.from_dataset)
      that expose methods to collect ROI polygons (get_planar_roi_measurement_groups)
      and produce NumPy label maps (seg_image.get_pixels_by_source_instance). This
      enables standardized integration of DICOM annotations with PyTorch and TensorFlow
      training/inference pipelines while maintaining clinical interoperability.'
    references:
    - https://doi.org/10.1007/s10278-022-00683-y
    used_in_bridge2ai: false
  - id: B2AI_APP:173
    category: B2AI:Application
    name: DICOM De-identification and Anonymization for AI Datasets
    description: Pydicom is used for metadata de-identification in DICOM anonymization
      pipelines preparing imaging datasets for AI research. A peer-reviewed pipeline
      implemented "a custom Python program utilizing the Pydicom library (v2.2.2)"
      to programmatically remove PHI from DICOM attributes while preserving required
      attributes for standard compliance, achieving successful metadata de-identification
      on all images in a dataset sampled from 37,866 prior de-identification requests
      (1,391 exam validation sample). The XNAT/MIDI-B deidentification pipeline explicitly
      names pydicom among Python libraries used for "deidentification of metadata"
      within broader workflows that combine metadata redaction with OCR-based pixel
      text removal for burned-in annotations. A knee MRI preprocessing study for deep
      learning lists "PyDicom and Simple ITK" as Python packages used for DICOM preprocessing
      alongside dataset anonymization steps. These applications demonstrate pydicom's
      role in institutional-scale anonymization workflows that strip/modify DICOM
      metadata attributes to prepare privacy-compliant datasets for ML development,
      often integrated with pixel-processing tools for comprehensive de-identification.
    references:
    - https://doi.org/10.1007/s10278-024-01098-7
    - https://doi.org/10.48550/arxiv.2504.20657
    - https://doi.org/10.26574/maedica.2024.19.3.526
    used_in_bridge2ai: false
- id: B2AI_STANDARD:884
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2025-02-16'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CSVY format
  formal_specification: https://github.com/leeper/csvy
  is_open: true
  name: CSVY
  purpose_detail: CSVY is a lightweight file format that enhances comma-separated
    values (CSV) files by embedding a YAML metadata header at the beginning of the
    file, enabling self-describing tabular datasets that combine the simplicity and
    ubiquity of CSV with structured metadata that documents data provenance, column
    definitions, licenses, and processing history. The format consists of a YAML
    front matter section delimited by triple-dash lines (---) followed by standard
    CSV data, allowing a single file to carry both human-readable and machine-readable
    documentation alongside the actual data values. The YAML header can specify field
    names, data types, units of measurement, missing value codes, column descriptions,
    dataset-level metadata like title, author, creation date, version, license terms,
    and citations, making CSVY particularly valuable for open science, data repositories,
    and reproducible research where data provenance and documentation are essential.
    Unlike traditional CSV files that rely on external documentation or README files
    to explain column meanings and data collection context, CSVY embeds this information
    directly within the data file itself, reducing the risk of metadata loss when
    files are shared, archived, or transferred between collaborators. The format maintains
    full backward compatibility with CSV parsers when they ignore the YAML header,
    while CSVY-aware tools can extract both the metadata and data, enabling applications
    to automatically validate data types, apply unit conversions, handle missing values
    according to documented conventions, and present contextual information to users.
    CSVY has gained adoption in scientific computing, environmental monitoring, survey
    research, and government open data initiatives where datasets require rich documentation
    but must remain accessible to users with basic spreadsheet tools. Programming
    libraries in R (csvy package), Python (pandas with yaml), and Julia provide native
    CSVY support, allowing researchers to read metadata-annotated tabular data with
    a single function call that populates data frames with appropriate column types
    and preserves documentation in object attributes. The format addresses a common
    pain point in data science workflows where CSV files are exchanged without adequate
    context about variable definitions, measurement protocols, or quality flags, often
    leading to misinterpretation or improper analysis. For machine learning applications,
    CSVY enables self-documenting training datasets where the header can specify feature
    engineering details, class labels, train/test splits, preprocessing steps, and
    benchmark results, facilitating reproducibility and proper citation of datasets
    used in published models. CSVY exemplifies the trend toward literate data formats
    that integrate documentation with data representation, making tabular datasets
    more FAIR (Findable, Accessible, Interoperable, Reusable) compliant while maintaining
    the practical simplicity that has made CSV a universal data exchange format for
    decades.
  requires_registration: false
  url: https://github.com/leeper/csvy
- id: B2AI_STANDARD:885
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:37
  contribution_date: '2025-02-16'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: openSMILE software
  formal_specification: https://github.com/audeering/opensmile
  has_relevant_organization:
  - B2AI_ORG:117
  is_open: true
  name: openSMILE
  purpose_detail: openSMILE is an open-source audio feature extraction toolkit.
  requires_registration: false
  url: https://www.audeering.com/research/opensmile/
  used_in_bridge2ai: true
- id: B2AI_STANDARD:886
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:36
  contribution_date: '2025-02-16'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Praat software
  formal_specification: https://github.com/praat/praat
  has_relevant_organization:
  - B2AI_ORG:117
  is_open: true
  name: Praat
  purpose_detail: Praat is software for working with voice data, including speech
    analysis, segmentation, and synthesis.
  related_to:
  - B2AI_STANDARD:887
  requires_registration: false
  url: https://www.fon.hum.uva.nl/praat/
  used_in_bridge2ai: true
- id: B2AI_STANDARD:887
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:36
  contribution_date: '2025-03-05'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Parselmouth software
  formal_specification: https://github.com/YannickJadoul/Parselmouth
  has_relevant_organization:
  - B2AI_ORG:117
  is_open: true
  name: Parselmouth
  purpose_detail: Parselmouth is a Python library for working with Praat software.
    Parselmouth directly accesses Praat's C/C++ code (which means the algorithms and
    their output are exactly the same as in Praat) and provides efficient access to
    the program's data, but also provides an interface that looks no different from
    any other Python library.
  related_to:
  - B2AI_STANDARD:886
  requires_registration: false
  url: https://parselmouth.readthedocs.io/en/stable/
  used_in_bridge2ai: true
- id: B2AI_STANDARD:888
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2025-03-13'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: BigTIFF format
  is_open: true
  name: BigTIFF
  purpose_detail: BigTIFF is an image format. It is a variant of the TIFF format that
    uses 64-bit offsets thereby supporting files up to 18,000 petabytes in size, vastly
    transcending TIFF's normal 4 GB limit. Since the format also supports all of the
    normal features and header tags of TIFF_6 and the extended metadata offered by
    GeoTIFF, it provides good service in the GIS domain, medical imaging, and other
    applications that employ large scanners or cameras.
  related_to:
  - B2AI_STANDARD:383
  requires_registration: false
  url: https://www.loc.gov/preservation/digital/formats/fdd/fdd000328.shtml
- id: B2AI_STANDARD:889
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:37
  contribution_date: '2025-03-13'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: TorchAudio library
  has_relevant_organization:
  - B2AI_ORG:117
  is_open: true
  name: torchaudio
  purpose_detail: TorchAudio is an official PyTorch domain library that provides
    building blocks for audio and speech processing in deep learning research and
    production applications, offering I/O operations, signal transforms, datasets,
    and pretrained models optimized for GPU acceleration and seamless integration
    with PyTorch's tensor operations and neural network modules. As part of the PyTorch
    ecosystem, torchaudio implements essential audio preprocessing primitives including
    waveform loading from diverse formats (WAV, MP3, FLAC, Vorbis, Opus), resampling,
    spectrogram computation (STFT, mel-scale, MFCC), audio augmentation (time/frequency
    masking, pitch shifting, noise injection), and feature extraction methods used
    in speech recognition, speaker identification, audio classification, and music
    information retrieval. The library provides unified access to standard audio datasets
    (LibriSpeech, VCTK, CommonVoice, YesNo) with consistent preprocessing pipelines,
    enabling reproducible benchmarking across research groups. TorchAudio includes
    reference implementations and pretrained weights for state-of-the-art models including
    Wav2Vec2, HuBERT, Conformer, Emformer, and Tacotron2, allowing researchers to
    fine-tune speech foundation models on domain-specific tasks or use them as feature
    extractors for downstream applications. The library's functional API separates
    stateless transforms from stateful modules, enabling flexible composition of audio
    processing pipelines that can be scripted with TorchScript for deployment in production
    environments without Python dependencies. TorchAudio supports both CPU and CUDA
    operations with automatic batching and differentiable transforms, allowing audio
    preprocessing to be included in end-to-end trainable pipelines where gradient
    flow through feature extraction improves model performance for tasks like source
    separation, speech enhancement, and audio generation. The library maintains compatibility
    with torchaudio.pipelines for high-level task-specific interfaces (automatic speech
    recognition, speaker verification, speech synthesis) that bundle models, preprocessing,
    and post-processing into single callable units suitable for inference workflows.
    TorchAudio is extensively used in industry and academia for applications ranging
    from voice assistants and podcast transcription to environmental sound classification,
    music genre recognition, acoustic event detection, and bioacoustic monitoring
    of wildlife vocalizations. For biomedical applications, torchaudio processes respiratory
    sounds (cough, wheeze, lung sounds), cardiac auscultation recordings, speech biomarkers
    for neurological disorders, and voice-based mental health assessment, enabling
    AI-driven diagnostics in telehealth and remote patient monitoring scenarios. The
    library's active development by Meta AI Research and the PyTorch Audio SIG ensures
    regular updates with new models, optimized kernels (Sox, FFmpeg, Kaldi-compatible
    implementations), and compatibility with the latest PyTorch features including
    torch.compile for accelerated inference and distributed training primitives for
    scaling audio model training across multiple GPUs and nodes.
  related_to:
  - B2AI_STANDARD:816
  requires_registration: false
  url: https://pytorch.org/audio/stable/index.html
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:91
    category: B2AI:Application
    name: Biomedical Audio Analysis and Speech-Based Diagnostics
    description: torchaudio is used in AI applications for processing and analyzing
      biomedical audio signals including speech patterns for neurological assessment,
      respiratory sounds for pulmonary diagnosis, and cardiac auscultation for automated
      heart disease detection. Deep learning models built with torchaudio process
      audio biomarkers such as cough sounds for COVID-19 screening, voice characteristics
      for Parkinson's disease detection, and lung sounds for pneumonia classification.
      The library's preprocessing capabilities, pretrained models, and integration
      with PyTorch enable researchers to develop audio-based AI diagnostics that can
      be deployed on mobile devices for remote patient monitoring, telehealth applications,
      and resource-limited settings where traditional diagnostic equipment is unavailable.
    used_in_bridge2ai: false
  - id: B2AI_APP:174
    category: B2AI:Application
    name: Self-Supervised Learning Pipelines for Speech Foundation Models
    description: 'TorchAudio provides pretrained self-supervised learning (SSL) pipelines
      and fine-tuning recipes for speech foundation models including Wav2Vec 2.0,
      HuBERT, XLS-R, and WavLM, enabling researchers to leverage large-scale pretrained
      encoders for downstream tasks. The library supplies "models and pre-trained
      pipelines for Wav2Vec 2.0, HuBERT, XLS-R, and WavLM" where "each pre-trained
      pipeline relies on the weights... and thus produces identical outputs" to upstream
      implementations, ensuring reproducibility when fine-tuning on domain-specific
      data. TorchAudio includes "end-to-end training recipes that allow for pre-training
      and fine-tuning HuBERT models from scratch" on LibriSpeech and custom datasets.
      A downstream spoken language understanding study explicitly "used the pretrained
      ASR models provided by TorchAudio," including WAV2VEC2_ASR_BASE_960H and HUBERT_ASR_LARGE
      checkpoints, to build joint-CTC SLU systems for intent classification and dialogue
      act recognition. The pretrained SSL encoders are designed for CTC fine-tuning
      and serve as feature extractors for tasks beyond ASR, demonstrating TorchAudio''s
      role as the primary distribution mechanism for PyTorch-compatible speech foundation
      models in research and production.'
    references:
    - https://doi.org/10.48550/arxiv.2310.17864
    - https://doi.org/10.1109/icassp49357.2023.10096795
    used_in_bridge2ai: false
  - id: B2AI_APP:175
    category: B2AI:Application
    name: ASR Model Training, Streaming Inference, and CTC Decoding
    description: 'TorchAudio implements complete automatic speech recognition training
      and inference pipelines including Conformer transducer, RNN-Transducer, and
      streaming Emformer architectures with production-ready decoders and forced alignment.
      The library provides "end-to-end training recipes" for Conformer and RNN-T,
      and introduces a "streaming-capable transformer-based acoustic model Emformer"
      that enables "real-time inference on CPU" for audio-visual ASR. For decoding,
      TorchAudio supplies a high-performance CTC beam search decoder supporting "both
      lexicon and lexicon-free decoding" with KenLM language model integration and
      custom neural network support, plus the "only publicly available CUDA-compatible
      CTC decoder" for GPU-accelerated inference. The library includes the "first
      publicly available GPU-based solution for computing forced alignments" via a
      "CTC-based forced alignment implementation" that supports both CPU and CUDA
      execution. StreamReader/StreamWriter APIs enable "chunked streaming of audio/video
      tensors from files and remote sources," supporting online ASR pipelines entirely
      in PyTorch with examples demonstrating how to "stream audio chunk by chunk from
      a remote video file." These components operationalize full ASR workflows from
      data loading through model training, streaming inference, beam search decoding,
      and timestamp alignment for subtitling and transcription applications.'
    references:
    - https://doi.org/10.48550/arxiv.2310.17864
    used_in_bridge2ai: false
  - id: B2AI_APP:176
    category: B2AI:Application
    name: Audio Preprocessing, Feature Extraction, and Data Augmentation
    description: 'TorchAudio''s functional, transform, and sox_effects modules provide
      comprehensive preprocessing and augmentation capabilities for ML training pipelines,
      with GPU-accelerated and TorchScript-compatible implementations suitable for
      production deployment. The library implements core preprocessing operations including
      "Spectrogram/InverseSpectrogram, mel-frequency cepstrum coefficients (MFCC),
      spectral centroid, phase vocoder and Griffin-Lim, resampling, and other filters"
      as torch.nn.Module subclasses that integrate seamlessly with PyTorch model pipelines
      via torch.nn.Sequential. The sox_effects module exposes "58 different sound
      effects, such as resampling, pitch shift" for data augmentation, with all transforms
      "fully torchscriptable" to enable compilation for inference optimization. TorchAudio
      emphasizes "GPU compute capability, automatic differentiation, and production
      readiness," allowing audio preprocessing to be included in end-to-end trainable
      pipelines where gradient flow through feature extraction can improve model performance.
      The library serves as "the foundational audio layer for downstream PyTorch toolkits
      (ESPnet, SpeechBrain, NeMo, fairseq)" by providing standardized, well-tested
      building blocks that avoid reimplementation of basic operations across projects.
      Transforms support runtime and inference acceleration through JIT compilation
      and "support [for] TorchScript and PyTorch-native quantization and leverage
      PyTorch 2.0''s Accelerated Transformers... to speed up training and inference,"
      with benchmarking on AWS p4d A100 hardware demonstrating performance parity
      or improvements over reference implementations.'
    references:
    - https://doi.org/10.48550/arxiv.2310.17864
    - https://doi.org/10.1109/icassp43922.2022.9747236
    used_in_bridge2ai: false
  - id: B2AI_APP:177
    category: B2AI:Application
    name: Multichannel Speech Enhancement and MVDR Beamforming
    description: 'TorchAudio 2.1 introduces mask-based MVDR (Minimum Variance Distortionless
      Response) beamforming and multichannel speech enhancement components that integrate
      with STFT/ISTFT processing and can be trained end-to-end with ASR models. The
      library implements a "mask-based MVDR beamforming model" where "the MVDR module
      is applied to the masks and multi-channel spectrum to produce the beamforming
      weights" and the "enhanced waveform is derived via inverse STFT," with training
      using "Ci-SDR as the loss function." The enhancement pipeline is designed for
      joint optimization with downstream tasks, as demonstrated by experiments "jointly
      leveraging TorchAudio''s mask-based MVDR beamforming model, Conformer transducer
      model, and TorchAudio-Squim" to perform "multi-channel speech enhancement, ASR,
      and speech quality assessment all within TorchAudio." The MVDR module supports
      integration with both Conformer transducer and Wav2Vec-2.0-based ASR models,
      enabling researchers to build end-to-end pipelines from multichannel microphone
      array input through beamforming, enhancement, recognition, and quality assessment.
      Experiments validate "the efficacy of TorchAudio''s MVDR module" for improving
      ASR performance in noisy and reverberant environments, with all components implemented
      as differentiable PyTorch modules that support GPU acceleration and gradient-based
      optimization of enhancement parameters jointly with acoustic model weights.'
    references:
    - https://doi.org/10.48550/arxiv.2310.17864
    used_in_bridge2ai: false
  - id: B2AI_APP:178
    category: B2AI:Application
    name: Reference-Less Speech Quality Assessment and Evaluation
    description: TorchAudio-Squim provides reference-less speech quality assessment
      tools that estimate perceptual and objective metrics including STOI (Short-Time
      Objective Intelligibility), PESQ (Perceptual Evaluation of Speech Quality),
      Si-SDR (Scale-Invariant Signal-to-Distortion Ratio), and MOS (Mean Opinion Score)
      without requiring clean reference signals, enabling integrated evaluation and
      training-time feedback within speech processing pipelines. The tool allows researchers
      to "estimate STOI, PESQ, and Si-SDR" during model training and inference, supporting
      quality-aware optimization and automated assessment of enhancement, separation,
      and synthesis outputs. TorchAudio-Squim is designed for integration with other
      library components, as demonstrated in experiments combining MVDR beamforming,
      Conformer/Wav2Vec-2.0 ASR, and "TorchAudio-Squim for quality assessment," providing
      a unified framework for training and evaluating multichannel enhancement pipelines
      with perceptual quality feedback. The reference-less approach is particularly
      valuable for real-world applications where clean reference signals are unavailable,
      such as live enhancement of telephony, conferencing, or broadcast audio, and
      for self-supervised or unsupervised training regimes where quality metrics guide
      model optimization without paired clean/noisy training data. By providing standardized,
      GPU-accelerated quality estimators as PyTorch modules, TorchAudio-Squim enables
      researchers to incorporate perceptual loss functions and quality monitoring
      directly into training loops, supporting development of enhancement and generation
      models optimized for human-perceived speech quality.
    references:
    - https://doi.org/10.48550/arxiv.2310.17864
    used_in_bridge2ai: false
- id: B2AI_STANDARD:890
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2025-03-13'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: RadElement Common Data Elements (CDEs) for Radiology
  is_open: true
  name: RadElement
  purpose_detail: RadElement is an open-source initiative that defines and curates
    standardized Common Data Elements (CDEs) for radiology research, providing machine-readable
    structured reporting templates that enable consistent capture of imaging findings,
    measurements, and clinical assessments across institutions, PACS vendors, and
    research consortia. Developed by the Radiological Society of North America (RSNA)
    with collaborative input from specialty societies, RadElement addresses the fragmentation
    of radiology reporting by creating consensus-driven data element sets organized
    into modular templates for specific clinical scenarios (lung nodule characterization,
    liver lesion reporting, prostate MRI, breast density assessment, stroke imaging
    protocols) that specify precise terminology, permissible values, measurement methods,
    and reporting conventions. Each CDE includes metadata describing the element's
    definition, units of measurement, data type, value sets drawn from controlled
    vocabularies (RadLex, SNOMED CT, LOINC), cardinality constraints, and provenance
    documentation linking to source guidelines from ACR, RSNA, and disease-specific
    professional societies. RadElement templates are expressed in JSON Schema format
    and distributed through the RadElement.org web platform, which provides search,
    browsing, and API access to the CDE library, enabling software developers to integrate
    standardized reporting into radiology information systems, structured reporting
    tools, and clinical decision support applications. By standardizing the representation
    of imaging observations, RadElement facilitates secondary use of radiology data
    for clinical research, quality improvement, and AI model development, as structured
    reports can be automatically parsed and aggregated across large patient cohorts
    without manual chart review or natural language processing of free-text reports.
    For AI/ML applications, RadElement provides ground-truth annotations and training
    labels derived from structured clinical reports, enables automated quality control
    of annotated imaging datasets, and supports development of AI-generated structured
    reports that populate CDE fields with model predictions for radiologist review.
    The initiative has released CDE sets for high-impact imaging applications including
    lung cancer screening (Lung-RADS), liver tumor response assessment (LI-RADS),
    prostate cancer staging (PI-RADS), traumatic brain injury, COVID-19 pneumonia,
    and pediatric appendicitis, with ongoing expansion driven by community contributions
    and feedback from pilot implementations. RadElement supports FAIR data principles
    by making imaging research data more Findable (standardized metadata schema),
    Accessible (open web platform and APIs), Interoperable (controlled vocabularies
    and FHIR mappings), and Reusable (versioned templates with clear licensing), addressing
    a critical need for semantic interoperability in multi-site imaging studies and
    federated learning scenarios where consistent data representation is essential
    for combining datasets. The platform tracks CDE usage statistics, version history,
    and validator tools that check report conformance to template specifications,
    promoting reproducibility and data quality in radiology research while reducing
    the burden on investigators to develop ad hoc data dictionaries for each new study.
  requires_registration: false
  url: https://www.radelement.org/
- id: B2AI_STANDARD:891
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2025-03-13'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FAIR Genomes Semantic Model
  formal_specification: https://github.com/fairgenomes/fairgenomes-semantic-model/blob/main/fair-genomes.yml
  is_open: true
  name: FAIRGenomes
  purpose_detail: FAIR Genomes is a semantic metadata model that provides a comprehensive,
    extensible framework for describing human genomic and phenotypic datasets in accordance
    with FAIR principles (Findable, Accessible, Interoperable, Reusable), addressing
    the critical challenge of standardizing metadata across diverse genomics studies,
    biobanks, and clinical genomics programs to enable data discovery, integration,
    and responsible sharing. The model defines a structured vocabulary organized into
    modular components covering study metadata (project descriptions, consent codes,
    data access policies), subject/patient attributes (demographics, ancestry, clinical
    diagnoses using ontology terms from HPO, MONDO, ORDO), sample characteristics
    (tissue types, preservation methods, collection protocols referencing UBERON and
    OBI), sequencing information (platforms, library preparation, coverage metrics,
    file formats), and variant/molecular data annotations (variant calling pipelines,
    reference genomes, quality metrics). FAIR Genomes is expressed as a machine-readable
    YAML schema (fair-genomes.yml) that specifies element definitions, data types,
    cardinality constraints, recommended ontologies and controlled vocabularies for
    each field, and mappings to existing standards including GA4GH Phenopackets, European
    Genome-phenome Archive (EGA) metadata schema, and Dublin Core terms. The model
    emphasizes use of established biomedical ontologies (Human Phenotype Ontology
    for phenotypes, NCIT for cancer types, LOINC for lab measurements, SNOMED CT
    for clinical concepts) to ensure semantic interoperability and enable automated
    reasoning, cross-dataset queries, and ontology-based data harmonization essential
    for meta-analyses and federated research networks. FAIR Genomes supports capture
    of consent and data use restrictions through structured encoding of informed consent
    types, data use ontology (DUO) terms, and return-of-results policies, facilitating
    compliant data sharing and automated assessment of whether a dataset can be used
    for a proposed research purpose. The schema is designed for extensibility through
    defined extension mechanisms that allow research communities to add domain-specific
    modules (rare disease registries, cancer genomics, population cohorts) while maintaining
    core compatibility, and it provides versioning and provenance tracking to document
    schema evolution over time. Implementation tooling includes Python and R libraries
    for validating metadata against the FAIR Genomes schema, converting between FAIR
    Genomes and other metadata formats (VCF headers, PED files, Phenopackets JSON),
    and generating metadata submission templates for data producers. For clinical
    genomics and precision medicine applications, FAIR Genomes enables integration
    of research genomic datasets with electronic health records, supports clinical
    variant interpretation workflows that require patient phenotype and family history
    context, and facilitates development of machine learning models that leverage
    rich phenotypic annotations for variant pathogenicity prediction and genomic risk
    scoring. The model has been adopted by European genomic data infrastructures including
    the European Rare Disease Research Infrastructure (RD-Connect), Solve-RD consortium,
    and national genome initiatives implementing federated analysis frameworks where
    standardized metadata is essential for distributed query execution without moving
    sensitive patient data across institutional boundaries.
  requires_registration: false
  url: https://github.com/fairgenomes/fairgenomes-semantic-model
- id: B2AI_STANDARD:893
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:15
  contribution_date: '2025-03-13'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Recommended Metadata for Biological Images (REMBI)
  is_open: true
  name: REMBI
  publication: doi:10.1038/s41592-021-01166-8
  purpose_detail: REMBI (Recommended Metadata for Biological Images) is a community-developed
    specification that defines minimum metadata standards for biological imaging experiments
    across light microscopy, electron microscopy, and other imaging modalities, addressing
    the critical need for reproducible, interpretable, and reusable image data in
    biomedical research by establishing consensus guidelines for documenting experimental
    context, sample preparation, imaging acquisition parameters, and image processing
    workflows. Published in Nature Methods (2021) through a collaborative effort involving
    microscopy core facilities, imaging scientists, data managers, and standards organizations,
    REMBI provides a hierarchical framework organized into four tiers of metadata
    granularityfrom essential contextual information required for all imaging datasets
    to detailed technical parameters needed for advanced image analysis and computational
    modeling. The specification covers study-level metadata (biological question,
    experimental design, organism/cell line identifiers linked to ontologies), specimen
    preparation details (fixation methods, labeling strategies, mounting media), imaging
    acquisition parameters specific to each modality (objective magnification, numerical
    aperture, excitation/emission wavelengths, laser power, detector settings, pixel
    size, bit depth, acquisition speed), and image processing provenance (software
    versions, algorithms applied, parameter settings, visualization adjustments like
    brightness/contrast). REMBI emphasizes use of controlled vocabularies and ontologies
    including the Biological Imaging Methods Ontology (FBbi), Cell Ontology (CL),
    Uberon anatomy ontology, and Chemical Entities of Biological Interest (ChEBI)
    for fluorophores and dyes, ensuring semantic consistency and enabling automated
    metadata validation and cross-dataset queries. The guidelines are intentionally
    flexible to accommodate the diversity of biological imagingfrom high-content
    screening and live-cell time-lapse to super-resolution and correlative light-electron
    microscopywhile maintaining core requirements that ensure datasets can be properly
    cited, understood without direct contact with original investigators, and computationally
    processed using documented protocols. REMBI is designed for integration with emerging
    imaging data repositories (Image Data Resource, BioImage Archive, Cell Image
    Library), OMERO image management systems, and Electronic Lab Notebook platforms,
    with implementation efforts producing metadata capture templates, validation tools,
    and export utilities that embed REMBI-compliant metadata into OME-TIFF and OME-ZARR
    file formats. For AI/ML applications in biological imaging, REMBI metadata enables
    proper documentation of training datasets, supports domain adaptation by revealing
    systematic differences in imaging protocols across labs, facilitates development
    of imaging biomarkers by linking acquisition parameters to biological outcomes,
    and ensures that deep learning models trained on specific microscopy modalities
    can be appropriately applied to new datasets or flagged when imaging conditions
    fall outside training distribution. The standard has been adopted by major microscopy
    facilities as a framework for user training and data management policies, is referenced
    in data management plans for imaging-intensive grants, and serves as a foundation
    for specialized extensions addressing modality-specific requirements in cryo-electron
    microscopy, spatial transcriptomics imaging, and high-throughput phenotypic screening.
  requires_registration: false
  url: https://docs.google.com/spreadsheets/d/1Ck1NeLp-ZN4eMGdNYo2nV6KLEdSfN6oQBKnnWU6Npeo/edit?gid=1023506919#gid=1023506919
- id: B2AI_STANDARD:894
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - dataregistry
  concerns_data_topic:
  - B2AI_TOPIC:5
  - B2AI_TOPIC:23
  description: PRoteomics IDEntifications Database
  has_relevant_organization:
  - B2AI_ORG:116
  is_open: true
  name: PRIDE
  publication: doi:10.1093/nar/gkae1011
  purpose_detail: The PRIDE PRoteomics IDEntifications (PRIDE) Archive database is
    a centralized, standards compliant, public data repository for mass spectrometry
    proteomics data, including protein and peptide identifications and the corresponding
    expression values, post-translational modifications and supporting mass spectra
    evidence (both as raw data and peak list files). PRIDE is a core member in the
    ProteomeXchange (PX) consortium, which provides a standardised way for submitting
    mass spectrometry based proteomics data to public-domain repositories. Datasets
    are submitted to ProteomeXchange via PRIDE and are handled by expert bio-curators.
    All PRIDE public datasets can also be searched in ProteomeCentral, the portal
    for all ProteomeXchange datasets.
  requires_registration: false
  url: https://www.ebi.ac.uk/pride/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:92
    category: B2AI:Application
    name: Proteomics Data Mining and Peptide Identification
    description: PRIDE (PRoteomics IDEntifications) database is used in AI applications
      for training deep learning models on mass spectrometry proteomics data, enabling
      improved peptide identification, protein quantification, and post-translational
      modification prediction. Machine learning systems leverage PRIDE's extensive
      repository of annotated spectra to develop neural networks for de novo peptide
      sequencing, spectral quality assessment, and cross-linking analysis. AI models
      trained on PRIDE data improve upon traditional database search algorithms by
      learning complex patterns in fragmentation spectra, enabling identification
      of novel peptides, non-canonical modifications, and proteoforms. The database's
      standardized mzML and mzIdentML formats facilitate reproducible AI research
      in clinical proteomics, biomarker discovery, and precision medicine applications.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:895
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  - implementation_maturity_production
  - standards_process_maturity_final
  concerns_data_topic:
  - B2AI_TOPIC:5
  description: American Standard Code for Information Interchange ASCII File Format
    Guidelines for Earth Science Data
  formal_specification: https://www.earthdata.nasa.gov/s3fs-public/imported/ESDS-RFC-027v1.1.pdf
  has_relevant_organization:
  - B2AI_ORG:114
  is_open: true
  name: ASCII File Format Guidelines for Earth Science Data
  purpose_detail: The ASCII File Format Guidelines for Earth Science Data is a NASA
    Earth Science Data and Information System (ESDIS) standard (ESDS-RFC-027v1.1)
    that establishes conventions for structuring plain-text ASCII files to store,
    distribute, and archive Earth science measurements, model outputs, and derived
    products in a human-readable, platform-independent format that remains accessible
    across decades of data preservation while supporting automated ingestion into
    scientific analysis workflows. Developed to address the proliferation of inconsistent
    ASCII file formats across NASA missions and data centers, the guidelines specify
    best practices for file organization including mandatory header sections containing
    metadata (data provider, creation date, geographic coverage, temporal range, variable
    definitions, units of measurement, missing value codes, quality flags), delimited
    data records (comma-separated, tab-separated, or fixed-width columns), and documentation
    of coordinate systems and datum references essential for geospatial data interpretation.
    The standard emphasizes self-describing files where all information necessary
    to interpret the data is embedded within the file itself, reducing dependency
    on external documentation that can be lost or separated from data files during
    long-term archival or inter-institutional data sharing. ASCII format conventions
    include character encoding specifications (7-bit ASCII for maximum compatibility),
    line terminator guidelines (handling platform differences between Unix, Windows,
    Mac), numeric precision recommendations, date/time representation standards (ISO
    8601 preferred), and strategies for representing hierarchical or multidimensional
    data within the inherent flatness of text files. The guidelines address practical
    considerations for Earth science data including representation of gridded datasets
    (latitude/longitude arrays, satellite swath data, model output grids), time series
    from ground stations and buoys, trajectory data from aircraft and autonomous vehicles,
    and derived products like statistical summaries and aggregated climatologies.
    While acknowledging that binary formats (NetCDF, HDF5, GeoTIFF) are more efficient
    for large-scale scientific datasets, the ASCII guidelines serve critical use cases
    including rapid prototyping and exploratory analysis, data exchange with legacy
    systems and non-specialist users, archival copies for human readability and format
    migration resilience, and compliance with data publication requirements from journals
    and funding agencies mandating human-readable supplementary data files. For interdisciplinary
    research connecting Earth science with biomedical and public health applications,
    ASCII representations of environmental exposure variables (air quality indices,
    temperature extremes, precipitation patterns, vegetation indices, land use changes)
    enable integration with epidemiological datasets often stored in simple text or
    CSV formats, supporting studies of climate-health relationships, environmental
    justice assessments, and One Health research connecting human, animal, and ecosystem
    health. The standard has influenced broader scientific data practices including
    the design of CSV dialect specifications, contributed to development of metadata
    standards for observational data, and serves as a reference for NASA Earth science
    data producers preparing data for distribution through the Earthdata system and
    long-term preservation in NASA's Earth Observing System Data and Information System
    (EOSDIS) archive.
  requires_registration: false
  url: https://www.earthdata.nasa.gov/about/esdis/esco/standards-practices/ascii-file-format-guidelines-earth-science-data
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:93
    category: B2AI:Application
    name: Environmental and Climate Health Data Integration
    description: ASCII File Format Guidelines for Earth Science Data are used in AI
      applications that integrate environmental and climate data with biomedical datasets
      to model health impacts of environmental change, predict disease patterns related
      to climate factors, and develop early warning systems for climate-sensitive
      health outcomes. Machine learning models leverage standardized ASCII representations
      of temperature, precipitation, air quality, and other environmental variables
      to train on relationships between environmental exposures and health outcomes
      such as vector-borne disease transmission, heat-related illness, respiratory
      disease exacerbation, and mental health impacts. The standardized format enables
      AI systems to integrate NASA Earth observations with clinical and epidemiological
      data for One Health applications bridging human, animal, and environmental health.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:896
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - datamodel
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:14
  description: Geographic Informations Systems Toolchain
  has_relevant_organization:
  - B2AI_ORG:76
  - B2AI_ORG:115
  is_open: true
  name: GIS Toolchain
  purpose_detail: The GIS toolchain consists of extensions to the OMOP schema, extensions
    to the OMOP Vocabulary, and GIS-specific software for acquiring and working with
    geospatial data. Together, these enable researchers to use health-related attributes
    of the regions where patients live in OHDSI study cohort definitions. For example,
    you can use the GIS toolchain to define cohorts that include regional data on
    exposure to toxicants or social deprivation along with EHR data on relevant health
    outcomes. The toolchain also includes informatics resources that support integration
    with the main OHDSI tool stack (HADES) and integration with externally supported
    solutions for geocoding and for finding and deriving relevant data sources from
    catalogs of available data sources. Importantly, the toolchain allows integrated
    analysis of geospatial and EHR data without sharing any sensitive patient location
    data.
  requires_registration: false
  url: https://ohdsi.github.io/GIS/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:94
    category: B2AI:Application
    name: Geospatial Health Analytics and Environmental Exposure Modeling
    description: GIS Toolchain (from OHDSI) is used in AI applications for integrating
      geographic information with clinical data to train models that account for environmental
      exposures, social determinants of health, and spatial disease patterns. Machine
      learning systems leverage geospatial features derived from this toolchain to
      develop prediction models for infectious disease spread, environmental health
      impacts, cancer cluster detection, and health equity analysis. AI applications
      combine geocoded patient addresses with environmental data layers (air quality,
      greenspace, food access) to create location-aware risk models that inform population
      health interventions and resource allocation. The toolchain's integration with
      OMOP CDM enables spatiotemporal analysis at scale across observational health
      databases.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:897
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  - standards_process_maturity_development
  - implementation_maturity_pilot
  concerns_data_topic:
  - B2AI_TOPIC:4
  description: Medical Imaging Common Data Model
  has_relevant_organization:
  - B2AI_ORG:76
  - B2AI_ORG:115
  is_open: true
  name: MI-CDM
  publication: doi:10.1007/s10278-024-00982-6
  purpose_detail: The rapid growth of artificial intelligence (AI) and deep learning
    techniques require access to large inter-institutional cohorts of data to enable
    the development of robust models, e.g., targeting the identification of disease
    biomarkers and quantifying disease progression and treatment efficacy. The Observational
    Medical Outcomes Partnership Common Data Model (OMOP CDM) has been designed to
    accommodate a harmonized representation of observational healthcare data. This
    study proposes the Medical Imaging CDM (MI-CDM) extension, adding two new tables
    and two vocabularies to the OMOP CDM to address the structural and semantic requirements
    to support imaging research. The tables provide the capabilities of linking DICOM
    data sources as well as tracking the provenance of imaging features derived from
    those images. The implementation of the extension enables phenotype definitions
    using imaging features and expanding standardized computable imaging biomarkers.
    This proposal offers a comprehensive and unified approach for conducting imaging
    research and outcome studies utilizing imaging features.
  related_to:
  - B2AI_STANDARD:98
  - B2AI_STANDARD:243
  requires_registration: false
  used_in_bridge2ai: true
  url: https://doi.org/10.1007/s10278-024-00982-6
  has_application:
  - id: B2AI_APP:95
    category: B2AI:Application
    name: Medical Imaging AI Data Standardization and Multi-Site Studies
    description: MI-CDM (Medical Imaging Common Data Model) is used in AI applications
      to standardize heterogeneous imaging metadata across institutions, enabling
      training of robust deep learning models on diverse multi-site imaging datasets.
      The common data model facilitates AI development by providing consistent representation
      of imaging protocols, scanner parameters, patient demographics, and clinical
      annotations across different PACS systems and imaging centers. Machine learning
      systems leverage MI-CDM to perform federated learning on distributed imaging
      data without raw image transfer, enable systematic bias detection across sites,
      and develop models that generalize across different scanner manufacturers and
      acquisition protocols. The standardization is critical for regulatory-grade
      AI where model validation requires diverse, well-characterized datasets.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:898
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:21
  description: Cytoscape Exchange
  has_relevant_organization:
  - B2AI_ORG:116
  is_open: true
  name: CX
  purpose_detail: Cytoscape Exchange (CX) format is a network exchange format, designed
    as a flexible structure for transmission of networks. It is designed for flexibility,
    modularity, and extensibility, and as a message payload in common REST protocols.
    It is not intended as an in-memory data model for use in applications.
  requires_registration: false
  url: https://cytoscape.org/cx/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:96
    category: B2AI:Application
    name: Network Biology and Graph Neural Networks
    description: CX (Cytoscape Exchange) format is used in AI applications for representing
      biological networks that serve as input to graph neural networks for tasks such
      as protein function prediction, drug-target interaction prediction, and disease
      gene prioritization. Machine learning models leverage CX's standardized representation
      of nodes, edges, and network attributes to train graph-based deep learning architectures
      that capture complex biological relationships. AI systems use CX-encoded networks
      for multi-omics data integration, pathway analysis, and systems biology modeling
      where network topology and node features inform predictions. The format enables
      sharing of network models across platforms and reproducible AI research in network
      medicine, synthetic biology, and computational drug discovery.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:899
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  description: Anndata h5ad format
  formal_specification: https://github.com/scverse/anndata/blob/main/src/anndata/_io/h5ad.py
  has_relevant_organization:
  - B2AI_ORG:116
  is_open: true
  name: h5ad
  purpose_detail: h5ad is an HDF5-based file format specifically designed for storing
    annotated data matrices produced by the AnnData Python package, serving as the
    standard data structure for single-cell genomics analysis and computational biology
    workflows in the scverse ecosystem. The format efficiently stores high-dimensional
    biological datasets by representing data as an AnnData object containing a primary
    data matrix (typically cell-by-gene expression counts stored as sparse or dense
    arrays), cell-level annotations (obs), gene-level annotations (var), unstructured
    annotations (uns), pairwise cell relationships (obsp), pairwise gene relationships
    (varp), and multi-dimensional arrays indexed by observations or variables (obsm,
    varm) such as dimensionality reduction embeddings (PCA, UMAP, t-SNE) or batch-effect
    correction results. Built on the HDF5 hierarchical data format, h5ad enables partial
    I/O operations allowing selective reading and writing of specific dataset components
    without loading entire files into memory, critical for datasets with millions
    of cells that exceed typical RAM capacity. The format supports both sparse matrix
    representations (CSR, CSC formats from scipy.sparse) for efficient storage of
    scRNA-seq data where most gene expression values are zero, and dense arrays for
    continuous measurements or embeddings. h5ad files preserve complete analytical
    provenance by storing preprocessing parameters, quality control metrics, normalization
    methods, clustering results, differential expression statistics, and trajectory
    inference outcomes alongside raw and processed data in a single self-contained
    file. Integration with the broader scverse ecosystem (Scanpy for analysis, scVI
    for deep generative modeling, CellRank for trajectory inference, Squidpy for spatial
    transcriptomics) ensures interoperability across tools, while compatibility with
    R through the anndata R package and Seurat conversion utilities enables cross-platform
    workflows. The format is extensively used in machine learning applications for
    cell type annotation, batch correction with deep learning models (scVI, scANVI),
    gene regulatory network inference, spatial transcriptomics analysis, and multi-modal
    data integration combining scRNA-seq with scATAC-seq, CITE-seq antibody counts,
    or spatial imaging data. h5ad has become the de facto standard for single-cell
    data sharing in repositories like the Human Cell Atlas, CZI CELLxGENE, and Single
    Cell Portal, enabling reproducible research and collaborative data analysis across
    the single-cell genomics community.
  requires_registration: false
  url: https://anndata.readthedocs.io/en/stable/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:97
    category: B2AI:Application
    name: Single-Cell Genomics and Deep Learning Integration
    description: h5ad format (HDF5-based AnnData) is the standard file format for
      AI applications in single-cell genomics, enabling efficient storage and access
      of large-scale single-cell RNA-seq datasets for training deep learning models.
      AI systems leverage h5ad's structured representation of cell-by-gene expression
      matrices, cell metadata, and dimensionality reduction embeddings to perform
      cell type classification, trajectory inference, gene regulatory network inference,
      and batch effect correction. Deep learning frameworks integrate seamlessly with
      h5ad through the AnnData API, supporting applications in cancer cell identification,
      developmental biology, immune profiling, and drug response prediction at single-cell
      resolution. The format's efficient sparse matrix storage enables training on
      datasets with millions of cells.
    used_in_bridge2ai: false
  - id: B2AI_APP:168
    category: B2AI:Application
    name: scvi-tools Probabilistic Modeling and Integration with AnnData
    references:
    - https://doi.org/10.1038/s41587-021-01206-w
    description: The scvi-tools Python library uses h5ad/AnnData as the primary data
      interface for probabilistic deep learning models in single-cell genomics, with
      models consuming AnnData objects directly for training and inference. The framework
      provides setup_anndata to register tensor locations from AnnData objects, enabling
      models like scVI, scANVI, and totalVI to operate on h5ad files via code such
      as "SCVI.setup_anndata(adata, batch_key='donor')" followed by "model = SCVI(adata)".
      The AnnDataLoader component "reads data from the AnnData object and automatically
      structures it for training or for downstream analysis with the trained model,"
      supporting joint multimodal processing of transcriptomics and proteomics data.
      Batch integration is enabled through batch_key parameters, and model-specific
      requirements (UMI counts, labels_key) are handled through the AnnData interface.
      The reimplementation of methods like Stereoscope and CellAssign within scvi-tools
      allows these models to "be used from Jupyter notebooks and with AnnData objects,"
      with visualization via Scanpy taking less than 20 lines of code. This demonstrates
      h5ad as the standard format for training variational autoencoders, performing
      batch correction, cell type annotation, and multimodal integration in the single-cell
      ecosystem.
    used_in_bridge2ai: false
  - id: B2AI_APP:169
    category: B2AI:Application
    name: PyTorch Integration for ML Training on Single-Cell Data
    references:
    - https://doi.org/10.21105/joss.04371
    - https://doi.org/10.1101/2021.12.16.473007
    description: AnnData provides native PyTorch integration for machine learning
      workflows through the AnnLoader DataLoader interface and AnnCollection for lazy
      concatenation of larger-than-memory datasets, enabling seamless training of
      deep learning models on h5ad files. The format uses "language-independent, hierarchical
      on-disk formats HDF5 and zarr (commonly .h5ad)" with a one-to-one mapping between
      in-memory and on-disk structures, allowing compressed sparse matrices (CSR,
      CSC) to be stored as data, indices, and indptr arrays while tabular data is
      stored columnar. This design enables partial I/O operations where models can
      read specific data components without loading entire files into memory, critical
      for training on million-cell datasets. The PyTorch integration via AnnLoader
      and AnnCollection allows h5ad-backed objects to serve directly as model input,
      supporting training workflows that combine numpy-centered tools (scikit-learn,
      UMAP) with PyTorch-based deep learning. Major consortia including the Human
      Cell Atlas and HuBMAP distribute single-cell datasets through h5ad format, and
      the ecosystem explicitly includes scvi-tools for deep generative modeling, demonstrating
      h5ad as the interchange format connecting data repositories, preprocessing workflows,
      and ML model training in single-cell genomics.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:900
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Clinical Data Interchange Standards Consortium (CDISC) Dataset-JavaScript
    Object Notation
  formal_specification: https://www.cdisc.org/standards/data-exchange/dataset-json/dataset-json-v1-1
  is_open: true
  name: Dataset-JSON
  purpose_detail: CDISC Dataset-JSON is a JSON-based schema specifically designed
    for exchanging tabular datasets in clinical studies. It is based on CDISC Dataset-JSON
    version 1.0 with enhancements, including smaller file sizes, additional metadata,
    and simpler processing. The format supports file and Application Programming Interface
    based data exchange, is widely supported across technologies, and can link to
    Define-XML for additional metadata. Dataset-JSON has the potential to replace
    Statistical Analysis System (SAS) version 5 XPORT Transport Format (XPT) for submission
    of electronic study data to regulatory agencies.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:15
  url: https://www.cdisc.org/standards/data-exchange/dataset-json
- id: B2AI_STANDARD:901
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: European Data Format Plus
  has_relevant_organization:
  - B2AI_ORG:115
  is_open: true
  name: EDF+
  publication: doi:10.1016/S1388-2457(03)00123-8
  purpose_detail: An extension of the European Data Format (EDF) that maintains EDF
    compatibility while adding capability to store annotations, events, and stimuli
    alongside the signal data. It also allows for storing interrupted (non-contiguous)
    recordings in a single file. EDF+ can save most EEG, PSG, ECG, EMG, and Evoked
    Potential data that cannot be saved into common hospital information systems.
    The Persyst universal EEG reader supports this format.
  related_to:
  - B2AI_STANDARD:105
  requires_registration: false
  url: https://www.edfplus.info/specs/edfplus.html
  used_in_bridge2ai: true
- id: B2AI_STANDARD:902
  category: B2AI_STANDARD:DataStandard
  collection:
  - datamodel
  - standards_process_maturity_draft
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Archival Resource Key Persistent Identifiers
  formal_specification: https://datatracker.ietf.org/doc/draft-kunze-ark/
  has_relevant_organization:
  - B2AI_ORG:116
  is_open: true
  name: ARK PIDs
  purpose_detail: Archival Resource Key (ARK) identifiers are persistent URLs designed
    to support long-term access to information objects. ARKs can identify digital
    objects (documents, databases, images, software), physical objects (books, artifacts),
    living beings, and intangible objects (concepts, services). ARKs are characterized
    by their internal "ark:" label, their NAAN (Name Assigning Authority Number) identifying
    the naming organization, and features like metadata access through inflections
    (adding ? to the URL). They are free to create and use, with no fees to assign
    or use ARKs, and can be hosted on any web server or through the global N2T.net
    resolver.
  requires_registration: false
  url: https://arks.org/about/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:98
    category: B2AI:Application
    name: Persistent Dataset Identification for ML Reproducibility
    description: ARK (Archival Resource Key) persistent identifiers are used in AI
      applications to create stable, long-term references to training datasets, model
      artifacts, and research outputs, ensuring reproducibility and provenance tracking
      in machine learning research. AI systems leverage ARK identifiers to maintain
      citations to specific versions of datasets used in model training, enabling
      verification of results and compliance with data governance policies. The identifier
      scheme's flexibility supports both fine-grained object identification (individual
      data files) and collections (entire datasets), which is essential for documenting
      complex ML pipelines. ARK PIDs facilitate data sharing in federated AI research
      while maintaining clear attribution and enabling auditable access logs for sensitive
      biomedical data.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:903
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:7
  - B2AI_TOPIC:4
  contribution_date: '2025-05-29'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Clinical Dataset Structure
  formal_specification: https://github.com/AI-READI/cds-specification
  has_relevant_organization:
  - B2AI_ORG:114
  is_open: true
  name: CDS
  publication: doi:10.5281/zenodo.10867040
  purpose_detail: The Clinical Dataset Structure (CDS) is a standardized way to organize
    and describe clinical research datasets to make them readily interoperable and
    easily reusable by humans and machines. It addresses the challenge of integrating
    multiple data modalities from clinical studies by providing a simple, intuitive
    file and directory structure. CDS organizes data by datatype at the root level,
    with each datatype directory structured according to applicable standards or a
    recommended hierarchy of modality/device/participant directories. The standard
    includes specifications for metadata files that document the dataset content,
    structure, and participant information, optimizing datasets for AI-readiness and
    secondary analysis.
  requires_registration: false
  url: https://cds-specification.readthedocs.io/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:99
    category: B2AI:Application
    name: Clinical Decision Support Rule Integration and ML Hybridization
    description: CDS (Clinical Decision Support) specification is used in AI applications
      to integrate machine learning models with traditional rule-based clinical decision
      support systems, enabling hybrid AI approaches that combine interpretable rules
      with data-driven predictions. AI systems leverage CDS Hooks and standardized
      interfaces to deploy ML models as FHIR-based services that provide real-time
      recommendations during clinical workflows, such as drug-drug interaction checking
      augmented with personalized risk predictions, or sepsis alerts that combine
      guideline-based criteria with neural network early warning scores. The specification
      enables AI applications to surface predictions within EHR user interfaces at
      appropriate decision points, while maintaining auditability through standardized
      request/response formats and provenance metadata.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:904
  category: B2AI_STANDARD:DataStandard
  collection:
  - datamodel
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2025-05-29'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: A collaborative, community-developed schema for structured data on
    the Internet
  formal_specification: https://github.com/schemaorg/schemaorg
  has_relevant_organization:
  - B2AI_ORG:116
  is_open: true
  name: Schema.org
  purpose_detail: Schema.org is a collaborative, community activity founded by Google,
    Microsoft, Yahoo, and Yandex with a mission to create, maintain, and promote schemas
    for structured data on the Internet. It provides a common vocabulary that webmasters
    can use to mark up their pages in ways that can be understood by major search
    engines and other applications. The Schema.org vocabulary consists of a set of
    types (e.g., Person, Event, Organization), properties (e.g., name, location, startDate),
    and relationships that can be used with many different encoding formats including
    RDFa, Microdata, and JSON-LD. As of 2024, over 45 million web domains use Schema.org
    markup with over 450 billion Schema.org objects. The vocabulary is continuously
    evolving through an open community process managed by the W3C Schema.org Community
    Group.
  requires_registration: false
  url: https://schema.org/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:100
    category: B2AI:Application
    name: Biomedical Knowledge Extraction and Semantic Search
    description: Schema.org vocabularies, particularly the biomedical extensions (BioSchemas),
      are used in AI applications for automated knowledge extraction from web resources,
      semantic annotation of datasets, and training large language models on structured
      biomedical information. AI systems leverage Schema.org markup to extract structured
      data about proteins, genes, diseases, clinical trials, and medical conditions
      from web pages, enabling automated knowledge base construction and question-answering
      systems. The vocabulary supports AI-driven dataset discovery, metadata standardization
      for machine learning pipelines, and training of biomedical language models that
      understand relationships between biological entities. Schema.org's widespread
      adoption makes it valuable for web-scale biomedical data mining and federated
      search applications.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:905
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:28
  contribution_date: '2025-05-29'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Thermo Fisher RAW mass spectrometry data format
  has_relevant_organization:
  - B2AI_ORG:116
  is_open: false
  name: Thermo Fisher RAW
  purpose_detail: The Thermo Fisher RAW format is a proprietary file format designed
    for storing mass spectrometry data generated by Thermo Fisher Scientific instruments.
    It contains detailed information about mass spectra, chromatograms, instrument
    parameters, and metadata from experimental runs. The format supports multiple
    mass spectrometry techniques and is accessed through Thermo's software tools like
    MSFileReader or more recent RawFileReader. While it's proprietary, various conversion
    tools allow transformation to open formats like mzML or mzXML for broader compatibility
    with third-party analysis software. The format is widely used in proteomics, metabolomics,
    and other mass spectrometry-based research applications where preserving the complete
    experimental context is essential for data interpretation and analysis.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:123
  url: https://www.thermofisher.com/us/en/home/industrial/mass-spectrometry.html
  used_in_bridge2ai: true
- id: B2AI_STANDARD:906
  category: B2AI_STANDARD:TrainingProgram
  concerns_data_topic:
  - B2AI_TOPIC:4
  - B2AI_TOPIC:52
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: 'Tutorial: Developing and Evaluating Your Extract, Transform, Load
    (ETL) Process to the OMOP CDM'
  has_relevant_organization:
  - B2AI_ORG:76
  is_open: true
  name: OHDSI 2024 Global Symposium ETL Tutorial
  purpose_detail: In this video tutorial, students will learn about the tools and
    practices developed by the OHDSI community to support the journey to establish
    and maintain an ETL to standardize your data to OMOP CDM and enable standardized
    evidence generation across a data network.
  requires_registration: false
  url: https://www.youtube.com/watch?v=H69dC7f-edQ
