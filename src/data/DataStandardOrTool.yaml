data_standardortools_collection:
- id: B2AI_STANDARD:1
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: .ACE format
  formal_specification: https://web.archive.org/web/20100609072313/http://bcr.musc.edu/manuals/CONSED.txt
  is_open: true
  name: .ACE format
  purpose_detail: The ACE file format is a specification for storing data about genomic
    contigs. The original ACE format was developed for use with Consed, a program
    for viewing, editing, and finishing DNA sequence assemblies. ACE files are generated
    by various assembly programs, including Phrap, CAP3, Newbler, Arachne, AMOS (sequence
    assembly) (more specifically Minimo) and Tigr Assembler v2.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/ACE_(genomic_file_format)
- id: B2AI_STANDARD:2
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - policy
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: 2023 NIH Data Management and Sharing Policy
  is_open: true
  name: DMS
  purpose_detail: NIH has issued the Data Management and Sharing (DMS) policy (effective
    January 25, 2023) to promote the sharing of scientific data. Sharing scientific
    data accelerates biomedical research discovery, in part, by enabling validation
    of research results, providing accessibility to high-value datasets, and promoting
    data reuse for future research studies. Under the DMS policy, NIH expects that
    investigators and institutions do the following. Plan and budget for the managing
    and sharing of data, Submit a DMS plan for review when applying for funding, Comply
    with the approved DMS plan.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:67
  url: https://sharing.nih.gov/data-management-and-sharing-policy/about-data-management-and-sharing-policy/data-management-and-sharing-policy-overview
  used_in_bridge2ai: true
- id: B2AI_STANDARD:3
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Access to Biological Collections Data Schema
  is_open: true
  name: ABCD
  purpose_detail: The Access to Biological Collections Data (ABCD) Schema is an evolving
    comprehensive standard for the access to and exchange of data about specimens
    and observations (a.k.a. primary biodiversity data). The ABCD Schema attempts
    to be comprehensive and highly structured, supporting data from a wide variety
    of databases. It is compatible with several existing data standards. Parallel
    structures exist so that either (or both) atomised data and free-text can be accommodated.
    Version 1.2 is currently in use with the GBIF (Global Biodiversity Information
    Facility) and BioCASE (Biological Collection Access Service for Europe) networks.
    Apart from the GBIF and BioCASE networks, the potential for the application of
    ABCD extends to internal networks, or in-house legacy data access (e.g. datasets
    from external sources that shall not be converted and integrated into an institution's
    own data, but be kept separately, though easily accessible). By defining relations
    between terms, ABCD is a step towards an ontology for biological collections.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:93
  url: https://abcd.tdwg.org/
  has_application:
  - id: B2AI_APP:1
    category: B2AI:Application
    name: DiSSCo Digital Specimen Architecture AI-Assisted Curation
    description: The Digital Specimen Architecture uses FAIR Digital Object type descriptions
      based on TDWG standards including ABCD. This enables registered AI services
      to automatically discover digital specimens, execute allowed actions, and attach
      machine-readable annotations such as automated extraction from images, relation
      creation for knowledge graphs, and standardization or correction. ABCD defines
      the object structure and attributes in the FDO, allowing AI services to operate
      uniformly across collections and propagate outputs to aggregators such as GBIF
      and GeoCASe.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.3897/biss.7.112678
  - id: B2AI_APP:102
    category: B2AI:Application
    name: Hespi Computer Vision and OCR Pipeline for Herbarium Sheets
    description: Hespi integrates object detection, OCR and handwriting recognition,
      and a multimodal LLM for post-processing and authority control in herbarium
      digitization. The pipeline explicitly situates digitization within community
      standards, citing ABCD alongside Darwin Core. Extracted fields from labels and
      sheet components are mapped to ABCD and DwC fields to produce interoperable
      records consumable by downstream AI and analytics systems.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.48550/arxiv.2410.08740
  - id: B2AI_APP:103
    category: B2AI:Application
    name: ML-Ready Benchmark Datasets via ABCD Standardization
    description: Work on improving transcribed digital specimen data highlights ABCD
      and Darwin Core as the target schema for interoperable outputs and schema-based
      annotation systems such as AnnoSys. This ecosystem underpins ML-ready benchmark
      datasets for herbarium images and semi-automated workflows for data cleaning
      and georeferencing, facilitating training and evaluation of computer vision
      and NLP models while keeping outputs in ABCD and DwC formats for data exchange.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1093/database/baz129
- id: B2AI_STANDARD:4
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: AGP format
  is_open: true
  name: AGP
  purpose_detail: AGP format describes the assembly of a larger sequence object from
    smaller objects. The large object can be a contig, a scaffold (supercontig), or
    a chromosome. Each line (row) of the AGP file describes a different piece of the
    object, and has the column entries defined below. Extended comments follow. It
    does not serve for either a description of how sequence reads were assembled,
    or a description of the alignments between components used to construct a larger
    object. Not all of the information in proprietary assembly files can be represented
    in the AGP format. It is also not for recording the spans of features like repeats
    or genes.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:120
  url: https://www.ncbi.nlm.nih.gov/assembly/agp/AGP_Specification/
- id: B2AI_STANDARD:5
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:3
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Analytical Information Markup Language
  is_open: true
  name: AnIML
  purpose_detail: The Analytical Information Markup Language (AnIML) is the emerging
    ASTM XML standard for analytical chemistry data. It is currently in pre-release
    form. It is a combination of a highly flexible core schema that defines XML tagging
    for any kind of analytical information; A set of technique definition documents.
    These XML files, one per analytical technique, apply tight constraints to the
    flexible core and in turn are defined by the Technique Schema; Extensions to Technique
    Definitions are possible to accommodate vendor- and institution-specific data
    fields. Mission Statement Our goal is to serve as the open-source development
    platform for a new XML standard for Analytical Chemistry Information. The project
    is a collaborative effort between many groups and individuals and is sanctioned
    by the ASTM subcommittee E13.15. http://animl.cvs.sourceforge.net/viewvc/animl/schema/animl-core.xsd
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:8
  url: https://www.animl.org/
- id: B2AI_STANDARD:6
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Animal Research Reporting In Vivo Experiments
  is_open: true
  name: ARRIVE
  publication: doi:10.1371/journal.pbio.3000411
  purpose_detail: Guidelines intended to improve the reporting of animal experiments.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:121
  url: https://arriveguidelines.org/
- id: B2AI_STANDARD:7
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Annotated ECG standard
  formal_specification: https://www.hl7.org/implement/standards/product_brief.cfm?product_id=70
  is_open: true
  name: aECG
  purpose_detail: Provides a common means of electronically storing both the ECG wave
    form and associated annotations.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: https://www.hl7.org/implement/standards/product_brief.cfm?product_id=70
- id: B2AI_STANDARD:8
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:9
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Annotation and Image Markup schema
  formal_specification: https://github.com/NCIP/annotation-and-image-markup
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: AIM
  purpose_detail: 'The Annotation and Image Markup (AIM) project is the first initiative
    to propose and create a standard means of adding information and knowledge to
    medical images in a clinical environment, enabling automatic searching of image
    content. AIM provides a comprehensive solution to imaging challenges including
    the lack of agreed-upon syntax for annotation and markup, standardized semantics
    for annotations, and common formats for annotations and markup. The AIM Model
    captures descriptive information for images with user-generated graphical symbols
    into a single common information source. The project includes multiple components:
    the AIM Template Service (a web service for uploading and downloading AIM templates),
    the AIM Template Builder (a Java application for creating templates with well-defined
    questions and answer choices), and reference implementations like AIM on ClearCanvas
    Workstation. AIM captures results in terms of image regions of interest, semantic
    descriptions, inferences, calculations, and quantitative features derived by computer
    programs. It is interoperable with DICOM-SR and HL7-CDA standards while providing
    unique advantages through an explicit semantic model of imaging results.'
  related_to:
  - B2AI_STANDARD:98
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:71
  url: https://github.com/NCIP/annotation-and-image-markup
  has_application:
  - id: B2AI_APP:2
    category: B2AI:Application
    name: PACS-to-AIM Conversion for ML-Ready Label Generation
    description: A deployed workflow converts proprietary vendor DICOM presentation
      state annotations from commercial PACS systems into AIM XML using the AIM API,
      enabling standardization of legacy radiologist annotations for machine learning.
      A Python module matches lesions across longitudinal studies via 3D coordinates,
      producing AIM files that are imported into ePAD where lesions are linked over
      time and quantitative metrics are computed. This AIM conversion pipeline unlocks
      historical radiologist annotations from PACS for supervised training, radiomics
      pipelines, and generation of high-quality labeled datasets for deep learning,
      while ensuring interoperability and enabling large-scale analysis across institutions.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1007/s10278-019-00191-6
  - id: B2AI_APP:104
    category: B2AI:Application
    name: ePAD AIM-Based Radiomics and ML Data Platform
    description: ePAD stores annotations and quantitative imaging features in AIM
      XML and exposes RESTful web services allowing plugins and external tools to
      retrieve AIM annotations and associated images for downstream analysis. Plugins
      compute biomarkers and save results back into AIM format, while integrations
      with external pipelines such as QIFP and pyRadiomics enable feature extraction
      with outputs persisted in AIM. This AIM-centric architecture facilitates standardized
      training data management, radiomic feature extraction, and programmatic access
      for machine learning workflows, cohort analyses, and interoperable analytics
      environments for quantitative imaging research.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.18383/j.tom.2018.00055
- id: B2AI_STANDARD:9
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ANSI/CTA Standard - The Use of Artificial Intelligence in Health Care
    Trustworthiness
  is_open: false
  name: ANSI/CTA-2090
  purpose_detail: This standard outlines key principles for ensuring trustworthiness
    in AI applications within healthcare, focusing on human trust, technical reliability,
    and regulatory compliance. It provides a framework for evaluating AI systems in
    clinical settings, emphasizing transparency, accountability, and ethical considerations.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:4
  - B2AI_ORG:122
  url: https://shop.cta.tech/products/cta-2090
  has_application:
  - id: B2AI_APP:3
    category: B2AI:Application
    name: Pre-Development ML Scoping and Data Provenance Documentation
    description: CTA-2090 recommends concrete developer actions for AI/ML workflows
      including listing potential use cases before development to properly scope algorithm
      functionality and limits, documenting whether datasets are raw or pre-processed
      and what preprocessing was performed, and understanding how original data were
      collected to identify potential biases. These practices operationalize the standard's
      emphasis on clear intended use, data lineage, traceability, and documentation
      requirements for trustworthy ML pipelines in healthcare, ensuring teams establish
      proper foundations during planning and data preparation stages.
    used_in_bridge2ai: false
    references:
    - https://www.fdli.org/wp-content/uploads/2023/01/9-Ross.pdf
  - id: B2AI_APP:105
    category: B2AI:Application
    name: Bias Assessment and Post-Deployment Monitoring
    description: CTA-2090 guidance is applied through explicit testing for racial
      and other biases including testing on vulnerable populations, contractual diversity
      and representation benchmarks for training data, and inventories to review,
      screen, retrain, and prevent bias during development. Post-deployment operationalization
      includes ongoing real-world testing after approval and continuous review of
      algorithm results during clinical use. These practices implement the standard's
      expectations for bias identification, mitigation, representative data, lifecycle
      monitoring, and real-world performance auditing in healthcare AI systems.
    used_in_bridge2ai: false
    references:
    - https://www.fdli.org/wp-content/uploads/2023/01/9-Ross.pdf
  - id: B2AI_APP:106
    category: B2AI:Application
    name: Z-Inspection Stakeholder Co-Design and Trust Assessment
    description: Healthcare AI systems apply CTA-2090 trustworthiness principles through
      Z-Inspection, an ethically aligned co-design methodology involving interdisciplinary
      stakeholders to examine ethical, technical, medical, and legal implications
      during development and deployment. Assessments uncover dataset bias risks, deployment
      shortcomings such as accent-related accuracy degradation, and protocol effects
      on ML output accuracy. Recommended practices include building explainability
      into models, documenting how decisions are generated, and conducting real-world
      validation that translates the standard's transparency, fairness, and reproducibility
      dimensions into concrete governance and evaluation activities.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.48550/arxiv.2206.09887
- id: B2AI_STANDARD:10
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Applied Biosystems sequence read binary format file
  is_open: false
  name: AB1
  purpose_detail: A binary version of raw DNA sequence reads from Applied Biosystems
    sequencing analysis software. Also known as ABIF.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:123
  url: https://www.thermofisher.com/us/en/home/life-science/sequencing/sanger-sequencing.html
- id: B2AI_STANDARD:11
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Applied Biosystems sequence read binary format file
  formal_specification: https://tools.thermofisher.com/content/sfs/manuals/4346366_DNA_Sequenc_Analysis_5_1_UG.pdf
  is_open: false
  name: ABI
  purpose_detail: A binary version of raw DNA sequence reads from Applied Biosystems
    sequencing analysis software.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:123
  url: https://tools.thermofisher.com/content/sfs/manuals/4346366_DNA_Sequenc_Analysis_5_1_UG.pdf
- id: B2AI_STANDARD:12
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ARB software binary alignment format
  is_open: true
  name: ARB
  publication: doi:10.1093/nar/gkh293
  purpose_detail: A binary alignment format used by the ARB package.
  requires_registration: false
  url: http://www.arb-home.de/documentation.html
- id: B2AI_STANDARD:13
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Archetype Definition Language
  formal_specification: https://specifications.openehr.org/releases/AM/latest/ADL1.4.html
  is_open: true
  name: ADL
  purpose_detail: ADL (Archetype Definition Language) provides a formal, human-readable
    syntax for expressing constraint-based models of clinical information structures.
    It enables the definition of reusable archetypes that constrain generic reference
    models (such as openEHR's) to represent specific clinical concepts like blood
    pressure measurements or problem lists. The language consists of cADL (constraint
    ADL) for structural definitions, dADL (data ADL) for metadata and terminology
    bindings, and an assertion language for business rules. ADL archetypes support
    multi-lingual terminology, specialization hierarchies, and versioning, making
    them suitable for creating maintainable, semantically interoperable health information
    systems. Tools exist for authoring, compiling, and validating ADL archetypes,
    with XML exchange formats also supported. The syntax is designed to be accessible
    to both clinical domain experts and technical implementers.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:79
  url: https://specifications.openehr.org/releases/AM/latest/ADL1.4.html
- id: B2AI_STANDARD:14
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Argonaut Data Query Implementation Guide
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: StructureDefinition-argo-careplan
  purpose_detail: Specifications for sharing single sets of patient care plans. Based
    on FHIR R2.
  related_to:
  - B2AI_STANDARD:109
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: http://www.fhir.org/guides/argonaut/r2/StructureDefinition-argo-careplan.html
- id: B2AI_STANDARD:15
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:16
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Article Minimum Information Standard
  is_open: true
  name: AMIS
  purpose_detail: The curation process is significantly slowed down by missing information
    in the articles analyzed. The identity of the clones used to generate ISH probes
    and the precise sequences tested in reporter assays constituted the most frequent
    omissions. To help authors ensure in the future that necessary information is
    present in their article, the Article Minimum Information Standard (AMIS) guidelines
    have been defined. The guideline describes the mandatory (and useful) information
    that should be mentioned in literature articles to facilitate the curation process.
    These guidelines extend the minimal information defined by the MISFISHIE format
    (Deutsch at al. 2008, Nature Biotechnology).
  requires_registration: false
  url: https://www.aniseed.fr/aniseed/default/submit_data?module=aniseed&action=default:submit_data#tab-4
- id: B2AI_STANDARD:16
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Axt Alignment Format
  is_open: true
  name: Axt
  purpose_detail: The Axt (Alignment eXTended) format is a simple text-based format
    developed by UCSC Genome Browser for storing pairwise DNA sequence alignments
    between two species or sequences. Each alignment block in an Axt file consists
    of four lines - a summary line containing alignment number, chromosome names,
    alignment start and end positions for both sequences, strand information, and
    alignment score, followed by the aligned sequence from the first genome, the aligned
    sequence from the second genome, and a blank separator line. The format uses zero-based
    coordinates for the first sequence and supports alignments on either strand, with
    sequences from the negative strand reverse-complemented. Axt files are particularly
    useful for representing whole-genome alignments between species (such as human-mouse
    or human-chimp comparisons) generated by alignment tools like BLASTZ or LASTZ.
    The format's straightforward structure makes it easy to parse programmatically
    and convert to other formats. UCSC provides utilities including axtToMaf for converting
    to MAF (Multiple Alignment Format), axtChain for chaining together alignments,
    and axtBest for selecting the best alignment for each position. While Axt efficiently
    represents pairwise alignments, it has been largely superseded by MAF and chain/net
    formats for more complex multi-way alignments and hierarchical alignment representations
    in comparative genomics workflows.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:119
  url: https://genome.ucsc.edu/goldenPath/help/axt.html
- id: B2AI_STANDARD:17
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: BAM indexing format file
  is_open: true
  name: BAI
  purpose_detail: A file containing the index for a Binary Alignment Map (BAM) file.
  requires_registration: false
  url: https://www.ncbi.nlm.nih.gov/tools/gbench/tutorial6/
- id: B2AI_STANDARD:18
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: BEDgraph format
  is_open: true
  name: BEDgraph
  purpose_detail: The bedGraph format allows display of continuous-valued data in
    track format.
  related_to:
  - B2AI_STANDARD:19
  - B2AI_STANDARD:20
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:119
  url: http://genome.ucsc.edu/goldenPath/help/bedgraph.html
- id: B2AI_STANDARD:19
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Big Browser Extensible Data Format
  is_open: true
  name: bigBed
  purpose_detail: The bigBed format is an indexed binary format for storing genome
    annotation data, developed by UCSC Genome Browser as a high-performance alternative
    to text-based BED files for large datasets. BigBed files store annotation items
    representing either simple genomic features or linked collections of exons, maintaining
    BED semantics while enabling efficient region-specific data transfer. Files are
    created using the bedToBigBed utility which converts sorted BED files into compressed
    binary format with built-in indexing, requiring chromosome sizes files and optionally
    AutoSql (.as) format definitions for describing standard and custom fields. The
    format supports BED3 through BED12 plus additional user-defined fields, with features
    including itemRgb color specification, extra searchable indices via the -extraIndex
    parameter for track hub item searches, and trackDb settings for mouseOver labels,
    field filtering, and URL transformations. Only the portions of bigBed files needed
    for the currently displayed chromosomal region are transferred to the browser,
    dramatically improving performance compared to full BED file loading. The format
    supports web-accessible hosting via HTTP, HTTPS, or FTP with sparse file caching,
    and includes utilities (bigBedToBed, bigBedInfo, bigBedSummary) for extracting
    and querying data. BigBed files are widely used in UCSC Genome Browser custom
    tracks, track hubs for consortia data sharing, and integrated with genome analysis
    workflows requiring scalable annotation storage and rapid genomic region queries.
  related_to:
  - B2AI_STANDARD:18
  - B2AI_STANDARD:20
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:119
  url: https://genome.ucsc.edu/goldenPath/help/bigBed.html
- id: B2AI_STANDARD:20
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Big Wiggle Format
  is_open: true
  name: bigWig
  purpose_detail: The bigWig format is an indexed binary file format developed by
    UCSC for efficient storage and visualization of dense, continuous genomic data
    displayed as graphs in genome browsers. Created from wiggle (wig) or bedGraph
    files using the wigToBigWig or bedGraphToBigWig utilities, bigWig files enable
    rapid data access by transferring only the portions needed to display specific
    genomic regions, rather than entire datasets. This sparse file caching mechanism
    provides dramatically faster performance than text-based formats when working
    with large-scale datasets such as ChIP-seq, RNA-seq coverage, methylation levels,
    or conservation scores. The format supports various visualization options including
    customizable graph types (bar or points), scaling parameters, smoothing windows,
    logarithmic transformations, and color schemes. BigWig files can also display
    sequence logos using the dynseq feature, which scales nucleotide characters by
    base-resolution scores. The format includes utilities for data extraction (bigWigToBedGraph,
    bigWigToWig, bigWigSummary, bigWigAverageOverBed) and file inspection (bigWigInfo),
    making it suitable for both visualization and downstream computational analysis.
  related_to:
  - B2AI_STANDARD:18
  - B2AI_STANDARD:19
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:119
  url: https://genome.ucsc.edu/goldenPath/help/bigWig.html
- id: B2AI_STANDARD:21
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Binary Alignment Map / Compressed Reference-oriented Alignment Map
  has_relevant_organization:
  - B2AI_ORG:117
  - B2AI_ORG:116
  is_open: true
  name: BAM/CRAM
  purpose_detail: BAM (Binary Alignment/Map) and CRAM (Compressed Reference-oriented
    Alignment Map) are binary formats for storing DNA sequence alignments to reference
    genomes, maintained by the GA4GH Large Scale Genomics work stream. BAM is the
    binary equivalent of the text-based SAM (Sequence Alignment/Map) format, providing
    efficient storage and retrieval of aligned sequencing reads with quality scores,
    alignment positions, CIGAR strings, and optional tags defined in the SAMtags specification.
    CRAM (currently version 3.x) achieves superior compression ratios by storing differences
    from a reference sequence rather than full sequence data, using custom compression
    codecs detailed in the CRAMcodecs specification. Both formats support indexing
    (BAI for BAM, CRAI for CRAM, and CSI as a more scalable successor) enabling rapid
    random access to genomic regions. The formats are widely supported by genomics
    toolkits including samtools, htslib, htsjdk, and GATK, with standard operations
    for sorting, merging, filtering, and format conversion. CRAM offers significant
    storage savings particularly important for large-scale projects, while maintaining
    full compatibility with SAM/BAM workflows. Both formats support the htsget protocol
    for parallel streaming access and can be wrapped with crypt4gh encryption for
    secure data sharing.
  related_to:
  - B2AI_STANDARD:22
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:34
  url: https://samtools.github.io/hts-specs/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:5
    category: B2AI:Application
    name: DeepVariant Deep Learning Variant Calling
    description: DeepVariant uses BAM/CRAM alignment files as input to train convolutional
      neural networks that classify candidate genomic variants with high accuracy
      across multiple sequencing technologies. The model transforms aligned reads
      into image-like pileup tensors that capture base qualities, mapping qualities,
      and strand information, enabling the CNN to learn complex patterns that distinguish
      true genetic variants from sequencing artifacts and alignment errors. This approach
      achieves state-of-the-art performance on benchmark datasets and generalizes
      well across Illumina, PacBio, and Oxford Nanopore sequencing platforms without
      technology-specific parameter tuning.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1038/nbt.4235
    - https://doi.org/10.1038/s41587-021-01108-x
  - id: B2AI_APP:101
    category: B2AI:Application
    name: Deep Learning for Structural Variant Detection
    description: Deep learning models trained on BAM/CRAM files enable detection of
      complex structural variants including deletions, duplications, inversions, and
      translocations that are challenging for traditional callers. Neural networks
      analyze read depth, split reads, discordant read pairs, and local assembly features
      from aligned data to identify structural rearrangements, with applications in
      cancer genomics where somatic structural variants drive tumorigenesis and treatment
      resistance. These models improve sensitivity for detecting variants in repetitive
      regions and provide better breakpoint resolution than conventional methods.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1038/s41467-022-28289-w
    - https://doi.org/10.1093/bioinformatics/btab732
- id: B2AI_STANDARD:22
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Binary Alignment Map format
  formal_specification: https://samtools.github.io/hts-specs/SAMv1.pdf
  is_open: true
  name: BAM
  purpose_detail: A BAM file (.bam) is the binary version of a SAM file.
  related_to:
  - B2AI_STANDARD:21
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:34
  url: https://en.wikipedia.org/wiki/Binary_Alignment_Map
  used_in_bridge2ai: true
- id: B2AI_STANDARD:23
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Binary sequence information Format
  is_open: true
  name: 2bit
  purpose_detail: A .2bit file stores multiple DNA sequences (up to 4 Gb total) in
    a compact randomly-accessible format. The file contains masking information as
    well as the DNA itself.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:119
  url: http://genome.ucsc.edu/FAQ/FAQformat.html#format7
- id: B2AI_STANDARD:24
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Binary variant call format
  is_open: true
  name: BCF
  purpose_detail: A binary version of the variant call format (VCF).
  requires_registration: false
  url: https://samtools.github.io/bcftools/bcftools.html
- id: B2AI_STANDARD:25
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: BioCompute Object standard
  has_relevant_organization:
  - B2AI_ORG:44
  is_open: true
  name: BioCompute
  publication: doi:10.5731/pdajpst.2016.006734
  purpose_detail: 'BioCompute Objects (BCOs) are a formal standard for representing
    bioinformatics computational workflows and analyses, designed to facilitate communication
    and reproducibility in high-throughput sequencing research. Developed through
    a collaborative effort and formally recognized as IEEE Standard 2791-2020, BCOs
    structure critical workflow information into standardized domains including provenance,
    usability, extension, description, execution, parametric, input/output, and error
    domains. BCOs are represented in JSON format adhering to JSON schema draft-07,
    making them both human and machine readable. The standard addresses the challenge
    of documenting complex bioinformatics methods by providing predictable structure
    and stability for workflow communication. Upon assignment of a digital etag, three
    domains become immutable to ensure integrity: the Parametric Domain, Execution
    Domain, and I/O Domain. BCOs support regulatory compliance including FDA Title
    21 CFR Part 11 considerations for electronic records and digital signatures, with
    hash values and encryption keys generated from execution and parametric domains
    for validation purposes.'
  requires_registration: false
  url: https://docs.biocomputeobject.org/user_guide/
- id: B2AI_STANDARD:26
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:20
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Biolink Model
  has_relevant_organization:
  - B2AI_ORG:70
  is_open: true
  name: Biolink
  publication: doi:10.1111/cts.13302
  purpose_detail: Biolink Model is a comprehensive, high-level data model designed
    to standardize types and relationships in biological knowledge graphs. It provides
    a consistent framework for representing biological knowledge across various databases
    and formats, covering entities such as genes, diseases, chemical substances, organisms,
    genomics, phenotypes, and pathways. The model incorporates object-oriented classification
    and graph-oriented features, with a core set of hierarchical, interconnected classes
    (categories) and relationships (predicates). Biolink Model includes over 400 classes
    ranging from molecular entities like genes and proteins to clinical concepts like
    diseases and treatments, plus comprehensive predicates for describing relationships
    such as "treats," "causes," "associated_with," and "regulates." It supports advanced
    features like qualifiers for contextualizing relationships, evidence attribution,
    and knowledge provenance tracking. The model is particularly valuable for translational
    science applications, enabling integration of data from diverse sources including
    clinical databases, molecular biology repositories, and literature mining systems.
    It serves as the foundational schema for knowledge graphs in the Biomedical Data
    Translator project and other large-scale biomedical data integration efforts.
  requires_registration: false
  url: https://biolink.github.io/biolink-model/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:6
    category: B2AI:Application
    name: Translator Knowledge Graph for Drug Repurposing
    description: NCATS Biomedical Data Translator uses Biolink Model to integrate
      diverse biomedical knowledge sources into a unified knowledge graph supporting
      AI-driven drug repurposing and mechanism discovery. The standardized Biolink
      schema enables graph neural networks to perform multi-hop reasoning across chemical-protein-disease
      relationships, identifying candidate therapeutics by connecting drugs to diseases
      through intermediate biological entities. The Translator system's reasoning
      agents leverage Biolink predicates to score and rank hypothesized drug-disease
      associations based on mechanistic evidence paths learned from integrated data.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1111/cts.13302
- id: B2AI_STANDARD:27
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Biological Expression Language
  formal_specification: https://language.bel.bio/
  is_open: true
  name: BEL
  purpose_detail: The Biological Expression Language (BEL) is a domain-specific language
    for representing scientific findings from life sciences literature in a computable,
    structured format. BEL captures causal and correlative relationships between biological
    entities (genes, proteins, complexes, biological processes, pathways) along with
    their experimental and publication context, enabling systematic knowledge representation
    and integration. BEL statements are expressed as triples (subject-relationship-object)
    that can be combined into biological networks and knowledge graphs. Each BEL assertion
    is packaged as a "nanopub" that includes provenance information, experimental
    conditions, and citations, allowing relationships to be properly evaluated in
    context. The language supports standard biological nomenclatures (Gene Ontology,
    HGNC, ChEBI, etc.) through namespace integration and provides a simplified syntax
    that is more accessible than traditional chemical notation. BEL's computable format
    enables applications in reverse causal reasoning, heat diffusion algorithms, prior
    knowledge for machine learning models, and detection of contradictory findings
    across literature. The standard is maintained by the BEL Language Committee with
    updates managed through BEL Enhancement Proposals (BEPs).
  requires_registration: false
  url: https://bel.bio/
- id: B2AI_STANDARD:28
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Biomedical Research Integrated Domain Group Model
  formal_specification: https://github.com/CBIIT/bridg-model/
  has_relevant_organization:
  - B2AI_ORG:15
  - B2AI_ORG:31
  - B2AI_ORG:40
  - B2AI_ORG:71
  is_open: true
  name: BRIDG Model
  publication: doi:10.1093/jamia/ocx004
  purpose_detail: The Biomedical Research Integrated Domain Group (BRIDG) Model is
    a collaborative effort engaging stakeholders from the Clinical Data Interchange
    Standards Consortium (CDISC), the HL7 BRIDG Work Group, the US National Cancer
    Institute (NCI), and the US Food and Drug Administration (FDA). The goal of the
    BRIDG Model is to produce a shared view of the dynamic and static semantics for
    the domain of basic, pre-clinical, clinical, and translational research and its
    associated regulatory artifacts.
  requires_registration: false
  url: https://bridgmodel.nci.nih.gov/
- id: B2AI_STANDARD:29
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: BioPAX standard
  is_open: true
  name: BioPAX
  publication: doi:10.1038/nbt.1666
  purpose_detail: BioPAX is a standard language that aims to enable integration, exchange,
    visualization and analysis of biological pathway data. Specifically, BioPAX supports
    data exchange between pathway data groups and thus reduces the complexity of interchange
    between data formats by providing an accepted standard format for pathway data.
    By offering a standard, with well-defined semantics for pathway representation,
    BioPAX allows pathway databases and software to interact more efficiently. In
    addition, BioPAX enables the development of pathway visualization from databases
    and facilitates analysis of experimentally generated data through combination
    with prior knowledge. The BioPAX effort is coordinated closely with that of other
    pathway related standards initiatives namely; PSI-MI, SBML, CellML, and SBGN in
    order to deliver a compatible standard in the areas where they overlap.
  requires_registration: false
  url: http://www.biopax.org/
- id: B2AI_STANDARD:30
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Bioschemas
  formal_specification: https://github.com/BioSchemas/specifications
  has_relevant_organization:
  - B2AI_ORG:28
  is_open: true
  name: Bioschemas
  purpose_detail: Bioschemas is a community-driven initiative that extends Schema.org
    to improve the findability and discoverability of life sciences resources on the
    web through structured markup. The project makes two main contributions to the
    life sciences community - proposing new types and properties to Schema.org specifically
    for life science resources, and defining usage profiles over existing Schema.org
    types that specify essential, recommended, and optional properties for consistent
    markup. Bioschemas profiles significantly simplify the markup process by reducing
    complex Schema.org types to manageable subsets while ensuring compatibility with
    search engines like Google Dataset Search. The initiative has achieved major recognition
    with six Bioschemas types (BioChemEntity, ChemicalSubstance, Gene, MolecularEntity,
    Protein, Taxon) officially included in Schema.org version 13.0. Endorsed by the
    European Research Council and serving as a flagship policy of ELIXIR, Bioschemas
    supports FAIR data principles by enabling automated discovery, collation, and
    analysis of distributed life sciences resources including datasets, software applications,
    training materials, and biological entities across the web ecosystem.
  requires_registration: false
  url: https://bioschemas.org/
- id: B2AI_STANDARD:31
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Biospecimen Reporting for Improved Study Quality
  has_relevant_organization:
  - B2AI_ORG:72
  is_open: true
  name: BRISQ
  publication: doi:10.1002/cncy.20147
  purpose_detail: Human biospecimens are subject to a number of different collection,
    processing, and storage factors that can significantly alter their molecular composition
    and consistency. These biospecimen preanalytical factors, in turn, influence experimental
    outcomes and the ability to reproduce scientific results. Currently, the extent
    and type of information specific to the biospecimen preanalytical conditions reported
    in scientific publications and regulatory submissions varies widely. To improve
    the quality of research utilizing human tissues, it is critical that information
    regarding the handling of biospecimens be reported in a thorough, accurate, and
    standardized manner. The Biospecimen Reporting for Improved Study Quality (BRISQ)
    recommendations outlined herein are intended to apply to any study in which human
    biospecimens are used. The purpose of reporting these details is to supply others,
    from researchers to regulators, with more consistent and standardized information
    to better evaluate, interpret, compare, and reproduce the experimental results.
    The BRISQ guidelines are proposed as an important and timely resource tool to
    strengthen communication and publications around biospecimen-related research
    and help reassure patient contributors and the advocacy community that the contributions
    are valued and respected
  requires_registration: false
  url: https://doi.org/10.1002/cncy.20147
- id: B2AI_STANDARD:32
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:20
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: BioXSD
  formal_specification: https://github.com/bioxsd/bioxsd
  has_relevant_organization:
  - B2AI_ORG:28
  is_open: true
  name: BioXSD
  publication: doi:10.1093/bioinformatics/btq391
  purpose_detail: 'BioXSD is a unified data model and family of interoperable exchange
    formats (XML, JSON, YAML, and binary serializations) designed to represent basic
    bioinformatics data types including biological sequences (DNA, RNA, protein),
    multiple sequence alignments, annotated feature records with genomic coordinates
    and associated metadata, database cross-references with accession identifiers,
    and provenance metadata aligned with the W3C PROV standard for tracking data
    origins and transformations. The data model is expressed as an XML Schema (currently
    version 1.1) defining a hierarchical namespace (http://bioxsd.org/BioXSD-1.1)
    with complex types for sequence records containing raw sequence strings with quality
    scores, feature annotations with start/end positions and strand information, alignment
    objects with gap representations and consensus sequences, and rich metadata fields
    for database identifiers, organism taxonomy, publication references, and analysis
    provenance. BioXSD is designed to fill the gap between specialized domain-specific
    XML formats (SBML for systems biology, MAGE-ML for microarrays, PDBML for protein
    structures, PhyloXML for phylogenetics) by providing a canonical intermediate
    exchange format that is sufficiently rich to enable lossless conversions among
    diverse bioinformatics file formats (FASTA, GenBank, EMBL, UniProt, GFF, BED)
    while remaining simple enough for straightforward implementation across multiple
    programming languages and web service architectures. The format has been developed
    through a "blue-collar" community-driven approach initiated by the EMBRACE project
    and 1st DBCLS BioHackathon participants, with data-type definitions semantically
    annotated using EDAM ontology terms to provide globally defined controlled vocabularies
    for tool interfaces and service descriptions. BioXSD supports both RESTful HTTP
    web services and WS-I compliant SOAP web services, interoperating with standard
    HTTP, SOAP, and XML libraries in common programming languages (Python, Java, Perl,
    R) without requiring additional infrastructure beyond standard HTTP and XML parsers.
    The BioXSD|GTrack family developed by ELIXIR Norway offers multiple serialization
    options with lossless interconversion: tree-structured formats (BioXSD XML, BioJSON,
    BioYAML) for hierarchical data and web service APIs, tabular GTrack for genome
    features compatible with tools expecting columnar input, binary BTrack for efficient
    storage and processing, and GSuite for integrative multi-track analyses, all sharing
    a common data model with consistent semantics across serializations. BioXSD has
    been adopted by multiple bioinformatics service providers including CBS Denmark
    (MaxAlign, ProP, NetNES), University of Bergen Norway (BLAST), IBCP France (ClustalW,
    GorIV), and TU Munich Germany (FreeContact protein contact prediction), demonstrating
    practical deployment in production web service infrastructures. The format is
    particularly valuable for machine learning and computational biology workflows
    where BioXSD serves as the standardized exchange format for ML tool outputs (FreeContact
    uses BioXSD for residue-residue contact predictions from mean-field Direct Coupling
    Analysis and PSICOV sparse inverse covariance methods), enables construction of
    datatype ontologies for annotating ML dataset repositories and assembling data
    mining workflows (OntoDT uses BioXSD to improve representation of bioinformatics
    datatypes for querying algorithm repositories), facilitates integration of diverse
    tools into automated analysis pipelines through consistent data serialization,
    supports reproducible computational research by encoding complete provenance chains
    documenting data transformations and parameter settings, and enables federated
    data sharing where BioXSD''s semantic annotations allow automated discovery and
    composition of compatible services without requiring custom format converters
    for each tool pair, thereby reducing the N-squared integration problem to N conversions
    between native formats and the canonical BioXSD representation.'
  requires_registration: false
  url: http://bioxsd.org/
  has_application:
  - id: B2AI_APP:7
    category: B2AI:Application
    name: FreeContact Protein Contact Prediction ML Output Format
    description: FreeContact implements statistical learning-based approaches for protein
      residue-residue contact prediction using mean-field Direct Coupling Analysis
      and PSICOV sparse inverse covariance methods. The tool explicitly supports BioXSD
      as an output format to facilitate integration into bioinformatics workflows
      and web services, with planned BioXSD input support. This demonstrates BioXSD
      serving as the standardized exchange format for machine learning tool outputs,
      enabling incorporation of ML-based contact predictions into interoperable analysis
      pipelines.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1186/1471-2105-15-85
  - id: B2AI_APP:107
    category: B2AI:Application
    name: OntoDT Datatype Ontology for ML Dataset Repositories
    description: The OntoDT generic ontology of datatypes uses BioXSD to improve representation
      of basic bioinformatics datatypes in exchange formats, enabling construction
      of taxonomies for datasets, data mining tasks, generalizations, and algorithms.
      OntoDT can be used for annotation and querying of machine learning dataset repositories
      and for constructing data mining workflows. This links BioXSD datatype representations
      to ML and data mining ontologies, supporting standardized workflow assembly
      through consistent bioinformatics datatype semantics.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1016/j.ins.2015.08.006
- id: B2AI_STANDARD:33
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:22
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Brain Imaging Data Structure
  formal_specification: https://github.com/bids-standard/bids-specification
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:3
  is_open: true
  name: BIDS
  publication: doi:10.1038/sdata.2016.44
  purpose_detail: The Brain Imaging Data Structure (BIDS) is a simple and intuitive
    way to organize and describe data. This document defines the BIDS specification,
    which provides many details to help implement the standard. It includes the core
    specification as well as many extensions to specific brain imaging modalities,
    and increasingly also to other kinds of data.
  requires_registration: false
  url: https://bids-specification.readthedocs.io/en/stable/
  has_application:
  - id: B2AI_APP:8
    category: B2AI:Application
    name: BIDS Apps for ML-Ready Preprocessing and Feature Extraction
    description: Containerized BIDS Apps such as fMRIPrep, MRIQC, and MRtrix3 Connectome
      accept BIDS datasets and emit standardized derivatives including preprocessed
      time series, image quality metrics, and connectivity matrices that function
      directly as inputs or labels for ML workflows. The BIDS Apps ecosystem uses
      containerization to provide reproducible preprocessing pipelines across systems
      including HPC environments via Singularity, enabling consistent feature extraction
      for training machine learning models on neuroimaging data.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1371/journal.pcbi.1005209
  - id: B2AI_APP:108
    category: B2AI:Application
    name: PyBIDS and MNE-BIDS Programmatic Dataset Assembly
    description: PyBIDS provides programmatic access to query BIDS datasets and their
      metadata and to organize derivatives in BIDS-Derivatives format, facilitating
      reproducible train-validation splits and feature assembly for ML pipelines.
      MNE-BIDS provides an integration layer for MNE-Python enabling standardized
      ingestion of EEG, MEG, and iEEG BIDS data into analysis pipelines that feed
      ML methods. These tools enable automated dataset selection, metadata-driven
      curation, and scalable learning across neuroimaging studies.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1162/imag_a_00103
  - id: B2AI_APP:109
    category: B2AI:Application
    name: BIDS Derivatives and Connectivity Specifications for ML Features
    description: The BIDS Connectivity extension and Derivatives specifications define
      interoperable formats for structural and functional connectivity matrices, seed-based
      maps, tractograms, and tractometry across modalities including sMRI, fMRI, DWI,
      PET, EEG, iEEG, and MEG. These standardized derivative schemas enable common
      ML features to be shared and allow benchmarking of ML models on consistent feature
      representations, supporting reproducible radiomics and connectomics analyses.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1162/imag_a_00103
  - id: B2AI_APP:110
    category: B2AI:Application
    name: XGBoost-Based DICOM to BIDS Automated Conversion
    description: An XGBoost classifier trained on DICOM metadata achieves 99.5% accuracy
      in classifying MRI acquisition types and automatically transforms unstructured
      clinical imaging into BIDS datasets with little to no user intervention. This
      ML-enabled conversion tool reduces manual curation burden, accelerates creation
      of standardized training corpora, and enables clinical imaging data to be rapidly
      prepared for downstream machine learning analyses in BIDS format.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1007/s12021-024-09659-5
- id: B2AI_STANDARD:34
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - diagnosticinstrument
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Brief Fatigue Inventory
  formal_specification: http://www.npcrc.org/files/news/brief_fatigue_inventory.pdf
  is_open: true
  name: BFI
  purpose_detail: The Brief Fatigue Inventory (BFI) is used to rapidly assess the
    severity and impact of cancer-related fatigue. An increasing focus on cancer-related
    fatigue emphasized the need for sensitive tools to assess this most frequently
    reported symptom. The six interference items correlate with standard quality-of-life
    measures.
  requires_registration: false
  url: https://www.mdanderson.org/research/departments-labs-institutes/departments-divisions/symptom-research/symptom-assessment-tools/brief-fatigue-inventory.html
- id: B2AI_STANDARD:35
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - diagnosticinstrument
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Brief Pain Inventory
  formal_specification: http://www.npcrc.org/files/news/briefpain_short.pdf
  is_open: true
  name: BPI
  purpose_detail: The Brief Pain Inventory (BPI) rapidly assesses the severity of
    pain and its impact on functioning. The BPI has been translated into dozens of
    languages, and it is widely used in both research and clinical settings.
  requires_registration: false
  url: https://www.mdanderson.org/research/departments-labs-institutes/departments-divisions/symptom-research/symptom-assessment-tools/brief-pain-inventory.html
- id: B2AI_STANDARD:36
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Browser Extensible Data Format
  formal_specification: https://github.com/samtools/hts-specs/blob/master/BEDv1.pdf
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:53
  is_open: true
  name: BED
  purpose_detail: The Browser Extensible Data (BED) format is a flexible, tab-delimited
    text format designed for defining annotation tracks in genome browsers, particularly
    the UCSC Genome Browser. BED format provides a standardized way to represent genomic
    features with positional information, supporting both simple coordinate-based
    annotations and complex multi-exon structures. The format consists of 12 possible
    fields, with the first three (chromosome, start position, end position) being
    mandatory and nine additional optional fields providing detailed feature information
    including name, score, strand orientation, thick start/end coordinates for coding
    regions, RGB color values for visualization, and block structures for representing
    discontinuous features like exons and introns. BED files use zero-based, half-open
    coordinate system where the start position is inclusive and the end position is
    exclusive, enabling precise genomic interval representation. The format supports
    various annotation types from simple genomic intervals to complex gene models,
    making it essential for genomic data visualization, comparative genomics, and
    functional annotation workflows in bioinformatics research.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:119
  url: https://genome.ucsc.edu/FAQ/FAQformat.html#format1
  has_application:
  - id: B2AI_APP:219
    category: B2AI:Application
    name: StratoMod Variant-Calling Error Prediction with Interpretable ML
    description: StratoMod employs Explainable Boosting Machine (EBM) classifiers to
      predict sequencing and variant-calling errors (true positives, false positives,
      false negatives) using BED files as the primary data structure throughout the
      workflow. VCF-derived variant labels are converted to 0-based BED format with
      coordinate shifts (start position reduced by 1) to enable accurate intersection
      with genomic context features, and SNV/INDEL label BED files are merged using
      bedtools -loj to produce per-variant binary labels (TP/FP/FN mapped to 0/1)
      for precision and recall models. Assembly-based benchmark regions from DeFrABB/dipcall
      defining 1:1 haplotype alignments (excluding gaps and large repeats) are represented
      as BED intervals delimiting evaluation zones where variants are considered for
      benchmarking. Numerous genomic-context feature tracks including GEM-based low-mappability
      regions (low-mappabilityall.bed.gz), nonunique mappability intervals (nonunique_l250_*.bed.gz),
      and SINE/LTR repeat annotations are provided as BED files, converted to binary
      features by setting interval lengths to 1 or appending a column indicating membership
      (e.g., "in hard-to-map region"), and intersected with variant positions using
      bedtools intersect -loj to derive feature values for model training. Missing
      values from non-overlapping intervals are filled with -1 or 0 depending on feature
      semantics, and the trained EBM model achieves interpretable predictions explaining
      technology-specific and callset-specific differences in false negative rates,
      demonstrating BED as the canonical format for labels, evaluation strata, and
      feature engineering in genomic ML pipelines.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.21203/rs.3.rs-2613736/v1
  - id: B2AI_APP:220
    category: B2AI:Application
    name: Deep Learning Epigenomic Profile Prediction with Peak-Centered Datasets
    description: A comprehensive evaluation framework for deep learning models predicting
      chromatin accessibility and epigenomic profiles from DNA sequence (including
      BPNet, Basenji, and other CNN/RNN architectures) uses BED-format IDR (Irreproducible
      Discovery Rate) peak files from ENCODE as binary classification labels and to
      construct peak-centered training and evaluation datasets. IDR peak BED files
      are selected per ATAC-seq experiment, merged across cell lines to create unified
      peak sets, and used to extract fixed-size sequence windows (e.g., 2 kb) centered
      on peak summits, ensuring each training sequence contains at least one peak
      across all cell lines being modeled. This peak-centered dataset construction
      contrasts with coverage-threshold approaches using bigWig files, enabling systematic
      comparison between binary peak-based modeling (where BED peaks serve as targets)
      and quantitative signal regression. Replicate 1 peak BEDs are used to build
      train/validation/test partitions while replicate 2 provides an experimental
      performance ceiling, and the resulting models are evaluated on downstream tasks
      including variant effect prediction and model interpretability analysis using
      the GOPHER framework, demonstrating BED as the standard format for defining
      regulatory element locations as binary labels in sequence-to-function deep learning.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1038/s42256-022-00570-9
  - id: B2AI_APP:221
    category: B2AI:Application
    name: Pybedtools ML Feature Engineering and Interval Operations
    description: Pybedtools, a Python wrapper for BEDTools, provides essential genomic
      interval manipulation capabilities that serve as preprocessing and feature engineering
      infrastructure for machine learning pipelines operating on BED-format data.
      The library enables set algebra operations (intersection, subtraction, addition)
      to extract unique or overlapping genomic features across multiple BED files,
      facilitating construction of positive/negative training sets by identifying
      variants overlapping functional annotations versus background regions. The closestBed
      -d functionality computes distances from query intervals (e.g., SNPs stored
      as BED) to nearest target features (e.g., gene boundaries from BED annotations),
      appending distance values as numeric columns that can serve as continuous features
      or distance-based targets for regression models. Interval objects expose elements
      by integer index (e.g., [-1] to retrieve appended distance field) and by named
      core attributes (chromosome, start, end, name, strand), enabling efficient feature
      extraction and label assignment within Python-based ML workflows. The stream=True
      option yields Python iterable Interval objects without writing intermediate
      files, reducing disk I/O and enabling scalable batch processing for large training/evaluation
      datasets, making pybedtools a foundational utility for genomic ML preprocessing
      where BED intervals represent labeled regions, feature annotations, or evaluation
      zones.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1093/bioinformatics/btr539
  - id: B2AI_APP:222
    category: B2AI:Application
    name: LanceOtron Deep Learning Peak Caller with BED-Based Labeling
    description: LanceOtron is a deep learning peak caller for ATAC-seq, ChIP-seq,
      and DNase-seq that uses convolutional neural networks with enrichment scoring,
      employing BED format throughout the supervised learning pipeline for candidate
      peak generation, manual labeling, and model output. Putative peak regions are
      generated by three methods (MACS2 and two mean-based enrichment approaches)
      and aggregated within randomly selected 1 Mb windows for labeling, with overlapping
      candidate peaks from multiple calling permutations resolved using pybedtools
      (Python implementation of BEDTools) by randomly selecting one overlapping region
      per cluster for manual visual inspection. Regions sized 50 bp to 2 kb are visually
      labeled as 'peak' or 'noise' based on clear signal patterns, producing a labeled
      training set of 736,753 regions (5,016 peaks, 731,737 noise) covering 499 Mb,
      with region coordinates maintained in BED-compatible interval format. An algorithmic
      labeling pipeline supplements manual labels by smoothing coverage signals with
      a 400 bp rolling mean, marking coordinates exceeding 4-fold the chromosome mean
      as enriched, merging adjacent enriched coordinates, and retaining candidate
      peaks within the 50 bp2 kb size range (recursively adjusting thresholds for
      larger regions). The trained deep neural network produces predicted peak coordinates
      exported as BED files, enabling direct integration with downstream genomic analysis
      workflows and genome browsers, demonstrating BED as both the labeled training
      data format and the standard model output format for supervised learning-based
      peak calling.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2021.01.25.428108
  - id: B2AI_APP:223
    category: B2AI:Application
    name: Tumor-Only Somatic Variant Classification with TabNet and BED-Based Preprocessing
    description: An attentive deep learning tumor-only somatic mutation classifier
      using TabNet, LightGBM, and XGBoost models to distinguish somatic from germline
      variants employs BED files for capture kit target definition, genomic blacklist
      filtering, and tumor mutational burden (TMB) normalization. Exon target BED
      files for three capture kits (Agilent SureSelect Human All Exon V6, IDT xGen
      Exome Research Panel V2, Illumina TruSeq Exome) were obtained from manufacturer
      websites in hg19 coordinates and converted to hg38 using UCSC liftOver tool
      to ensure consistent coordinate systems across datasets, with resulting target
      footprints of 33.0, 37.3, and 63.5 megabases computed from the lifted BED intervals.
      A BED-format genomic blacklist (hg38_simple_repeats.bed from UCSC) is applied
      as a snpblacklist filter to exclude SNPs in tandem repeat regions that are prone
      to sequencing errors and alignment artifacts. Target footprint sizes derived
      from capture kit BED files are used to normalize TMB estimates, computing a
      patient-weighted average target size of 41 Mb across cohorts to enable fair
      comparisons of mutation burden between samples sequenced with different capture
      kits. The TabNet attention-based neural network and gradient boosting models
      (LightGBM, XGBoost) are trained on variant features to predict somatic versus
      germline status, achieving high accuracy agnostic of tissue type and capture
      kit by incorporating BED-based preprocessing that standardizes genomic coordinates,
      filters problematic regions, and normalizes coverage-dependent features, illustrating
      BED as essential infrastructure for coordinate system management and quality
      control in clinical genomic ML applications.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2021.12.07.471513
- id: B2AI_STANDARD:37
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Business Process Modeling Notation
  has_relevant_organization:
  - B2AI_ORG:10
  is_open: true
  name: BPMN
  purpose_detail: A graphical notation that depicts the steps in a business process.
    BPMN depicts the end-to-end flow of a business process. The notation has been
    specifically designed to coordinate the sequence of processes and the messages
    that flow between different process participants in a related set of activities.
  requires_registration: false
  url: https://www.omg.org/bpmn/index.htm
- id: B2AI_STANDARD:38
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Case Management Model and Notation
  has_relevant_organization:
  - B2AI_ORG:10
  is_open: true
  name: CMMN
  purpose_detail: 'Case Management Model and Notation (CMMN) is an Object Management
    Group (OMG) standard that defines a common meta-model, graphical notation, and
    XML-based interchange format for modeling and expressing adaptive case management
    processes, designed to complement BPMN by capturing work methods that involve
    unpredictable activity sequences, knowledge worker decision-making, and event-driven
    responses to evolving situations rather than rigidly structured process flows.
    CMMN centers on the concept of a case as a living information container that aggregates
    all relevant data, documents, and relationships for a particular matter (patient
    treatment, legal case, insurance claim, research project), with a case file serving
    as the central repository that evolves dynamically as the case progresses through
    various stages and milestones. The notation provides graphical symbols for case
    plan modeling including discretionary tasks that knowledge workers can choose
    to perform based on their expertise, required tasks with mandatory completion,
    event listeners that trigger activities when specific conditions occur, sentries
    (entry and exit criteria) that guard stage transitions using boolean expressions,
    case file items representing information assets with lifecycle states, milestones
    marking significant achievement points, and human tasks requiring user interaction
    with form-based data collection. CMMN supports the Adaptive Case Management paradigm
    where humans remain "firmly in the driver''s seat" making contextualized decisions
    while the system provides structure, suggestions, and automated responses to predictable
    patterns, enabling flexibility for complex scenarios like clinical care coordination
    where treatment paths depend on patient-specific factors, diagnostic results,
    and expert clinical judgment rather than predetermined protocols. The standard
    defines an XSD schema for serializing case models in XML format with complete
    semantic preservation, enabling tool interoperability where case definitions created
    in one CMMN-compliant platform can be exchanged, imported, and executed in another,
    supporting collaborative process design across organizational boundaries. CMMN
    integrates with the OMG "triple crown" of process improvement standards: BPMN
    for structured predictable processes, CMMN for adaptive knowledge-intensive work,
    and DMN (Decision Model and Notation) for explicit business rule specification,
    allowing organizations to model hybrid workflows where standardized BPMN subprocesses
    can be embedded within CMMN cases and decisions can be delegated to DMN decision
    tables with inputs from the case file. In healthcare and biomedical research,
    CMMN is particularly valuable for modeling patient care pathways where treatment
    decisions depend on longitudinal observations and multidisciplinary consultation,
    clinical trial management where protocol deviations require documented case-by-case
    assessments, rare disease diagnosis workflows involving iterative testing and
    specialist referrals, personalized medicine programs tailoring interventions to
    genetic profiles and biomarker responses, and research data management where data
    collection, quality control, and analysis activities adapt to preliminary findings
    and emerging hypotheses. For AI and machine learning applications, CMMN provides
    a framework where case file data can train predictive models to suggest next-best
    actions (recommending diagnostic tests based on symptom patterns and prior case
    outcomes), ML-generated risk scores and classification results can populate case
    file items triggering sentries that automatically initiate appropriate response
    protocols, natural language processing can extract structured information from
    case notes and clinical narratives to enrich the case file, reinforcement learning
    agents can optimize case routing and resource allocation by learning from historical
    case resolution patterns, and human-in-the-loop ML systems can embed model predictions
    as discretionary tasks where domain experts review AI suggestions before taking
    action, creating audit trails documenting the interplay between automated intelligence
    and human expertise throughout case lifecycles.'
  requires_registration: false
  url: https://www.omg.org/cmmn/
- id: B2AI_STANDARD:39
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Case Report guidelines
  is_open: true
  name: CARE
  purpose_detail: 'The CARE (CAse REport) guidelines are internationally developed
    reporting standards created by an expert group to improve the accuracy, transparency,
    and usefulness of medical case reports by providing a structured checklist and
    framework for documenting individual patient encounters, clinical observations,
    and treatment outcomes with explicit focus on why particular observations are
    important in the context of existing knowledge. The CARE checklist comprises 13
    items organized across four main sections: Title (indicating case report with
    key identifying information), Keywords (2-5 words identifying topics), Abstract
    (structured summary with Introduction, Patient Information, Clinical Findings,
    Timeline, Diagnostic Assessment, Therapeutic Interventions, Follow-up and Outcomes,
    Discussion including takeaway lessons, Patient Perspective), and comprehensive
    Main Text sections including Introduction (brief background and context with literature
    review), Patient Information (de-identified demographic and clinical characteristics,
    chief complaints, relevant medical/family/psychosocial history), Clinical Findings
    (diagnostic tests, assessment methods, imaging, laboratory results), Timeline
    (chronological table of key dates and events from symptom onset through treatment
    and follow-up), Diagnostic Assessment (diagnostic reasoning including differential
    diagnoses, challenges, prognostic characteristics), Therapeutic Intervention (types
    of intervention with dosages/durations/administration details, changes to interventions
    with rationales), Follow-up and Outcomes (clinician- and patient-assessed outcomes
    including adverse events, test results, treatment adherence, tolerability), Discussion
    (strengths/limitations, relevant medical literature, rationale for conclusions,
    takeaway lessons including clinical significance), Patient Perspective (when possible,
    patient''s own account of symptoms, diagnosis, treatment experiences), and Informed
    Consent (documentation of patient consent for publication). The guidelines support
    the EQUATOR Network''s mission to improve health research reporting by helping
    authors reduce bias risk through systematic documentation requirements, increase
    transparency through explicit reporting of timeline and decision-making processes,
    and provide early signals about what interventions work for which patients under
    which circumstances, particularly valuable for rare conditions, novel treatments,
    unexpected adverse events, and unusual disease presentations where randomized
    controlled trials are impractical. CARE guidelines have been endorsed by multiple
    medical journals and publishers (BMJ Case Reports, Journal of Medical Case Reports,
    JAMA, Lancet, BMC), translated into multiple languages (Spanish, Portuguese, German,
    Japanese, Korean, Chinese, Turkish), and integrated into online tools like CARE-writer
    (an application for organizing, formatting, and writing systematic case reports
    as preprints or journal submissions). The guidelines benefit multiple healthcare
    stakeholders: patients reviewing and comparing therapeutic options through transparent
    outcome reporting, clinicians engaging in peer-to-peer communication at conferences
    and in communities of practice, researchers developing testable hypotheses from
    clinical settings (for example, case reports of Zika-associated microcephaly in
    2016 led to rapid hypothesis generation and public health responses), educators
    using real-world systematic case reports for case-based learning curricula, authors
    simplifying the case report writing process through structured guidance, and journal
    editors implementing standardized peer review criteria. For AI and machine learning
    applications, CARE-compliant case reports provide high-quality structured training
    data where the standardized format enables automated extraction of clinical variables
    (demographics, symptoms, test results, interventions, outcomes) for populating
    electronic health record databases and clinical data warehouses, natural language
    processing models can be trained on the consistent narrative structure to identify
    entities (diseases, drugs, procedures) and relationships (causal links between
    interventions and outcomes, temporal sequences from timeline sections), case-based
    reasoning systems can retrieve similar historical cases for differential diagnosis
    support by comparing structured patient information and clinical findings across
    repositories, machine learning classifiers can predict treatment success likelihood
    by learning patterns from follow-up outcome sections across aggregated case reports,
    phenotype extraction algorithms can identify rare disease characteristics and novel
    symptom clusters from standardized clinical findings descriptions, and federated
    learning approaches can train models across distributed case report databases without
    pooling sensitive patient data by leveraging the common CARE structure for consistent
    feature representation, enabling meta-analysis of rare events and personalized
    medicine insights that would be impossible with traditional aggregate study designs.'
  requires_registration: false
  url: https://www.care-statement.org/
- id: B2AI_STANDARD:40
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDISC Case Report Tabulation Data Definition Specification
  is_open: true
  name: Define
  purpose_detail: The CDISC Case Report Tabulation Data Definition Specification (define.xml)
    Version 1.0 reflects changes from a comment period through the Health Level 7
    (HL7) Regulated Clinical Research Information Management Technical Committee (RCRIM)
    in December 2003 (www.hl7.org) and CDISC's website in September 2004 as well as
    the work done by the define.xml team in conjunction with the CDISC ODM team to
    add functionality, features, and additional documentation. This document specifies
    the standard for providing Case Report Tabulations Data Definitions in an XML
    format for submission to regulatory authorities (e.g., FDA). The XML schema used
    to define the expected structure for these XML files is based on an extension
    to the CDISC Operational Data Model (ODM). The 1999 FDA electronic submission
    (eSub) guidance and the electronic Common Technical Document (eCTD) documents
    specify that a document describing the content and structure of the included data
    should be provided within a submission. This document is known as the Data Definition
    Document (e.g., define.pdf in the 1999 guidance). The Data Definition Document
    provides a list of the datasets included in the submission along with a detailed
    description of the contents of each dataset. To increase the level of automation
    and improve the efficiency of the Regulatory Review process, define.xml can be
    used to provide the Data Definition Document in a machine-readable format.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:15
  url: https://www.cdisc.org/standards/data-exchange/define-xml
- id: B2AI_STANDARD:41
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDISC Controlled Terminology for Analysis Dataset Model
  formal_specification: https://evs.nci.nih.gov/ftp1/CDISC/ADaM/
  is_open: true
  name: CDISC ADaM
  purpose_detail: 'CDISC ADaM (Analysis Dataset Model) Controlled Terminology is a
    comprehensive standardized vocabulary maintained by the NCI Enterprise Vocabulary
    Services (EVS) that defines the semantic concepts, codes, and codelists required
    for creating analysis-ready datasets in clinical trials following the CDISC ADaM
    standard, enabling consistent representation of derived variables, analysis parameters,
    statistical methodologies, and data derivation techniques used in regulatory submissions
    to FDA, EMA, and other health authorities. The terminology encompasses multiple
    codelist categories including Derivation Type (DTYPE) defining 30+ imputation
    and data derivation methods (LOCF last observation carried forward, BOCF baseline
    observation carried forward, AVERAGE, MAXIMUM, MINIMUM, INTERPOLATION, EXTRAPOLATION,
    LLOQ lower limit of quantification, PHANTOM record creation for missing visits,
    ML maximum likelihood), Date and Time Imputation Flags (DATEFL, TIMEFL) indicating
    the level of imputation for partial dates (Y for year imputed, M for month-day
    imputed, D for day imputed, H/M/S for time components), Parameter Type (PARAMTYP)
    distinguishing derived parameters calculated from other parameters versus directly
    observed measurements, Analysis Stratum (STRATA) defining subgroup variables like
    age categories, race, sex, tobacco product use status for stratified analyses,
    Subject Trial Status (SBJTSTAT) tracking completion/discontinuation/ongoing enrollment,
    and Pool for Integration (POOLINT) identifying combined datasets across multiple
    studies for integrated safety and efficacy analyses. ADaM terminology includes
    extensive support for clinical assessment instruments with parameter codes and
    names for validated scales: APACHE II (Acute Physiology and Chronic Health Evaluation
    II) with 20+ clinical parameters and total acute physiology score calculations,
    CHART-SF (Craig Handicap Assessment and Reporting Technique-Short Form) covering
    physical independence, cognitive independence, mobility, occupation, social integration,
    and economic self-sufficiency subscores across 19 questionnaire items, GAD-7 (Generalized
    Anxiety Disorder-7) and GDS (Geriatric Depression Scale Short Form) for mental
    health assessments, HAMD-17 (Hamilton Depression Rating Scale 17-item), and NEWS2
    (National Early Warning Score 2) for clinical deterioration monitoring with trigger
    thresholds. The terminology provides specialized codelists for tobacco regulatory
    science including Tobacco Product Category Response (TPCATRS) distinguishing cigarettes,
    cigars, electronic nicotine delivery systems (ENDS), heated tobacco products,
    roll-your-own products, smokeless tobacco, pipe tobacco, and waterpipe products,
    Tobacco Product Use Status Response (TPUSRS) classifying current/former/never
    use states, Tobacco Use Transition Response (TBUTRS) capturing initiation, cessation,
    relapse, and product switching behaviors, and Input Parameter (INPRM) terms for
    population modeling including birth rate, mortality probability, net migration
    rate, prevalence, transition probability for computational models of public health
    interventions. Each terminology term is mapped to NCI Thesaurus concept codes
    (C-codes) providing formal definitions, synonyms, and semantic relationships that
    enable automated reasoning and cross-standard harmonization with other CDISC terminologies
    (SDTM Study Data Tabulation Model, Protocol, CDASH Clinical Data Acquisition Standards
    Harmonization). ADaM terminology is maintained as extensible codelists where sponsor
    organizations can add study-specific terms for novel assessments or custom analyses
    while preserving the core standard vocabulary, with updates released quarterly
    incorporating new clinical scales, analysis methodologies, and regulatory requirements
    as clinical trial practices evolve. For AI and machine learning applications in
    clinical trial analytics, ADaM controlled terminology enables standardized feature
    engineering where derivation type codes inform algorithm selection (choosing appropriate
    imputation methods for missing data - ML models can learn which DTYPE values historically
    produced most accurate predictions for specific endpoint patterns), automated quality
    control by training classifiers to detect inconsistent terminology usage across
    analysis datasets flagging non-standard codes that may indicate data processing
    errors, meta-analysis and cross-study learning where standardized parameter codes
    allow pooling of results across trials (ML models trained on CHART-SF or GAD-7
    parameters from thousands of subjects across multiple studies to predict treatment
    response patterns), natural language processing to extract analysis specifications
    from statistical analysis plans and automatically generate corresponding ADaM
    terminology codelists, reinforcement learning for adaptive clinical trial designs
    where agents select optimal imputation strategies (LOCF vs BOCF vs ML estimation)
    based on interim analysis results, and federated learning across pharmaceutical
    companies where ADaM terminology provides the common semantic layer enabling collaborative
    model training on pooled efficacy/safety data without exposing proprietary study
    details, all while maintaining regulatory compliance through standardized documentation
    of every analysis decision and derivation method applied to clinical trial data.'
  related_to:
  - B2AI_STANDARD:42
  - B2AI_STANDARD:43
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:15
  url: https://evs.nci.nih.gov/ftp1/CDISC/ADaM/ADaM%20Terminology.html
- id: B2AI_STANDARD:42
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDISC Controlled Terminology for Data Collection for Protocol
  formal_specification: https://evs.nci.nih.gov/ftp1/CDISC/Protocol/
  is_open: true
  name: CDISC Protocol
  purpose_detail: Controlled terminology for biomedical protocols.
  related_to:
  - B2AI_STANDARD:41
  - B2AI_STANDARD:43
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:15
  url: https://evs.nci.nih.gov/ftp1/CDISC/Protocol/Protocol%20Terminology.html
- id: B2AI_STANDARD:43
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDISC Controlled Terminology for Therapeutic Area Standards
  is_open: true
  name: CDISC TAUGs
  purpose_detail: CDISC Therapeutic Area User Guides (TAUGs) are specialized extensions of CDISC foundational standards (SDTM, CDASH, ADaM) that provide disease-specific metadata, controlled terminology, implementation guidance, annotated case report form examples, and data collection recommendations tailored to particular therapeutic areas, enabling consistent, standardized representation of clinical trial data for specific disease domains while supporting global regulatory submissions to agencies including FDA, EMA, and PMDA. TAUGs address the reality that different therapeutic areas present unique clinical concepts, disease-specific assessments, biomarkers, imaging modalities, specialized endpoints, and treatment paradigms that cannot be adequately captured by foundational standards alone, requiring domain-specific supplements that formalize how to model disease progression measures (e.g., ALSFRS-R for ALS, UPDRS for Parkinson's), tumor staging systems (TNM classification for oncology), cardiac imaging parameters (ejection fraction, wall motion abnormalities for cardiovascular trials), respiratory function tests (FEV1, FVC for asthma/COPD), viral load measurements (HIV RNA copies/mL for HIV trials, HCV RNA for hepatitis C), cognitive batteries (ADAS-Cog, MMSE for Alzheimer's disease), pain scales and physical function assessments (WOMAC for osteoarthritis, HAQ-DI for rheumatoid arthritis), and disease-specific adverse events (cytokine release syndrome in CAR-T therapy, infusion reactions in biologics trials, diabetic ketoacidosis in diabetes studies). CDISC has developed TAUGs for over 30 therapeutic areas including Acute Kidney Injury (standardizing creatinine, urine output, RIFLE/AKIN/KDIGO criteria for AKI staging), Alzheimer's Disease (neuropsychological test batteries, MRI volumetrics, amyloid/tau biomarkers, Clinical Dementia Rating), Asthma (spirometry, peak flow, asthma control questionnaires, exacerbation definitions), Breast Cancer (hormone receptor status, HER2 testing, RECIST tumor measurements, pathological complete response), Cardiovascular Disease (ECG intervals, echocardiography parameters, cardiac biomarkers troponin/BNP, MACE endpoints myocardial infarction/stroke/cardiovascular death), CDAD/Clostridioides difficile (stool frequency, Bristol Stool Scale, toxin assays, colonoscopy findings), Colorectal Cancer (microsatellite instability, KRAS/NRAS/BRAF mutation status, CEA tumor marker), COPD (post-bronchodilator spirometry, CAT/SGRQ questionnaires, exacerbation rates, 6-minute walk distance), COVID-19 (SARS-CoV-2 PCR/antigen testing, oxygen saturation, WHO ordinal scale for clinical improvement, inflammatory markers CRP/D-dimer/ferritin), Crohn's Disease (CDAI, endoscopic scoring Harvey-Bradshaw Index, fecal calprotectin, imaging for strictures/fistulas), Diabetes Type 1 and Type 2 (HbA1c, fasting/postprandial glucose, continuous glucose monitoring metrics time-in-range/glucose variability, insulin dosing, hypoglycemia events classified by severity, diabetes-related complications retinopathy/nephropathy/neuropathy), Diabetic Kidney Disease (urine albumin-creatinine ratio, eGFR trajectory, kidney biopsy histology), Duchenne Muscular Dystrophy (6-minute walk test, North Star Ambulatory Assessment, pulmonary function, cardiac MRI for fibrosis, dystrophin expression from muscle biopsy), Dyslipidemia (LDL-C, HDL-C, triglycerides, apolipoprotein B, Lp(a), treatment targets per ATP III/ACC-AHA guidelines), Ebola (viral load quantification, symptom onset and progression, hemorrhagic manifestations, survival outcomes), Heart Failure (NYHA functional class, natriuretic peptides BNP/NT-proBNP, ejection fraction from echocardiography/cardiac MRI, 6-minute walk, heart failure hospitalizations, Kansas City Cardiomyopathy Questionnaire), Hepatitis C (HCV RNA quantification, genotype, liver stiffness measurements FibroScan, sustained virologic response SVR12/SVR24 definitions, liver enzymes ALT/AST), HIV (CD4 count, HIV-1 RNA viral load, antiretroviral resistance testing genotype/phenotype, opportunistic infection prophylaxis and occurrences), Huntington's Disease (CAG repeat length, Unified Huntington's Disease Rating Scale motor/cognitive/behavioral/functional assessments, caudate volume from MRI), Hypertension (ambulatory blood pressure monitoring, home BP measurements, hypertensive urgency/emergency definitions, target organ damage assessment), Influenza (viral culture/PCR, symptom diaries, secondary bacterial infections, hospitalization for respiratory complications), Major Depressive Disorder (Hamilton Depression Rating Scale, MADRS, PHQ-9, remission defined as HAMD-17 7 or MADRS 10, suicidal ideation Columbia Suicide Severity Rating Scale), Multiple Sclerosis (relapse definitions, Expanded Disability Status Scale EDSS, MRI lesion counts T2/gadolinium-enhancing, optical coherence tomography for retinal nerve fiber layer), Non-Small Cell Lung Cancer (PD-L1 expression, EGFR/ALK mutation status, RECIST 1.1, iRECIST for immunotherapy trials, progression-free survival, overall survival), Obesity (BMI, waist circumference, body composition DEXA, metabolic parameters glucose/insulin/lipids, weight loss maintenance), Osteoarthritis (WOMAC pain/stiffness/function subscales, joint space width from radiographs, MRI cartilage morphology, rescue analgesic use), Parkinson's Disease (Unified Parkinson's Disease Rating Scale UPDRS motor/non-motor sections, Hoehn and Yahr staging, levodopa equivalent daily dose, "on"/"off" time diaries, DaTscan imaging), Prostate Cancer (PSA, Gleason score, TNM staging, bone scan for metastases, androgen deprivation therapy metrics testosterone levels), Psoriasis (Psoriasis Area and Severity Index PASI, body surface area affected, physician's global assessment, DLQI quality of life), Rheumatoid Arthritis (Disease Activity Score DAS28, American College of Rheumatology ACR20/50/70 response criteria, tender/swollen joint counts, CRP/ESR, rheumatoid factor/anti-CCP antibodies, radiographic progression Sharp/van der Heijde scores), Schizophrenia (Positive and Negative Syndrome Scale PANSS, Clinical Global Impression CGI, extrapyramidal symptoms scales, metabolic monitoring glucose/lipids/weight), Solid Tumors (RECIST 1.1 target/non-target lesions, disease control rate, duration of response, circulating tumor DNA, quality of life EORTC QLQ-C30), Transplantation (rejection episodes classified by Banff criteria for kidney/ISHLT for heart/lung, graft function creatinine/ejection fraction/FEV1, immunosuppressant drug levels tacrolimus/cyclosporine, donor-specific antibodies), Tuberculosis (sputum smear microscopy, culture, GeneXpert MTB/RIF, chest radiography, time to culture conversion, treatment outcomes per WHO definitions cure/completed/failed/died/lost-to-follow-up), and Ulcerative Colitis (Mayo Clinic Score, endoscopic appearance, histology, fecal calprotectin, clinical remission definitions). Each TAUG document typically includes scope defining included diseases and study phases, general considerations for therapeutic area-specific trial design and data collection, SDTM implementation guidance showing how to represent disease-specific findings in domains (LB for laboratory, EG for ECG, procedures in PR, questionnaires in QS with appropriate test codes from CDISC controlled terminology or standardized instruments), CDASH recommendations for case report form design with annotated CRF examples demonstrating question text, response options, and mapping to SDTM variables, ADaM considerations for analysis datasets including derivation of composite endpoints (e.g., MACE in cardiovascular, ACR response in rheumatology, sustained virologic response in hepatitis C), disease-specific controlled terminology extensions adding terms not in foundational CDISC CT (e.g., specific tumor histologies, rare adverse events unique to therapeutic area, proprietary assessment scales), and therapeutic area data standards team (TAUG team) roster listing volunteer subject matter experts from pharmaceutical companies, CROs, regulatory agencies, and academia who developed the guidance. TAUGs facilitate consistent data collection and submission practices across sponsors for the same disease enabling regulatory reviewers to more efficiently evaluate trials in a therapeutic area when data follows common standards, support integrated analyses pooling data across multiple trials (meta-analyses, network meta-analyses, indirect treatment comparisons) by ensuring harmonized endpoint definitions and data structures, enable development of therapeutic area-specific data repositories and disease registries where standardized data from multiple sources can be aggregated for real-world evidence generation and post-marketing surveillance, accelerate clinical trial startup by providing sponsors with ready-to-use CRF templates and SDTM implementation conventions reducing need to reinvent standards for each protocol, improve data quality by clarifying ambiguous data collection scenarios through worked examples (e.g., how to represent partial responses in oncology, how to handle missed doses in diabetes insulin trials, how to code cardiovascular events when patient experiences multiple MIs), support machine learning and AI applications in clinical research where standardized therapeutic area data enables training of predictive models across trials (e.g., predicting treatment response from baseline characteristics, early biomarker changes predicting long-term outcomes, natural language processing extracting adverse events from narrative fields when standardized coding applied), facilitate regulatory interactions where sponsors and reviewers can reference agreed-upon TAUG conventions avoiding lengthy discussions about data representation choices, enable global harmonization as TAUGs incorporate perspectives from FDA, EMA, PMDA ensuring data structures support requirements across regulatory regions reducing need for region-specific datasets, and position CDISC standards as living, evolving specifications where TAUGs are updated as therapeutic landscapes change (new biomarkers discovered, novel endpoints validated, emerging therapies like gene therapy/CAR-T requiring new data models). TAUG development follows CDISC's consensus-driven process involving public review periods where stakeholders comment on draft guidance, therapeutic area teams incorporating feedback iteratively, final publication as official CDISC standard with version control, and post-publication maintenance addressing implementation questions through Q&A documents and hosting therapeutic area-focused webinars and workshops. Integration with broader CDISC ecosystem positions TAUGs alongside foundational standards (SDTM, CDASH, ADaM), protocol standards (Protocol Representation Model defining study design elements adaptable to therapeutic area requirements), controlled terminology (CDISC CT with therapeutic area extensions), and Define-XML metadata (documenting therapeutic area-specific dataset structures and derivations), creating comprehensive infrastructure for standardized clinical research data management. Machine learning applications leveraging TAUG-standardized data span predictive modeling for patient stratification where therapeutic area-specific baseline covariates (disease severity scores, biomarkers, imaging parameters) predict treatment response enabling enrichment designs selecting patients most likely to benefit, trial simulation using historical control data from prior TAUG-compliant trials to optimize adaptive designs and sample size calculations, safety signal detection across therapeutic area where standardized adverse event coding and laboratory data enable aggregate analysis identifying rare safety signals not apparent in individual trials, natural language processing training on narrative fields (medical history, adverse event descriptions, concomitant medication indications) mapped to standardized codes improving automated coding accuracy, and real-world evidence generation linking TAUG-compliant clinical trial data to claims databases and electronic health records using common data models (OMOP, Sentinel) with TAUG-defined phenotype algorithms for disease identification and outcome ascertainment ensuring consistency between RCT populations and real-world cohorts. TAUGs exemplify how therapeutic area expertise, clinical research best practices, and data standardization converge to create actionable implementation guidance transforming abstract foundational standards into concrete disease-specific data models that balance rigor of standardization with flexibility to capture unique clinical phenomena, ultimately advancing the goal of making clinical trial data more FAIR (Findable, Accessible, Interoperable, Reusable) through discipline-specific conventions that respect domain knowledge while enabling cross-trial integration and secondary use of clinical research data for regulatory decision-making, evidence synthesis, and translational research accelerating therapeutic development and improving patient outcomes across diverse disease areas.
  related_to:
  - B2AI_STANDARD:41
  - B2AI_STANDARD:42
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:15
  url: https://www.cdisc.org/standards/therapeutic-areas
- id: B2AI_STANDARD:44
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDISC Controlled Terminology Standards for Data Aggregation through
    Study Data Tabulation Model (including QRS, Medical Device and Pharmacogenomics
    Data)
  formal_specification: https://evs.nci.nih.gov/ftp1/CDISC/SDTM/
  is_open: true
  name: SDTM
  purpose_detail: A standard for organizing and formatting data to streamline processes
    in collection, management, analysis and reporting.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:15
  url: https://www.cdisc.org/standards/foundational/sdtm
- id: B2AI_STANDARD:45
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDISC Controlled Terminology Standards for Data Collection through
    Clinical Data Acquisition Standards Harmonization
  formal_specification: https://evs.nci.nih.gov/ftp1/CDISC/SDTM/
  is_open: true
  name: CDASH
  purpose_detail: The Clinical Data Acquisition Standards Harmonization (CDASH) is
    a CDISC foundational standard that establishes consistent data collection practices
    across clinical studies and sponsors. CDASH provides a standardized framework
    for designing case report forms (CRFs) and electronic data capture (eCRF) systems
    by defining a common set of data collection fields and structures. The standard
    specifies field names, labels, prompt questions, and controlled terminology for
    capturing clinical observations, ensuring that data can be directly traced to
    the Study Data Tabulation Model (SDTM) without extensive post-collection transformation.
    By harmonizing data collection at the source, CDASH reduces data cleaning efforts,
    improves data quality, and accelerates the transition from collection to analysis.
    The standard covers various therapeutic areas and data domains including demographics,
    vital signs, adverse events, concomitant medications, and laboratory results.
    CDISC provides a library of ready-to-use, CDASH-compliant annotated eCRFs in multiple
    formats (PDF, HTML, XML) that can be implemented as-is or customized for specific
    study needs, facilitating regulatory submission and data review processes.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:15
  url: https://www.cdisc.org/standards/foundational/cdash
- id: B2AI_STANDARD:46
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDISC Dataset-XML
  is_open: true
  name: CDISC Dataset
  purpose_detail: CDISC Dataset-XML, which was released for comment under the name
    StudyDataSet-XML but was renamed to avoid confusion with the CDISC SDS team,
    is a new standard used to exchange study datasets in an XML format. The purpose
    of Dataset-XML is to support the interchange of tabular data for clinical research
    applications using ODM-based XML technologies. The Dataset-XML model is based
    on the CDISC Operational Data Model (ODM) standard and should follow the metadata
    structure defined in the CDISC Define-XML standard. Dataset-XML can represent
    any tabular dataset including SDTM, ADaM, SEND, or non-standard legacy datasets.
    Some noteworthy items relating to Dataset-XML v1.0 include alternative to SAS
    Version 5 Transport (XPT) format for datasets ODM-based model for representation
    of SEND, SDTM, ADaM or legacy datasets Capable of supporting CDISC regulatory
    data submissions Based on Define-XML v2 or v1 metadata, easy to reference Dataset-XML
    supports all language encodings supported by XML.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:15
  url: https://www.cdisc.org/standards/data-exchange/dataset-xml
- id: B2AI_STANDARD:47
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDISC Laboratory Data Model
  is_open: true
  name: CDISC LAB
  purpose_detail: LAB provides a standard model for the acquisition and exchange of
    laboratory data, primarily between labs and sponsors or CROs. The LAB standard
    was specifically designed for the interchange of lab data acquired in clinical
    trials.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:15
  url: https://www.cdisc.org/standards/data-exchange/lab
  has_application:
  - id: B2AI_APP:9
    category: B2AI:Application
    name: HL7 FHIR to CDISC Laboratory Mapping for ML-Ready EHR Data
    description: The HL7 FHIR to CDISC Joint Mapping Implementation Guide includes
      mappings for the Laboratory domain that translate FHIR lab resources into CDISC
      variables, linked with CDISC LB-to-LOINC mapping guidance to leverage real-world
      data for clinical trials. This harmonization enables EHR laboratory observations
      to be transformed into standardized, ML-ready lab features with consistent semantic
      coding via LOINC, supporting downstream machine learning analyses, pooled studies,
      and data mining across clinical trial and real-world datasets.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.47912/jscdm.162
  - id: B2AI_APP:111
    category: B2AI:Application
    name: Metadata-Driven ETL for Scalable Lab Dataset Construction
    description: CDISC Laboratory Model implementations use metadata-driven ETL approaches
      to produce standards-compliant lab datasets from source systems, reducing manual
      data preparation through automated transformation logic that operates at the
      metadata level rather than requiring code changes. This metadata-driven approach
      is foundational for building scalable, ML-ready dataset construction pipelines
      that can consistently process laboratory data across multiple studies and sites,
      enabling reproducible preprocessing for machine learning workflows.
    used_in_bridge2ai: false
    references:
    - https://www.lexjansen.com/pharmasug/2002/proceed/DM/dm01.pdf
  - id: B2AI_APP:112
    category: B2AI:Application
    name: Integrated SDTM Laboratory Data for Pooled ML Analytics
    description: Integration of clinical trial and real-world data using CDISC standards
      including the Laboratory domain produces harmonized SDTM and ADaM datasets that
      enable pooled analyses and data mining across multiple studies and sponsors.
      Standardized laboratory variables with CDISC Controlled Terminology facilitate
      dataset aggregation, warehousing, and reuse, supporting pooled machine learning
      analyses and real-world evidence research by providing consistent lab feature
      representations that serve as inputs for predictive modeling and downstream
      analytics.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.47912/jscdm.128
- id: B2AI_STANDARD:48
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDISC Operational Data Model
  is_open: true
  name: CDISC ODM
  purpose_detail: The CSDIC ODM is a vendor-neutral, platform-independent format for
    exchanging and archiving clinical and translational research data, along with
    their associated metadata, administrative data, reference data, and audit information.
    ODM facilitates the regulatory-compliant acquisition, archival and exchange of
    metadata and data.
  related_to:
  - B2AI_STANDARD:689
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:15
  url: https://www.cdisc.org/standards/data-exchange/odm
  has_application:
  - id: B2AI_APP:10
    category: B2AI:Application
    name: Clinical Trial Data Integration and Predictive Modeling
    description: CDISC ODM (Operational Data Model) is used in AI applications for
      standardizing clinical trial data exchange, enabling machine learning models
      to train on multi-study datasets and predict trial outcomes, patient enrollment,
      and safety events. AI systems leverage ODM's XML-based representation of study
      metadata, case report forms, and clinical data to automatically harmonize data
      from different trials, perform cross-study analyses, and develop predictive
      models for trial design optimization. The standard enables AI applications that
      forecast patient dropout rates, identify optimal sites for recruitment based
      on historical data, and detect protocol deviations through anomaly detection.
      ODM's structured format facilitates automated quality control and regulatory
      submission preparation through AI-driven validation.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1016/j.cct.2019.105820
- id: B2AI_STANDARD:49
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDISC Protocol Representation Model
  is_open: true
  name: CDISC PRM
  purpose_detail: The CDISC Protocol Representation Model Version 1.0 (PRM V1.0) is
    intended for those involved in the planning and design of a research protocol.
    The model focuses on the characteristics of a study and the definition and association
    of activities within the protocols, including arms and epochs. PRM V1.0 also includes
    the definitions of the roles that participate in those activities. The scope of
    this model includes protocol content including Study Design, Eligibility Criteria,
    and the requirements from the ClinicalTrials.gov and World Health Organization
    (WHO) registries. The majority of business requirements were provided by subject
    matter experts in clinical trial protocols. PRM V1.0 is based on the BRIDG Release
    3.0 Protocol Representation sub-domain. It includes all classes in the BRIDG Protocol
    Representation sub-domain plus some classes from other BRIDG sub-domains, generally
    classes required for ClinicalTrials.gov and the WHO registries.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:15
  url: https://www.cdisc.org/standards/foundational/protocol
- id: B2AI_STANDARD:50
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDISC Standard for the Exchange of Nonclinical Data
  is_open: true
  name: SEND
  purpose_detail: The CDISC SEND is intended to guide the organization, structure,
    and format of standard nonclinical tabulation datasets for interchange between
    organizations such as sponsors and CROs and for submission to the US Food and
    Drug Administration (FDA)
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:15
  url: https://www.cdisc.org/standards/foundational/send
- id: B2AI_STANDARD:51
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDISC Study Design Model in XML
  is_open: true
  name: CDISC SDM
  purpose_detail: The CDISC Study Design Model in XML (SDM-XML) version 1.0 allows
    organizations to provide rigorous, machine-readable, interchangeable descriptions
    of the designs of their clinical studies, including treatment plans, eligibility
    and times and events. As an extension to the existing CDISC Operational Data Model
    (ODM) specification, SDM-XML affords implementers the ease of leveraging existing
    ODM concepts and re-using existing ODM definitions. SDM-XML defines three key
    sub-modules  Structure, Workflow, and Timing  permitting various levels of detail
    in any representation of a clinical studys design, while allowing a high degree
    of authoring flexibility. The specification document is available for download
    as a PDF file. A ZIP file containing the XML Schemas, several examples, and an
    SDM-XML element and attribute reference also is available.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:15
  url: https://www.cdisc.org/standards/data-exchange/sdm-xml
- id: B2AI_STANDARD:52
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CHADO XML interchange Format
  formal_specification: https://github.com/GMOD/Chado
  has_relevant_organization:
  - B2AI_ORG:125
  is_open: true
  name: CHADO
  publication: doi:10.1093/bioinformatics/btm189
  purpose_detail: Chado is a modular schema covering many aspects of biology, not
    just sequence data. Chado-XML has exactly the same scope as the Chado schema.
  requires_registration: false
  url: https://github.com/GMOD/Chado
- id: B2AI_STANDARD:53
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Chain Format for pairwise alignment
  is_open: true
  name: chain
  purpose_detail: The Chain format is a compact file format developed by UCSC for
    representing pairwise genomic sequence alignments that permit gaps in both the
    reference and query sequences simultaneously. Each chain alignment consists of
    a header line specifying score, chromosome identifiers, sizes, strands, and coordinates
    for both sequences, followed by alignment data lines describing ungapped blocks
    and the gaps between them. The format uses zero-based half-open intervals for
    coordinates and includes support for reverse-complement alignments through strand
    indicators. Chain files are particularly useful for comparing whole genomes or
    large genomic regions, representing syntenic relationships, structural variations,
    and evolutionary rearrangements. They are commonly used in genome browsers and
    liftOver tools for mapping genomic coordinates between different genome assemblies
    or between species. The format supports "snake" or rearrangement display modes
    that visualize complex structural variants including inversions, duplications,
    and translocations. Chain alignments can be produced by tools like BLASTZ and
    are widely used in comparative genomics workflows.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:119
  url: http://genome.ucsc.edu/goldenPath/help/chain.html
- id: B2AI_STANDARD:54
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:27
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CHARMM Card File Format
  is_open: true
  name: CARD
  purpose_detail: The CARD file format is the standard means in CHARMM for providing
    a human readable and writable coordinate file.
  requires_registration: false
  url: https://charmm-gui.org/charmmdoc/io.html
- id: B2AI_STANDARD:55
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:3
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Chemical Markup Language
  is_open: true
  name: CML
  purpose_detail: CML (Chemical Markup Language) is an XML language designed to hold
    most of the central concepts in chemistry. It was the first language to be developed
    and plays the same role for chemistry as MathML for mathematics and GML for geographical
    systems. CML covers most mainstream chemistry and especially molecules, reactions,
    solid-state, computation and spectroscopy. Since it has a special flexible approach
    to numeric science it also covers a very wide range of chemical properties, parameters
    and experimental observation. It is particularly concerned with the communication
    between machines and humans, and machines to machines. It has been heavily informed
    by the current chemical scholarly literature and chemical databases. XML is a
    mainstream approach providing semantics for science, such as MathML, SBML/BIOPAX
    (biology), GML and KML (geo) SVG (graphics) and NLM-DTD, ODT and OOXML (documents).
    CML provides support for most chemistry, especially molecules, compounds, reactions,
    spectra, crystals and computational chemistry (compchem). CML has been developed
    by Peter Murray-Rust and Henry Rzepa since 1995. It is the de facto XML for chemistry,
    accepted by publishers and with more than 1 million lines of Open Source code
    supporting it. CML can be validated and built into authoring tools (for example
    the Chemistry Add-in for Microsoft Word). A list of CML-compliant and CML-aware
    software can be found on the software page. The infrastructure includes legacy
    converters, dictionaries and conventions, Semantic Web and Linked Open Data. There
    are several versions of the CML schema. The most recent schema is schema 3. This
    essentially relaxes many of the constraints imposed in the previous stable release
    (schema 2.4), allowing users to put together the elements and attributes in a
    more flexible manner to fit the data that they want to represent more easily.
  requires_registration: false
  url: https://www.xml-cml.org/
- id: B2AI_STANDARD:56
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:4
  - B2AI_TOPIC:35
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ClinGen Interpretation Model
  formal_specification: https://github.com/clingen-data-model/clingen-interpretation
  is_open: true
  name: ClinGen Interpretation
  purpose_detail: The ClinGen Interpretation Model is a comprehensive data model and
    exchange format designed to capture, structure, and communicate the clinical interpretation
    of genetic variants. It supports the representation of evidence, reasoning, provenance,
    and contextual information underlying variant pathogenicity assessments, aligning
    with ACMG/AMP guidelines and community standards. The model enables detailed documentation
    of what evidence was used, how it was applied, who performed each interpretive
    act, and how interpretations build upon prior knowledge. It is flexible enough
    to represent both simple assertions and fully evidence-based interpretations,
    and is implemented using industry-standard JSON-LD for interoperability. The ClinGen
    Interpretation Model is closely aligned with the Monarch Initiative's SEPIO ontology,
    supporting integration with other biomedical data models and facilitating automated
    reasoning, reproducibility, and data sharing in clinical genomics.
  requires_registration: false
  url: https://dataexchange.clinicalgenome.org/interpretation/
  has_application:
  - id: B2AI_APP:11
    category: B2AI:Application
    name: LitGen Semi-Supervised Literature Recommendation and Evidence Classification
    description: LitGen uses semi-supervised deep learning trained on ClinGen VCI-curated
      papers annotated with ACMG/AMP-aligned evidence types, incorporating curator
      explanations and unlabeled ClinGen-linked papers to improve proxy labels for
      literature recommendation and evidence-type classification. The system leverages
      ClinGen Variant Curation Interface annotations to predict which papers support
      specific ACMG/AMP evidence criteria, achieving 7.9-12.6% relative performance
      improvement over supervised-only models and reducing manual curation burden
      for variant interpretation workflows.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1142/9789811215636_0007
  - id: B2AI_APP:113
    category: B2AI:Application
    name: ACMG/AMP Feature-Based Variant Pathogenicity Classification
    description: Machine learning models explicitly encode ACMG/AMP evidence criteria
      as features in penalized logistic regression to produce probabilistic pathogenicity
      scores and rank variants of uncertain significance for prioritization. The approach
      treats ClinGen/ACMG guideline-based evidence levels as structured ML features,
      enabling guidelines-informed classification that slightly outperforms some in
      silico scores on certain VUS datasets and improves prioritization for clinical
      decision support in hereditary disease gene panels.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1038/s41598-022-06547-3
  - id: B2AI_APP:114
    category: B2AI:Application
    name: ClinGen SVI Calibration of Computational Predictors to PP3/BP4
    description: A probabilistic framework maps continuous scores from missense variant
      prediction tools to ACMG/AMP PP3 and BP4 evidence strength levels using ClinGen/ACMG
      standards, establishing score intervals for Supporting, Moderate, and Strong
      evidence. This calibration enables automated, standardized assignment of computational
      evidence to ClinGen-aligned interpretation workflows, with most tools achieving
      Supporting level and several reaching Moderate or Strong evidence thresholds
      for pathogenic or benign classification.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1016/j.ajhg.2022.10.013
  - id: B2AI_APP:115
    category: B2AI:Application
    name: CGBench LLM Benchmarking for ClinGen Evidence Reasoning
    description: CGBench uses ClinGen VCI and GCI entries with expert explanations
      as ground truth to evaluate large language models on evidence extraction, strength
      scoring, and explanation concordance for variant interpretation. The benchmark
      shows moderate extraction performance with GPT-4o achieving 49% precision and
      79% recall, improved explanation concordance with few-shot prompting reaching
      70%, but limited strength-change prediction accuracy, revealing both capabilities
      and gaps in LLM reasoning for ClinGen-aligned genomic curation tasks.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.48550/arxiv.2510.11985
- id: B2AI_STANDARD:57
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CLSI Next-Generation In Vitro Diagnostic Interface
  is_open: false
  name: CLSI AUTO16
  purpose_detail: Exchange of data about and produced by in vitro diagnostic tests.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:16
  url: https://clsi.org/standards/products/automation-and-informatics/documents/auto16/
- id: B2AI_STANDARD:58
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  - B2AI_TOPIC:26
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CLUSTAL-W Alignment Format
  is_open: true
  name: MSF
  purpose_detail: The MSF (Multiple Sequence Format) is a standardized file format
    produced by CLUSTAL-W for representing multiple sequence alignments of proteins
    or nucleic acids. It includes sequence metadata, alignment length, checksums for
    data integrity verification, and displays aligned sequences in blocks with position
    numbering. The format supports gap characters and ambiguity codes, making it widely
    compatible with bioinformatics tools for phylogenetic analysis, sequence conservation
    studies, and comparative genomics. MSF files preserve alignment quality information
    and are human-readable while maintaining machine-parseable structure for automated
    processing.
  requires_registration: false
  url: https://bioinfo.nhri.edu.tw/gcg/doc/11.0/clustalw+.html
- id: B2AI_STANDARD:59
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  - B2AI_TOPIC:26
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CLUSTAL-W Dendrogram Guide File Format
  is_open: true
  name: DND
  purpose_detail: Format for the tree (or dendrogram) used to guide the a multiple
    sequence alignment process.
  requires_registration: false
  url: https://bioinfo.nhri.edu.tw/gcg/doc/11.0/clustalw+.html
- id: B2AI_STANDARD:60
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CODE-EHR best-practice framework for the use of structured electronic
    health-care records in clinical research
  is_open: true
  name: CODE-EHR
  publication: doi:10.1016/S2589-7500(22)00151-0
  purpose_detail: "The CODE-EHR (Clinical Outcomes Data in EHR) framework establishes minimum and preferred standards for the design, conduct, analysis, and reporting of clinical research studies using routinely collected electronic health record data, developed through international consensus by the BigData@Heart consortium and European Society of Cardiology with participation from patients, patient advocacy groups, regulators, government agencies, leading medical journals, professional societies, academic institutions, pharmaceutical industry, and payers to enhance transparency, reproducibility, and confidence in EHR-based research findings across the disease spectrum. The framework addresses critical challenges in secondary use of healthcare data including data quality uncertainties, heterogeneous coding systems, variable completeness, linkage complexities, phenotype definition ambiguities, analytical reproducibility gaps, and governance variability that limit comparability across studies and stakeholder confidence in observational research compared to randomized controlled trials. CODE-EHR provides step-by-step guidance across five key domains with minimum standards (essential requirements for methodological rigor) and preferred standards (aspirational best practices for future research): Dataset construction and linkage requires clarifying data sources (single institution EHR, regional health information exchanges, national registries, claims databases), temporal coverage (date ranges for data availability), patient population characteristics (age, sex, demographics), completeness assessment (proportion of expected records present, missingness patterns), and linkage methodology when integrating multiple datasets (deterministic vs. probabilistic matching algorithms, linkage quality metrics including match rates and false positive/negative rates, handling of duplicate records); Data fit for purpose mandates documenting coding systems used (ICD-9/10/11 for diagnoses, CPT/HCPCS for procedures, RxNorm/ATC for medications, LOINC for laboratory tests, SNOMED CT for clinical findings), data transformations and manipulations (unit conversions, date/time standardization, free-text to structured data mapping via natural language processing), data quality assessment including validation against external references or chart review gold standards (positive predictive value, sensitivity, specificity of coded diagnoses), plausibility checks (biologically implausible values, temporal logic violations), and missing data characterization (mechanismsmissing completely at random, missing at random, missing not at randomwith implications for analytical validity); Disease outcome and definitions requires transparent specification of all computable phenotype definitions using explicit code lists and algorithms for patient identification (inclusion/exclusion criteria with specific codes and lookback periods), exposure/treatment definitions (medication exposure windows, dosing requirements, persistence criteria), procedures and interventions (with timing relative to outcomes), comorbidity definitions (Charlson Comorbidity Index, Elixhauser conditions with code mappings), and outcomes (primary and secondary endpoints with validation evidence, composite outcome components, censoring rules, competing risks), enabling other researchers to reproduce cohort selection, validate findings in independent datasets, and iteratively refine definitions; Analysis standards specify analytical approaches with sufficient detail for replication including cohort flow diagrams (CONSORT-style reporting showing number of patients screened, excluded at each step with reasons, final analytical cohort), handling of missing data (complete case analysis, multiple imputation methods and assumptions, inverse probability weighting), confounding control strategies (covariate adjustment, propensity score methodsmatching, stratification, inverse probability of treatment weightinginstrumental variables, difference-in-differences, regression discontinuity), exposure and outcome timing definitions (index date, baseline period for covariate assessment, washout periods, follow-up windows, grace periods for treatment discontinuation), statistical models (regression specifications, survival analysis censoring assumptions, time-varying covariates, model diagnostics), sensitivity analyses exploring robustness to design choices (alternative phenotype definitions, lookback period durations, covariate sets, statistical approaches), and reporting of effect estimates with measures of uncertainty (confidence intervals, p-values with multiple testing corrections if applicable); Ethics and governance domain addresses consent frameworks (broad consent for secondary research, opt-out mechanisms, consent waiver justifications), data privacy safeguards including de-identification approaches (HIPAA Safe Harbor method removing 18 identifier types, expert determination, synthetic data generation, aggregation/suppression of small cells), data access and sharing policies (open data, controlled access via data use agreements, federated query systems preventing data egress, differential privacy mechanisms), institutional review board or ethics committee approvals with protocol registration in public registries (ClinicalTrials.gov, EU Clinical Trials Register) documenting study objectives and methods prior to data access reducing selective reporting and p-hacking, patient and public involvement in research design and interpretation (patient advisory boards, plain language study summaries, return of results to participants), and transparent conflict of interest disclosures from investigators and funders. For artificial intelligence and machine learning applications on EHR data, CODE-EHR principles guide trustworthy model development where dataset construction standards ensure training/validation/test data provenance and quality are documented enabling assessment of dataset shift and domain adaptation needs; data fitness standards support feature engineering transparency (derived variable definitions, temporal feature construction like lookback windows for aggregating lab trends or medication counts, handling of sparse/irregular time-series); outcome definition standards ensure prediction targets have clinical validity (validated phenotypes, ground truth labels with known accuracy, clear clinical utility for the prediction task); analysis standards extend to ML-specific requirements including training procedures (cross-validation strategies, hyperparameter tuning without test set leakage, ensemble methods, regularization), performance metrics beyond AUROC (calibration curves, decision curve analysis quantifying net benefit, sensitivity/specificity at clinically relevant thresholds, performance across patient subgroups assessing fairness), model interpretability and explainability (SHAP values, attention weights, concept activation vectors, counterfactual explanations), uncertainty quantification (conformal prediction, Bayesian credible intervals), external validation on independent healthcare systems or time periods, prospective evaluation measuring real-world clinical impact not just predictive accuracy, and continuous monitoring for model drift requiring retraining; governance standards for ML include algorithmic transparency (model cards documenting intended uses, training data characteristics, performance benchmarks, fairness metrics, limitations), consent considerations for automated decision-making, data sharing enabling independent model validation and adversarial testing, and regulatory pathways (FDA software as medical device, CE marking in Europe). CODE-EHR also informs federated learning on multi-institutional EHR data where standards ensure harmonized phenotype definitions across sites, consistent data quality metrics, coordinated governance frameworks (distributed institutional review boards, federated data use agreements, privacy-preserving analytics), and evaluation of model generalization versus site-specific performance heterogeneity. The framework supports causal inference from observational EHR data by emphasizing directed acyclic graphs documenting assumed causal relationships and confounders, sensitivity analyses for unmeasured confounding (E-values quantifying minimum strength of unmeasured confounders to nullify findings), negative control outcomes (exposures or outcomes with no plausible causal pathway serving as bias indicators), and triangulation with evidence from diverse study designs (comparing EHR findings to randomized trials, Mendelian randomization using genetic instruments, sibling designs controlling familial confounding). Ultimately, CODE-EHR establishes a methodological and reporting standard elevating EHR-based research to scientific rigor comparable to prospective studies, enabling reproducibility through transparent reporting, facilitating evidence synthesis through standardized definitions, supporting regulatory and clinical guideline decision-making through credible observational evidence, and fostering patient and public trust through ethical governance and stakeholder engagement, positioning EHR data as a core research infrastructure for precision medicine, comparative effectiveness research, pharmacovigilance, health services research, and AI-driven clinical decision support."
  requires_registration: false
  url: https://www.bigdata4betterhearts.eu/News/ID/146/More-about-the-CODE-EHR-approach-and-framework
- id: B2AI_STANDARD:61
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Coherent X-ray Imaging Data Bank format
  is_open: true
  name: CXI
  purpose_detail: The CXI format was created as common format for all the data in
    the Coherent X-ray Imaging Data Bank (CXIDB). Naturally its scope is all experimental
    data collected during Coherent X-ray Imaging experiments as well as all data generated
    during the analysis of the experimental data.
  requires_registration: false
  url: https://www.cxidb.org/cxi.html
- id: B2AI_STANDARD:62
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Collaborative Computing Project for the NMR community data model
  is_open: true
  name: CCPN
  publication: doi:10.1002/prot.20449
  purpose_detail: The CCPN Data Model for macromolecular NMR is intended to cover
    all data needed for macromolecular NMR spectroscopy from the initial experimental
    data to the final validation. It serves for exchange of data between programs,
    for storage, data harvesting, and database deposition. The data model proper is
    an abstract description of the relevant data and their relationships - it is implemented
    in the modeling language UML. From this CCPN autogenerates interfaces (APIs) for
    various languages, format description and I/O routines, and documentation.
  requires_registration: false
  url: https://sites.google.com/site/ccpnwiki/home/documentation/ccpnmr-analysis/core-concepts/the-ccpn-data-model
  has_application:
  - id: B2AI_APP:12
    category: B2AI:Application
    name: CcpNmr-Integrated Deep Learning for HNCA Backbone Assignment
    description: Deep neural networks integrated with the CcpNmr AnalysisAssign software
      analyze HNCA spectrum line shapes to yield amino acid type probabilities for
      automated protein backbone assignment. The networks are trained on synthetic
      databases with realistic instrumental artifacts and noise, taking 2D slices
      of pyruvate-patterned HNCA spectra as input and outputting probability tables
      that combine with primary sequence information for rapid assignment. This ML
      integration within the CCPN software framework accelerates NMR analysis workflows,
      though networks require retraining when experimental parameters change.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1126/sciadv.ado0403
- id: B2AI_STANDARD:63
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Common Fund Data Ecosystem Crosscut Metadata Model
  formal_specification: https://osf.io/bq6k9/
  is_open: true
  name: CFDE C2M2
  purpose_detail: The Common Fund Data Ecosystem (CFDE) Crosscut Metadata Model (C2M2)
    is a flexible, extensible data model designed to harmonize metadata across NIH
    Common Fund data coordinating centers (DCCs) to enable integrated search, discovery,
    and analysis. C2M2 provides a standardized framework for describing biomedical
    research assets including files, biosamples, subjects, and collections with their
    associated metadata using controlled vocabularies and ontologies. The model consists
    of core entities (file, biosample, subject, project, collection) connected through
    relationships that capture data provenance, experimental context, and biological
    associations. C2M2 submissions include data tables describing these entities along
    with controlled vocabulary term usage tables that link to standard ontologies
    for enhanced semantic interoperability. The model supports supra-schematic constraints
    validated by dedicated tools, and includes ontology reference files for adding
    display names, descriptions, and synonyms to controlled terms. C2M2 enables the
    CFDE Workbench portal to provide unified search and browsing across heterogeneous
    Common Fund datasets, facilitating cross-program data discovery and promoting
    FAIR (Findable, Accessible, Interoperable, Reusable) principles for biomedical
    research data.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:68
  url: https://docs.nih-cfde.org/en/latest/c2m2/draft-C2M2_specification/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:13
    category: B2AI:Application
    name: CFDE Workbench ML-Ready Data Harmonization and Knowledge Graph Integration
    description: The CFDE Workbench uses C2M2 to harmonize metadata and host processed
      data in standardized, AI-ready formats that can be loaded as data frames and
      integrated into ML workflows. C2M2-derived metadata feeds PostgreSQL databases
      supporting complex queries for ML dataset preparation, while planned conversion
      to Neo4j knowledge graphs will align database structure with the metadata model
      to enhance search performance and facilitate ML applications. This harmonization
      across Common Fund programs reduces barriers to assembling training corpora
      and interoperable inputs for cross-program AI/ML analyses.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2025.02.04.636535
  - id: B2AI_APP:116
    category: B2AI:Application
    name: LLM-Powered Chatbot with C2M2-Grounded Workflow Automation
    description: The CFDE Workbench integrates an LLM chatbot using OpenAI assistants
      API that answers Common Fund program and data questions while executing API
      calls based on standardized workflow specifications. The chatbot leverages C2M2-harmonized
      resources and codified ETL workflows through the Playbook Workflow Builder to
      automate discovery and retrieval, using function calling to ensure comprehensive
      and reproducible results. This direct AI application is enabled by C2M2-backed
      infrastructure that provides the structured metadata and APIs necessary for
      the LLM to interact with heterogeneous datasets.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2025.02.04.636535
  - id: B2AI_APP:117
    category: B2AI:Application
    name: FAIRshake Assessment of C2M2-Derived Assets for AI-Readiness
    description: The CFDE Workbench subjects metadata, processed data, and code assets
      derived from C2M2 submissions to automated FAIR assessments via FAIRshake. These
      assessments evaluate findability, accessibility, interoperability, and reusability
      of C2M2-derived assets that support AI/ML tools including the Playbook Workflow
      Builder, GeneSetCart, DD-KG-UI, and CFDE-GSE. By operationalizing AI-readiness
      through systematic FAIRness evaluation, this application ensures that C2M2-structured
      datasets meet the accessibility and interoperability requirements necessary
      for downstream ML consumption.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2025.02.04.636535
  - id: B2AI_APP:118
    category: B2AI:Application
    name: Ecosystem-Level Metadata Harmonization for Cross-Program ML Dataset Discovery
    description: The CFDE catalog uses C2M2 to ingest diverse Data Coordinating Center
      metadata into a unified model supporting centralized indexing and search across
      NIH Common Fund programs. This harmonization with controlled vocabularies and
      ontologies minimizes barriers to cross-program dataset discovery and reuse,
      which is prerequisite for assembling training corpora and interoperable inputs
      for AI/ML analyses. C2M2's evolving model adapts to community standards and
      emerging AI/ML technologies, enabling effective search and retrieval mechanisms
      essential for training AI models on biomedical data while promoting FAIR principles.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2021.11.05.467504
- id: B2AI_STANDARD:64
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Common Metadata Elements for Cataloging Biomedical Datasets
  formal_specification: https://figshare.com/articles/dataset/Common_Metadata_Elements_for_Cataloging_Biomedical_Datasets/1496573?file=3377663
  is_open: true
  name: Common Metadata
  purpose_detail: This dataset outlines a proposed set of core, minimal metadata elements
    that can be used to describe biomedical datasets, such as those resulting from
    research funded by the National Institutes of Health. It can inform efforts to
    better catalog or index such data to improve discoverability. The proposed metadata
    elements are based on an analysis of the metadata schemas used in a set of NIH-supported
    data sharing repositories. Common elements from these data repositories were identified,
    mapped to existing data-specific metadata standards from to existing multidisciplinary
    data repositories, DataCite and Dryad, and compared with metadata used in MEDLINE
    records to establish a sustainable and integrated metadata schema.
  requires_registration: false
  url: https://figshare.com/articles/dataset/Common_Metadata_Elements_for_Cataloging_Biomedical_Datasets/1496573
- id: B2AI_STANDARD:65
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - diagnosticinstrument
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: COmprehensive Score for financial Toxicity A FACIT Measure of Financial
    Toxicity
  has_relevant_organization:
  - B2AI_ORG:30
  is_open: true
  name: FACIT-COST
  publication: doi:10.1002/cncr.30369
  purpose_detail: Developed in conjunction with the University of Chicago, the COST
    is a patient-reported outcome measure that describes the financial distress experienced
    by cancer patients. Since its initial publication, an additional item from the
    FACIT System has been included to screen for financial toxicity and to provide
    a good global summary item for financial toxicity.
  requires_registration: false
  url: https://www.facit.org/measures/FACIT-COST
- id: B2AI_STANDARD:66
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Consolidated Clinical Document Architecture
  formal_specification: http://www.hl7.org/implement/standards/product_brief.cfm?product_id=492
  is_open: true
  name: C-CDA
  purpose_detail: A widely-used, XML-based format for electronic health records. Superceded
    by FHIR document standards.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:40
  url: https://www.healthit.gov/topic/standards-technology/consolidated-cda-overview
- id: B2AI_STANDARD:67
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Consolidated criteria for reporting qualitative research
  formal_specification: http://www.cnfs.net/modules/module2/story_content/external_files/13_COREQ_checklist_000017.pdf
  is_open: true
  name: COREQ
  publication: doi:10.1093/intqhc/mzm042
  purpose_detail: 'COREQ (Consolidated Criteria for Reporting Qualitative Research)
    is a comprehensive 32-item reporting checklist specifically designed to improve
    the transparency, rigor, and completeness of qualitative research reports, particularly
    those involving interviews and focus groups. This evidence-based guideline addresses
    three major domains essential for qualitative research reporting: research team
    and reflexivity (covering the researcher''s credentials, occupation, experience,
    gender, and relationship with participants), study design (including theoretical
    framework, participant selection, setting, and data collection methods), and analysis
    and findings (encompassing data analysis approaches, verification procedures,
    and presentation of findings). COREQ serves as a critical quality assessment tool
    for researchers, editors, reviewers, and readers, helping ensure that qualitative
    studies provide sufficient detail for proper evaluation and potential replication.
    The checklist promotes methodological transparency and supports the credibility
    and trustworthiness of qualitative health research by standardizing reporting
    expectations across the research community.'
  requires_registration: false
  url: http://www.cnfs.net/modules/module2/story_content/external_files/13_COREQ_checklist_000017.pdf
- id: B2AI_STANDARD:68
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Consolidated Health Economic Evaluation Reporting Standards
  formal_specification: https://www.ispor.org/heor-resources/good-practices/article/consolidated-health-economic-evaluation-reporting-standards-2022-cheers-2022-statement-updated-reporting-guidance-for-health-economic-evaluations
  is_open: true
  name: CHEERS
  publication: doi:10.1136/bmj.f1049
  purpose_detail: The Consolidated Health Economic Evaluation Reporting Standards
    (CHEERS) statement is an attempt to consolidate and update previous health economic
    evaluation guidelines efforts into one current, useful reporting guidance. The
    primary audiences for the CHEERS statement are researchers reporting economic
    evaluations and the editors and peer reviewers assessing them for publication.
  requires_registration: true
  url: https://www.ispor.org/heor-resources/good-practices/article/consolidated-health-economic-evaluation-reporting-standards-2022-cheers-2022-statement-updated-reporting-guidance-for-health-economic-evaluations
- id: B2AI_STANDARD:69
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Consolidated Standards of Reporting Trials
  is_open: true
  name: CONSORT
  publication: doi:10.1136/bmj.c332
  purpose_detail: CONSORT (Consolidated Standards of Reporting Trials) is an evidence-based
    minimum set of recommendations for reporting randomized controlled trial (RCT)
    results, developed to address inadequate reporting that prevents proper assessment
    of trial quality and generalizability. The CONSORT 2025 Statement consists of
    a 30-item checklist and flow diagram covering how trials are designed, analyzed,
    and interpreted, with detailed recommendations for reporting participant recruitment,
    randomization procedures, interventions, outcomes, sample size calculations, statistical
    methods, baseline characteristics, results for each outcome, harms, interpretation,
    and generalizability. The flow diagram tracks progress of all participants through
    each trial stage (enrollment, allocation, follow-up, analysis) with numbers and
    reasons for exclusions and losses. CONSORT is companion to SPIRIT (Standard Protocol
    Items - Recommendations for Interventional Trials) which provides a 34-item checklist
    for protocol reporting. Both guidelines emphasize transparent and complete reporting
    to enable readers to critically appraise, interpret, and apply research findings.
    CONSORT includes numerous extensions for specific trial designs (cluster randomized,
    non-inferiority, pragmatic trials), interventions (herbal, acupuncture, non-pharmacological
    treatment), and data types. The guidelines are supported by Explanation and Elaboration
    documents with detailed rationale and examples, online tools (COBWEB for report
    writing, SEPTRE for protocol management), translations in multiple languages,
    and endorsement by hundreds of medical journals worldwide as part of the EQUATOR
    Network research reporting standards.
  requires_registration: false
  url: https://www.consort-statement.org/
- id: B2AI_STANDARD:70
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Continua Design Guidelines
  formal_specification: https://members.pchalliance.org/document/dl/2148
  is_open: true
  name: Continua
  purpose_detail: The Continua Design Guidelines, developed and maintained by the Personal Connected Health Alliance (PCHA, formerly Continua Health Alliance), comprise a comprehensive suite of interoperability specifications, certification requirements, and implementation guidelines that enable end-to-end data exchange between personal health devices (blood pressure monitors, glucose meters, pulse oximeters, weight scales, activity trackers, thermometers, peak flow meters, medication dispensers), mobile health applications, electronic health record systems, and cloud-based health platforms through standardized communication protocols (Bluetooth Low Energy, USB, NFC, Wi-Fi), data formats (IEEE 11073 Personal Health Device standards for device-to-gateway communication, HL7 FHIR for clinical system integration), and semantic interoperability frameworks (LOINC codes for observations, UCUM units of measure, SNOMED CT for clinical concepts) creating a unified ecosystem where consumers, patients, clinicians, and researchers can reliably exchange personal health data across heterogeneous devices and systems without vendor lock-in or custom integration efforts. Established in 2006 as a non-profit industry consortium bringing together medical device manufacturers (Omron, A&D Medical, Nonin, iHealth), technology companies (Intel, Qualcomm, IBM), healthcare providers (Kaiser Permanente, Mayo Clinic), and standards organizations (HL7, IEEE, Bluetooth SIG, USB-IF), the Continua Guidelines address critical barriers to personal health device interoperability including proliferative proprietary data formats requiring device-specific software, lack of semantic standardization where "blood pressure" measurements from different devices use inconsistent units or nomenclature, fragmented connectivity with incompatible wireless protocols, and absent security/privacy frameworks for protecting sensitive health data in home and mobile environments. The Continua framework defines a layered architecture where the Device layer specifies personal health device profiles implementing IEEE 11073-20601 optimized exchange protocol for point-of-care devices with domain information models for specific device types (11073-10407 blood pressure, 11073-10415 weighing scale, 11073-10404 pulse oximeter, 11073-10417 glucose meter defining data structures, measurement units, device status codes), the Transport layer standardizes communication mechanisms (Bluetooth Health Device Profile HDP, USB Personal Healthcare Device Class PHDC, Zigbee Health Care Profile, NFC Forum Type 4 Tag), the Gateway/Aggregation layer describes personal health gateways (smartphones, dedicated hubs, set-top boxes) that collect data from multiple devices and forward to health management services via internet protocols, the Health & Fitness Service Interface layer specifies cloud/server APIs for receiving, storing, and exposing personal health data (RESTful web services, HL7 FHIR endpoints), and the WAN Interface layer defines wide-area network connectivity standards ensuring secure transmission between gateways and remote services (TLS/SSL encryption, OAuth2 authentication, IHE XDS.b document sharing). Continua's certification program validates product conformance where devices, gateways, and backend systems undergo testing against specification requirements (protocol compliance, data format correctness, error handling, security measures) by authorized test labs, earning Continua certification marks signaling to consumers and healthcare organizations that products interoperate seamlessly within the Continua ecosystema blood pressure monitor certified to Continua guidelines will successfully pair with any Continua-certified smartphone app or gateway and transmit readings in standardized IEEE 11073 format interpretable by any Continua-compliant health management platform without custom drivers or configuration. The semantic interoperability layer maps device-generated observations to standardized clinical terminologies where blood pressure readings include LOINC codes (8480-6 for systolic, 8462-4 for diastolic), glucose measurements specify units in mg/dL or mmol/L with UCUM codes, body weight uses standardized units (kg, lb), and qualitative observations reference SNOMED CT concepts (248646004 for irregular pulse detected by blood pressure monitor), enabling clinical decision support systems, population health analytics, and electronic health record integration to consume personal health device data alongside laboratory results, vital signs captured in clinical settings, and patient-reported outcomes with consistent semantic interpretation. For artificial intelligence and machine learning applications, Continua's standardization enables development of device-agnostic predictive models where training datasets aggregate personal health measurements from diverse device manufacturers (Omron blood pressure monitors, Dexcom continuous glucose monitors, Fitbit activity trackers, Withings scales) into unified representations with consistent units, timestamps, and metadata (device type, measurement context like fasting glucose vs. postprandial), supporting supervised learning for hypertension risk stratification (logistic regression, gradient boosting on longitudinal home blood pressure series predicting cardiovascular events), diabetes management (glucose forecasting with LSTMs or attention-based sequence models predicting hypoglycemic episodes from CGM time-series enabling proactive interventions), weight management (time-series clustering identifying weight trajectory phenotypessteady loss, plateaus, reboundsinforming personalized coaching), and heart failure decompensation early warning (multi-task learning combining daily weights, blood pressure trends, activity levels, symptom surveys to predict hospitalizations). Remote patient monitoring programs leverage Continua interoperability where chronic disease management platforms (for diabetes, hypertension, heart failure, COPD) integrate data streams from multiple Continua-certified devices worn or used by patients at home, applying machine learning algorithms for automated anomaly detection (sudden weight gain >2kg over 2 days flagging fluid retention in heart failure, blood glucose readings consistently >180 mg/dL triggering medication titration alerts, oxygen saturation <88% in COPD patients initiating clinical outreach) and personalized feedback generation (reinforcement learning agents trained on longitudinal patient data recommending behavioral modificationsexercise, medication adherence, dietary changestimed to maximize engagement and health outcomes). Clinical trials and real-world evidence studies benefit from Continua standardization where decentralized trial designs distribute Continua-certified devices to participants (blood pressure cuffs, glucometers, spirometers, ECG patches) capturing protocol-defined endpoints remotely with data automatically transmitted to electronic data capture systems in standardized formats, reducing site visit burden, improving adherence, and enabling larger, more diverse study populations; machine learning on trial datasets identifies treatment effect heterogeneity (which patient subgroups benefit most from interventions based on baseline device-measured physiology), predicts dropout risk (time-series models forecasting non-adherence from engagement patterns in device usage), and discovers digital biomarkers (features extracted from continuous device data correlating with clinical outcomesheart rate variability from pulse oximeter predicting medication response, glucose variability metrics predicting complications). Federated learning architectures for personal health devices exploit Continua interoperability where machine learning models train locally on smartphones or gateways receiving standardized device data, then share model updates (not raw data) with central servers, preserving privacy while enabling population-level model improvements; for example, a glucose prediction model fine-tunes on each patient's CGM data (Continua-formatted IEEE 11073 streams) and shares gradients to update global model, or anomaly detection models learn normal patterns from individual home blood pressure trajectories and contribute to ensemble detectors identifying pathological deviations. Digital therapeutics and closed-loop intervention systems integrate Continua devices where therapeutic mobile apps deliver evidence-based interventions (cognitive behavioral therapy for diabetes distress, medication reminders, exercise coaching) and adapt content based on real-time device feedbackan app escalates support when glucometer data shows poor control, a hypertension app adjusts medication reminders when blood pressure readings remain elevated, a heart failure app coordinates care team notifications when smart scale detects weight gain exceeding thresholds, all enabled by Continua's semantic standardization ensuring device readings drive clinical logic reliably across device brands. Population health surveillance and public health monitoring can leverage aggregated, de-identified personal health device data where Continua standardization enables pooling of home blood pressure measurements (identifying community-level hypertension control trends), continuous glucose monitoring data (tracking diabetes management effectiveness across populations), and activity tracker data (assessing physical activity patterns, sedentary behavior prevalence) with machine learning models identifying geographic, demographic, or socioeconomic disparities in chronic disease control, predicting outbreaks or exacerbations (flu activity from increased thermometer readings, asthma exacerbations from decreased peak flow measurements correlated with environmental triggers), and evaluating public health intervention effectiveness, ultimately positioning Continua as the interoperability foundation enabling a scalable, device-diverse personal health data ecosystem where AI-driven analytics, clinical decision support, remote monitoring, digital therapeutics, and precision medicine applications can operate across heterogeneous device landscapes, empowering patients as data generators contributing to their own care and advancing population health through standardized, machine-readable personal health observations.
  requires_registration: false
  url: https://www.pchalliance.org/continua-design-guidelines
- id: B2AI_STANDARD:71
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Critical care data exchange format
  formal_specification: https://github.com/autonlab/auviewer
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:16
  is_open: true
  name: CCDEF
  publication: doi:10.1088/1361-6579/abfc9b
  purpose_detail: The Critical Care Data Exchange Format (CCDEF) is an HDF5-based, self-describing, hierarchical file format specifically designed for efficient storage, exchange, and real-time streaming of high-resolution, multiparametric physiological waveforms and clinical data from intensive care unit (ICU) monitoring systems, addressing critical challenges in critical care informatics including data volume (continuous waveforms sampled at 125-500 Hz generate gigabytes per patient-day), heterogeneity (diverse devices from multiple manufacturers producing incompatible formats), real-time requirements (streaming data for bedside decision support and closed-loop control systems), and interoperability (enabling secondary use for research, quality improvement, and machine learning model development). Developed through collaboration between Carnegie Mellon University's Auton Lab and clinical partners, CCDEF leverages the HDF5 (Hierarchical Data Format version 5) scientific data framework to provide efficient binary storage with transparent compression (gzip, LZF codecs reducing file sizes by 60-90% while maintaining rapid random access), self-describing metadata (attributes documenting units, sampling rates, calibration parameters, device identifiers, alarm thresholds embedded within the file eliminating external documentation dependencies), hierarchical organization (nested groups for Patient/Admission/Device/Parameter structuring data logically with extensible schemas accommodating new measurement modalities without format versioning), and platform-independent access (HDF5 libraries for Python, R, MATLAB, C/C++, Java enabling cross-platform analytics). The CCDEF schema defines a standardized tree structure where the root contains global metadata (format version, institution identifier, de-identification flags), patient-level groups store demographics and admission metadata (age, sex, diagnosis codes, APACHE/SOFA severity scores), time-series datasets represent physiological waveforms (ECG leads I/II/III/V, arterial blood pressure ABP, pulmonary artery pressure PAP, central venous pressure CVP, respiratory plethysmography, SpO2, end-tidal CO2 EtCO2, intracranial pressure ICP) as chunked arrays with attributes specifying sampling frequency (typically 125-500 Hz), units (mmHg, mV, %), calibration coefficients, and acquisition timestamps synchronized to UTC or relative time-since-admission, while derived-parameter datasets store lower-frequency clinical measurements (heart rate, blood pressure systolic/diastolic/mean computed from waveforms, respiratory rate, temperature) and laboratory results (arterial blood gases pH/PaCO2/PaO2, lactate, troponin, BUN/creatinine, complete blood counts) as time-stamped tabular data, and event annotation datasets document clinical interventions (intubation, extubation, vasopressor boluses, fluid resuscitation), alarms (brady-/tachycardia, hypotension, desaturation), and artifact labels (movement, sensor disconnection, calibration periods) essential for supervised machine learning. The format's chunked storage strategy partitions long time-series into fixed-duration segments (e.g., 10-minute chunks) enabling efficient partial reads where analytics workflows query specific time windows without loading entire patient stays (days to weeks of continuous data), supporting distributed computing frameworks (Spark, Dask) that parallelize feature extraction and model inference across chunks, and real-time streaming scenarios where incoming data continuously appends new chunks to open CCDEF files with minimal write overhead, facilitating online monitoring algorithms and closed-loop interventions. CCDEF's real-time streaming capability enables bedside AI applications where machine learning models consume CCDEF streams for early warning score calculation (predicting sepsis onset, hemodynamic decompensation, respiratory failure hours in advance), automated arrhythmia detection (classifying ventricular tachycardia, atrial fibrillation, PVCs from ECG waveforms using CNNs), and alarm management systems (filtering false alarms via multimodal signal fusion reducing alarm fatigue while maintaining high sensitivity for true events), with model predictions written back to CCDEF event annotations creating audit trails for clinical validation and regulatory compliance. For artificial intelligence and machine learning applications, CCDEF provides a standardized data substrate for developing and validating ICU prediction models where large multi-center critical care databases (MIMIC-III/IV waveform matched subsets, eICU Collaborative Research Database, HiRID high-time-resolution ICU dataset) could be distributed in CCDEF format enabling reproducible preprocessing pipelines (artifact removal, signal quality assessment, waveform segmentation), harmonized feature engineering (extracting heart rate variability metrics, pulse pressure variation, respiratory variability, spectral features from Fourier transforms of ECG/ABP/respiratory waveforms, morphological features from QRS complexes and ABP pulse shapes), and consistent train/validation/test splits across studies, addressing reproducibility challenges where custom data formats and preprocessing code create barriers to benchmarking and independent validation. Deep learning architectures for critical care time-series benefit from CCDEF's efficient waveform access where 1D CNNs, LSTMs, and transformer models train on raw or minimally processed physiological signals stored as HDF5 datasets, leveraging GPU-accelerated data loaders that read CCDEF chunks in parallel (TensorFlow tf.data pipelines, PyTorch DataLoader with HDF5 backend) achieving high throughput for training on terabyte-scale waveform corpora, with self-supervised pretraining on unlabeled CCDEF waveforms (contrastive learning, autoencoding) followed by fine-tuning on labeled subsets (mortality, acute kidney injury, ventilator weaning success) improving sample efficiency compared to training from scratch. Multi-modal fusion models leverage CCDEF's unified format to integrate waveforms (high-frequency cardiorespiratory dynamics), laboratory values (intermittent biochemical measurements), clinical notes (extracted via NLP), and interventions (medications, ventilator settings) stored within the same hierarchical structure, training architectures (attention-based fusion, graph neural networks treating modalities as heterogeneous nodes, hierarchical RNNs with separate encoders per modality) that learn complementary information across time-scales (beat-to-beat waveform variability, hourly lab trends, daily disease trajectories) for holistic patient state representations improving prognostic accuracy. Federated learning and privacy-preserving machine learning on critical care data benefit from CCDEF's standardization where multiple hospitals locally train models on their CCDEF-formatted ICU data and share model updates (gradients, model weights) without exchanging patient-level data, with CCDEF's de-identification metadata flags documenting whether timestamps have been shifted, identifiers removed, and rare diagnoses generalized according to HIPAA Safe Harbor guidelines, enabling privacy-preserving consortia to develop and validate AI models on diverse patient populations while respecting data governance constraints. Causal inference and counterfactual reasoning in critical care employ CCDEF's comprehensive capture of interventions and outcomes where researchers train causal models (marginal structural models, G-computation, causal forests, neural causal effect estimators) on observational CCDEF data to estimate treatment effects (vasopressor strategies, ventilator settings, transfusion thresholds) accounting for time-varying confounding, with CCDEF's granular temporal resolution enabling discovery of optimal treatment timing and dosing policies via reinforcement learning (offline RL with batch-constrained Q-learning, fitted Q-iteration) trained on historical CCDEF trajectories, potentially improving clinical decision making through AI-derived precision medicine strategies tailored to individual patient physiology captured in CCDEF waveforms. The format's extensibility future-proofs critical care data infrastructure by accommodating emerging measurement modalities (continuous glucose monitors, microdialysis brain chemistry, wearable sensors, point-of-care ultrasound clips, portable chest X-rays) as new HDF5 datasets or groups without breaking backward compatibility, and genomic data integration (whole exome/genome sequences, polygenic risk scores, pharmacogenomic variants) stored alongside physiological data enables development of integrative models predicting sepsis susceptibility, drug metabolism variability, or recovery trajectories based on both genomic risk and real-time physiology, ultimately positioning CCDEF as the interoperable foundation for data-driven critical care research where standardized, machine-readable, comprehensive capture of ICU patient physiology enables reproducible AI development, multi-center validation, regulatory evaluation, and clinical deployment of intelligent monitoring and decision support systems improving outcomes in the most acutely ill patients.
  related_to:
  - B2AI_STANDARD:339
  requires_registration: false
  url: https://ccdef.org/
- id: B2AI_STANDARD:72
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Crypt4GH format
  formal_specification: https://github.com/samtools/hts-specs
  has_relevant_organization:
  - B2AI_ORG:34
  is_open: true
  name: Crypt4GH
  purpose_detail: Crypt4GH is a file format specification developed by the Global
    Alliance for Genomics and Health (GA4GH) for secure storage and transmission of
    genomic and health-related data. The format provides end-to-end encryption and
    authentication, allowing sensitive genomic data to be stored in public repositories
    while maintaining confidentiality. It uses modern cryptographic standards including
    Curve25519 for key agreement, ChaCha20 for encryption, and Poly1305 for authentication.
    The format supports multiple recipients through per-recipient encrypted headers,
    enabling controlled data sharing where different users can decrypt the same file
    using their own private keys. Crypt4GH is designed for streaming access, allowing
    applications to decrypt and process data incrementally without loading entire
    files into memory. The format includes metadata protection and supports selective
    decryption of file segments, making it suitable for large-scale genomic datasets.
    Existing bioinformatics tools can be adapted with minimal modifications to read
    and write Crypt4GH-encrypted data, facilitating adoption in genomics workflows.
  requires_registration: false
  url: https://samtools.github.io/hts-specs/crypt4gh.pdf
- id: B2AI_STANDARD:73
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:27
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Crystallographic Information File format
  is_open: true
  name: CIF
  purpose_detail: The acronym CIF is used both for the Crystallographic Information
    File, the data exchange standard file format of Hall, Allen & Brown (1991) (see
    Documentation), and for the Crystallographic Information Framework, a broader
    system of exchange protocols based on data dictionaries and relational rules expressible
    in different machine-readable manifestations, including, but not restricted to,
    Crystallographic Information File and XML.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/Crystallographic_Information_File
- id: B2AI_STANDARD:74
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - codesystem
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Current Procedural Terminology
  formal_specification: https://www.cms.gov/Medicare/Fraud-and-Abuse/PhysicianSelfReferral
  has_application:
  - id: B2AI_APP:263
    category: B2AI:Application
    name: Anesthesiology CPT auto-coding from EHR
    description: Machine learning models for automated prediction of anesthesia Current Procedural Terminology (CPT) codes from perioperative electronic health record data using support vector machines and label-embedding attentive neural networks trained on 1,164,343 procedures from 16 hospitals. Procedure text extracted from EHR served as the most informative input with actual submitted claim data from billing specialists as reference standard. Achieved 87.9% single-best accuracy and 96.8% top-3 accuracy on test set with strong external holdout validation performance. Using tunable confidence threshold, SVM reached 96.4% accuracy for 47.0% of cases and label-embedding model achieved 94.4% accuracy for 62.2% of cases. Demonstrates automated coding value for quality improvement, research, reimbursement, and cost reduction given high manual billing costs ($170-$215 per case) and observed CPT error rates (up to 38%). Supports scalable code assignment reducing administrative burden while improving accuracy and consistency compared to manual specialist coding.
    references:
    - https://doi.org/10.1097/aln.0000000000003150
  - id: B2AI_APP:264
    category: B2AI:Application
    name: Pathology report CPT prediction with NLP
    description: Natural language processing and machine learning pipeline predicting primary CPT codes (88302, 88304, 88305, 88307, 88309) and 38 ancillary CPT codes from pathology reports using corpus of 93,039 reports. Compared XGBoost, support vector machines, and BERT models with advanced topic modeling (20 topics) preprocessing on diagnostic text alone versus all report subfields (gross description, microscopic description, additional comments). XGBoost outperformed BERT when using all report subfields, achieving higher accuracy than prior published work. Model errors tended to occur between CPT codes of similar complexity. Applied model explanation techniques to identify informative report subcomponents. Applications extend beyond auto-coding to detecting mis-billing and fraud/waste, standardizing pathology report text, and estimating physician productivity metrics tied to relative value units (RVUs) for workload assessment and resource allocation.
    references:
    - https://doi.org/10.4103/jpi.jpi_52_21
  - id: B2AI_APP:265
    category: B2AI:Application
    name: Operative note CPT auto-coding comparison
    description: Comparative evaluation of classical machine learning and deep learning approaches for automated CPT code generation from operative note dictations in spine surgery setting. Random Forest model achieved AUC 0.94 and AUPRC 0.85 for CPT prediction and outperformed LSTM deep model (Random Forest ~87% weighted accuracy), demonstrating that classical supervised approaches can be competitive with neural networks given structured operative note text. Systematic evaluation across musculoskeletal operative notes corpus (~126,789 notes) found TF-IDF and classical machine learning methods often outperformed BERT-family transformer models on common CPT codes while offering better interpretability for clinical adoption. Highlights importance of feature engineering and model selection tailored to CPT prediction task characteristics, dataset size, and deployment requirements balancing accuracy with explainability for billing and compliance workflows.
    references:
    - https://doi.org/10.1177/21925682211062831
    - https://doi.org/10.1101/2022.10.10.22280852
  - id: B2AI_APP:266
    category: B2AI:Application
    name: Cloud-based end-to-end CPT/ICD auto-coding pipeline
    description: Commercial deep learning NLP system for automated medical coding mapping clinical text in electronic health records to standard procedure codes (CPT-4) and diagnosis codes (ICD-10-CM) with cloud-based claim submission, validation, and auditing infrastructure. Pipeline uses trained deep learning models on annotated EHRs with multiple NLP stages for entity recognition, context disambiguation, and code mapping supporting automatic CPT assignment from clinical notes with point-of-care coding suggestions. Real-time claim validation, auditing, and feedback loops leverage cloud infrastructure for HIPAA-compliant deployment. Vendor-reported outcomes include approximately 70% reduction in claim processing time and 23% improvement in coding accuracy at aggregate system level, though per-code performance metrics not publicly disclosed. Demonstrates emerging commercial deployment patterns for AI-driven revenue cycle automation integrating CPT/HCPCS coding with downstream billing workflows and fraud detection capabilities.
    references:
    - https://doi.org/10.63282/3050-922x.ijeret-v4i4p104
  - id: B2AI_APP:267
    category: B2AI:Application
    name: Unsupervised fraud detection on CPT billing patterns
    description: Unsupervised machine learning pipeline applied to Medicare, Medicaid, and outpatient claims data for detecting billing anomalies, CPT code mismatches, and fraudulent procedure coding patterns addressing estimated >$60B annual Medicare/Medicaid losses. Models analyze CPT/HCPCS usage patterns to flag aberrant billing behaviors without labeled fraud examples using anomaly detection on procedure code sequences, frequency distributions, and temporal patterns. Reported substantial improvements in fraud detection rates and significant reduction in manual audit efforts enabling automated prioritization of high-risk claims for investigation. Addresses challenges of data standardization across heterogeneous claims systems, inconsistent terminologies hindering model generalizability, and regulatory compliance requirements (HIPAA, GDPR) for deploying AI in healthcare fraud detection. Emphasizes need for transparency, routine audits for bias, and alignment with CMS program integrity goals ensuring AI-driven fraud detection augments rather than replaces human expertise in complex billing integrity assessments.
    references:
    - https://al-kindipublishers.org/index.php/jcsts/article/download/11010/9926
  - id: B2AI_APP:268
    category: B2AI:Application
    name: CPT AI Taxonomy for coding AI-enabled services
    description: American Medical Association CPT AI Taxonomy framework (Appendix S, effective January 1, 2022) establishing standardized coding methodology for AI-enabled clinical services by categorizing machine work as Assistive (AI assists physician decision-making), Augmentative (AI enhances physician work), or Autonomous (AI performs clinical task independently). Framework provides language describing work performed by machines relative to physician work, requires CPT descriptors align with device technological features and output, characterizes device-physician interaction, and ensures codes are discrete and differentiable supporting correct valuation, coverage determination, and payment policy. Applies to both Category I (established procedures) and Category III (emerging technology) CPT codes with guidance for code change applications including technical specifications justifying taxonomy placement. Example implementation includes autonomous point-of-care diabetic retinal examination (CPT 92229) transitioning descriptor from "automated" to "autonomous" reflecting independent AI capability. Addresses regulatory context where FDA has reviewed/authorized numerous Software as Medical Device (SaMD) products requiring consistent coding framework enabling integration of AI services into healthcare coverage and reimbursement systems, supporting transparent valuation of machine-performed clinical work.
    references:
    - https://doi.org/10.1038/s41746-022-00723-5
  is_open: false
  name: CPT
  purpose_detail: "Current Procedural Terminology (CPT) is a comprehensive, standardized code set maintained by the American Medical Association (AMA) that provides uniform nomenclature for describing medical, surgical, and diagnostic services performed by physicians and other healthcare providers, serving as the universal language for reporting outpatient and office procedures to payers including Medicare, Medicaid, and private insurance for reimbursement, enabling consistent documentation of clinical services across healthcare systems, supporting quality measurement and performance monitoring, facilitating healthcare research through structured procedure data, and underpinning revenue cycle management where accurate CPT coding directly impacts provider reimbursement (with manual coding costs $170-$215 per case and error rates up to 38% in complex specialties). Originally developed in 1966 by the AMA to standardize reporting of surgical procedures and adopted by CMS in 1983 as part of the Healthcare Common Procedure Coding System (HCPCS Level I), the CPT code set contains over 10,000 five-digit numeric codes organized into three categories where Category I codes describe established procedures and services widely performed across healthcare settings (organized by specialty sections: Evaluation & Management, Anesthesia, Surgery, Radiology, Pathology & Laboratory, Medicine), Category II codes capture performance measurement data for quality reporting and registry submissions (optional supplemental tracking codes documenting evidence-based care components like hemoglobin A1c testing in diabetes management, with alphanumeric format ending in 'F'), and Category III codes designate emerging technologies and procedures under development or investigation (temporary alphanumeric codes ending in 'T' facilitating data collection on new services before sufficient evidence accumulates for Category I inclusion, such as early telehealth services or novel imaging modalities). Each CPT code specifies the work relative value unit (wRVU) reflecting physician time, effort, skill, and intensity required for the procedure, practice expense RVU covering overhead costs, and professional liability insurance RVU, combined with geographic adjustment factors to calculate Medicare reimbursement rates through the Resource-Based Relative Value Scale (RBRVS) system, with private payers negotiating contracted rates often referenced to Medicare fee schedules modified by percentage multipliers. The CPT Editorial Panela multidisciplinary committee including physicians from major medical specialty societies, the Health Care Professionals Advisory Committee, CPT advisors from Blue Cross Blue Shield, America's Health Insurance Plans, CMS, and the American Hospital Associationreviews code change applications quarterly, evaluating proposals for new codes, revisions to existing descriptors, or deletions based on criteria including clinical distinctness (service must be distinct from existing codes), frequency of use (widespread adoption across providers), FDA approval for devices/drugs where applicable, and evidentiary basis (published clinical evidence supporting safety, efficacy, and medical necessity), with approved changes implemented in annual CPT updates released each January following a rigorous consensus process balancing innovation, clinical utility, and billing precision. CPT modifierstwo-character alphanumeric suffixes appended to codes (e.g., -25 for significant separately identifiable evaluation and management service, -59 for distinct procedural service, -RT/LT for right/left anatomical designation, -50 for bilateral procedures)provide additional context about how services were performed, enabling more accurate reimbursement for complex or unusual circumstances without proliferating primary codes. For artificial intelligence and machine learning applications, CPT codes serve multiple critical functions where automated code assignment (auto-coding) leverages supervised natural language processing and machine learning to map clinical documentation (operative notes, pathology reports, anesthesia records, EHR clinical narratives) directly to appropriate CPT codes, with deep learning models (support vector machines, label-embedding neural networks, BERT transformers, XGBoost) trained on large annotated corpora (e.g., 1.16 million procedures, 93,000+ pathology reports, 126,000+ operative notes) achieving 85-96% accuracy depending on specialty, input data completeness, and top-k prediction tolerance, addressing high manual coding costs and error rates while supporting scalable code assignment for quality improvement, research cohort identification, revenue optimization, and compliance monitoring. Machine learning-based fraud, waste, and abuse detection systems apply unsupervised anomaly detection and outlier analysis to claims databases containing CPT/HCPCS billing patterns, identifying aberrant utilization (excessive frequency of high-reimbursement procedures, code mismatches indicating upcoding or unbundling violations, temporally implausible procedure sequences suggesting fabricated claims) without requiring labeled fraud examples, with Medicare/Medicaid program integrity initiatives leveraging these models to flag high-risk claims for investigation addressing estimated >$60 billion annual improper payments, though deployment faces challenges including data standardization across heterogeneous claims systems, inconsistent terminologies across regional carriers, and regulatory requirements for transparency, auditability, and bias monitoring to ensure AI augments rather than replaces human expert judgment in complex fraud assessments. EHR-based phenotyping and outcomes prediction incorporate CPT codes as structured features or prediction targets where supervised models use billed procedure codes alongside diagnosis codes (ICD-10-CM), medication orders (RxNorm), and laboratory results (LOINC) to construct patient phenotypes for epidemiological research, clinical trial recruitment, risk stratification (predicting hospital readmissions, post-operative complications, healthcare utilization), and longitudinal disease progression modeling, with the ubiquity and portability of CPT codes across EHR systems (Epic, Cerner, Allscripts) and claims databases (Medicare, Medicaid, commercial payers) enabling federated learning where multiple institutions collaboratively train models on locally held data without sharing patient-level information, though CPT-based phenotyping confronts limitations including incomplete capture of clinical detail (codes designed for billing may not reflect nuanced clinical presentations), temporal lag between service delivery and claims submission (weeks to months delay reducing utility for real-time predictions), and code selection variability across providers and specialties (physician discretion and billing optimization incentives introduce noise in code distributions). The CPT AI Taxonomy (Appendix S, effective January 2022) establishes a structured framework for coding AI-enabled clinical services by classifying machine work into three categoriesAssistive (AI provides information to support physician decision-making, e.g., computer-aided detection highlighting suspicious regions in mammography with final interpretation by radiologist), Augmentative (AI performs portions of the clinical task with physician oversight and interpretation, e.g., automated coronary artery calcium scoring from CT with physician review and clinical integration), and Autonomous (AI independently performs complete clinical task generating actionable output without real-time physician involvement, e.g., autonomous diabetic retinopathy screening CPT 92229 producing diagnostic recommendations directly from retinal images)requiring CPT code descriptors for AI services to explicitly characterize device technological features, AI-physician interaction model, and discrete outputs supporting accurate valuation through the AMA/Specialty Society RVS Update Committee (RUC) process, coverage policy development by payers determining medical necessity criteria and patient eligibility, and payment rate determination balancing technology costs, clinical value, and budget impact, with the taxonomy addressing the regulatory and reimbursement infrastructure needed to integrate FDA-authorized Software as Medical Device (SaMD) products into routine clinical workflows where transparent CPT coding enables health systems to track AI utilization, measure clinical outcomes, assess return on investment, and satisfy quality reporting requirements. Natural language processing systems for automated CPT coding face deployment challenges including class imbalance (common procedures like routine office visits vastly outnumber complex specialty procedures in training data, requiring oversampling, focal loss functions, or hierarchical classification strategies), code similarity and confusion (many CPT codes describe clinically similar procedures with subtle distinctions in technique, anatomical location, or complexity leading to prediction errors between adjacent codes, e.g., pathology specimen processing codes 88302-88309 differing primarily by tissue complexity and examination extent), generalizability across institutions and specialties (models trained on one hospital's documentation practices, EHR templates, and physician dictation styles may underperform when deployed elsewhere without transfer learning or domain adaptation), explainability and trust (black-box deep learning models producing CPT predictions lack transparency needed for billing compliance and error correction, motivating attention mechanisms, saliency maps, and model-agnostic explainability techniques like SHAP values to surface which clinical text features drive code assignments), integration with clinical workflows (auto-coding systems must interoperate with EHR billing modules, provide real-time point-of-care suggestions during documentation, flag low-confidence predictions for human review, and support iterative feedback loops where corrected codes retrain models, requiring robust software engineering, user interface design, and change management), and regulatory compliance (automated coding tools constitute clinical decision support potentially subject to FDA oversight depending on intended use and risk classification, require validation against coding guidelines published by AMA and specialty societies, and must document audit trails for claims submitted to government payers satisfying anti-fraud regulations and Office of Inspector General scrutiny). CPT codes also serve as labels in supervised learning tasks beyond auto-coding where models predict post-procedure outcomes (complications, readmissions, mortality) conditional on performed procedures represented by CPT codes, estimate procedure duration and operating room utilization for surgical scheduling optimization (random forests predicting case length from CPT codes, patient characteristics, and surgeon experience), forecast healthcare costs and resource consumption for population health management and value-based care contracts (CPT-based procedure costs aggregated with diagnosis-based expected utilization producing risk-adjusted spending predictions), and power clinical natural language generation systems that automatically compose procedure notes, operative summaries, or patient instructions by retrieving templates associated with billed CPT codes then personalizing with patient-specific details extracted from structured EHR data. The interplay between CPT coding accuracy, revenue integrity, and AI/ML applications creates a feedback loop where improved automated coding reduces administrative costs and accelerates reimbursement, the resulting high-quality coded data enables more sophisticated ML models for clinical decision support and operations optimization, and AI-generated insights into coding patterns inform coding guideline refinements and fraud detection strategies, ultimately positioning CPT as not merely a billing code set but a foundational structured vocabulary enabling AI-driven transformation of healthcare delivery, payment models, and quality measurement while demanding ongoing evolution of the code set, taxonomy frameworks, and regulatory policies to keep pace with technological innovation in both clinical procedures and the AI systems that document, analyze, and optimize them."
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:3
  url: https://www.ama-assn.org/amaone/cpt-current-procedural-terminology
  used_in_bridge2ai: true
- id: B2AI_STANDARD:75
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Data Documentation Initiative Lifecycle
  is_open: true
  name: DDI-Lifecycle
  purpose_detail: The freely available international DDI standard describes data that
    result from observational methods in the social, behavioral, economic, and health
    sciences. DDI is used to document data in over 60 countries of the world.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:23
  url: https://ddialliance.org/Specification/DDI-Lifecycle/3.3/
- id: B2AI_STANDARD:76
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Dead simple owl design pattern exchange format
  formal_specification: https://github.com/INCATools/dead_simple_owl_design_patterns
  has_relevant_organization:
  - B2AI_ORG:58
  is_open: true
  name: DOS-DP
  publication: doi:10.1186/s13326-017-0126-0
  purpose_detail: A simple design pattern system that can easily be consumed, whatever
    your code base, for OWL ontologies.
  requires_registration: false
  url: https://oboacademy.github.io/obook/tutorial/dosdp-template/
- id: B2AI_STANDARD:77
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Decision Model and Notation
  has_relevant_organization:
  - B2AI_ORG:10
  is_open: true
  name: DMN
  purpose_detail: A modeling language and notation for the precise specification of
    business decisions and business rules.
  requires_registration: false
  url: https://www.omg.org/dmn/
- id: B2AI_STANDARD:78
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Description Language for Taxonomy
  is_open: true
  name: DELTA
  purpose_detail: When taxonomic descriptions are prepared for input to computer programs,
    the form of the coding is usually dictated by the requirements of a particular
    program or set of programs. This restricts the type of data that can be represented,
    and the number of other programs that can use the data. Even when working with
    a particular program, it is frequently necessary to set up different versions
    of the same basic data, for example, when using restricted sets of taxa or characters
    to make special-purpose keys. The potential advantages of automation, especially
    in connexion with large groups, cannot be realized if the data have to be restructured
    by hand for every operation. The DELTA (DEscription Language for TAxonomy) system
    was developed to overcome these problems. It was designed primarily for easy use
    by people rather than for convenience in computer programming, and is versatile
    enough to replace the written description as the primary means of recording data.
    Consequently, it can be used as a shorthand method of recording data, even if
    computer processing of the data is not envisaged.
  requires_registration: false
  url: https://www.delta-intkey.com/
- id: B2AI_STANDARD:79
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - audiovisual
  - fileformat
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 10 Media Storage and File Format for Media Interchange
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part10.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOMDIR
  purpose_detail: This Part of the DICOM Standard specifies a general model for the
    storage of Medical Imaging information on removable media. The purpose of this
    Part is to provide a framework allowing the interchange of various types of medical
    images and related information on a broad range of physical storage media.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:80
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 11 Media Storage Application Profiles
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part11.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 11
  purpose_detail: This Part of the DICOM Standard specifies application specific subsets
    of the DICOM Standard to which an implementation may claim conformance. Such a
    conformance statement applies to the interoperable interchange of medical images
    and related information on storage media for specific clinical uses.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:81
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 12 Media Formats and Physical Media for Media Interchange
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part12.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 12
  purpose_detail: This Part of the DICOM Standard facilitates the interchange of information
    between digital imaging computer systems in medical environments. This interchange
    will enhance diagnostic imaging and potentially other clinical applications. The
    multi-part DICOM Standard defines the services and data that shall be supplied
    to achieve this interchange of information.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:82
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 14 Grayscale Standard Display Function
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part14.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 14
  purpose_detail: PS3.14 specifies a standardized Display Function for display of
    grayscale images. It provides examples of methods for measuring the Characteristic
    Curve of a particular Display System for the purpose of either altering the Display
    System to match the Grayscale Standard Display Function, or for measuring the
    conformance of a Display System to the Grayscale Standard Display Function. Display
    Systems include such things as monitors with their associated driving electronics
    and printers producing films that are placed on light-boxes or alternators.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:83
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 15 Security and System Management Profiles
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part15.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 15
  purpose_detail: This Part of the DICOM Standard specifies Security and System Management
    Profiles to which implementations may claim conformance. Security and System Management
    Profiles are defined by referencing externally developed standard protocols, such
    as TLS, ISCL, DHCP, and LDAP, with attention to their use in a system that uses
    DICOM Standard protocols for information interchange.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:84
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 16 Content Mapping Resource
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part16.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DCMR
  purpose_detail: DICOM Part 16 Content Mapping Resource (DCMR) is a comprehensive
    component of the DICOM standard that provides the foundational vocabulary and
    semantic structures used throughout medical imaging systems worldwide. DCMR defines
    the Templates and Context Groups that standardize the representation of clinical
    concepts, measurements, and observations within DICOM objects, ensuring consistent
    interpretation of medical imaging data across different systems, vendors, and
    healthcare institutions. The Content Mapping Resource includes structured templates
    for various types of medical imaging reports, measurements, and annotations, covering
    specialties such as radiology, cardiology, ophthalmology, and oncology. These
    templates define the specific data elements, their relationships, and the controlled
    terminologies that should be used when creating structured reports and enhanced
    imaging objects. DCMR plays a crucial role in enabling interoperability, supporting
    artificial intelligence applications in medical imaging, and facilitating the
    exchange of semantically rich imaging information in clinical practice and research
    environments.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:85
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 17 Explanatory Information
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part17.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 17
  purpose_detail: 'DICOM Part 17: Explanatory Information serves as the comprehensive
    educational and interpretive companion to the DICOM (Digital Imaging and Communications
    in Medicine) Standard, providing normative annexes that clarify implementation
    requirements and extensive informative annexes that explain the rationale, context,
    and practical application of DICOM specifications across the complete multi-part
    standard maintained by NEMA (National Electrical Manufacturers Association) and
    the Medical Imaging & Technology Alliance. This part functions as the bridge between
    the formal, technically precise language of the normative DICOM specifications
    (Parts 1-21) and the practical needs of implementers, including medical device
    manufacturers, PACS vendors, healthcare IT developers, clinical engineers, radiologists,
    and AI/ML researchers who must correctly interpret and apply DICOM protocols in
    real-world medical imaging systems. Part 17 contains normative annexes that define
    mandatory implementation rules, unique identifier (UID) allocation schemes, conformance
    statement structures, and security profiles referenced throughout other DICOM
    parts, making compliance verification and interoperability testing possible. The
    informative annexes provide contextual explanations, use case scenarios, implementation
    guidelines, and best practices that illuminate the "why" behind DICOM''s design
    decisions, including historical context for legacy features maintained for backward
    compatibility, clinical workflow considerations that shaped information model
    choices, and trade-offs between flexibility and interoperability in attribute
    definitions. For AI and machine learning applications in medical imaging, Part
    17''s explanatory material is invaluable for understanding how to correctly encode
    AI-generated annotations (segmentations, structured reports, parametric maps)
    in DICOM formats, interpret manufacturer-specific private tags that may contain
    proprietary imaging parameters needed for model training, navigate the complexities
    of coordinate system transformations between image acquisition space and patient
    anatomical space, and ensure that AI model outputs maintain proper DICOM conformance
    for integration into clinical PACS workflows. The part includes detailed explanations
    of DICOM''s information object definitions (IODs), service-object pair (SOP) classes,
    and application entities that form the foundation for network communication between
    imaging modalities, archives, workstations, and AI inference servers. Part 17
    clarifies the distinction between DICOM''s composite and normalized information
    models, explains the role of Type 1 (required), Type 2 (required but may be empty),
    and Type 3 (optional) attributes in ensuring semantic interoperability, and provides
    guidance on handling multi-frame images, enhanced MR/CT IODs, and whole slide
    imaging pyramids that are increasingly common in pathology AI applications. For
    researchers building AI training datasets from clinical DICOM archives, Part 17''s
    explanatory material helps interpret DICOM metadata for proper cohort selection,
    understand acquisition parameter variations that affect image characteristics
    and model generalizability, and recognize quality control indicators embedded
    in DICOM headers that may signal artifacts or protocol deviations requiring exclusion
    from training sets. The part also explains DICOM''s approach to encoding patient
    orientation, anatomical landmarks, and spatial registration information critical
    for multi-modal image fusion and longitudinal studies where AI models must track
    anatomical changes over time or correlate findings across different imaging modalities
    (CT, MRI, PET, ultrasound). Part 17''s annexes document the rationale behind DICOM''s
    privacy and security frameworks, including de-identification profiles and attribute
    confidentiality levels essential for creating shareable AI training datasets that
    comply with HIPAA, GDPR, and institutional review board requirements. The explanatory
    information supports regulatory submissions by providing the interpretive context
    needed to demonstrate DICOM conformance in FDA 510(k) applications and EU Medical
    Device Regulation (MDR) technical documentation for AI-enabled medical imaging
    products. Updated with each DICOM standard revision, Part 17 incorporates explanations
    of new supplements and correction proposals, ensuring that implementers understand
    the evolution of the standard and can plan migration paths for existing systems
    while maintaining backward compatibility with legacy imaging equipment and archives.'
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:86
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 18 Web Services
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part18.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOMweb
  purpose_detail: PS3.18 specifies web services (using the HTTP family of protocols)
    for managing and distributing DICOM (Digital Imaging and Communications in Medicine)
    Information Objects, such as medical images, annotations, reports, etc. to healthcare
    organizations, providers, and patients. The term DICOMweb is used to designate
    the RESTful Web Services described here.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:87
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 19 Application Hosting
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part19.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 19
  purpose_detail: This Part of the DICOM Standard defines an interface between two
    software applications. One application, the Hosting System, provides the second
    application with data, such as a set of images and related data. The second application,
    the Hosted Application, analyzes that data, potentially returning the results
    of that analysis, for example in the form of another set of images and/or structured
    reports, to the first application. Such an Application Program Interface (API)
    differs in scope from other portions of the DICOM Standard in that it standardizes
    the data interchange between software components on the same system, instead of
    data interchange between different systems.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:88
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 2 Conformance
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part02.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 2
  purpose_detail: An implementation need not employ all the optional components of
    the DICOM Standard. After meeting the minimum general requirements, a conformant
    DICOM implementation may utilize whatever SOP Classes, communications protocols,
    Media Storage Application Profiles, optional (Type 3) Attributes, codes and controlled
    terminology, etc., needed to accomplish its designed task.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:89
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 20 Imaging Reports using HL7 Clinical Document Architecture
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part20.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_relevant_organization:
  - B2AI_ORG:40
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 20
  purpose_detail: This Part of the DICOM Standard specifies templates for the encoding
    of imaging reports using the HL7 Clinical Document Architecture Release 2 (CDA
    R2, or simply CDA) Standard. Within this scope are clinical procedure reports
    for specialties that use imaging for screening, diagnostic, or therapeutic purposes.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:90
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:15
  - standards_process_maturity_final
  - implementation_maturity_production
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 21 Transformations between DICOM and other Representations
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part21.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 21
  purpose_detail: This Part of the DICOM Standard specifies the transformations between
    DICOM and other representations of the same information.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:91
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 22 Real-Time Communication
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part22.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 22
  purpose_detail: This Part of the DICOM Standard specifies an SMPTE ST 2110-10 based
    service, relying on RTP, for the real-time transport of DICOM metadata. It provides
    a mechanism for the transport of DICOM metadata associated with a video or an
    audio flow based on the SMPTE ST 2110-20 and SMPTE ST 2110-30, respectively.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:92
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 3 Information Object Definitions
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part03.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 3
  purpose_detail: This Part of the DICOM Standard specifies the set of Information
    Object Definitions (IODs) that provide an abstract definition of real-world objects
    applicable to communication of digital medical information. For each IOD, this
    Part specifies any necessary information for the semantic description of the IOD,
    relationships to associated real-world objects relevant to the IOD, Attributes
    that describe the characteristics of the IOD.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:93
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 4 Service Class Specifications
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part04.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 4
  purpose_detail: This Part of the DICOM Standard specifies the set of Service Class
    Definitions that provide an abstract definition of real-world activities applicable
    to communication of digital medical information.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:94
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 5 Data Structures and Encoding
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part05.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 5
  purpose_detail: In this Part of the Standard the structure and encoding of the Data
    Set is specified. In the context of Application Entities communicating over a
    network, a Data Set is that portion of a DICOM Message that conveys information
    about real world objects being managed over the network. A Data Set may have other
    contexts in other applications of this Standard; e.g., in media exchange the Data
    Set translates to file content structure.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:95
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 6 Data Dictionary
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part06.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 6
  purpose_detail: This Part of the DICOM Standard is PS 3.6 of a multi-part standard
    produced to facilitate the interchange of information between digital imaging
    computer systems in medical environments. This interchange will enhance diagnostic
    imaging and potentially other clinical applications. The multi-part DICOM Standard
    covers the protocols and data that shall be supplied to achieve this interchange
    of information.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:96
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 7 Message Exchange
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part07.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DIMSE
  purpose_detail: This Part of the DICOM Standard specifies the DICOM Message Service
    Element (DIMSE). The DIMSE defines an Application Service Element (both the service
    and protocol) used by peer DICOM Application Entities for the purpose of exchanging
    medical images and related information.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:97
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DICOM Part 8 Network Communication Support for Message Exchange
  formal_specification: http://dicom.nema.org/medical/dicom/current/output/html/part08.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM Part 8
  purpose_detail: The Communication Protocols specified in this Part of PS3 closely
    fit the ISO Open Systems Interconnection Basic Reference Model (ISO 7498-1, see
    Figure 1-1). They relate to the following layers - Physical, Data Link, Network,
    Transport, Session, Presentation and the Association Control Services (ACSE) of
    the Application layer. The communication protocols specified by this Part are
    general purpose communication protocols (TCP/IP) and not specific to this Standard.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  subclass_of:
  - B2AI_STANDARD:98
  url: https://www.dicomstandard.org/current
  used_in_bridge2ai: true
- id: B2AI_STANDARD:98
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Digital Imaging And Communications In Medicine
  formal_specification: https://www.dicomstandard.org/current
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  - B2AI_SUBSTRATE:19
  has_relevant_organization:
  - B2AI_ORG:115
  - B2AI_ORG:114
  - B2AI_ORG:117
  has_training_resource:
  - B2AI_STANDARD:849
  is_open: true
  name: DICOM
  purpose_detail: Digital Imaging and Communications in Medicine (DICOM) is the international
    standard for medical imaging information and related data. DICOM defines the
    formats and communication protocols for medical images (CT, MRI, ultrasound, X-ray,
    etc.) and associated metadata including patient demographics, study information,
    equipment parameters, and image acquisition details. The standard encompasses
    file formats, network protocols, and information models that enable interoperability
    across imaging devices, PACS (Picture Archiving and Communication Systems), workstations,
    and clinical information systems. DICOM supports structured reporting through
    DICOM-SR for encoding measurements, annotations, and AI-generated findings in
    a standardized machine-readable format. The standard includes specialized modules
    for radiation dose tracking, workflow management, and integration of AI inference
    results. DICOM's comprehensive metadata structure and pixel data encoding make
    it essential for training and deploying AI models in medical imaging, while its
    widespread clinical adoption ensures that AI tools can integrate seamlessly into
    existing radiological workflows and health IT infrastructure.
  related_to:
  - B2AI_STANDARD:691
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:25
  url: https://www.dicomstandard.org/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:33
    category: B2AI:Application
    name: NCI Imaging Data Commons FAIR Dataset Harmonization for ML Training
    description: The NCI Imaging Data Commons uses DICOM to harmonize large-scale
      public imaging collections with AI-derived annotations, supporting transparent
      cohort selection and reproducible ML development. IDC encodes images and derived
      artifacts (volumetric segmentations, slice-level annotations, radiomics features)
      consistently using DICOM Segmentation objects and Structured Reports, enabling
      FAIR (findable, accessible, interoperable, reusable) datasets. Cloud-ready tools
      and libraries including highdicom lower barriers for ML developers to produce
      and consume DICOM-compliant training labels and outputs directly from Python
      workflows, facilitating dataset quality control and reproducibility for AI model
      development.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1038/s41597-023-02864-y
  - id: B2AI_APP:119
    category: B2AI:Application
    name: DICOM Segmentation and Structured Report Encoding for Supervised Learning
      Labels
    description: Research datasets are re-encoded into DICOM annotation objects to
      make labels directly consumable by ML pipelines. LIDC-IDRI lung nodule annotations
      were converted to DICOM Segmentation (SEG) objects and DICOM Structured Reports
      following template TID1500, providing interoperable nodular masks and qualitative/quantitative
      assessments. Disease-specific datasets like HCC-TACE distribute DICOM-SEG tumor/liver
      labels for algorithm training and evaluation. DICOM-SEG metadata tags capture
      algorithm provenance (Segment algorithm type, Segmentation type) essential for
      data curation, reproducibility, and supervised ML workflows, while software
      stacks (DCMTK, ITK, dcmqi, pydicom-seg) enable conversion and pipeline integration.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1186/s13244-021-01081-8
  - id: B2AI_APP:120
    category: B2AI:Application
    name: Vendor-Agnostic PACS-Integrated Shadow Testing Pipeline for Near-Real-Time
      Segmentation
    description: A containerized, DICOM-compatible pipeline integrates PACS with ML
      inference for near-real-time clinical shadow testing of segmentation algorithms.
      PACS sends studies via classic DICOM services (C-STORE/C-FIND/C-MOVE) to an
      on-premises GPU host that performs nnU-Net inference and converts results to
      DICOM Segmentation objects and Structured Reports encoding volumetry. Results
      are stored in a DCM4CHEE archive and visualized in OHIF web viewer, enabling
      clinical evaluation, human-in-the-loop corrections, and dataset curation. This
      demonstrates concrete PACS-to-inference-to-reporting workflows using established
      DICOM network protocols for AI integration into radiology operations.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.3389/fmed.2023.1241570
  - id: B2AI_APP:121
    category: B2AI:Application
    name: DICOM Structured Report TID1500 for Machine-Readable AI Results and Radiomics
    description: Quantitative AI outputs including radiomics features and imaging
      biomarkers are standardized via DICOM Structured Report template TID1500 measurement
      reports, providing machine-readable, auditable AI results that facilitate downstream
      research and clinical decision support. DICOM SR encodes semantic measurements
      with standardized format and lexicon, acting as a big-data container for multimodal
      patient data integration. DICOM Parametric Map IODs preserve derived pixelwise
      feature maps (kurtosis, texture) as DICOM objects for reproducible feature extraction.
      Integration guidance from standards bodies emphasizes encoding AI outputs into
      SR to scale deployment across vendors and sites.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1186/s13244-021-01081-8
    - https://doi.org/10.1117/1.jmi.7.1.016502
  - id: B2AI_APP:122
    category: B2AI:Application
    name: DICOMweb RESTful Services for Scalable Cloud and Web-Based ML Pipelines
    description: DICOMweb RESTful services (WADO-RS, QIDO-RS, STOW-RS) connect external
      ML applications and viewers to enterprise archives over HTTP, enabling scalable
      web and cloud AI pipelines. DICOMweb client APIs allow external apps to retrieve,
      query, and store DICOM objects without classic DIMSE protocols, facilitating
      integration with modern web architectures. Frameworks stream DICOM and metadata
      from hospital PACS into research compute clusters to power real-time ML and
      operational analytics. Examples include Niffler for PACS-to-research pipelines
      with metadata extraction and ML analytics, and EMPAIA decentralized platform
      for running third-party AI on DICOM data at scale.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1016/j.cmpb.2024.108113
  - id: B2AI_APP:123
    category: B2AI:Application
    name: DICOM-WSI and DICOMweb for Computational Pathology ML Development
    description: DICOM Whole Slide Imaging (DICOM-WSI) and DICOMweb enable interoperable
      storage, visualization, and annotation of whole-slide images in web viewers
      for computational pathology. These standards support collecting standardized
      annotations and displaying AI inference outputs including segmentation masks,
      heat maps, and image-derived measurements within the DICOM ecosystem. DICOM-WSI
      brings pathology images into uniform DICOM workflows to facilitate ML model
      development and validation, while web viewers (e.g., Visilab Viewer) provide
      platforms for annotating training data and reviewing AI results in digital pathology
      contexts.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1038/s41597-023-02864-y
    - https://doi.org/10.1016/j.cmpb.2024.108113
  - id: B2AI_APP:124
    category: B2AI:Application
    name: IODeep Proposed IOD for DNN Model Management and Federated Learning in
      PACS
    description: The proposed IODeep DICOM Information Object Definition stores deep
      neural network architectures and weights inside PACS as non-patient objects,
      enabling model selection via DICOM tag metadata (modality, anatomical region,
      disease). Inference runs client-side or on dedicated servers proximal to viewers,
      with predicted ROIs saved as RT Structure Sets for physician validation and
      retraining. IODeep supports model lifecycle management including fine-tuning
      with hospital data and federated learning scenarios by sharing model metadata
      without moving patient data. The design uses Unlimited Text VR for JSON architecture
      descriptions and weight URIs, with parsers for TensorFlow/PyTorch instantiation,
      demonstrating an emerging pattern for managing AI models within DICOM-compliant
      workflows.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1016/j.cmpb.2024.108113
  - id: B2AI_APP:125
    category: B2AI:Application
    name: DICOM GSPS and RT Structure Sets for AI Annotation Presentation and Clinical
      Feedback
    description: DICOM Grayscale Softcopy Presentation State (GSPS) ensures consistent
      overlay and display of AI annotations across workstations, while RT Structure
      Set (RTSS) objects store physician-validated contours after AI suggestion, supporting
      clinical quality assurance and retraining workflows. RTSS is widely supported
      for storing ROI contours and labels for model training, enabling human-in-the-loop
      correction loops where radiologists review AI predictions, validate or correct
      them, and create ground-truth datasets. Integration of CAD results into PACS
      using DICOM SR and GSPS, combined with DICOM routers for communication and correction,
      supports maturity levels from investigational AI display through continuously
      updated deployed models driven by radiologist interactions.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1117/1.jmi.7.1.016502
    - https://doi.org/10.1016/j.cmpb.2024.108113
- id: B2AI_STANDARD:99
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Direct Applicability Statement for Secure Health Transport
  formal_specification: https://wiki.directproject.org/w/images/e/e6/Applicability_Statement_for_Secure_Health_Transport_v1.2.pdf
  has_relevant_organization:
  - B2AI_ORG:26
  is_open: true
  name: Direct Standard
  purpose_detail: Describes how to use SMTP, S/MIME, and X.509 certificates to securely
    transport health information over the Internet.
  requires_registration: false
  url: https://wiki.directproject.org/w/images/e/e6/Applicability_Statement_for_Secure_Health_Transport_v1.2.pdf
- id: B2AI_STANDARD:100
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Distributed Sequence Annotation System
  formal_specification: https://static-content.springer.com/esm/art%3A10.1186%2F1471-2105-2-7/MediaObjects/12859_2001_8_MOESM1_ESM.pdf
  is_open: true
  name: DAS
  publication: doi:10.1186/1471-2105-2-7
  purpose_detail: The Distributed Annotation System (DAS) is a client-server protocol and data federation framework enabling decentralized, lightweight integration of heterogeneous biological sequence annotations from multiple independent data sources without requiring centralized data warehousing or schema harmonization, allowing genome browsers and bioinformatics applications to dynamically retrieve and overlay annotations (gene structures, protein domains, sequence variants, regulatory elements, expression data, evolutionary conservation, functional predictions) from distributed annotation servers on-demand as users navigate genomic coordinates or protein sequences. Developed in 2001 by the Wellcome Trust Sanger Institute and European Bioinformatics Institute to address the challenge of integrating the rapidly growing diversity of genome annotation sourcesranging from curated databases (Ensembl, UCSC Genome Browser, UniProt) to experimental datasets (ChIP-seq peaks, RNA-seq coverage, GWAS associations) to computational predictions (transcription factor binding sites, splice variants, structural motifs)without each resource needing to replicate all others' data, DAS defines a simple HTTP-based XML protocol where a reference server provides coordinate systems (genome assemblies like GRCh38, protein sequences like UniProt IDs) establishing the shared addressing scheme, and annotation servers implement a standardized REST API exposing endpoints for querying features within specified coordinate ranges (GET /das/source/features?segment=chr1:1000000,2000000 returns all annotations overlapping that genomic region as XML feature elements with start/end positions, feature type ontology terms, scores, orientation, and links to external resources), enabling genome browsers (Ensembl, IGB, Gbrowse, UCSC Genome Browser via DAS tracks) to federate queries across dozens of annotation sources and render integrated multi-layer visualizations where each data source contributes a separate track without pre-loading all data locally. The DAS architecture specifies a sources endpoint (GET /das/sources) returning a registry of available annotation datasets with metadata (organism, coordinate system, version, capabilities, description), a types endpoint enumerating feature categories available from a source (gene, exon, SNP, binding_site using Sequence Ontology terms for semantic interoperability), a features endpoint retrieving annotations for coordinate ranges with optional filtering by feature type or category, a sequence endpoint allowing reference servers to provide underlying DNA/protein sequences, and a stylesheet endpoint defining visual rendering hints (colors, glyphs, heights) for client-side display customization, collectively creating a lightweight, stateless, RESTful federation model where each server maintains autonomy over its data update schedule, access policies, and internal schemas while presenting a uniform query interface. The decentralized annotation federation paradigm enables community-driven annotation contributions where individual research groups, consortia, or computational prediction pipelines can independently deploy DAS servers exposing their specialized annotations (tissue-specific regulatory elements from ENCODE, disease variants from ClinVar, orthology relationships from Ensembl Compara, mass spectrometry peptide mappings from proteomics experiments, CRISPR guide RNA predictions from design tools) without requiring approval or coordination from central genome databases, democratizing annotation dissemination and accelerating data sharing by reducing barriers (no need to submit to centralized repositories, format conversions, or publication delays), while clients benefit from accessing a comprehensive, continually updated annotation landscape by federating queries across the DAS registry. DAS supports multi-organism comparative genomics where a client application displays syntenic regions across species (human, mouse, zebrafish) by querying DAS servers for each organism's genome with coordinate transformations (liftover between assemblies, orthologous coordinate mapping) handled by specialized alignment servers that expose alignment blocks as DAS features, enabling users to visualize conserved regulatory elements, identify evolutionarily constrained sequences as potential functional elements, and compare gene structures across phylogenetic groups. The system accommodates temporal versioning where DAS sources specify genome assembly versions (GRCh37 vs. GRCh38, mm9 vs. mm10) and annotation release dates, allowing clients to query historical states for reproducibility and longitudinal studies tracking how annotations evolve as evidence accumulates, with provenance metadata documenting annotation methods (experimental vs. computational, manual curation vs. automated prediction) and confidence scores enabling evidence-weighted integration where high-confidence annotations are visually emphasized. For artificial intelligence and machine learning applications, DAS infrastructure enables dynamic feature construction for genomic machine learning models by federating retrieval of multi-modal annotations (sequence conservation PhyloP scores, histone modification ChIP-seq signals, chromatin accessibility DNase-seq peaks, transcription factor binding PWM matches, RNA-seq expression levels, variant population frequencies from gnomAD, pathogenicity predictions from CADD/DANN, tissue-specific enhancer predictions from deep learning models like Enformer) across coordinate ranges corresponding to regulatory elements, gene loci, or variant positions, generating rich feature matrices for training supervised classifiers predicting regulatory activity, variant pathogenicity, or tissue-specific gene expression; the federated architecture avoids data replication by querying annotations on-the-fly during feature engineering pipelines, with caching layers mitigating latency. Deep learning models for functional genomics leverage DAS-federated annotations as training labels where ChIP-seq peaks served via DAS define positive examples of transcription factor binding sites, histone modifications indicate active promoters/enhancers, and CAGE-seq peaks mark transcription start sites, enabling convolutional neural networks to learn sequence motifs predictive of these functional annotations from DNA sequences retrieved via DAS reference servers; model predictions can be published back as new DAS annotation tracks (predicted binding sites, chromatin state segmentations from ChromHMM/Segway, variant effect predictions) completing a cycle where ML-generated annotations become available for federated querying by other researchers and subsequent model iterations. Metagenomics and microbiome research benefit from DAS-like federated annotation where metagenomic contigs assembled from environmental samples are annotated with gene predictions served by one DAS source, taxonomic assignments from another (BLAST hits to RefSeq, marker gene phylogenies), functional annotations from enzyme databases (EC numbers, KEGG pathways), and antibiotic resistance gene predictions from specialized databases (CARD, ResFinder), enabling integrated analysis of community functional potential across multiple annotation dimensions without consolidating heterogeneous databases. Precision medicine applications employ DAS to federate patient variant annotations where a clinical genome browser queries the patient's VCF coordinates against DAS servers providing ClinVar pathogenicity assertions, gnomAD population frequencies, functional predictions (SIFT, PolyPhen-2, CADD scores from separate computational servers), drug-gene interaction annotations from PharmGKB, and literature evidence from PubMed mining, synthesizing a comprehensive variant interpretation without requiring local copies of all databases, with machine learning models trained on federated annotations to predict variant clinical significance (benign, pathogenic, variant of uncertain significance VUS) using ensemble features from multiple DAS sources improving classification accuracy beyond single-source models. The DAS protocol's extensibility supports emerging data types where new feature categories (3D chromatin interactions from Hi-C as arcs between genomic coordinates, RNA structure predictions, nanopore long-read alignments with base modification calls, single-cell ATAC-seq pseudo-bulk tracks aggregated by cell type, spatial transcriptomics histology-linked expression) can be exposed through DAS servers as new sources join the federation, and clients incorporating DAS client libraries (Bio::Das::Lite in Perl, pydas in Python, Java DAS clients) dynamically discover and integrate these novel annotation layers, fostering an open, evolvable ecosystem where innovation in annotation methods (new experimental assays, improved computational predictions, AI-generated annotations) rapidly propagates to end-users through the lightweight DAS federation layer without requiring software updates or data migrations, ultimately enabling AI-driven genomics research where machine learning models training on comprehensive, multi-source genomic annotations benefit from the decentralized, continually updated knowledge encoded in the distributed DAS annotation network spanning experimental databases, computational prediction servers, and AI model inference APIs.
  requires_registration: false
  url: https://static-content.springer.com/esm/art%3A10.1186%2F1471-2105-2-7/MediaObjects/12859_2001_8_MOESM1_ESM.pdf
- id: B2AI_STANDARD:101
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ecological Metadata Language
  is_open: true
  name: EML
  publication: doi:10.5063/F11834T2
  purpose_detail: A metadata standard developed for the earth, environmental and ecological
    sciences.
  requires_registration: false
  url: https://eml.ecoinformatics.org/
- id: B2AI_STANDARD:102
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Economic Botany Data Collection Standard
  is_open: true
  name: EBDCS
  purpose_detail: This standard provides a system whereby uses of plants (in their
    cultural context) can be described, using standardised descriptors and terms,
    and attached to taxonomic data sets. It resulted from discussions at the International
    Working Group on Taxonomic Databases for Plant Sciences (TDWG) between 1989 and
    1992. Users and potential users of the standard include economic botanists and
    ethnobotanists whose purpose is to record all known information about the uses
    of a taxon; educationalists, taxonomists, biochemists, anatomists etc. who wish
    to record plant use, often at a broad level; economic botany collection curators
    who need to describe accurately the uses and values of specimens in their collections;
    bibliographers who need to describe plant uses referred to in publications and
    to apply keywords consistently for ease of data retrieval. While this standard
    is still in use, it is no longer actively maintained (labelled as prior on the
    TDWG website).
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:93
  url: https://www.tdwg.org/standards/economic-botany/
- id: B2AI_STANDARD:103
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - codesystem
  concerns_data_topic:
  - B2AI_TOPIC:26
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Enzyme Commission Number
  is_open: true
  name: EC
  publication: doi:10.1126/science.150.3697.719
  purpose_detail: The Enzyme Commission (EC) number system provides a hierarchical, internationally standardized nomenclature and classification scheme for enzymes based on the chemical reactions they catalyze rather than their protein structure, sequence homology, or evolutionary relationships, enabling unambiguous identification and cross-referencing of enzymatic functions across biochemistry, molecular biology, genomics, metabolomics, and systems biology research. Established in 1961 by the International Union of Biochemistry and Molecular Biology (IUBMB) and published in the foundational 1965 report "Enzyme Nomenclature," the EC system assigns each characterized enzyme a unique four-level numerical identifier (EC X.Y.Z.W) where the first digit indicates the major reaction class (1=oxidoreductases catalyzing oxidation-reduction, 2=transferases transferring functional groups, 3=hydrolases catalyzing hydrolysis, 4=lyases cleaving bonds by mechanisms other than hydrolysis or oxidation, 5=isomerases catalyzing intramolecular rearrangements, 6=ligases forming bonds with ATP cleavage, 7=translocases moving molecules across membranes), the second digit specifies the subclass refining the reaction type (such as EC 1.1.* for oxidoreductases acting on the CH-OH group of donors, EC 2.7.* for transferases transferring phosphorus-containing groups like kinases), the third digit further narrows substrate specificity or reaction mechanism (EC 2.7.1.* for phosphotransferases with alcohol group as acceptor), and the fourth digit uniquely identifies the individual enzyme (EC 2.7.1.1 for hexokinase catalyzing ATP + D-hexose  ADP + D-hexose 6-phosphate), creating a hierarchical classification framework with ~7,700 entries as of 2025 covering characterized enzymatic activities with recommended systematic names, common synonyms, reaction equations with substrate/product stoichiometry, cofactor requirements, and literature references documenting catalytic mechanisms. The EC system serves as the authoritative enzyme nomenclature standard integrated into major biological databases including UniProt protein knowledgebase (mapping protein sequences to enzymatic functions via EC annotations), KEGG pathway database (representing metabolic and signaling pathways as networks of EC-numbered reactions), BRENDA comprehensive enzyme information system (curating kinetic parameters, substrate specificities, and organism-specific properties for each EC number), MetaCyc/BioCyc metabolic pathway collections (using EC numbers to define reaction steps in organism-specific and reference pathway reconstructions), Reactome pathway knowledgebase (annotating human biochemical reactions with EC classifications), and ENZYME database at ExPASy (the reference repository maintained by Swiss Institute of Bioinformatics mirroring IUBMB nomenclature committee recommendations with monthly updates reflecting newly characterized enzymes and reclassifications). The hierarchical EC structure enables functional annotation where genome sequencing projects assign putative enzymatic functions to predicted protein-coding genes through sequence homology searches against characterized enzymes, with tools like BLAST, HMMer, and InterPro identifying conserved catalytic domains corresponding to specific EC classes; metabolic network reconstruction leverages EC annotations to infer organism-specific biochemical capabilities by mapping genomic enzyme repertoires to known metabolic pathways, predicting auxotrophies, biosynthetic capacities, and catabolic capabilities; and comparative enzymology uses EC-based functional profiles to analyze metabolic evolution, horizontal gene transfer of enzymatic functions, and adaptation to environmental niches by quantifying presence/absence patterns of enzyme classes across phylogenetic groups. For artificial intelligence and machine learning applications, EC numbers provide structured functional labels enabling supervised learning where enzyme function prediction models train on sequence-to-EC mappings using architectures such as convolutional neural networks on amino acid sequences (DeepEC, ECPred), graph neural networks on protein structure representations (predicting catalytic residues and EC class from 3D coordinates), and pre-trained protein language models (ESM, ProtTrans) fine-tuned for multi-label EC classification treating the hierarchical EC system as nested prediction tasks (first predicting reaction class EC X.-.-.-, then subclass, then sub-subclass, then specific enzyme), with evaluation metrics including hierarchical precision/recall accounting for partial correctness when predictions match broader EC categories. Machine learning on EC-annotated enzyme datasets supports metabolic engineering applications where models predict substrate promiscuity and catalytic efficiency for enzyme variants, guiding directed evolution campaigns by forecasting which mutations enhance desired activities (kcat/KM for target substrates) or alter substrate specificity (broadening or narrowing accepted substrates), with training data derived from mutagenesis studies, kinetic assays, and high-throughput screening experiments linked to EC-numbered parent enzymes. Natural language processing extracts enzyme-function relationships from literature by mining PubMed abstracts for sentences mentioning EC numbers alongside organism names, experimental conditions, kinetic parameters, and inhibitors, populating knowledge graphs that connect genes, proteins, EC functions, metabolites, pathways, and phenotypessuch graphs trained using graph embedding methods (node2vec, TransE) or graph neural networks enable link prediction inferring novel enzyme-metabolite associations, pathway gap-filling identifying missing enzymatic steps in incomplete reconstructions, and drug target prioritization by predicting which enzymes are essential for pathogen viability or disease progression. Metabolomics and proteomics data integration benefits from EC-based functional categorization where untargeted metabolomics detects thousands of small molecules, and pathway enrichment analysis tests whether detected metabolites over-represent substrates or products of specific EC classes, implicating corresponding enzymatic activities as dysregulated in disease states; proteomics quantifies enzyme abundances, and flux balance analysis (FBA) of genome-scale metabolic models constrained by EC-annotated reaction stoichiometry predicts metabolic fluxes under different conditions, with machine learning models trained on multi-omics datasets (transcriptomics, proteomics, metabolomics) to predict flux distributions and growth phenotypes. Pharmacology and drug discovery leverage EC classifications where therapeutic enzyme targets (such as EC 3.4.24.* matrix metalloproteinases in cancer metastasis, EC 3.4.25.1 angiotensin-converting enzyme ACE in hypertension, EC 2.7.10.* protein tyrosine kinases in oncogenic signaling) guide rational drug design, with machine learning models predicting compound-enzyme binding affinities and selectivity profiles by training on chemical structure-EC activity datasets from ChEMBL and PubChem BioAssay, enabling virtual screening of compound libraries against specific EC-numbered enzyme families and off-target prediction assessing whether candidate drugs inhibit unintended EC classes causing adverse effects. Metagenomics and microbiome research employ EC-based functional profiling where shotgun metagenomic sequencing of environmental or host-associated microbial communities is followed by gene prediction and EC annotation, yielding community-level enzyme repertoires; tools like HUMAnN (HMP Unified Metabolic Analysis Network) quantify the abundance of EC-numbered pathways, revealing functional shifts in microbiomes across health/disease states or environmental perturbations, with machine learning classifiers trained on EC abundance profiles distinguishing disease phenotypes (inflammatory bowel disease vs. healthy controls, dysbiosis states) and random forests identifying which EC functions (e.g., butyrate-producing enzymes EC 2.8.3.*, bile salt hydrolases EC 3.5.1.24) are most predictive of clinical outcomes. Systems biology integrates EC-annotated metabolic models with gene regulatory networks and signaling pathways, where constraint-based modeling (FBA, flux variability analysis) predicts metabolic fluxes through EC-numbered reactions under genetic or environmental perturbations, and machine learning emulators (neural networks trained on simulation ensembles) accelerate exploration of design spaces for metabolic engineering or predict emergent phenotypes from multi-scale models coupling genome-scale metabolism (EC-defined reactions), transcriptional regulation (transcription factor binding), and environmental sensing. The EC classification system also underpins ontology-based data integration where the Gene Ontology (GO) molecular function terms reference EC numbers to define enzymatic activities (GO:0004396 hexokinase activity is defined with EC 2.7.1.1), enabling semantic reasoning over biological knowledge graphs that link genes annotated with GO/EC terms to pathways, phenotypes, diseases, and drugs, supporting AI-driven hypothesis generation such as identifying candidate enzymes for synthetic biology chassis organisms, predicting metabolic vulnerabilities in cancer cells (synthetic lethality with specific EC-numbered enzyme knockouts), or discovering novel biocatalysts for industrial applications by querying EC-structured enzyme databases for activities matching desired chemical transformations (stereoselectivity, substrate scope, cofactor independence), ultimately providing the standardized functional vocabulary essential for reproducible enzyme annotation, interoperable metabolic databases, and machine learning models that predict, design, and optimize enzymatic functions across biochemistry, biomedicine, biotechnology, and synthetic biology.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:61
  url: https://iubmb.qmul.ac.uk/enzyme/rules.html
- id: B2AI_STANDARD:104
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - diagnosticinstrument
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: EORTC Quality of Life questionnaires
  is_open: true
  name: EORTC QLQ
  purpose_detail: 'The European Organisation for Research and Treatment of Cancer
    (EORTC) Quality of Life Questionnaires (QLQ) comprise a modular, multidimensional
    patient-reported outcome (PRO) instrument system designed to assess health-related
    quality of life (HRQoL) in cancer patients participating in clinical trials and
    routine oncology care. Developed through rigorous international collaboration
    since the 1980s, the system consists of a 30-item core questionnaire (QLQ-C30)
    applicable across cancer types, covering five functional scales (physical, role,
    emotional, cognitive, social), three symptom scales (fatigue, nausea/vomiting,
    pain), six single-item symptoms (dyspnea, insomnia, appetite loss, constipation,
    diarrhea, financial difficulties), and a global health/quality-of-life scale,
    supplemented by over 20 disease-specific modules (e.g., QLQ-LC29 for lung cancer,
    QLQ-BR45 for breast cancer, QLQ-CR29 for colorectal cancer), treatment-specific
    modules (QLQ-CIPN20 for chemotherapy-induced neuropathy, QLQ-IO38 for immunotherapy),
    and symptom-specific modules. Items use 4-point Likert scales ("not at all" to
    "very much") transformed into 0100 scores, with established minimal clinically
    important differences (MCIDs, typically 510 points), robust psychometric properties
    (Cronbach''s  0.700.90, test-retest reliability, construct validity with performance
    status and disease stage), and translations into over 100 languages with cultural
    validation. In clinical practice, EORTC QLQ data inform treatment decision-making,
    toxicity monitoring, and supportive care interventions, with HRQoL endpoints increasingly
    influencing regulatory approvals (FDA, EMA) and reimbursement decisions. For AI
    and machine learning applications, the structured PRO datasets enable predictive
    modeling of treatment completion, survival, and hospitalization; time-series analysis
    of symptom trajectories for early deterioration detection; NLP extraction of concerns
    from free-text comments; dimensionality reduction to identify symptom clusters
    and patient phenotypes; and multi-task learning across correlated functional and
    symptom domains, with large-scale international databases (EORTC Quality of Life
    Group repository, ARCAD) supporting meta-learning and external validation for
    generalizable, patient-centered AI systems in oncology.'
  requires_registration: true
  url: https://qol.eortc.org/questionnaires/
- id: B2AI_STANDARD:105
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: European Data Format
  has_application:
  - id: B2AI_APP:258
    category: B2AI:Application
    name: Sleep-EDF 1D-CNN sleep staging
    description: Automated sleep stage classification using one-dimensional convolutional neural networks (1D-CNN) trained on raw polysomnography (EEG and EOG) signals from Sleep-EDF and Sleep-EDFX databases achieving high multi-class accuracy for 2-6 stage classification. The 1D-CNN architecture processes raw PSG time-series without manual feature engineering, learning characteristic sleep waveforms directly from EDF-formatted recordings. Reported accuracies on Sleep-EDF were 98.06% (2-class), 94.64% (3-class), 92.36% (4-class), 91.22% (5-class), and 91.00% (6-class); on Sleep-EDFX 97.62%, 94.34%, 92.33%, 90.98%, and 89.54% respectively, demonstrating that deep learning on EDF biosignal files achieves near-human performance for automated sleep staging supporting large-scale sleep disorder screening and research.
    references:
    - https://doi.org/10.3390/ijerph16040599
  - id: B2AI_APP:259
    category: B2AI:Application
    name: Sleep-EDF weighted Random Forest + HMM staging
    description: Automated sleep staging using improved weighted Random Forest (WRF) with continuous wavelet packet transform (WPT) features combined with Hidden Markov Model (HMM) temporal smoothing on Sleep-EDF and Sleep-EDF Expanded double-channel EEG (Fpz-Cz and Pz-Oz). Recordings segmented into 30-second epochs then 29 overlapping 2-second subepochs for frequency-domain and statistical feature extraction, with feature selection removing low-variance features and class weighting addressing imbalance, followed by HMM exploiting temporal dependencies between adjacent sleep stages. Achieved subject-non-independent accuracy 93.20% and Cohen's kappa 0.890; subject-independent accuracy 91.97% and kappa 0.874; best double-channel performance 94.34% accuracy and kappa 0.886; best single-subject 96.3% accuracy and kappa 0.912. Demonstrates effective classical ML approach on EDF-formatted PSG with improved N1 stage recognition, providing interpretable feature-based alternative to end-to-end deep learning for clinical sleep medicine.
    references:
    - https://doi.org/10.32604/cmes.2020.08731
  - id: B2AI_APP:260
    category: B2AI:Application
    name: SeizeIT2 wearable EDF seizure detection benchmark
    description: "Wearable epilepsy dataset providing behind-the-ear EEG, ECG, EMG, accelerometer, and gyroscope data in EDF format with annotations in JSON/TSV for standardized seizure detection algorithm development. Dataset includes 125 patients with focal epilepsy (80% training split sub-001 to sub-096 with 704 seizures, 20% validation split sub-097 to sub-125 with 182 seizures), recordings aligned with full-scalp video-EEG monitoring, corrupted flat-line segments removed. Provides two baseline pipelines: feature-based ML using SVM (1-25 Hz Butterworth band-pass, 2-second windows 50% overlap, RMS thresholding 13-150 V, 42 features from 2-channel behind-the-ear EEG, 5x background undersampling) and deep learning ChronoNet (convolutional + recurrent layers, 250 Hz resampling, 2-second segments 75% overlap seizures/50% background). Annotations document onset, duration, lateralization, localization, vigilance state, and per-channel visibility. Enables reproducible comparison of wearable seizure detection algorithms on standardized EDF-formatted biosignals supporting ambulatory epilepsy monitoring and closed-loop intervention systems."
    references:
    - https://doi.org/10.1038/s41597-025-05580-x
  - id: B2AI_APP:261
    category: B2AI:Application
    name: EDF-based EEG deep learning survey benchmarks
    description: Comprehensive survey documenting widespread use of EDF-formatted EEG datasets (Sleep-EDF, CHB-MIT pediatric epilepsy with ~969 hours and 173 seizures, TUH EEG Corpus with 60,000+ recordings) underpinning deep learning research for sleep staging, seizure detection, and EEG abnormality detection. Sleep-EDF typical 5-stage classification accuracies reported at 82-88% overall with advanced models (deep CNNs, CNN+BiLSTM, U-Net variants U-Time, temporal convolutional networks, Transformer hybrids, self-supervised pretraining) achieving 87-89% accuracy and Cohen's kappa 0.75-0.82 by learning characteristic sleep waveforms from 30-second epochs. Cascaded RNN pipelines report ~95% on 6-class CAP Sleep. CHB-MIT continuous scalp EEG and TUH/TUSZ corpora enable training CNN, CNN+RNN, and image-based CNN architectures for clinical seizure detection and abnormality identification. Survey highlights that EDF standardization enables comparative evaluation across laboratories, reproducible benchmarking, and transfer learning from public datasets to clinical applications, with deep learning architectures approaching or exceeding human expert performance on EDF-standardized biosignal corpora.
    references:
    - https://doi.org/10.56578/ataiml040304
  is_open: true
  name: EDF
  purpose_detail: The European Data Format (EDF) is a simple and flexible format for
    exchange and storage of multichannel biological and physical signals, particularly
    in clinical neurophysiology, sleep medicine, and cardiology. Originally developed
    in 1990 by European biomedical engineers through the "Methodology for the Analysis
    of the Sleep-Wakefulness Continuum" project funded by the European Community,
    EDF became the de-facto standard for EEG and PSG recordings. The format is hardware
    and software independent, supporting various montages, transducers, prefiltering
    options, and sampling frequencies. EDF enables creation of common databases for
    sleep records, comparative analysis of algorithms across laboratories, and standardized
    evaluation of manual and automatic analysis methods. The format has been extended
    to EDF+, which maintains backward compatibility while adding support for interrupted
    recordings, annotations, stimuli, events, and analysis results such as deltaplots,
    QRS parameters, and sleep stages. EDF+ can store any medical recording including
    EMG, evoked potentials, and ECG data, with stricter specifications that enable
    automatic electrode localization and calibration. Both formats are freely available
    and widely adopted in commercial equipment and multicenter research projects.
  requires_registration: false
  url: https://www.edfplus.info/
- id: B2AI_STANDARD:106
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - diagnosticinstrument
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: European Laryngological Society guidelines
  is_open: true
  name: ELS
  publication: doi:10.1007/s004050000299
  purpose_detail: A multidimensional set of minimal basic measurements suitable for
    all common dysphonias is proposed. It includes five different approaches - perception
    (grade, roughness, breathiness), videostroboscopy (closure, regularity, mucosal
    wave and symmetry), acoustics (jitter, shimmer, Fo-range and softest intensity),
    aerodynamics (phonation quotient), and subjective rating by the patient.
  requires_registration: false
  url: https://doi.org/10.1007/s004050000299
- id: B2AI_STANDARD:107
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - diagnosticinstrument
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: EuroQol Five Dimension Three Level descriptive system
  is_open: true
  name: EQ-5D-3L
  purpose_detail: The EQ-5D-3L descriptive system comprises the following five dimensions
    - mobility, self-care, usual activities, pain/discomfort and anxiety/depression.
    Each dimension has 3 levels - no problems, some problems, and extreme problems.
    The patient is asked to indicate his/her health state by ticking the box next
    to the most appropriate statement in each of the five dimensions. This decision
    results into a 1-digit number that expresses the level selected for that dimension.
    The digits for the five dimensions can be combined into a 5-digit number that
    describes the patients health state.
  requires_registration: true
  url: https://euroqol.org/eq-5d-instruments/eq-5d-3l-about/
- id: B2AI_STANDARD:108
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FAIR Cookbook
  has_relevant_organization:
  - B2AI_ORG:28
  is_open: true
  name: FAIR Cookbook
  publication: doi:10.5281/zenodo.7156792
  purpose_detail: 'The FAIR Cookbook is an open-access, community-driven, online resource
    developed by the ELIXIR Europe consortium that provides practical, actionable
    guidance for implementing FAIR (Findable, Accessible, Interoperable, Reusable)
    data principles in life sciences research through structured "recipes"step-by-step
    tutorials addressing specific data management challenges across the research lifecycle.
    Organized into thematic sections covering data management planning, metadata standards,
    ontologies and terminologies, data deposition and licensing, interoperability
    frameworks (semantic web technologies, APIs, linked data), reproducibility and
    provenance tracking, and domain-specific implementations (genomics, proteomics,
    metabolomics, imaging, clinical data), each recipe follows a standardized format
    including objectives, intended audience, requirements, ingredients (tools, standards,
    resources), step-by-step instructions with code snippets, worked examples, expected
    outcomes, and references to relevant standards (schema.org, DCAT, Dublin Core,
    ISA model, RO-Crate, Bioschemas, EDAM ontology) and infrastructure (repositories,
    registries, validation tools). The cookbook addresses common barriers to FAIR
    adoptionsuch as selecting appropriate metadata schemas, implementing persistent
    identifiers (DOIs, ORCIDs, RRIDs), choosing open licenses (CC-BY, CC0), exposing
    data through machine-readable APIs, and integrating semantic technologieswith
    pragmatic solutions tailored to diverse stakeholder needs (wet-lab researchers,
    bioinformaticians, data stewards, repository managers, funders, publishers). Hosted
    on GitHub with Jupyter Book rendering, the FAIR Cookbook operates as a living
    document with continuous community contributions, version control, peer review
    via pull requests, and Zenodo DOI versioning for citability, embodying FAIR principles
    in its own infrastructure (content licensed CC-BY 4.0, source code openly available,
    machine-readable structured markup). For AI and machine learning workflows, the
    cookbook provides critical guidance on making training datasets FAIR-compliant:
    recipes cover automated metadata extraction and validation, semantic annotation
    of features and labels using ontologies (enabling cross-dataset harmonization
    and federated learning), implementing provenance standards (PROV-O, CWLProv) for
    reproducible preprocessing pipelines, packaging models and data together in research
    objects (RO-Crate with ML-specific profiles), exposing datasets through standardized
    APIs for programmatic access (GraphQL, SPARQL endpoints, OpenAPI-compliant REST
    services), and integrating with ML platforms (Hugging Face Datasets, TensorFlow
    Datasets, PyTorch DataLoaders) that consume FAIR metadata for dataset discovery
    and versioning, ultimately supporting the creation of well-documented, reusable,
    and interoperable AI-ready data resources that accelerate model development, facilitate
    external validation, and enable responsible AI practices through transparent data
    lineage and governance.'
  requires_registration: false
  url: https://faircookbook.elixir-europe.org/
- id: B2AI_STANDARD:109
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Fast Healthcare Interoperability Resources
  has_relevant_organization:
  - B2AI_ORG:114
  - B2AI_ORG:117
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: FHIR
  purpose_detail: Fast Healthcare Interoperability Resources (FHIR) is a standard
    for exchanging healthcare information electronically, developed by HL7. FHIR defines
    a set of modular components called "resources" that represent discrete clinical
    and administrative concepts (patients, observations, medications, procedures,
    diagnostic reports, etc.) with well-defined data structures, terminology bindings,
    and RESTful API interactions. Each resource can be retrieved, created, updated,
    and searched via standard HTTP operations, enabling flexible system integration
    and mobile/web application development. FHIR supports multiple exchange paradigms
    including RESTful APIs, messaging, documents, and services, with extensibility
    mechanisms (profiles, extensions) to adapt to local requirements while maintaining
    interoperability. The standard leverages modern web technologies (JSON, XML),
    common terminologies (SNOMED CT, LOINC, RxNorm), and implementation guides to
    facilitate rapid deployment across EHR systems, clinical decision support tools,
    patient-facing applications, and research platforms. FHIR's structured, granular
    data representation and standardized APIs make it particularly suitable for AI/ML
    applications that require consistent access to longitudinal patient data, clinical
    observations, and healthcare workflows for training predictive models and deploying
    real-time decision support at the point of care.
  related_to:
  - B2AI_STANDARD:720
  - B2AI_STANDARD:688
  - B2AI_STANDARD:693
  - B2AI_STANDARD:694
  - B2AI_STANDARD:697
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:40
  url: https://www.hl7.org/fhir/overview.html
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:34
    category: B2AI:Application
    name: Cumulus Federated EHR Learning with Bulk FHIR and AI/NLP
    description: The Cumulus platform operationalizes SMART/HL7 Bulk FHIR Access API
      for standardized data export across multiple healthcare institutions, then applies
      AI and natural language processing for computable phenotyping to define cohorts
      and outcomes from both structured and unstructured EHR data. The SMART Text2FHIR
      pipeline extracts insights from clinical texts and converts them into structured
      FHIR data elements for analysis. Only aggregate outputs leave each institution,
      enabling privacy-preserving federated learning across sites for public health
      monitoring and research while maintaining data sovereignty and interoperability
      through standardized FHIR exchange.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2024.02.02.24301940
  - id: B2AI_APP:126
    category: B2AI:Application
    name: FHIR Observation Encoding of Radiology AI Findings for Interoperable Integration
    description: A framework models radiology AI outputs (such as pulmonary nodule
      characteristics from automated detection algorithms) as FHIR Observation resources
      embedded within DiagnosticReport objects, standardizing AI-generated findings
      alongside radiologist reports. This FHIR-based encoding enables interoperable
      downstream use, long-term tracking of imaging findings, and integration with
      electronic health records for clinical decision-making. The structured FHIR
      representation facilitates reproducible data management, cohort identification,
      and longitudinal analysis of AI-detected imaging biomarkers across healthcare
      systems.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1093/jamia/ocae134
  - id: B2AI_APP:127
    category: B2AI:Application
    name: MyDigiTwin FHIR Harmonization for Federated Learning Cohort Studies
    description: The MyDigiTwin federated learning research infrastructure uses a
      FHIR-based data harmonization framework to standardize cohort study variables
      across multiple sites for cardiovascular risk prediction modeling. The pipeline
      successfully generated approximately 150,000 FHIR bundles from Lifelines cohort
      variable data, demonstrating practical large-scale FHIR use for multi-center
      ML. This FHIR harmonization ensures interoperability and facilitates the structuring,
      standardization, and exchange of healthcare data across federated learning nodes,
      enabling distributed model training while data remains at local institutions.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.3233/shti240735
  - id: B2AI_APP:128
    category: B2AI:Application
    name: SMART on FHIR Integration for AI-Driven Clinical Decision Support Systems
    description: SMART on FHIR provides a modular framework enabling third-party AI-driven
      clinical decision support applications to integrate seamlessly into health information
      systems. Implementations include automated breast cancer diagnosis via ultrasound
      with FHIR-standardized diagnostic metadata, ECG stream analysis frameworks for
      cloud-native AI processing, and machine learning-enhanced architectures that
      use FHIR to standardize clinical and imaging data flow for AI-based risk assessment.
      FHIR's standardized APIs and resource models enable deployment of predictive
      models across clinical domains, improving adherence to guidelines and enhancing
      diagnostic accessibility through interoperable, scalable CDSS implementations.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.20944/preprints202509.0818.v1
  - id: B2AI_APP:129
    category: B2AI:Application
    name: FHIR-Based Analytics Platforms for Clinical Predictive Model Deployment
    description: FHIR data models and APIs support analytics frameworks and deployment
      of clinical predictive models including sepsis prediction, distributed phenotyping
      analytics platforms, and personalized medicine applications. Implementations
      provide patterns for converting FHIR resources to analysis-ready formats, hosting
      predictive models as web services, and automating management of audit files
      and structured medical data. Platforms like doc.ai leverage FHIR analytical
      capabilities for personalized medicine, while distributed phenotyping systems
      use FHIR to enable clinical decision-making across multiple healthcare sites
      with standardized data access and model integration.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.3390/healthcare11121729
- id: B2AI_STANDARD:110
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FASTA Sequence Format
  is_open: true
  name: FASTA
  purpose_detail: A text-based data format for nucleotide and amino acid sequences.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/FASTA_format
  used_in_bridge2ai: true
- id: B2AI_STANDARD:111
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FASTQ Sequence and Sequence Quality Format
  has_relevant_organization:
  - B2AI_ORG:116
  - B2AI_ORG:114
  is_open: true
  name: FASTQ
  purpose_detail: FASTQ is a text-based file format for storing nucleotide sequences
    along with their corresponding per-base quality scores, making it the de facto
    standard for raw sequencing data from high-throughput sequencing platforms (Illumina,
    PacBio, Oxford Nanopore, etc.). Each sequence record consists of four lines containing
    a sequence identifier, the raw nucleotide sequence, a separator line, and ASCII-encoded
    quality scores (Phred scores) representing the confidence of each base call. The
    quality scores enable downstream tools to weight bases appropriately during alignment,
    variant calling, and assembly operations. FASTQ supports various quality encoding
    schemes (Phred+33, Phred+64) and accommodates reads of variable length from different
    sequencing technologies. The format's simultaneous representation of sequence
    and quality information is essential for quality control, error correction, read
    filtering, and AI/ML applications that require confidence metrics for training
    models on sequencing data quality assessment, base calling improvement, contamination
    detection, and metagenomic classification tasks.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/FASTQ_format
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:35
    category: B2AI:Application
    name: seqQscorer Automated FASTQ Quality Control Using Machine Learning
    description: seqQscorer computes features from FASTQ files including per-base
      and per-read quality metrics, overrepresented sequences, k-mer counts, and mapping-derived
      statistics, then trains tree-based (Random Forest) and deep learning classifiers
      to automatically flag low- versus high-quality NGS files. The system uses FASTQ-derived
      quality scores as core predictive features with labels from ENCODE curation,
      creating generalizable models across assays (RNA-seq, ChIP-seq, DNase-seq, ATAC-seq)
      and species. This ML-based automated QC provides decision support for curators
      and researchers, saving resources by identifying poor-quality experiments early
      in analysis pipelines.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1186/s13059-021-02294-2
  - id: B2AI_APP:130
    category: B2AI:Application
    name: MiniScrub CNN-Based Nanopore Read Error Scrubbing Using FASTQ Quality Pileups
    description: MiniScrub encodes Oxford Nanopore read-to-read overlaps and per-base
      qualities from FASTQ into image-like pileup channels that capture minimizer
      matches, base quality scores, and inter-minimizer distances. A convolutional
      neural network trained on these FASTQ-derived representations identifies and
      removes low-quality read segments de novo, without requiring reference alignment
      or hybrid error correction. This CNN-based scrubbing improves downstream assembly
      quality by reducing misassemblies and indel errors, demonstrating how FASTQ
      sequence and quality information can be transformed into ML-friendly image representations
      for quality classification tasks.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1186/s12859-019-3103-z
  - id: B2AI_APP:131
    category: B2AI:Application
    name: MAC-ErrorReads Supervised Classification for Filtering Erroneous NGS Reads
    description: MAC-ErrorReads formulates erroneous-read filtration as supervised
      binary classification using features extracted from FASTQ sequences via TF-IDF
      encoding (treating sequence tokens/k-mers like text). Multiple supervised algorithms
      including Naive Bayes models are trained to distinguish erroneous versus accurate
      reads, retaining higher-quality reads and improving downstream assembly quality
      and genomic coverage. This ML approach addresses increasing sequencing throughput
      by providing automated, scalable read filtering that complements traditional
      k-mer-based and alignment-based error-handling methods across both short-read
      and long-read NGS platforms.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1186/s12859-024-05681-1
  - id: B2AI_APP:132
    category: B2AI:Application
    name: SeqTagger CTC-CRF Basecaller for Nanopore Barcode Demultiplexing with Quality
      Filtering
    description: SeqTagger demultiplexes direct RNA nanopore datasets by training
      a CTC-CRF DNA basecalling model (using Bonito software) to decode barcode sequences
      from signal data, producing FASTQ basecalls that are then mapped to reference
      barcodes. Per-read median base quality from FASTQ is used to filter out potential
      misassignments, achieving high precision (approximately 99%) and recall (approximately
      95%) for demultiplexing. This workflow demonstrates how FASTQ basecalled sequences
      and their quality scores enable accurate barcode assignment and quality-based
      filtering in direct RNA sequencing, contrasting with image-based CNN approaches
      by operating in sequence space.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2024.10.29.620808
  - id: B2AI_APP:133
    category: B2AI:Application
    name: Deep Learning Variant Calling from FASTQ-Aligned Read Pileups
    description: Deep learning variant callers including DeepVariant, Clair3, and
      DNAscope operate on pileup tensors built from FASTQ-aligned reads, incorporating
      per-base quality scores as input features alongside base identity. These models
      transform FASTQ-derived alignments into image-like or tensor representations
      that preserve sequence context and quality information, then apply CNNs or specialized
      architectures for genotype prediction. Benchmarks demonstrate that these deep
      learning approaches outperform traditional algorithms for SNP and indel calling
      across platforms (Illumina, PacBio HiFi, Oxford Nanopore), with quality scores
      explicitly used to weight evidence and calibrate predictions through methods
      like GATK's variant quality score recalibration (VQSR).
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2024.03.15.585313
    - https://doi.org/10.1002/0471250953.bi1110s43
  - id: B2AI_APP:134
    category: B2AI:Application
    name: CNN-Based eDNA Read Classification and rRNA Detection from FASTQ
    description: Deep learning models for metagenomics and environmental DNA analysis
      map FASTQ reads directly to taxonomic labels using k-mer embeddings or sequence
      representations, enabling fast and scalable classification with comparable accuracy
      to conventional pipelines. CNN-based workflows process raw FASTQ at high throughput
      for taxonomic assignment, while tools like RiboDetector use bidirectional LSTM
      models to identify ribosomal RNA reads in short-read data for contaminant removal.
      These approaches transform FASTQ sequences (and optionally quality-derived features)
      into token/k-mer embeddings or sequence inputs for classification tasks, demonstrating
      rapid and accurate read-level identification that reduces misclassification
      in downstream RNA-seq and metagenomic workflows.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2024.03.15.585313
  - id: B2AI_APP:135
    category: B2AI:Application
    name: fastp FASTQ Preprocessing for ML-Ready Feature Generation
    description: fastp performs ultra-fast all-in-one FASTQ preprocessing including
      quality control, adapter trimming, quality filtering, UMI processing, and base
      correction in a single scan, generating standardized outputs used as inputs
      or targets for ML workflows. The tool produces per-base quality profiles, k-mer
      occurrence tables, overrepresented-sequence positions, adapter detection results,
      and paired-end overlap corrections that serve as features for ML quality prediction
      models, denoising/error-correction training data, and metadata for classifiers.
      UMI-processed consensus reads reduce noise for ML training, while comprehensive
      pre- and post-filtering metrics enable automated quality assessment and artifact
      detection across NGS workflows, facilitating large-scale dataset generation
      for ML model development.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1093/bioinformatics/bty560
- id: B2AI_STANDARD:112
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - policy
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Federal Policy for the Protection of Human Subjects
  has_relevant_organization:
  - B2AI_ORG:39
  is_open: true
  name: Common Rule
  purpose_detail: The Federal Policy for the Protection of Human Subjects or the Common
    Rule was published in 1991 and codified in separate regulations by 15 Federal
    departments and agencies, as listed below. The HHS regulations, 45 CFR part 46,
    include four subparts - subpart A, also known as the Federal Policy or the Common
    Rule; subpart B, additional protections for pregnant women, human fetuses, and
    neonates; subpart C, additional protections for prisoners; and subpart D, additional
    protections for children. Each agency includes in its chapter of the Code of Federal
    Regulations [CFR] section numbers and language that are identical to those of
    the HHS codification at 45 CFR part 46, subpart A. For all participating departments
    and agencies the Common Rule outlines the basic provisions for IRBs, informed
    consent, and Assurances of Compliance. Human subject research conducted or supported
    by each federal department/agency is governed by the regulations of that department/agency.
  requires_registration: false
  url: https://www.hhs.gov/ohrp/regulations-and-policy/regulations/common-rule/index.html
  used_in_bridge2ai: true
- id: B2AI_STANDARD:113
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FHIR Common Data Models Harmonization
  formal_specification: https://build.fhir.org/ig/HL7/cdmh/profiles.html
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: CDMH
  purpose_detail: The Common Data Models Harmonization (CDMH) FHIR Implementation
    Guide is an HL7-sponsored specification that provides standardized mappings and
    transformation rules for converting observational clinical data from multiple
    common data models (CDMs) into HL7 FHIR format, enabling interoperability between
    patient-centered outcomes research networks, federated data networks, and learning
    health systems that use different data representation schemas while preserving
    semantic fidelity and supporting cross-network queries for comparative effectiveness
    research, quality improvement initiatives, and post-market surveillance studies.
    CDMH focuses on harmonizing four major CDMs widely adopted in the United States
    research infrastructurePCORNet (Patient-Centered Outcomes Research Network),
    i2b2/ACT (Informatics for Integrating Biology and Bedside / Accrual to Clinical
    Trials), OMOP (Observational Medical Outcomes Partnership), and FDA Sentineleach
    of which employs distinct data schemas, terminology bindings, and structural conventions
    that historically impeded data sharing and federated analysis across research
    consortia. The Implementation Guide defines FHIR resource profiles, ConceptMaps,
    StructureMaps, and transformation logic that specify how core clinical entities
    (patient demographics, encounters, diagnoses, procedures, medications, laboratory
    results, vital signs) represented in PCORNet's relational model, i2b2/ACT's star
    schema with ontology-driven observations, OMOP's standardized vocabularies and
    fact tables, or Sentinel's distributed query architecture are mapped to corresponding
    FHIR resources (Patient, Encounter, Condition, Procedure, MedicationRequest,
    Observation) with appropriate code system translations and cardinality adjustments.
    By providing bidirectional mappings, CDMH enables research networks to expose
    their existing CDM data through FHIR APIs without requiring complete data migration,
    allowing external applications, quality measure engines, and clinical decision
    support tools built on FHIR to access multi-network datasets while preserving
    local data governance and privacy controls. The harmonization approach addresses
    semantic challenges including vocabulary alignment (mapping between SNOMED CT,
    RxNorm, ICD-10-CM, LOINC, and CDM-specific code systems like PCORNet value sets
    or OMOP concept codes), temporal representation differences (event dates versus
    observation periods versus time spans), and structural mismatches (flat tables
    versus hierarchical resources, normalized schemas versus denormalized views).
    CDMH supports use cases in comparative effectiveness research where investigators
    need to pool patient cohorts from multiple networks to achieve sufficient sample
    sizes for detecting treatment effects, rare disease studies, or subgroup analyses,
    with the FHIR-based harmonization layer enabling federated queries that execute
    locally at each network site and aggregate de-identified summary statistics without
    transferring patient-level data across organizational boundaries. For regulatory
    applications, CDMH facilitates FDA Sentinel's integration with FHIR-enabled clinical
    data sources, enabling post-market safety surveillance queries to span both claims-based
    data (traditional Sentinel domain) and EHR-based clinical observations (accessible
    via FHIR), improving signal detection for adverse events that require clinical
    context beyond billing codes. The Implementation Guide provides validation rules,
    conformance testing criteria, and quality metrics (data completeness, mapping
    coverage, terminology alignment rates) to assess the fidelity of CDM-to-FHIR
    transformations and identify data quality issues requiring remediation. CDMH integration
    with SMART on FHIR enables patient-facing applications (patient-reported outcomes
    capture, longitudinal health records, research participation platforms) to interact
    with research network data through standardized APIs, supporting patient engagement
    in research and transparent data sharing with participants. The harmonization
    framework supports machine learning and AI applications by providing unified access
    to training datasets aggregated from multiple CDMs, with FHIR's structured representation
    facilitating feature engineering, phenotype definition, and model validation across
    diverse source systems. As research networks adopt FHIR natively (via FHIR-native
    CDMs or dual-model architectures), CDMH serves as a transition pathway preserving
    investments in existing CDM implementations while enabling incremental migration
    toward standards-based interoperability. The IG is maintained by HL7's Clinical
    Interoperability Council with input from PCORnet, i2b2, OMOP/OHDSI, and FDA Sentinel
    stakeholder communities, ensuring alignment with evolving CDM specifications and
    FHIR release cycles.
  related_to:
  - B2AI_STANDARD:109
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: https://build.fhir.org/ig/HL7/cdmh/
- id: B2AI_STANDARD:114
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:13
  - B2AI_TOPIC:35
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FHIR Genomics Operations
  formal_specification: https://github.com/FHIR/genomics-operations
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: Genomics Operations
  publication: doi:10.1093/jamia/ocac246
  purpose_detail: A standardized suite of genomics operations in FHIR designed to
    support a wide range of clinical scenarios, such as variant discovery; clinical
    trial matching; hereditary condition and pharmacogenomic screening; and variant
    reanalysis.
  related_to:
  - B2AI_STANDARD:109
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: http://build.fhir.org/ig/HL7/genomics-reporting/operations.html
- id: B2AI_STANDARD:115
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FHIR Provenance Resource
  formal_specification: https://www.hl7.org/fhir/provenance-definitions.html
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: Provenance
  purpose_detail: '"The FHIR Provenance resource provides comprehensive tracking of
    information about activities that create, revise, delete, or sign healthcare resources,
    describing the entities and agents involved in these processes. Based on the W3C
    Provenance specification, it establishes a critical foundation for assessing authenticity,
    enabling trust, and allowing reproducibility in healthcare data systems. The resource
    supports three key components from W3C Provenance: Entity (physical, digital,
    or conceptual things with fixed aspects), Agent (persons, devices, systems, organizations,
    or care teams bearing responsibility for activities), and Activity (time-bound
    processes that act upon entities). Provenance resources serve as record-keeping
    assertions that capture context information for quality, reliability, and trustworthiness
    assessments. The standard supports multiple use cases including tracking data
    lineage in clinical workflows, enabling audit trails for regulatory compliance,
    supporting clinical decision-making through data origin verification, and facilitating
    data integration from multiple healthcare systems. It includes digital signature
    capabilities for integrity verification and non-repudiation, supports versioning
    and unique identification requirements, and provides mechanisms for recording
    import and transformation activities in healthcare interoperability scenarios."'
  related_to:
  - B2AI_STANDARD:109
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: https://www.hl7.org/fhir/provenance.html
- id: B2AI_STANDARD:116
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FHIR Resource CodeSystem
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: CodeSystem
  purpose_detail: The CodeSystem resource is used to declare the existence of and
    describe a code system or code system supplement and its key properties, and optionally
    define a part or all of its content.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: http://hl7.org/fhir/codesystem.html
  used_in_bridge2ai: true
- id: B2AI_STANDARD:117
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  - B2AI_TOPIC:32
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FHIR Resource ConceptMap
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: ConceptMap
  purpose_detail: The FHIR ConceptMap resource provides a structured, machine-readable representation of terminology mappings that define relationships between codes from different coding systems (such as SNOMED CT to ICD-10-CM, RxNorm to local hospital formularies, LOINC to institution-specific laboratory test codes, or HL7 v2 table codes to FHIR value sets), enabling semantic translation of clinical concepts across heterogeneous health information systems where different organizations, regions, or time periods have adopted different terminologies for fundamentally equivalent clinical meanings, with ConceptMaps serving as the authoritative specification of how concepts in a source system map to concepts in one or more target systems through explicitly documented equivalence relationships that preserve clinical intent during data exchange, system migration, federated queries across institutional boundaries, real-world evidence studies requiring harmonized phenotype definitions, and AI/ML applications necessitating consistent terminology representation for training data drawn from multiple sources. A ConceptMap resource defines mapping metadata including the canonical URL that serves as the unique, version-independent identifier for the map (enabling unambiguous reference in implementations, like "http://hl7.org/fhir/ConceptMap/103" for the standard ICD-9-CM to SNOMED CT map), human-readable name, title, and description providing context on the map's purpose and appropriate usage scenarios, version string and effective date range documenting temporal validity (critical for longitudinal studies where terminology mappings evolve as source or target systems release new versionsICD-10-CM annual updates, SNOMED CT bi-annual releases), publication status indicating lifecycle state (draft maps under development and review, active maps validated and approved for production use, retired maps superseded by newer versions with documented migration paths), publisher and contact information identifying the authoritative source responsible for map curation and maintenance (such as national standards bodies, professional societies, or multi-stakeholder collaboratives like the US National Library of Medicine maintaining UMLS Metathesaurus cross-terminology mappings), copyright and licensing terms governing map reuse (essential for proprietary terminology licenses requiring attribution, usage restrictions, or redistribution controls), jurisdiction and use context specifying appropriate usage scenarios (maps designed for clinical decision support vs billing/reimbursement vs quality measurement vs research analytics may differ in mapping granularity and equivalence semantics), and purpose statement articulating the clinical, administrative, or analytical rationale for the map's creation. The core mapping structure consists of source and target elements defining the conceptual spaces being mapped, where sourceUri or sourceCanonical references the value set or code system providing concepts to be translated (the "from" terminology like ICD-10-CM diagnosis codes), targetUri or targetCanonical references the value set or code system providing translation targets (the "to" terminology like SNOMED CT concepts), and group elements partition the overall mapping into logical subsets each specifying a narrower source and target scope (enabling efficient lookup and reducing mapping complexity by organizing translations hierarchicallyseparate groups for different ICD-10-CM chapters mapping to different SNOMED CT hierarchies). Within each group, element entries define individual concept mappings with source code specifying the code being translated (like ICD-10-CM "E11.9" for Type 2 diabetes mellitus without complications), source display providing human-readable text for the source concept (improving map reviewability and maintenance), and target array enumerating one or more translation targets each with target code (like SNOMED CT "44054006" for diabetes mellitus type 2), target display, equivalence indicator specifying the semantic relationship between source and target concepts using standardized codes (equivalent indicating exact semantic match suitable for bidirectional translation, equal for formally identical concepts differing only in representation, wider where target concept subsumes source meaningsource is more specific than target, subsumes synonym for wider, narrower where source concept encompasses targetsource is more general than target, specializes synonym for narrower, inexact for approximate matches where translation loses or adds semantic nuance, unmatched explicitly documenting that no suitable target exists for this source concept, disjoint indicating fundamentally incompatible meanings precluding any translation), comment providing human-readable notes on translation rationale or caveats (documenting clinical judgment, edge cases, or context-dependent validity), and dependsOn elements specifying additional properties or codes that must be considered for context-dependent mappings (such as patient age, anatomical location, or procedure context required to disambiguate translations where a single source code maps to different targets depending on clinical circumstances). ConceptMap resources support bidirectional vs unidirectional mapping specifications where bidirectional maps define reciprocal translations enabling roundtrip conversion (A maps to B AND B maps back to A, critical for system interoperability requiring lossless translation), unidirectional maps define one-way translations where reverse mapping is either undefined or requires a separate ConceptMap (common when mapping from granular source terminology like SNOMED CT 350,000+ active concepts to coarse target terminology like ICD-10-CM ~70,000 codes, where many distinct source concepts map to same target but target cannot unambiguously reverse-map to unique source), and unmapped elements document default behavior when source concepts lack explicit mappings (strategies include returning fixed code indicating "no map available", using source code unchanged in target system if permitted by use case, or raising an error to prevent silent data loss). The operational integration of ConceptMaps with FHIR infrastructure occurs through the $translate operation exposed by FHIR terminology servers, where clients submit requests specifying source code, source system, and target system (plus optional context parameters like dependsOn property values, reverse direction flag for bidirectional maps, and scope constraints), the terminology server queries its ConceptMap repository to locate applicable maps matching the source-to-target system pair and temporal validity constraints, executes the mapping logic including evaluating conditional dependencies and equivalence semantics, and returns a Parameters resource containing the translated target code(s), display text, equivalence type, and provenance indicating which ConceptMap and version were appliedthis $translate operation enables real-time, on-demand concept translation within clinical workflows without requiring applications to implement mapping logic internally, supporting batch translation operations that process datasets by translating columns of source codes to target codes in bulk (efficient for ETL pipelines populating analytical databases), and cached translation services that precompute and materialize common translations to optimize performance in high-throughput scenarios like near-real-time clinical decision support or streaming analytics on electronic health record data. For AI and machine learning applications, ConceptMap resources enable foundational data harmonization where federated learning initiatives training models across multiple healthcare institutions use ConceptMaps to translate institution-specific local terminologies to a common reference terminology before model training (ensuring that "CBC" at Hospital A, "complete blood count" at Hospital B, and LOINC code "58410-2" at Hospital C are recognized as equivalent features despite different source representations, preventing vocabulary fragmentation that would degrade model performance), natural language processing pipelines performing clinical entity linking use ConceptMaps to normalize extracted text mentions to preferred vocabularies (mapping free-text medication names to RxNorm, disease mentions to SNOMED CT, laboratory tests to LOINC) with equivalence type metadata guiding confidence scoring (equivalent mappings receiving higher confidence than inexact mappings), clinical decision support systems translate local order codes to knowledge base terminologies to enable guideline checking (mapping hospital formulary drug codes to RxNorm for drug-drug interaction screening, translating local procedure codes to SNOMED CT for appropriateness use criteria evaluation), data warehouse extract-transform-load processes use ConceptMaps to harmonize heterogeneous source systems to unified target schemas like OMOP Common Data Model or PCORnet Common Data Model (systematically translating diagnosis codes, procedure codes, medication codes, laboratory tests to standard_concept_id representations, documenting mapping provenance and equivalence semantics for sensitivity analyses), real-world evidence studies requiring longitudinal observational cohorts spanning multiple health systems leverage ConceptMaps to create harmonized phenotype definitions (defining "diabetes" cohort inclusion criteria as UNION of ICD-10-CM codes, SNOMED CT concepts, and local problem list terms after ConceptMap-based semantic alignment ensuring consistent case ascertainment despite source terminology heterogeneity), quality measurement programs use ConceptMaps to translate clinical data to value sets specified in measure definitions (CMS eCQM specifications, HEDIS measures requiring specific code sets with ConceptMaps enabling sites using alternative terminologies to demonstrate compliance), international research collaborations map national terminologies to shared reference standards (NHS Read Codes or SNOMED CT Clinical Terms UK edition to international SNOMED CT for multi-country studies, French CCAM procedure codes to ICD-10-PCS for comparative effectiveness research), interoperability testing frameworks validate roundtrip translation fidelity by applying forward and reverse ConceptMaps and checking that clinical meaning is preserved (detecting lossy or ambiguous mappings that could compromise data quality in health information exchanges), genomic medicine applications linking phenotype terminologies to genotype databases use ConceptMaps to translate clinical diagnosis codes to Human Phenotype Ontology (HPO) terms enabling variant prioritization algorithms that match patient phenotypes to gene-disease associations (mapping ICD-10 "Q87.0" Fragile X syndrome to HPO "HP:0001999" Abnormal facial shape and related terms), pharmacovigilance systems harmonize adverse event reports from different sources by mapping MedDRA terms, SNOMED CT concepts, and free-text descriptions to common representations supporting signal detection algorithms, and precision medicine data integration platforms use ConceptMaps to align multi-omics entities across measurement platforms (mapping mass spectrometry metabolite identifiers to HMDB codes, proteomics peptide sequences to UniProt identifiers, transcriptomics probe identifiers to HUGO gene symbols) enabling cross-platform statistical analysis and multi-modal machine learning models that integrate heterogeneous molecular phenotypes for treatment response prediction, with ConceptMap-documented semantic relationships providing the essential terminology translation infrastructure that enables AI/ML models to learn from distributed, terminology-diverse healthcare data while maintaining clinical validity and interpretability through explicit, auditable, version-controlled mappings that preserve semantic relationships and document translation limitations, ultimately supporting trustworthy AI where model training data provenance includes terminology harmonization metadata, federated analytics can reason about semantic equivalence across sites, and real-world evidence generation maintains clinical meaning across heterogeneous source terminologies through principled, transparent concept mapping.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: http://hl7.org/fhir/conceptmap.html
- id: B2AI_STANDARD:118
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FHIR Resource NamingSystem
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: NamingSystem
  purpose_detail: The FHIR NamingSystem resource provides a curated, machine-readable registry of identifier and coding systems used within healthcare information exchange, documenting the namespaces that issue unique symbols for identification of concepts (terminology codes like SNOMED CT concept identifiers, LOINC codes), people (patient medical record numbers, provider National Provider Identifiers NPI, staff employee IDs), devices (medical device unique device identifiers UDI, imaging equipment serial numbers), organizations (facility identifiers, payer organization IDs), and other entities requiring unambiguous identification across distributed healthcare systems, serving as the authoritative metadata layer that enables FHIR Identifier and Coding data types to reference external identifier systems with precise semantics ensuring correct interpretation, validation, and linkage of identifiers across institutional boundaries. A NamingSystem resource defines critical metadata for an identifier/coding system including the system's canonical URI (the authoritative namespace string used in FHIR Identifier.system and Coding.system elements, like "http://snomed.info/sct" for SNOMED CT or "http://hl7.org/fhir/sid/us-npi" for US National Provider Identifiers), human-readable name and description (contextual information for implementers about the system's purpose, scope, and appropriate usage), uniqueId elements specifying alternative identifiers for the same system (enabling recognition that "http://snomed.info/sct" and OID "2.16.840.1.113883.6.96" reference the same SNOMED CT system across different contextsURI-based references in modern FHIR implementations, OID-based references in legacy HL7 v2/v3 systems), type classification distinguishing identifier systems (which assign unique identifiers to specific instances like "patient 12345 at Hospital A") from codesystem systems (which define concept codes representing classes or types like "diabetes mellitus Type 2" as a SNOMED CT concept applicable to many patients), and usage context metadata including responsible organization (identifying issuing authority like College of American Pathologists for SNOMED CT, Regenstrief Institute for LOINC, CMS for NPIs), jurisdiction and preferred usage regions (documenting geographic scope like "US realm" for US Core profiles, "International" for globally applicable systems), and publication status tracking system lifecycle (draft systems under development, active systems in production use, retired systems being phased out with guidance on migration to successors). NamingSystem resources support identifier validation workflows where applications receiving identifiers check that the namespace portion matches a recognized NamingSystem, verify the identifier format conforms to system-specific rules documented in the resource (NPIs must be 10 digits passing Luhn algorithm checksum, ICD-10-CM codes follow specific alphanumeric patterns), and apply jurisdictional constraints ensuring identifiers are used appropriately (US Realm profiles require US-specific systems, cross-border exchanges may necessitate identifier mapping or alternative identification strategies). The uniqueId elements enable identifier normalization where systems receiving data with OID-based identifiers (common in HL7 v2 messages, CDA documents) translate them to URI-based identifiers for processing in FHIR pipelines, with NamingSystem lookups resolving "2.16.840.1.113883.6.88" OID to "http://www.nlm.nih.gov/research/umls/rxnorm" RxNorm URI ensuring consistent internal representation regardless of source format. The NamingSystem registry function is crucial for multi-institutional data integration where patient linking requires matching identifiers across institutions (recognizing that different hospitals' MRNs are distinct namespaces requiring explicit NamingSystem declarations like "http://hospital-a.org/fhir/sid/mrn" vs "http://hospital-b.org/fhir/sid/mrn" preventing accidental conflation of unrelated identifiers), provider directories aggregate credentials from multiple sources (matching NPIs, state medical license numbers, DEA numbers as distinct but co-referent identifiers for same provider using NamingSystem-based identity resolution), and medication reconciliation disambiguates drug identifiers from different systems (RxNorm concept codes, NDC product codes, local formulary IDs requiring explicit namespace documentation for accurate medication matching). For AI/ML applications, NamingSystem resources enable robust data preprocessing pipelines where ETL processes validate identifier formats against NamingSystem-documented patterns before loading data into analytical databases (rejecting malformed identifiers that would corrupt downstream analyses), federated learning initiatives leverage NamingSystem metadata to understand identifier scope and design privacy-preserving record linkage strategies (determining which identifier systems permit cross-institutional matching vs requiring local pseudonymization), natural language processing systems extracting identifiers from clinical text use NamingSystem format specifications to recognize identifier patterns in unstructured notes (detecting "NPI 1234567890" as a provider identifier, "MRN 98765432" as a patient identifier with appropriate namespace disambiguation), entity resolution models training on linked administrative and clinical data use NamingSystem-based features indicating identifier type and issuing organization to improve matching accuracy (weighting high-trust national identifiers like NPIs more heavily than potentially duplicate local identifiers), synthetic data generation for ML development uses NamingSystem specifications to produce realistic test identifiers following correct format rules and namespace conventions (generating valid-format but non-existent NPIs for testing, synthetic MRNs conforming to institutional patterns), clinical data warehouses implementing OMOP Common Data Model or i2b2 star schema map source system identifiers to standardized domains using NamingSystem-based crosswalks (translating hospital-specific identifiers to common representations while maintaining provenance), quality measurement systems validating data submissions against program requirements use NamingSystem checks ensuring submitted identifiers match expected namespaces (CMS quality reporting requiring specific identifier systems), and blockchain-based health information exchanges leverage NamingSystem resources as smart contract metadata ensuring cryptographically-signed clinical data references identifiers from trusted, auditable namespaces, ultimately supporting trustworthy AI where model training data provenance traces back through properly identified entities, federated analytics respect identifier scope and privacy constraints, and real-world evidence generation maintains identifier integrity across diverse source systems through explicit, machine-actionable namespace documentation.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: http://hl7.org/fhir/namingsystem.html
- id: B2AI_STANDARD:119
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FHIR Resource TerminologyCapabilities
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: TerminologyCapabilities
  purpose_detail: A TerminologyCapabilities resource documents a set of capabilities
    (behaviors) of a FHIR Terminology Server that may be used as a statement of actual
    server functionality or a statement of required or desired server implementation.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: http://hl7.org/fhir/terminologycapabilities.html
- id: B2AI_STANDARD:120
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FHIR Resource ValueSet
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: ValueSet
  purpose_detail: The FHIR ValueSet resource is a fundamental component of the Fast Healthcare Interoperability Resources (FHIR) standard that defines curated collections of codes drawn from one or more terminology systems (CodeSystems like SNOMED CT, LOINC, RxNorm, ICD-10) intended for use in specific clinical contexts, serving as the mechanism linking standardized medical vocabularies to coded data elements in FHIR resources where terminology binding ensures semantic interoperability, data quality, and consistent interpretation across heterogeneous healthcare information systems. A ValueSet resource instance specifies both intensional definitions (rule-based membership criteria like "all SNOMED CT concepts descending from '64572001|Disease' excluding neoplasms") and extensional definitions (explicit enumeration of permitted codes like specific ICD-10 diagnosis codes for diabetes complications), with metadata including unique canonical URL identifier (enabling unambiguous ValueSet references across systems), version information (supporting evolution of code sets over time while maintaining backward compatibility), publication status (draft/active/retired lifecycle states), publisher and contact information (attributing ValueSet authorship to standards organizations like HL7, CDC, WHO, or local healthcare authorities), copyright/licensing terms, and purpose/usage notes explaining clinical context and intended application. ValueSet composition uses include and exclude elements with filters enabling dynamic code selection where "include" adds codes matching criteria (concept descendants in hierarchy, codes with specific properties like "is-a bacterial infection", codes from particular code system versions), filter operators (is-a for subsumption, descendent-of for transitive closure, regex for pattern matching, in for membership testing against reference sets), and exclude rules remove unwanted codes from included sets, supporting complex value set definitions like "all cardiovascular procedures (SNOMED codes under 64915003) performed on adults (exclude pediatric-specific procedures) with CPT billing codes mapped via ConceptMap resources." ValueSets support expansion operations where FHIR terminology servers resolve intensional definitions into explicit code lists at specific timestamps, returning complete enumerations of valid codes with display names, designations in multiple languages, hierarchical relationships, and additional metadata, with expansions cacheable and versioned to ensure reproducible validation of historical data against time-appropriate code sets. The binding strength mechanism associates ValueSets with FHIR data elements through required bindings (only codes from ValueSet permitted, enforced validation), extensible bindings (ValueSet codes preferred but others allowed if no appropriate match exists, supporting local extensions), preferred bindings (suggested codes but not enforced), and example bindings (illustrative codes for demonstration), enabling graduated interoperability where core clinical concepts like vital signs, medications, procedures require strict standardization (required/extensible bindings to international ValueSets) while local workflow terms, institutional protocols, research-specific classifications permit flexibility (preferred/example bindings). ValueSet supplements and ConceptMap resources extend functionality by adding local designations, translations, or properties to codes (supplements augment base CodeSystems without modifying source terminologies) and mapping between equivalent concepts across terminology systems (ConceptMaps translating ICD-10 diagnoses to SNOMED CT, RxNorm medications to local formulary codes), supporting federated terminology services where institutions contribute local knowledge while maintaining alignment with national/international standards. For AI/ML applications, FHIR ValueSets enable semantic feature engineering where coded clinical data elements (diagnoses, procedures, medications, lab tests) expand from atomic codes into rich semantic features using ValueSet hierarchies and relationships, with NLP systems mapping free-text clinical notes to structured codes then validating against appropriate ValueSets ensuring coded output matches context (problem lists use diagnosis ValueSets, medication statements use RxNorm ValueSets), phenotype algorithms querying FHIR data using ValueSet-based inclusion/exclusion criteria to identify study cohorts (patients with "Type 2 Diabetes" ValueSet codes AND "Metformin" ValueSet medications AND HbA1c > 7% for real-world evidence studies), machine learning models incorporating ValueSet membership as categorical features or using semantic embeddings where code positions in ValueSet hierarchies inform similarity metrics (SNOMED CT subsumption relationships improving disease classification models), clinical decision support systems validating orders and alerts against ValueSet constraints (medication-allergy checking where allergy ValueSets define contraindications, lab result interpretation using ValueSet-defined normal ranges), automated chart review and quality measurement where NLP extracted codes validate against quality measure ValueSets (CMS Core Measures, HEDIS specifications), federated learning across institutions using standardized ValueSets ensuring consistent phenotype definitions and outcome measurements enabling model training on distributed EHR data without raw data sharing, and clinical trial recruitment where eligibility criteria expressed as FHIR ValueSet queries (OMOP phenotypes, PCORnet Common Data Model queries) identify eligible patients across research networks, supporting precision medicine initiatives requiring semantic interoperability where AI models learn from multi-institutional data consistently coded using shared ValueSets, real-world evidence generation from EHR data where ValueSet-based phenotyping ensures reproducible cohort identification, and healthcare analytics platforms where standardized ValueSets enable accurate cross-system queries, longitudinal patient tracking, and population health surveillance despite underlying terminology heterogeneity in source systems.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: http://hl7.org/fhir/valueset.html
- id: B2AI_STANDARD:121
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:2
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Flow Cytometry Standard format
  is_open: true
  name: FCS
  publication: doi:10.1002/cyto.a.20825
  purpose_detail: The flow cytometry data file standard provides the specifications
    needed to completely describe flow cytometry data sets within the confines of
    the file containing the experimental data. In 1984, the first Flow Cytometry Standard
    format for data files was adopted as FCS 1.0. This standard was modified in 1990
    as FCS 2.0 and again in 1997 as FCS 3.0. We report here on the next generation
    Flow Cytometry Standard data file format. FCS 3.1 is a minor revision based on
    suggested improvements from the community. The unchanged goal of the Standard
    is to provide a uniform file format that allows files created by one type of acquisition
    hardware and software to be analyzed by any other type.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/Flow_Cytometry_Standard
- id: B2AI_STANDARD:122
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FORCE11 Data Citation Principles
  is_open: true
  name: FORCE11 DC
  purpose_detail: The Data Citation Principles cover the purpose, function and attributes
    of citations. These principles recognize the dual necessity of creating citation
    practices that are both human understandable and machine-actionable.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:32
  url: https://force11.org/info/joint-declaration-of-data-citation-principles-final/
- id: B2AI_STANDARD:123
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - diagnosticinstrument
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Functional Assessment of Cancer Therapy - General
  is_open: true
  name: FACT-G
  purpose_detail: The Functional Assessment of Cancer Therapy - General (FACT-G) is
    a 27-item questionnaire designed to measure four domains of HRQOL in cancer patients
    - Physical, social, emotional, and functional well-being. Original development
    and validation involved 854 patients with cancer and 15 oncology specialists.
    An initial pool of 370 overlapping items for breast, lung, and colorectal cancer
    was generated by open-ended interviews with patients experienced with the symptoms
    of cancer and oncology professionals. Using preselected criteria, items were reduced
    to a 38-item general version. Factor and scaling analyses of these 38 items on
    545 patients with mixed cancer diagnoses resulted in the 27-item FACT-General
    (FACT-G). Coefficients of reliability and validity were uniformly high. The scale's
    ability to discriminate patients on the basis of stage of disease, performance
    status rating (PSR), and hospitalization status supports its sensitivity. It has
    also demonstrated sensitivity to change over time.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:30
  url: https://www.facit.org/measures/FACT-G
- id: B2AI_STANDARD:124
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - diagnosticinstrument
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Functional Assessment of Chronic Illness Therapy - Dyspnea
  is_open: true
  name: FACIT-Dyspnea
  publication: doi:10.1016/j.jval.2010.06.001
  purpose_detail: The Functional Assessment of Chronic Illness Therapy - Dyspnea
    (FACIT-Dyspnea) is a validated, patient-reported outcome (PRO) instrument designed
    to comprehensively assess the multidimensional impact of dyspnea (shortness of
    breath) on patients' functional status and health-related quality of life across
    chronic respiratory diseases, cardiovascular conditions, and oncologic illnesses
    where breathlessness is a primary symptom affecting daily activities, emotional
    well-being, and social participation. Developed by the FACIT measurement system
    group and published with rigorous psychometric validation in Value in Health (2010),
    the instrument comprises a 10-item short form (FACIT-Dyspnea-10) that captures
    physical limitations imposed by dyspnea (ability to walk, climb stairs, perform
    household tasks, exercise), functional impairments in activities of daily living
    (bathing, dressing, eating), emotional consequences (anxiety, frustration, fear
    related to breathlessness), and social impacts (ability to participate in social
    activities, work, and recreational pursuits). Each item uses a 5-point Likert
    scale (0 = not at all, 4 = very much) referencing the past 7 days, with total
    scores ranging from 0-40 where higher scores indicate greater dyspnea-related
    impairment and lower quality of life. The FACIT-Dyspnea-10 demonstrates strong
    internal consistency (Cronbach's alpha typically >0.85), test-retest reliability,
    convergent validity with other dyspnea measures (Medical Research Council dyspnea
    scale, Borg scale, visual analog scales), and responsiveness to clinical change
    following therapeutic interventions, making it suitable for both cross-sectional
    assessment and longitudinal monitoring of treatment response. Clinical applications
    span chronic obstructive pulmonary disease (COPD) management where dyspnea is
    the cardinal symptom driving healthcare utilization and disability, interstitial
    lung diseases (idiopathic pulmonary fibrosis, sarcoidosis, connective tissue
    disease-associated ILD) where progressive breathlessness correlates with disease
    severity and survival, heart failure populations where dyspnea on exertion guides
    functional classification and treatment escalation decisions, and cancer-related
    breathlessness in lung cancer patients or those experiencing treatment-related
    pulmonary toxicity (radiation pneumonitis, chemotherapy-induced lung injury, immunotherapy-related
    pneumonitis). The instrument is particularly valuable in clinical trials evaluating
    pharmacologic interventions (bronchodilators, anti-fibrotics, diuretics, oxygen
    therapy, opioids for refractory dyspnea), pulmonary rehabilitation programs, exercise
    training interventions, and palliative care approaches for advanced disease, where
    patient-centered outcomes and subjective symptom burden are critical endpoints
    complementing objective measures like spirometry, six-minute walk distance, or
    imaging findings. FACIT-Dyspnea scores correlate with healthcare resource utilization
    patterns (emergency department visits, hospitalizations for dyspnea exacerbations,
    long-term oxygen prescriptions), support risk stratification models predicting
    disease progression and mortality, and inform shared decision-making conversations
    about treatment goals, advance care planning, and hospice transitions when breathlessness
    becomes refractory to disease-modifying therapies. The instrument's brevity (10
    items, ~2-3 minute administration time) facilitates integration into routine clinical
    encounters, remote patient monitoring programs using electronic PRO (ePRO) platforms,
    and large-scale registry studies or epidemiological cohorts examining dyspnea
    prevalence and burden across populations. FACIT-Dyspnea has been translated into
    multiple languages with cultural adaptation and validation in international populations,
    supporting multi-national clinical trials and cross-cultural comparative effectiveness
    research. The instrument complements physiologic measures and biomarkers by capturing
    the lived experience of dyspnea from the patient perspective, addressing FDA and
    EMA guidance emphasizing patient-reported outcomes as essential endpoints in regulatory
    submissions for symptom-focused therapies. FACIT-Dyspnea data support health economic
    evaluations by quantifying the quality-of-life burden of dyspnea for cost-utility
    analyses (QALY calculations) and inform healthcare policy decisions regarding
    coverage of symptomatic therapies, pulmonary rehabilitation services, and palliative
    interventions for breathlessness management.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:30
  url: https://www.facit.org/measures/facit-dyspnea
- id: B2AI_STANDARD:125
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - diagnosticinstrument
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Functional Assessment of Non-Life Threatening Conditions
  is_open: true
  name: FANLTC
  purpose_detail: The Functional Assessment of Non-Life Threatening Conditions (FANLTC)
    is a 26-item version of the FACT-G designed to be administered to patients with
    non-life threatening conditions. The item from the FACT-G making reference to
    anxiety about death has been removed, and the instrument measures four domains
    of HRQOL - Physical, social/family, emotional and functional well-being.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:30
  url: https://www.facit.org/measures/FANLTC
- id: B2AI_STANDARD:126
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:13
  - B2AI_TOPIC:35
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Functional equivalence of genome sequencing analysis pipelines enables
    harmonized variant calling across human genetics projects
  formal_specification: https://github.com/CCDG/Pipeline-Standardization
  is_open: true
  name: Regier2018
  publication: doi:10.1038/s41467-018-06159-4
  purpose_detail: Regier2018 defines whole genome sequencing (WGS) data processing
    standards that enable functional equivalence (FE) across different analysis pipelines
    used by various research groups. These standards establish best practices for
    alignment, variant calling, and quality control steps in genomic analysis workflows,
    allowing different computational implementations to produce concordant results
    while still permitting innovation in pipeline optimization. The framework addresses
    key challenges in large-scale genomic studies by defining expected outputs, quality
    metrics, and validation procedures that ensure harmonized variant calling across
    projects. This standardization is particularly important for collaborative efforts
    like the Centers for Common Disease Genomics (CCDG), where multiple centers must
    produce comparable data that can be combined for downstream analysis. The standards
    cover reference genome usage, read alignment parameters, variant detection algorithms,
    and filtering criteria, providing a foundation for reproducible genomics research.
  requires_registration: false
  url: https://github.com/CCDG/Pipeline-Standardization
- id: B2AI_STANDARD:127
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Functional Genomics Experiment Markup Language
  is_open: true
  name: FuGE-ML
  purpose_detail: Functional genomics experiments present many challenges in data
    archiving, sharing and querying. As the size and complexity of data generated
    from such experiments grows, so does the requirement for standard data formats.
    To address these needs, the Functional Genomics Experiment [Object Model / Markup-Language]
    (FuGE-OM, FuGE-ML) has been created to facilitate the development of data standards.FuGE
    is a model of the shared components in different functional genomics domains.
    FuGE facilitates the development of data standards in functional genomics in two
    ways. 1. FuGE provides a model of common components in functional genomics investigations,
    such as materials, data, protocols, equipment and software. These models can be
    extended to develop modular data formats with consistent structure. 2. FuGE provides
    a framework for capturing complete laboratory workflows, enabling the integration
    of pre-existing data formats. In this context, FuGE allows the capture of additional
    metadata that gives formats a context within the complete workflow. FuGE is available
    as a UML model and an XML Schema
  requires_registration: false
  url: https://fuge.sourceforge.net/index.php
- id: B2AI_STANDARD:128
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Functional Genomics Experiment model for flow cytometry
  is_open: true
  name: FuGEFlow
  publication: doi:10.1186/1471-2105-10-184
  purpose_detail: FuGEFlow is a specialized extension of the Functional Genomics Experiment
    (FuGE) data model designed to provide comprehensive, machine-readable representation
    of flow cytometry experimental workflows, metadata, and data provenance in compliance
    with the Minimum Information about a Flow Cytometry Experiment (MIFlowCyt) reporting
    guidelines, developed through collaborative efforts between the ISAC Data Standards
    Task Force, British Columbia Cancer Agency, and NIH/NIAID to enable standardized
    data exchange, repository integration, and computational reproducibility in high-throughput
    flow cytometry research. The model comprises Flow-OM (Flow Cytometry Object Model)
    defined in UML and Flow-ML (Flow Markup Language) implemented as an XML schema
    generated via AndroMDA, extending FuGE's core classes to capture flow cytometry-specific
    entities including cytometer instrumentation (flow cells, lasers, optical detectors
    with voltage parameters), sample preparation protocols (staining, fixation, permeabilization),
    acquisition parameters, and analytical workflows while maintaining references
    to external standards including FCS (Flow Cytometry Standard) list-mode data
    files and Gating-ML gating transformation descriptions rather than duplicating
    raw fluorescence measurements. FuGEFlow maps all MIFlowCyt mandatory and optional
    metadata elements to FuGE classes or newly defined FuGEFlow extensions (e.g.,
    ListModeDataFile, extended Material and Equipment classes), enabling complete
    experiment documentation from biological source through data acquisition to computational
    analysis with explicit ontology term bindings for standardized vocabularies.
    The framework supports conversion to ISA-TAB (Investigation-Study-Assay) tabular
    format via XSL stylesheets, facilitating integration with broader functional
    genomics data repositories and cross-domain meta-analyses that combine flow cytometry
    with transcriptomics, proteomics, or metabolomics datasets. FuGEFlow's design
    enables software tool interoperability through the FuGE toolkit which generates
    programmatic interfaces from the object model, and its flexible architecture
    accommodates diverse experimental designs including time-course studies, dose-response
    assays, and complex multi-parametric analyses that were not anticipated during
    initial specification development. In the context of artificial intelligence and
    machine learning applications, FuGEFlow serves as an enabling infrastructure standard
    rather than a direct ML analysis frameworkwhile peer-reviewed literature does
    not document specific cases where ML models were trained or validated using FuGEFlow-encoded
    datasets, the standard's comprehensive metadata capture and standardized experiment
    representation provide the foundation for several potential ML use cases including
    cross-study dataset integration for training population-scale classification
    models, reproducible feature extraction pipelines that link raw FCS signals through
    Gating-ML transformations to derived cellular phenotypes, automated quality control
    systems that detect instrument drift or protocol deviations through metadata
    pattern analysis, and meta-learning approaches that generalize gating strategies
    or cell population identification algorithms across diverse experimental conditions
    documented in standardized metadata. The ISA-TAB conversion capability enables
    export to tabular formats commonly consumed by ML frameworks (pandas, scikit-learn,
    TensorFlow), while FuGEFlow's provenance tracking supports reproducible ML workflows
    where training data lineage, preprocessing steps, and analytical transformations
    must be documented for regulatory compliance or scientific validation. Public
    flow cytometry data repositories built on Flow-ML infrastructure (as planned in
    the original FuGEFlow publication with anticipated ImmPort integration) could
    serve as curated training corpora for supervised learning of cell type classifiers,
    automated gating algorithms, or anomaly detection systems, though such repositories'
    ML utilization has not been systematically reported in subsequent literature.
    The standard's ontology-based knowledge representation aligns with broader data
    mining best practices articulated by the minimum information community, where
    machine-readable experiment metadata enables computational hypothesis generation,
    experimental design optimization, and integration of biological knowledge with
    primary analytical resultscapabilities that form the semantic foundation for
    AI systems even when FuGEFlow itself is not explicitly invoked in ML method descriptions.
  requires_registration: false
  url: https://doi.org/10.1186/1471-2105-10-184
- id: B2AI_STANDARD:129
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - diagnosticinstrument
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Functionality Assessment Flowchart
  is_open: true
  name: FAF
  publication: doi:10.1186/s12885-015-1526-0
  purpose_detail: The Functionality Assessment Flowchart (FAF) is a structured clinical
    assessment tool designed to evaluate cancer patients' performance status through
    a standardized flowchart-based methodology. FAF provides a systematic approach
    to measuring functional capacity and activities of daily living in oncology patients,
    yielding performance status scores with high inter-observer agreement. The flowchart
    format guides clinicians through a series of binary decision points based on patient
    capabilities (ability to work, walk, self-care, etc.), resulting in consistent
    and reproducible assessments. FAF addresses limitations of traditional performance
    status scales by reducing subjectivity and improving reliability across different
    observers and clinical settings. The standardized scoring enables comparison of
    patient functional status over time, supports clinical decision-making for treatment
    planning, facilitates stratification in clinical trials, and provides structured
    data suitable for integration into electronic health records and outcomes research
    databases.
  requires_registration: false
  url: https://doi.org/10.1186/s12885-015-1526-0
- id: B2AI_STANDARD:130
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: GA4GH metadata model
  formal_specification: https://github.com/ga4gh-metadata/metadata-schemas
  is_open: true
  name: GA4GH
  purpose_detail: The Global Alliance for Genomics and Health (GA4GH) metadata model
    provides a standardized framework for representing and sharing genomic and clinical
    data across institutions and international borders, developed by a coalition of
    public and private stakeholders including research institutions, healthcare organizations,
    and technology companies. The model defines schemas for core genomic data types
    including variants, reads, references, annotations, and associated clinical phenotypes,
    using protocol buffers (protobuf) and JSON for interoperable data exchange. GA4GH
    schemas cover reference genomes and sequences, variant calls and annotations (compatible
    with VCF), sequencing reads and alignments (supporting BAM/CRAM), RNA quantification,
    continuous-valued genomic signals, genomic features and annotations, metadata
    about biosamples and individuals, phenotypic information using ontologies, and
    provenance tracking. The framework enables federated data access through APIs
    including hts-get for streaming genomic data, Beacon for discovery queries across
    datasets, Data Repository Service (DRS) for accessing data objects, Task Execution
    Service (TES) for running analysis workflows, and Workflow Execution Service (WES)
    for managing computational pipelines. GA4GH standards facilitate large-scale collaborative
    research initiatives, support FAIR principles (Findable, Accessible, Interoperable,
    Reusable), enable secure data sharing with privacy-preserving technologies, and
    provide the foundation for international genomics research networks, clinical
    genomics implementations, and precision medicine programs.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:34
  collection:
  - deprecated
  url: https://github.com/ga4gh-metadata/metadata-schemas?tab=readme-ov-file
- id: B2AI_STANDARD:131
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Gating-ML specification
  is_open: true
  name: Gating-ML
  purpose_detail: The Gating-ML specification represents a proposal on how to form
    unambiguous XML-based gate definitions that may be used independently as well
    as included as one of the components of ACS. Such a description of gates can facilitate
    the interchange and validation of data between different software packages with
    the potential of significant increase of hardware and software interoperability.
    The specification supports rectangular gates in n dimensions (i.e., from one-dimensional
    range gates up to n-dimensional hyper-rectangular regions), polygon gates in two
    (and more) dimensions, ellipsoid gates in n dimensions, decision tree structures,
    and Boolean collections of any of the types of gates. Gates can be uniquely identified
    and may be ordered into a hierarchical structure to describe a gating strategy.
    Gates may be applied on parameters as in list mode data files (e.g., FCS files)
    or on transformed parameters as described by any explicit parameter transformation.
    Therefore, since version 1.5, parameter transformation and compensation description
    are included as part of the Gating-ML specification.
  requires_registration: false
  url: http://flowcyt.sourceforge.net/gating/
- id: B2AI_STANDARD:132
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:26
  - B2AI_TOPIC:33
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: GenBank Sequence Format
  is_open: true
  name: GB
  purpose_detail: The GenBank format is a comprehensive text-based annotation format
    for nucleotide and protein sequences maintained by NCBI as part of the GenBank
    sequence database, one of the three components of the International Nucleotide
    Sequence Database Collaboration (INSDC) alongside EMBL and DDBJ. Each GenBank
    record contains a sequence along with extensive structured metadata organized
    into standardized fields including locus name and length, definition line, accession
    and version numbers, keywords, source organism with taxonomic classification,
    literature references with PubMed links, and feature table annotations. The feature
    table uses a hierarchical key-value structure to annotate biological features
    such as genes, coding sequences (CDS) with translation, regulatory elements, binding
    sites, variations, and experimental evidence, with controlled qualifiers and ontology
    terms. GenBank format supports protein translations, codon usage information,
    cross-references to other databases (UniProt, PDB, Gene, etc.), and provenance
    tracking of sequence submissions and updates. The rich annotation structure enables
    integration of genomic context, functional annotations, phylogenetic information,
    and experimental validation data in a single standardized record, making GenBank
    essential for comparative genomics, gene annotation pipelines, sequence analysis
    tools, and machine learning applications that leverage both sequence content and
    biological metadata for training predictive models of gene structure, function,
    and regulation.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/GenBank
- id: B2AI_STANDARD:133
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Gene Prediction File Format
  is_open: true
  name: GenePred
  purpose_detail: The genePred (Gene Prediction) format is a flexible tab-delimited
    table format developed by UCSC Genome Browser for representing gene structure
    predictions and annotations, widely used for storing transcript models, gene predictions
    from ab initio tools, and curated gene annotations. The basic genePred format
    contains 10 required fields - gene name, chromosome, strand, transcription start/end
    positions, coding sequence (CDS) start/end positions, exon count, comma-separated
    exon start positions, and comma-separated exon end positions. Extended versions
    (genePredExt) add fields for unique identifiers, CDS start/end status (complete/incomplete),
    exon frame information for each coding exon, and gene symbols or descriptions.
    The format efficiently represents complex transcript structures including alternative
    splicing isoforms, UTRs (untranslated regions), introns, and partial gene models.
    GenePred files support various gene prediction algorithms (GeneMark, Augustus,
    SNAP), RefSeq annotations, Ensembl gene builds, and GENCODE comprehensive gene
    sets. UCSC provides utilities including genePredToBed for BED format conversion,
    genePredToGtf for GTF conversion, and genePredToPsl for PSL format output. The
    format's compact structure and explicit exon coordinates make it ideal for genome
    browsers, annotation pipelines, and comparative genomics analyses requiring efficient
    storage and rapid retrieval of gene structure information across whole genomes.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:119
  url: https://genome.ucsc.edu/FAQ/FAQformat.html#format9
- id: B2AI_STANDARD:134
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:26
  - B2AI_TOPIC:33
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Gene Product Annotation Data format
  is_open: true
  name: GPAD
  purpose_detail: The Gene Ontology project provides annotations describing attributes
    of biological entities such as genes and gene products. The Gene Ontology has
    historically provided annotations via Gene Association Format (GAF), including
    GAF-1 and GAF-2. Ontologies are distributed separately, using an OWL serialization
    or OBO format. The use of GAF has some drawbacks. Combined representation of gene/gene
    product data and annotations leads to redundancy/repetition No way to represent
    gene/gene product metadata for unannotated genes Requirement to maintain backward
    compatibility makes it harder to introduce enhancements such as use of an ontology
    for evidence types GAF formats will continue to be supported, but the need for
    a way to represent genes/gene products separately from annotations, as well as
    the need to use the evidence ontology has lead to the creation of the GPAD (Gene
    Product Annotation Data) and GPI (Gene Product Information) formats, defined here.
    Whilst GPAD and GPI have been defined for use within the Gene Ontology Consortium
    for GO annotation, this specification is designed to be reusable for analagous
    ontology-based annotation - for example, gene phenotype annotation.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:36
  url: http://geneontology.org/docs/gene-product-association-data-gpad-format/
- id: B2AI_STANDARD:135
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Gene Transfer Format
  is_open: true
  name: GTF
  purpose_detail: GTF (Gene Transfer Format, GTF2.2) is an extension to, and backward
    compatible with, GFF2. The first eight GTF fields are the same as GFF. The feature
    field is the same as GFF, with the exception that it also includes the following
    optional values, 5UTR, 3UTR, inter, inter_CNS, and intron_CNS.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:119
  url: http://genome.ucsc.edu/FAQ/FAQformat.html#format4
- id: B2AI_STANDARD:136
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: GenePattern GeneSet Table Format
  is_open: true
  name: GMT
  purpose_detail: The GMT (Gene Matrix Transposed) format is a tab-delimited text
    file format developed by the Broad Institute for representing gene sets used in
    gene set enrichment analysis (GSEA) and related computational biology applications.
    Each line in a GMT file represents a single gene set with a simple structure containing
    the gene set name, description (or URL), and a tab-separated list of gene identifiers
    (typically gene symbols, Entrez IDs, or other standard identifiers). The format's
    simplicity and human readability make it ideal for storing and sharing curated
    gene set collections such as pathway databases (KEGG, Reactome), Gene Ontology
    term associations, disease gene signatures, transcription factor targets, and
    experimentally derived co-expression modules. GMT files are widely used by enrichment
    analysis tools (GSEA, Enrichr, WebGestalt), enable integration of custom gene
    sets into analysis workflows, support reproducible research through standardized
    gene set definitions, and facilitate machine learning applications that use gene
    set membership as features for classification, dimension reduction, and biological
    interpretation of high-throughput genomic data.
  requires_registration: false
  url: https://www.genepattern.org/file-formats-guide#GMT
- id: B2AI_STANDARD:137
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: General Feature Format
  formal_specification: https://github.com/The-Sequence-Ontology/Specifications
  is_open: true
  name: GFF
  purpose_detail: The General Feature Format (GFF) is a tab-delimited text file format
    developed by the Sequence Ontology Consortium for representing genomic features
    and annotations. GFF3, the current version, provides a flexible structure for
    describing genes, transcripts, exons, regulatory elements, and other sequence
    features with their genomic coordinates, strand orientation, phase information,
    and hierarchical relationships. Each line represents a single feature with nine
    standardized fields including sequence ID, source, feature type (from Sequence
    Ontology terms), start/end positions, score, strand, and attributes. The format
    supports parent-child relationships through ID and Parent attributes, enabling
    representation of complex gene structures with multiple transcripts and alternative
    splicing. GFF3 includes directives for metadata and embedded FASTA sequences.
    The format is widely used by genome browsers (UCSC, Ensembl, JBrowse), annotation
    pipelines, and analysis tools for visualizing and processing genomic data. Its
    human-readable structure and extensive tool support make it a standard choice
    for genome annotation exchange and long-term data archiving.
  requires_registration: false
  url: https://github.com/The-Sequence-Ontology/Specifications
- id: B2AI_STANDARD:138
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Genome Annotation File
  is_open: true
  name: GAF
  purpose_detail: The Gene Ontology Annotation File (GAF) format is a standardized,
    tab-delimited text file specification developed by the Gene Ontology Consortium
    for representing gene product annotations using GO terms. GAF 2.1 defines 17 fields
    per line, capturing essential information such as database identifiers, gene symbols,
    GO terms, evidence codes, references, taxonomic information, and annotation extensions.
    Each line encodes a single association between a gene product and a GO term, supported
    by evidence and references, enabling precise tracking of functional, process,
    and cellular component annotations. The format supports both required and optional
    fields, allowing for rich annotation detail and interoperability across databases.
    GAF is widely used for large-scale functional genomics, comparative biology, and
    integrative bioinformatics, providing a foundation for computational analysis,
    data sharing, and automated reasoning about gene function across species.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:36
  url: http://geneontology.org/docs/go-annotation-file-gaf-format-2.1/
- id: B2AI_STANDARD:139
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Genome Beacons
  formal_specification: https://github.com/ga4gh-beacon/beacon-v2
  is_open: true
  name: Beacon
  publication: doi:10.1002/humu.24369
  purpose_detail: Beacon v2 is a protocol/specification established by the Global
    Alliance for Genomics and Health initiative (GA4GH) that defines an open standard
    for federated discovery of genomic (and phenoclinic) data in biomedical research
    and clinical applications.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:34
  url: http://docs.genomebeacons.org/
- id: B2AI_STANDARD:140
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:13
  - B2AI_TOPIC:35
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Genome Variation Format
  formal_specification: https://github.com/The-Sequence-Ontology/Specifications/blob/master/gvf.md
  is_open: true
  name: GVF
  purpose_detail: "The Genome Variation Format (GVF) is a specialized extension of
    the Generic Feature Format version 3 (GFF3) designed specifically for representing
    sequence variants, structural variations, and genomic alterations at nucleotide
    resolution relative to a reference genome assembly, developed and maintained by
    the Sequence Ontology Consortium. GVF inherits GFF3's tab-delimited, nine-column
    structure (seqid, source, type, start, end, score, strand, phase, attributes)
    while adding variant-specific conventions and controlled vocabularies for describing
    single nucleotide variants (SNVs), insertions, deletions, inversions, translocations,
    copy number variations (CNVs), and complex genomic rearrangements with precise
    breakpoint coordinates and allelic information. The format uses Sequence Ontology
    (SO) terms in the 'type' column to classify variant types (SO:0001483 for SNV,
    SO:1000032 for indel, SO:0001019 for CNV, SO:0000199 for translocation), ensuring
    semantic consistency and computational interoperability across variant calling
    pipelines, annotation systems, and genomic databases. GVF's attribute column supports
    standardized key-value pairs including Variant_seq for alternate alleles, Reference_seq
    for reference alleles, Genotype for individual genotypes, Variant_freq for population
    allele frequencies, Variant_effect for functional consequences predicted by tools
    like VEP or SnpEff, and Dbxref for cross-references to variant databases (dbSNP,
    ClinVar, COSMIC, gnomAD). The format was designed to address limitations of earlier
    variant formats (such as ambiguities in coordinate systems and lack of standardized
    variant classification) while maintaining backward compatibility with existing
    GFF3 parsers and genome browsers. GVF files include structured pragma directives
    that specify format version,
    reference genome assembly (e.g., GRCh38, GRCm39), and chromosome/scaffold boundaries,
    enabling unambiguous interpretation of variant coordinates across different reference
    assemblies and coordinate systems. The format supports multi-sample variant data
    through the Genotype attribute, allowing representation of diploid or polyploid
    genotypes (homozygous reference, heterozygous, homozygous alternate) and phasing
    information for linked variants on the same haplotype. GVF is particularly valuable
    for documenting structural variations and complex genomic events that involve
    multiple breakpoints, nested features, or imprecise boundaries, where VCF's linear
    representation becomes cumbersome or ambiguous. Research applications include
    population genetics studies reporting allele frequencies and linkage disequilibrium
    patterns, clinical genomics workflows annotating pathogenic variants with ACMG
    classifications and penetrance estimates, cancer genomics projects cataloging
    somatic mutations and driver events, and comparative genomics analyses tracking
    orthologous variants across species. GVF files integrate with genome browsers
    (UCSC Genome Browser, Ensembl, IGV) for visualization of variant landscapes alongside
    gene annotations, regulatory elements, and conservation scores. The format's standardized
    structure enables systematic variant aggregation and meta-analysis, supporting
    databases like Ensembl Variation, NCBI dbSNP, and model organism databases that
    curate and distribute genome-wide variant catalogs. While VCF (Variant Call Format)
    has become the dominant standard for high-throughput sequencing variant calls
    due to its compact representation and extensive tool support, GVF remains relevant
    for scenarios requiring richer semantic annotations, complex structural variants
    with feature hierarchies, or integration with GFF3-based annotation pipelines
    where variant features are treated as genomic intervals in unified analytical
    workflows. The Sequence Ontology Consortium maintains the GVF specification and
    provides reference implementations, validation tools, and mappings to related
    standards (VCF, HGVS nomenclature) to facilitate format interconversion and data
    harmonization across genomic resources."
  requires_registration: false
  url: https://github.com/The-Sequence-Ontology/Specifications/blob/master/gvf.md
- id: B2AI_STANDARD:141
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Genomic Contextual Data Markup Language
  is_open: true
  name: GCDML
  publication: doi:10.1089/omi.2008.0A10
  purpose_detail: The Genomic Contextual Data Markup Language (GCDML) is an XML Schema
    developed by the Genomic Standards Consortium (GSC) to implement the Minimum Information
    about a Genome Sequence (MIGS), Minimum Information about a Metagenome Sequence
    (MIMS), and Minimum Information about a MARKer gene Sequence (MIMARKS) specifications.
    This sample-centric, strongly-typed schema provides a comprehensive set of descriptors
    for documenting the complete provenance of biological samples, from initial collection
    and environmental context through sequencing and subsequent analysis. GCDML enables
    standardized reporting of genomic and metagenomic data by defining required metadata
    fields that capture sampling conditions, laboratory processing methods, sequencing
    parameters, and analytical workflows. The schema facilitates data exchange between
    research groups, repositories, and databases, ensuring that essential contextual
    information needed to interpret sequence data is preserved and communicated effectively.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:38
  url: https://doi.org/10.1089/omi.2008.0A10
- id: B2AI_STANDARD:142
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Global Genome Biodiversity Network Data Standard
  is_open: true
  name: GGBN
  publication: doi:10.1093/database/baw125
  purpose_detail: In order to facilitate exchange of information on genomic samples
    and their derived data, the Global Genome Biodiversity Network (GGBN) Data Standard
    is intended to provide a platform based on a documented agreement to promote the
    efficient sharing and usage of genomic sample material and associated specimen
    information in a consistent way.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:93
  url: https://www.tdwg.org/standards/ggbn/
- id: B2AI_STANDARD:143
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:20
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: GlycoCT encoding scheme
  formal_specification: https://github.com/glycoinfo/GlycoCT
  is_open: true
  name: GlycoCT
  publication: doi:10.1016/j.carres.2008.03.011
  purpose_detail: '"GlycoCT (Glyco Connection Table) format is an advanced, comprehensive
    encoding scheme specifically designed to describe complex carbohydrate sequences
    with unprecedented precision and consistency in glycobioinformatics. Developed
    as version 4 (KIROI) by Stephan Herget and Rene Ranzinger in 2007, this format
    employs a controlled vocabulary based on IUPAC nomenclature rules to systematically
    name monosaccharides and utilizes a connection table approach rather than linear
    encoding, enabling accurate representation of branched and complex glycan structures.
    GlycoCT incorporates a sophisticated block concept to efficiently handle frequently
    occurring structural features such as repeating units, which are common in biological
    carbohydrates. The format exists in two complementary variants: a condensed form
    optimized for database applications with strict sorting rules ensuring uniqueness
    for use as primary keys, and a more verbose XML syntax that provides enhanced
    readability and machine processing capabilities. Released under Creative Commons
    Attribution 4.0 International License, GlycoCT represents a significant advancement
    toward achieving a unified and broadly accepted sequence format in glycobioinformatics,
    encompassing the capabilities of the heterogeneous landscape of existing digital
    encoding schemata while providing the foundation for developing ontological relationships
    between glycan structural terms and supporting automated glycan structure analysis."'
  requires_registration: false
  url: https://github.com/glycoinfo/GlycoCT
- id: B2AI_STANDARD:144
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: GTrack genomic data format
  formal_specification: https://github.com/gtrack/gtrack
  is_open: true
  name: GTrack
  publication: doi:10.1186/1471-2105-12-494
  purpose_detail: GTrack is a tabular format that was developed as part of the Genomic
    HyperBrowser system to provide a uniform representation of most types of genomic
    datasets. GTrack is able to replace common formats such as WIG, GFF, BED, FASTA,
    in addition to represent chromatin capture datasets, such as Hi-C and ChIA-PET.
  requires_registration: false
  url: https://github.com/gtrack/gtrack
- id: B2AI_STANDARD:145
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:20
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: GUID and Life Sciences Identifiers Applicability Statements
  formal_specification: https://github.com/tdwg/guid-as
  is_open: true
  name: GUID-AS
  purpose_detail: GUIDs are Globally Unique Identifiers which should be referentially
    consistent and resolvable in order to support tests of uniqueness and the acquisition
    of associated metadata. Further, permanent and robust resolution services need
    to be available. The TDWG Globally Unique Identifiers Task Group (TDWG GUID),
    after meeting twice in 2006, recommended the use of the Life Sciences Identifiers
    (LSID) to uniquely identify shared data objects in the biodiversity domain.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:93
  url: http://www.tdwg.org/standards/150
- id: B2AI_STANDARD:146
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Guidelines for Information About Therapy Experiments
  is_open: true
  name: GIATE
  publication: doi:10.1186/1756-0500-5-10
  purpose_detail: The Guidelines for Information About Therapy Experiments (GIATE)
    is a minimum information checklist developed to establish a consistent framework
    for transparently reporting the purpose, methods, and results of therapeutic experiments,
    particularly in preclinical and translational research contexts. GIATE provides
    structured reporting requirements covering experimental design, intervention details
    (therapeutic agents, dosing, administration routes, timing), subject characteristics,
    outcome measures, statistical methods, and results presentation. The guidelines
    aim to improve reproducibility, enable meta-analyses, facilitate comparison across
    studies, and enhance the quality of preclinical therapeutic research by ensuring
    complete documentation of experimental parameters that affect interpretation and
    translatability. GIATE addresses the need for standardized reporting of therapy
    experiments to support evidence synthesis, reduce publication bias, and improve
    the rigor and transparency of preclinical studies that inform clinical trial design
    and therapeutic development pipelines.
  requires_registration: false
  url: https://doi.org/10.1186/1756-0500-5-10
- id: B2AI_STANDARD:147
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - policy
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Health Insurance Portability and Accountability Act of 1996
  formal_specification: https://aspe.hhs.gov/reports/health-insurance-portability-accountability-act-1996
  is_open: true
  name: HIPAA
  purpose_detail: The Health Insurance Portability and Accountability Act of 1996
    (HIPAA) is a federal law that required the creation of national standards to protect
    sensitive patient health information from being disclosed without the patient's
    consent or knowledge. The US Department of Health and Human Services (HHS) issued
    the HIPAA Privacy Rule to implement the requirements of HIPAA. The HIPAA Security
    Rule protects a subset of information covered by the Privacy Rule.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:39
  url: https://www.hhs.gov/hipaa/index.html
  used_in_bridge2ai: true
- id: B2AI_STANDARD:148
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - codesystem
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Healthcare Common Procedure Coding System
  is_open: true
  name: HCPCS
  purpose_detail: The Healthcare Common Procedure Coding System (HCPCS) is a standardized
    code set maintained by the Centers for Medicare & Medicaid Services (CMS) for
    reporting medical procedures, products, supplies, and services to Medicare, Medicaid,
    and private health insurers for claims processing and reimbursement. HCPCS consists
    of two levels - Level I comprises CPT (Current Procedural Terminology) codes maintained
    by the American Medical Association for physician services and procedures, while
    Level II contains alphanumeric codes (A through V series) for items and services
    not covered by CPT including durable medical equipment (DME), prosthetics, orthotics,
    supplies, ambulance services, drugs administered via methods other than oral, and
    temporary procedures. Each HCPCS code is accompanied by modifiers that provide
    additional information about the service or item (laterality, level of service,
    unusual circumstances). The code set is updated quarterly to reflect new technologies,
    procedures, and supplies, and includes detailed descriptions, Medicare coverage
    policies, and pricing information. HCPCS enables standardized billing, facilitates
    claims adjudication, supports healthcare cost analysis and utilization research,
    and provides the foundation for healthcare payment systems and revenue cycle management
    across payers.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:17
  url: https://www.cms.gov/Medicare/Coding/HCPCSReleaseCodeSets
  used_in_bridge2ai: true
- id: B2AI_STANDARD:149
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Herbarium Information Standards and Protocols for Interchange of Data
  is_open: true
  name: HISPID3
  purpose_detail: The 'Herbarium Information Standards and Protocols for Interchange
    of Data' (HISPID) is a standard format for the interchange of electronic herbarium
    specimen information. HISPID has been developed by a committee of representatives
    from all major Australian herbaria. This interchange standard was first published
    in 1989, with a revised version published in 1993./nHISPID3 (version 3) is an
    accession-based interchange standard. Although many fields refer to attributes
    of the taxon they should be construed as applying to the specimen represented
    by the record, not to the taxon per se. The interchange of taxonomic, nomenclatural,
    bibliographic, typification, rare and endangered plant conservation, and other
    related information is not dealt with in this standard, unless it specifically
    refers to a particular accession (record). While this standard is still in use,
    it is no longer actively maintained (labelled as prior on the TDWG website).
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:93
  url: https://www.tdwg.org/standards/hispid3/
- id: B2AI_STANDARD:150
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Histoimmunogenetics Markup Language
  formal_specification: https://github.com/nmdp-bioinformatics/hml
  is_open: true
  name: HML
  purpose_detail: Histoimmunogenetics Markup Language (HML) is intended as a potentially
    general-purpose XML format for exchanging genetic typing data. This format supports
    NGS based genotyping methods, raw sequence reads, registered methodologies, reference
    data, complete reporting of allele and genotype ambiguity and MIRING compliant
    reporting.
  requires_registration: false
  url: https://bioinformatics.bethematchclinical.org/hla-resources/hml/
- id: B2AI_STANDARD:151
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HL7 Arden Syntax
  formal_specification: https://www.hl7.org/login/index.cfm?next=/implement/standards/product_brief.cfm?product_id=2
  is_open: true
  name: Arden
  publication: doi:10.1016/j.jbi.2012.02.001
  purpose_detail: The Arden Syntax for Medical Logic Systems Version 2.10 is the latest
    version of a formalism for clinical knowledge representation that can be used
    by clinicians, knowledge engineers, administrators and others to implement clinical
    decision support (CDS) solutions to help improve the quality and safety of care.
    Arden Syntax can be used to create a knowledge base for CDS systems that, when
    coupled with patient data, can generate patient-specific CDS interventions for
    improving patient care. The key change in Version 2.10 over Version 2.9 is inclusion
    of a normative XML representation for Arden Syntax. This was done because the
    use of XML facilitates the development of tools such as syntax checkers and editors
    that can help increase the correctness of executable knowledge modules, and this
    in turn will foster the augmentation of the development and production environments
    for Arden, thereby increasing its utility.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:40
  url: http://www.hl7.org/implement/standards/product_brief.cfm?product_id=372
- id: B2AI_STANDARD:152
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HL7 Clinical Document Architecture Implementation Guide - Data Provenance
  is_open: true
  name: HL7 CDA Data Provenance
  purpose_detail: The HL7 Clinical Document Architecture (CDA) Data Provenance Implementation
    Guide provides standardized guidance and templates for capturing and exchanging
    comprehensive provenance information about clinical and care-related information
    within CDA documents. The guide addresses the fundamental provenance questions
    - who created the information (author, organization, device), when it was created
    (timestamps, versioning), where it was created (facility, location), how it was
    created (method, procedure, source system), and why it was created (purpose, context).
    The implementation guide defines structured elements and coded values for documenting
    data transformations, aggregations, derivations, and chains of custody that are
    critical for clinical decision-making, regulatory compliance, and legal accountability.
    Provenance metadata supports trust and transparency in health information exchange,
    enables auditability and traceability of clinical data across systems and care
    settings, facilitates detection of data quality issues, and provides essential
    context for interpreting clinical information in research, quality improvement,
    and AI/ML applications where understanding data lineage and reliability is paramount.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:40
  url: http://www.hl7.org/implement/standards/product_brief.cfm?product_id=420
- id: B2AI_STANDARD:153
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HL7 FHIR Resource DiagnosticReport
  formal_specification: http://hl7.org/fhir/diagnosticreport-definitions.html
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: DiagnosticReport
  purpose_detail: Resource for findings and interpretation of diagnostic tests performed
    on patients, groups of patients, devices, and locations, and/or specimens derived
    from these. The report includes clinical context such as requesting and provider
    information, and some mix of atomic results, images, textual and coded interpretations,
    and formatted representation of diagnostic reports.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: http://hl7.org/fhir/diagnosticreport.html
- id: B2AI_STANDARD:154
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:9
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HL7 FHIR Resource GenomicStudy
  formal_specification: https://build.fhir.org/genomicstudy-definitions.html
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: GenomicStudy
  purpose_detail: GenomicStudy resource aims at delineating relevant information of
    a genomic study. A genomic study might comprise one or more analyses, each serving
    a specific purpose. These analyses may vary in method (e.g., karyotyping, CNV,
    or SNV detection), performer, software, devices used, or regions targeted.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: https://build.fhir.org/genomicstudy.html
- id: B2AI_STANDARD:155
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  - B2AI_TOPIC:20
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HL7 FHIR Resource MolecularSequence
  formal_specification: http://hl7.org/implement/standards/fhir/molecularsequence-definitions.html
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: MolecularSequence
  purpose_detail: The FHIR MolecularSequence resource provides a standardized structure
    for representing raw and processed biological sequence data including DNA, RNA,
    and amino acid sequences within the FHIR ecosystem, enabling integration of genomic
    information with clinical data. The resource captures observed sequences, reference
    sequences, sequence variations (SNPs, indels, structural variants), quality scores,
    read coverage information, and repository links to external sequence databases
    (GenBank, EMBL, RefSeq). MolecularSequence supports representation of sequence
    coordinates using zero-based or one-based numbering systems, strand orientation,
    and relationships between sequences (such as transcript-to-protein mappings). The
    resource can reference associated specimens, patients, or other subjects, and links
    to variant calling results, expression data, and functional annotations. MolecularSequence
    enables clinical genomics workflows including variant interpretation, pharmacogenomics
    decision support, molecular diagnostics reporting, and precision medicine applications
    by providing a FHIR-native way to exchange sequence data alongside phenotypic,
    diagnostic, and treatment information in electronic health records and clinical
    information systems.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: http://hl7.org/implement/standards/fhir/molecularsequence.html
- id: B2AI_STANDARD:156
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HL7 FHIR Resource Observation
  formal_specification: http://hl7.org/implement/standards/fhir/observation-definitions.html
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: Observation
  purpose_detail: The FHIR Observation resource is a central, versatile structure
    for representing measurements, assessments, and assertions made about a patient,
    device, location, or other subject in healthcare contexts. Observations encompass
    a wide range of clinical and research data including vital signs (blood pressure,
    heart rate, temperature), laboratory results (chemistry, hematology, microbiology),
    imaging measurements, clinical assessments (pain scores, functional status), device
    readings (glucose monitors, pulse oximeters), social determinants of health, genomic
    findings, and quality metrics. The resource uses coded values from standard terminologies
    (LOINC, SNOMED CT) to identify what was measured, supports numeric values with
    units (UCUM), coded results, textual findings, and multimedia attachments. Observation
    includes metadata for effective time, status, performer, method, specimen reference,
    reference ranges, and interpretation flags. The resource supports panels and components
    for organizing related observations (complete blood count, metabolic panel), enables
    longitudinal tracking through sequences and trends, and links to supporting evidence
    and derivations. Observations are fundamental to AI/ML applications requiring structured
    clinical phenotypes, time-series analytics, predictive modeling, and integration
    of diverse data types for clinical decision support and population health analysis.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: http://hl7.org/implement/standards/fhir/observation.html
- id: B2AI_STANDARD:157
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:6
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HL7 FHIR Resource Patient
  formal_specification: http://hl7.org/implement/standards/fhir/patient-definitions.html
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: Patient
  purpose_detail: The FHIR Patient resource provides a standardized structure for
    representing demographics and administrative information about individuals or animals
    receiving care or health-related services. The resource captures core demographic
    data including name (with multiple name types and use contexts), birth date, gender,
    address (with structured components and period of validity), telecom contact information
    (phone, email, with use and priority indicators), and multiple identifiers (medical
    record numbers, social security numbers, insurance IDs) with system and type codes.
    Patient supports marital status, multiple languages with proficiency levels, photo
    attachments, contact persons and their relationships, communication preferences,
    general practitioners, managing organization, and links to related patients (merged
    records, see-also relationships). The resource includes deceased indicator, multiple
    birth information, and animal-specific extensions for species and breed. Patient
    serves as a central hub linking to all other clinical resources (observations,
    conditions, procedures, medications) and enables patient matching, record linkage,
    demographic analytics, cohort identification, and population health management.
    The standardized demographics are essential for AI/ML applications requiring patient
    stratification, bias detection, social determinants analysis, and fair representation
    across diverse populations in clinical prediction models and decision support systems.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: http://hl7.org/implement/standards/fhir/patient.html
- id: B2AI_STANDARD:158
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HL7 FHIR Resource Specimen
  formal_specification: http://hl7.org/fhir/specimen-definitions.html
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: Specimen
  purpose_detail: The FHIR Specimen resource provides a standardized structure for
    representing any material sample taken from a biological entity (living or dead)
    or from a physical object or the environment, essential for laboratory testing,
    biobanking, and clinical diagnostics. The resource captures comprehensive specimen
    metadata including type (blood, tissue, urine, etc.), subject/patient association,
    collection details (method, body site, quantity, date/time), identifiers (accession
    number, container ID), processing and handling procedures (centrifugation, fixation,
    storage), parent-child specimen relationships for aliquots and derivatives, storage
    conditions (temperature, humidity), and current status (available, unavailable,
    unsatisfactory, entered-in-error). The resource supports complex laboratory workflows
    by tracking specimen provenance through collection, transport, processing, and
    analysis stages, with fields for collection procedure, additive substances, container
    types, and special handling requirements. Specimen integrates with other FHIR
    resources including Patient, Practitioner, ServiceRequest, DiagnosticReport, and
    Observation to create complete clinical laboratory information workflows. The
    resource enables biobank specimen catalogs, clinical trial sample tracking, public
    health surveillance specimen management, and research biorepository operations.
    FHIR Specimen supports interoperability between laboratory information systems
    (LIS), electronic health records (EHR), biobank management systems, and research
    databases, facilitating standardized specimen data exchange across healthcare
    and research ecosystems while maintaining compliance with regulations for specimen
    handling, consent, and data privacy.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  url: http://hl7.org/fhir/specimen.html
- id: B2AI_STANDARD:159
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:6
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HL7 Gender Harmony Project
  is_open: true
  name: GHP
  publication: doi:10.1093/jamia/ocab196
  purpose_detail: Currently, it is common that a single data element is used to capture
    both sex and gender information, often assuming these two items are one unified
    idea. This specification challenges that notion and proposes that independent
    consideration of sex and gender, and the assessment of their differences promotes
    the health of women, men, and people of diverse gender identities of all ages,
    avoiding systematic errors that generate results with a low validity (if any)
    in clinical studies. The Gender Harmony model describes an approach that can improve
    data accuracy for sex and gender information in health care systems.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:40
  url: http://www.hl7.org/implement/standards/product_brief.cfm?product_id=564
- id: B2AI_STANDARD:160
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HUPO-PSI TraML format
  is_open: true
  name: TraML
  purpose_detail: The HUPO PSI Mass Spectrometry Standards Working Group (MSS WG)
    has developed a specification for a standardized format for the exchange and transmission
    of transition lists for selected reaction monitoring (SRM) experiments.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:41
  url: https://www.psidev.info/traml
- id: B2AI_STANDARD:161
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:18
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: IEEE 360-2022 IEEE Standard for Wearable Consumer Electronic Devices
  is_open: false
  name: IEEE P360
  purpose_detail: An overview, terminology, and categorization for Wearable Consumer
    Electronic Devices (Wearables). It further outlines an architecture for a series
    of standard specifications that define technical requirements and testing methods
    for different aspects of Wearables, from basic security and suitableness of wearing
    to various functional areas like health, fitness, and infotainment, etc.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:44
  url: https://standards.ieee.org/ieee/360/6244/
- id: B2AI_STANDARD:162
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: IHE Basic Patient Privacy Consents
  formal_specification: https://profiles.ihe.net/ITI/TF/Volume1/ch-19.html
  is_open: true
  name: BPPC
  purpose_detail: A mechanism to record the patient privacy consent(s) and a method
    for Content Consumers to use to enforce the privacy consent appropriate to the
    use.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:46
  url: https://profiles.ihe.net/ITI/TF/Volume1/ch-19.html
- id: B2AI_STANDARD:163
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: IHE Clinical Decision Support Order Appropriateness Tracking
  is_open: true
  name: IHE CDS-OAT
  purpose_detail: Profile for Clinical Decision Support and Appropriate Use Criteria
    (AUC) information as received from the CDS Mechanism 145 (CDSM) on the order and
    charge transaction to the revenue cycle application that is responsible to create
    a claim.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:46
  url: https://www.ihe.net/uploadedFiles/Documents/Radiology/IHE_Rad_Suppl_CDS-OAT.pdf
- id: B2AI_STANDARD:164
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: IHE Clinical Research Document standard
  formal_specification: https://www.ihe.net/uploadedFiles/Documents/QRPH/IHE_QRPH_Suppl_CRD.pdf
  is_open: true
  name: CRD
  purpose_detail: The Clinical Research Document Profile (CRD) specifies a standard
    way to generate a clinical research document from EHR data provided in the CDA
    standard.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:46
  url: https://www.ihe.net/uploadedFiles/Documents/QRPH/IHE_QRPH_Suppl_CRD.pdf
- id: B2AI_STANDARD:165
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: IHE Cross-Community Patient Discovery Profile
  is_open: true
  name: IHE XCPD
  purpose_detail: Standardized means to locate communities that hold patient relevant
    health data and the translation of patient identifiers across communities holding
    the same patients data.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:46
  url: https://profiles.ihe.net/ITI/TF/Volume1/ch-27.html
- id: B2AI_STANDARD:166
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: IHE IT Infrastructure Technical Framework
  formal_specification: https://profiles.ihe.net/ITI/TF/index.html
  is_open: true
  name: IHE ITI
  purpose_detail: Technical profiles and definitions for IT use cases, transactions,
    content, and metadata.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:46
  url: https://profiles.ihe.net/ITI/index.html
- id: B2AI_STANDARD:167
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: IHE Laboratory Analytical Workflow
  formal_specification: https://www.ihe.net/uploadedFiles/Documents/PaLM/IHE_PaLM_TF_Vol1.pdf
  is_open: true
  name: LAW
  purpose_detail: Profile to support the analytical workflow between analyzers of
    the clinical laboratory and the IT systems managing their work
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:46
  url: https://www.ihe.net/resources/technical_frameworks/#PaLM
- id: B2AI_STANDARD:168
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: IHE Patient Care Device Profiles
  is_open: true
  name: IHE PCD
  purpose_detail: IHE Patient Care Device (PCD) Profiles are standards-based interoperability
    specifications developed by Integrating the Healthcare Enterprise (IHE) to address
    medical device communication challenges in clinical settings. The profiles follow
    IHE's technical framework development process including proposal, supplement development,
    public comment, trial implementation at Connectathon events, and finalization.
    Key PCD profiles include ACM (Alert Communication Management) for standardized
    alarm handling, DEC (Device Enterprise Communication) for device-to-enterprise
    integration, IDCO (Implantable Device Cardiac Observations) for cardiac device
    data, IPEC (Infusion Pump Event Communication) for infusion pump safety, PIV (Point
    of Care Infusion Verification) for medication verification, PCIM (Point of Care
    Identity Management) for device authentication, RDQ (Retrospective Data Query)
    for historical data retrieval, RTM (Rosetta Terminology Mapping) for standardized
    device terminology, and WCM (Waveform Communication Module) for physiological
    waveform data. These profiles leverage existing standards like HL7, DICOM, and
    IEEE 11073 to enable vendor-neutral device interoperability, supporting critical
    use cases such as automated vital signs documentation in electronic health records,
    integrated alarm management across monitoring systems, and safe medication administration
    workflows.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:46
  url: https://wiki.ihe.net/index.php?title=PCD_Profiles
- id: B2AI_STANDARD:169
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: IHE Patient Demographics Query Integration Profile
  is_open: true
  name: IHE PDQ
  purpose_detail: Standardized ways for multiple distributed applications to query
    a patient information server for a list of patients, based on user-defined search
    criteria, and retrieve a patients demographic (and, optionally, visit or visit-related)
    information directly into the application.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:46
  url: https://profiles.ihe.net/ITI/TF/Volume1/ch-8.html
- id: B2AI_STANDARD:170
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: IHE Retrieve Form for Data Capture
  is_open: true
  name: RFD
  purpose_detail: The RFD Profile provides a generic polling mechanism to allow an
    external agency to indicate issues with data that have been captured and enable
    the healthcare provider to correct the data. The profile does not dictate the
    mechanism employed or content required to achieve such corrections.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:46
  url: https://profiles.ihe.net/ITI/TF/Volume1/ch-17.html
- id: B2AI_STANDARD:171
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: IICC Digital Format for Publication of LOINC to Vendor IVD Test Results
  has_relevant_organization:
  - B2AI_ORG:53
  has_training_resource:
  - B2AI_STANDARD:848
  is_open: true
  name: LIVD
  purpose_detail: Industry-defined format to facilitate the publication and exchange
    of LOINC codes for vendor IVD test results.
  requires_registration: false
  url: https://ivdconnectivity.org/livd/
- id: B2AI_STANDARD:172
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:19
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Image Cytometry Standard
  is_open: true
  name: ICS
  publication: doi:10.1002/cyto.990110502
  purpose_detail: The Image Cytometry Standard (ICS) is a digital multidimensional
    image file format used in life sciences microscopy. It stores not only the image
    data, but also the microscopic parameters describing the optics during the acquisition.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/Image_Cytometry_Standard
- id: B2AI_STANDARD:173
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:15
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: imzML format
  has_relevant_organization:
  - B2AI_ORG:20
  is_open: true
  name: imzML
  publication: doi:10.1016/j.jprot.2012.07.026
  purpose_detail: The purpose of imzML is to facilitate the exchange and processing
    of mass spectrometry imaging data. This website is intended to provide all information
    neccesary to implement imzML.imzML was developed in the framework of the EU funded
    project COMPUTIS. The main goals during the development were complete description
    of MS imaging experiments and efficient storage of (very large) data sets. imzML
    is it not limited to MS imaging, but is also useful for other MS applications
    generating large data sets such as LC-FTMS. The current version is mzML 1.1.0.
    The metadata part of imzML is based on the mzML format by HUPO-PSI
  requires_registration: false
  url: https://ms-imaging.org/imzml/
- id: B2AI_STANDARD:174
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - codesystem
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: International Classification of Diseases 10th Revision Clinical Modification
  is_open: true
  name: ICD-10-CM
  purpose_detail: "ICD-10-CM (International Classification of Diseases, 10th Revision,
    Clinical Modification) is the United States' clinical modification of the WHO's
    ICD-10 classification system, mandated for reporting diagnoses and health conditions
    in all healthcare settings for billing, epidemiology, quality measurement, and
    public health surveillance since October 2015. Maintained by the National Center
    for Health Statistics (NCHS) in collaboration with CMS, ICD-10-CM provides over
    70,000 diagnosis codes with significantly expanded granularity compared to ICD-9-CM,
    enabling more precise documentation of disease severity, anatomical location,
    episode of care (initial encounter, subsequent encounter, sequela), and laterality
    (left, right, bilateral). The coding structure uses alphanumeric format with 3-7
    characters where the first character is alphabetic, second is numeric, third through
    seventh provide increasing specificity of body site, etiology, severity, and clinical
    details, with optional seventh characters as extensions for encounter context
    or injury staging. Major chapters include infectious diseases (A00-B99), neoplasms
    (C00-D49), endocrine and metabolic disorders (E00-E89), mental and behavioral
    disorders (F01-F99), nervous system (G00-G99), circulatory (I00-I99), respiratory
    (J00-J99), digestive (K00-K95), musculoskeletal (M00-M99), and external causes
    of morbidity (V00-Y99). Critical features include combination codes that capture
    multiple conditions or manifestations in single codes, placeholder 'x' characters
    to allow for future expansion, and seventh character extensions for obstetric
    outcomes, fracture healing status, and diabetes complications. ICD-10-CM drives
    healthcare reimbursement through diagnosis-related groups (DRGs), enables clinical
    decision support systems through structured diagnosis data, supports population
    health analytics identifying disease trends and risk factors, powers epidemiological
    surveillance for CDC reportable conditions tracking disease outbreaks, and provides
    standardized terminology for electronic health record (EHR) systems ensuring interoperability.
    Annual updates released October 1st incorporate new diseases, refined definitions,
    and code revisions reflecting medical advances, essential for medical billing
    specialists, clinical coders, health informaticians, public health researchers,
    and healthcare quality officers."
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:14
  url: https://www.cdc.gov/nchs/icd/icd-10-cm.htm
  used_in_bridge2ai: true
- id: B2AI_STANDARD:175
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - codesystem
  - standards_process_maturity_final
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: International Classification of Diseases 11th Revision
  is_open: true
  name: ICD-11
  purpose_detail: "ICD-11 (International Classification of Diseases, 11th Revision)
    is the World Health Organization's latest global standard for diagnostic health
    information, officially adopted by the World Health Assembly in 2019 and effective
    from January 2022, representing the most comprehensive revision in over three
    decades with fully digital-native architecture designed for electronic health
    systems and multilingual implementation. Developed through extensive international
    collaboration involving thousands of health professionals across 90 countries,
    ICD-11 contains over 17,000 diagnostic categories with significantly enhanced
    granularity, clinical detail, and scientific accuracy compared to ICD-10, incorporating
    advances in medical knowledge including genomic medicine, patient safety, traditional
    medicine integration, and sexual health classifications. The classification uses
    a multi-hierarchical structure with foundation component enabling flexible content
    management, allowing multiple parent categories and post-coordination where complex
    conditions are composed by combining codes for anatomy, etiology, severity, temporality,
    and other clinical dimensions. Major structural improvements include entirely
    new chapters for traditional medicine (validated practices from Chinese, Ayurvedic,
    and other systems), extension codes for functional assessment using WHO Disability
    Assessment Schedule 2.0 (WHODAS), detailed antimicrobial resistance documentation,
    gaming disorder and other emerging conditions, and significantly expanded mental
    health classifications with dimensional assessments. ICD-11 features built-in
    multilingual support covering Arabic, Chinese, English, French, Russian, Spanish
    with machine translation capabilities, linearizations tailored for different use
    cases (mortality reporting, morbidity statistics, primary care, clinical documentation),
    and modern URI-based coding enabling semantic web integration and FHIR compatibility.
    Critical technical features include embedded logical definitions using description
    logic enabling automated classification and consistency checking, standardized
    application programming interfaces (APIs) for EHR integration, and continuous
    online updating mechanism replacing decennial revision cycles with regular incremental
    updates. ICD-11 supports global health surveillance enabling real-time disease
    outbreak tracking, international health statistics comparability across countries
    and regions, research data standardization for epidemiological studies and clinical
    trials, health service planning through accurate disease burden measurement, and
    AI/machine learning applications through structured, semantically rich diagnostic
    data. Adoption varies globally with some countries implementing immediately while
    others transition gradually, essential for medical informaticians, public health
    authorities, clinical coders, WHO collaborating centers, and healthcare system
    planners."
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:100
  url: https://icd.who.int/en
- id: B2AI_STANDARD:176
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - codesystem
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: International Classification of Diseases 9th Revision Clinical Modification
  is_open: true
  name: ICD-9-CM
  purpose_detail: "ICD-9-CM (International Classification of Diseases, 9th Revision,
    Clinical Modification) was the official medical classification system used in
    the United States from 1979 until September 2015 for coding diagnoses and procedures
    in healthcare billing, epidemiological research, and health statistics, representing
    the U.S. clinical modification of WHO's ICD-9 with additional detail for morbidity
    classification. Developed by the National Center for Health Statistics (NCHS)
    and used by CMS for Medicare/Medicaid reimbursement, ICD-9-CM contained approximately
    13,000 diagnosis codes and 3,000 procedure codes, structured with 3-5 digit numeric
    format where first three digits represent disease category and subsequent digits
    add specificity for anatomical location, etiology, or manifestations. Major diagnostic
    chapters included infectious and parasitic diseases (001-139), neoplasms (140-239),
    endocrine and metabolic diseases (240-279), mental disorders (290-319), nervous
    system (320-389), circulatory system (390-459), respiratory system (460-519),
    digestive system (520-579), genitourinary system (580-629), and external causes
    (E codes E800-E999) documenting injury circumstances. The procedure classification
    (Volume 3) used 2-4 digit numeric codes organized by anatomical system, covering
    surgical operations, diagnostic procedures, and therapeutic interventions primarily
    for inpatient hospital settings. Despite decades of widespread adoption creating
    massive legacy datasets and establishing institutional coding workflows, ICD-9-CM
    became outdated due to limited specificity insufficient for modern medicine's
    diagnostic precision, exhausted code capacity with no room for new diseases or
    procedures, lack of laterality designation, and outdated medical terminology inconsistent
    with current clinical practice. The mandated transition to ICD-10-CM/PCS in October
    2015 required extensive healthcare industry preparation including EHR system updates,
    coder retraining, and billing system modifications, yet ICD-9-CM remains critically
    important for historical medical records analysis, longitudinal epidemiological
    studies spanning pre-2015 data, retrospective cohort studies, trend analysis requiring
    crosswalk mappings between ICD-9 and ICD-10, and legacy system maintenance. Applications
    include historical disease surveillance tracking public health trends, health
    services research analyzing treatment patterns, medical informatics developing
    code translation algorithms, and archival medical data management, essential for
    health informaticians managing legacy data, epidemiologists conducting temporal
    studies, medical archivists, and researchers working with pre-2015 healthcare
    datasets."
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:14
  url: https://www.cdc.gov/nchs/icd/icd9cm.htm
  used_in_bridge2ai: true
- id: B2AI_STANDARD:177
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: INTRPRT guidelines for transparent machine learning for medical image
    analysis
  is_open: true
  name: INTRPRT
  publication: doi:10.1038/s41746-022-00699-2
  purpose_detail: 'INTRPRT (INTerpretability, Reproducibility, Prediction, Trustworthiness)
    is a set of evidence-based design guidelines for developing transparent, trustworthy
    machine learning systems in medical image analysis, addressing critical barriers
    to clinical adoption of AI diagnostic tools. Published through a systematic literature
    review and expert consensus process, INTRPRT provides actionable recommendations
    across four core transparency dimensions: Interpretability (explaining model decisions
    through techniques like saliency maps, attention mechanisms, feature importance,
    and counterfactual explanations that clinicians can understand and validate against
    domain knowledge), Reproducibility (ensuring consistent results through standardized
    data preprocessing, version-controlled code, containerized environments, transparent
    reporting of hyperparameters, cross-validation strategies, and external validation
    on diverse patient cohorts), Prediction quality (reporting comprehensive performance
    metrics including sensitivity, specificity, AUC, confidence intervals, subgroup
    analyses stratified by demographics/disease severity, failure mode characterization,
    and uncertainty quantification), and Trustworthiness (building clinical confidence
    through user-centered design, formative research with radiologists and clinicians
    to understand workflow integration needs, prospective clinical validation studies,
    regulatory compliance with FDA/CE marking requirements, bias auditing across patient
    populations, and ongoing post-deployment monitoring). INTRPRT emphasizes human-centered
    design as the foundational first step, recommending formative user research (interviews,
    contextual inquiry, usability testing with radiologists in clinical settings)
    to elicit user needs, identify pain points in existing workflows, understand domain-specific
    requirements for explanation granularity and visualization preferences, and co-design
    interfaces that present AI predictions alongside interpretability artifacts (highlighted
    regions of interest, similar reference cases, diagnostic reasoning chains) in
    formats aligned with radiological training and cognitive processes. The guidelines
    address the "black box" problem in deep learning medical imaging by providing
    concrete implementation strategies: for interpretability, recommend layer-wise
    relevance propagation, GradCAM heatmaps, or concept-based explanations validated
    against radiologist annotations; for reproducibility, specify use of public benchmark
    datasets (MICCAI challenges, Medical Decathlon), standardized evaluation protocols,
    and open-source model repositories; for prediction, require reporting of calibration
    curves, decision thresholds optimized for clinical utility, and comparison to
    inter-reader agreement; for trustworthiness, mandate transparent disclosure of
    training data characteristics (demographic composition, imaging protocols, label
    sources), model limitations, and intended use statements. INTRPRT has influenced
    AI reporting standards (CONSORT-AI, SPIRIT-AI extensions), regulatory guidance
    (FDA''s "Proposed Regulatory Framework for Modifications to AI/ML-Based Software
    as a Medical Device"), journal submission requirements for AI research (Nature
    Medicine, Radiology: Artificial Intelligence), and institutional review processes
    for clinical AI deployment, providing a practical framework that bridges the gap
    between algorithmic innovation and responsible, effective integration of machine
    learning into diagnostic radiology, pathology, dermatology, and other image-intensive
    medical specialties.'
  requires_registration: false
  url: https://doi.org/10.1038/s41746-022-00699-2
- id: B2AI_STANDARD:178
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: IPSM Alignment Format
  formal_specification: https://github.com/INTER-IoT/ipsm-alignments
  is_open: true
  name: IPSM-AF
  purpose_detail: Alignment can be interpreted as a set of uni-directional mappings
    for transforming input RDF graph into output RDF graph. It is persisted in IPSM
    Alignment Format (IPSM-AF) that is based on a well known Alignment API Format.
  requires_registration: false
  url: https://inter-iot.readthedocs.io/projects/ipsm/en/latest/Configuration/Alignment-format/IPSM-alignment-format/
- id: B2AI_STANDARD:179
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ISA-Tab-Nano format
  has_relevant_organization:
  - B2AI_ORG:47
  is_open: true
  name: ISA-TAB-Nano
  publication: doi:10.1186/1472-6750-13-2
  purpose_detail: ISA-TAB-Nano specifies the format for representing and sharing information
    about nanomaterials, small molecules and biological specimens along with their
    assay characterization data (including metadata, and summary data) using spreadsheet
    or TAB-delimited files.
  requires_registration: false
  url: https://wiki.nci.nih.gov/display/icr/isa-tab-nano
- id: B2AI_STANDARD:180
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ISO 13606 standard for Electronic Health Record Communication
  formal_specification: https://www.iso.org/standard/67868.html
  has_relevant_organization:
  - B2AI_ORG:49
  is_open: false
  name: EN13606
  purpose_detail: A means for communicating part or all of the electronic health record
    (EHR) of one or more identified subjects of care between EHR systems, or between
    EHR systems and a centralised EHR data repository. It can also be used for EHR
    communication between an EHR system or repository and clinical applications or
    middleware components (such as decision support components), or personal health
    applications and devices, that need to access or provide EHR data, or as the representation
    of EHR data within a distributed (federated) record system.
  requires_registration: true
  url: http://www.en13606.org/
- id: B2AI_STANDARD:181
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ITU H.810, H.811, H.812, H.812.5, and H.813
  is_open: true
  name: ITU-T E-health
  purpose_detail: Standards for medical device communication.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:50
  url: https://www.itu.int/en/ITU-T/studygroups/2013-2016/16/Pages/rm/ehealth.aspx
- id: B2AI_STANDARD:182
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:3
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Joint Committee on Atomic and Molecular Physical Data standard
  is_open: true
  name: JCAMP-DX
  purpose_detail: The JCAMP-DX was one of the earliest specifications providing a
    standard file format for data exchange in mass spectrometry. It was initially
    developed for infrared spectrometry and related chemical and physical information
    between spectrometer data systems of different manufacture. It was also used later
    for nuclear magnetic resonance spectroscopy. JCAMP-DX is an ASCII based format
    and therefore not very compact even though it includes standards for file compression.
    All data are stored as labeled fields of variable length using printable ASCII
    characters. JCAMP-DX was officially released in 1988. JCAMP-DX was found impractical
    for today's large MS data sets, but it is still used for exchanging moderate numbers
    of spectra. IUPAC is currently in charge of its maintenance and the latest protocol
    is from 2005.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:51
  url: https://iupac.org/what-we-do/digital-standards/jcamp-dx/
- id: B2AI_STANDARD:183
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: K-mer countgraph
  is_open: true
  name: Countgraph
  publication: doi:10.12688/f1000research.6924.1
  purpose_detail: A format used by the khmer tool to represent k-mer counts and their
    occurences.
  related_to:
  - B2AI_STANDARD:782
  requires_registration: false
  url: https://khmer.readthedocs.io/en/v2.0/dev/binary-file-formats.html#countgraph
- id: B2AI_STANDARD:184
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - diagnosticinstrument
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Karnofsky Performance Scale
  is_open: true
  name: KPS
  publication: doi:10.1200/JCO.1984.2.3.187
  purpose_detail: A standard way of measuring the ability of cancer patients to perform
    ordinary tasks. The Karnofsky Performance Status scores range from 0 to 100. A
    higher score means the patient is better able to carry out daily activities. Karnofsky
    Performance Status may be used to determine a patient's prognosis, to measure
    changes in a patients ability to function, or to decide if a patient could be
    included in a clinical trial. Also called KPS.
  requires_registration: false
  url: http://www.npcrc.org/files/news/karnofsky_performance_scale.pdf
- id: B2AI_STANDARD:185
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: KEGG Markup Language
  is_open: true
  name: KGML
  purpose_detail: The KEGG Markup Language (KGML) is an exchange format of the KEGG
    pathway maps, which is converted from internally used KGML+ (KGML+SVG) format.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:52
  url: https://www.kegg.jp/kegg/xml/
- id: B2AI_STANDARD:186
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - deprecated
  concerns_data_topic:
  - B2AI_TOPIC:20
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Life sciences domain analysis model
  is_open: true
  name: LS-DAM
  publication: doi:10.1136/amiajnl-2011-000763
  purpose_detail: The LS DAM v2.2.1 is comprised of 130 classes and covers several
    core areas including Experiment, Molecular Biology, Molecular Databases and Specimen.
    Nearly half of these classes originate from the BRIDG model, emphasizing the semantic
    harmonization between these models. Validation of the LS DAM against independently
    derived information models, research scenarios and reference databases supports
    its general applicability to represent life sciences research.
  requires_registration: false
  url: https://doi.org/10.1136/amiajnl-2011-000763
- id: B2AI_STANDARD:187
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - codesystem
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Logical Observation Identifier Names and Codes
  has_relevant_organization:
  - B2AI_ORG:114
  - B2AI_ORG:115
  has_training_resource:
  - B2AI_STANDARD:848
  is_open: true
  name: LOINC
  purpose_detail: Tests, observations, diagnostics, and other clinical procedures.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:53
  url: loinc.org
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:37
    category: B2AI:Application
    name: Automated ML-Based Mapping of Free-Text Laboratory Codes to LOINC
    description: Machine learning classifiers including Random Forest, logistic regression,
      and ensemble methods automatically map free-text laboratory descriptors (test
      names, units, panels) from local EHR systems to standardized LOINC codes. Novel
      encoding methods vectorize heterogeneous lab terminology, and trained models
      achieve high accuracy (Random Forest approximately 94.5% top-1 accuracy; ensemble
      approaches up to 99% accuracy) on nationwide oncology EHR data from approximately
      280 US clinics. This automation dramatically reduces manual terminologist effort
      (which typically requires 6-8 hours per 1000 terms) and enables research-ready
      datasets by harmonizing laboratory data across institutions for downstream ML
      applications.
    used_in_bridge2ai: false
    references:
    - https://pmc.ncbi.nlm.nih.gov/articles/PMC8861721/
  - id: B2AI_APP:136
    category: B2AI:Application
    name: LLM-Based LOINC Standardization Using Pre-Trained T5 Embeddings
    description: Pre-trained T5 large language models with contrastive learning and
      few-shot fine-tuning retrieve and suggest top-k LOINC code candidates from local
      laboratory descriptions, supporting human-in-the-loop curation workflows. The
      approach uses contextual embeddings and data augmentation to handle acronyms,
      synonyms, misspellings, and missing metadata across LOINC's six dimensions (component,
      property, time, system, scale, method), generalizing to unseen LOINC targets
      without retraining. Training and evaluation use source-target pairs from MIMIC-III
      EHR data, demonstrating improved top-k retrieval performance for scalable cross-site
      laboratory harmonization given LOINC's large code space (over 80,000 codes).
    used_in_bridge2ai: false
    references:
    - https://proceedings.mlr.press/v193/tu22a/tu22a.pdf
  - id: B2AI_APP:137
    category: B2AI:Application
    name: Value-Based Laboratory Code Mapping to LOINC Using Result Distributions
    description: Statistical feature engineering on laboratory test result distributions
      (mean, median, quartiles, variance, skewness) combined with unit normalization
      enables AI mapping of in-house codes to LOINC via intermediate standards (JLAC10)
      without relying on test-name NLP. Applied to the J-DREAMS diabetes database
      (955,011 entries across 15 facilities, 51 analytes), classifiers achieved at
      least 70% mapping accuracy for 80.4% of analytes. This value-centric approach
      is particularly useful where NLP resources or medical corpora are limited, demonstrating
      that unit and value harmonization tied to LOINC groupings is critical for accurate
      automated mapping and international data sharing.
    used_in_bridge2ai: false
    references:
    - https://pmc.ncbi.nlm.nih.gov/articles/PMC12150744/
  - id: B2AI_APP:138
    category: B2AI:Application
    name: LOINC-Coded Laboratory Features for Improved Multi-Site Predictive Modeling
    description: Using LOINC-standardized laboratory features in predictive models
      significantly improves performance and cross-site transferability compared to
      unmapped local codes. In a 13-hospital UPMC heart failure cohort (2008-2012)
      predicting 30-day readmission, models trained with manually LOINC-mapped lab
      features consistently achieved significantly higher AUCs despite modest overall
      performance. LOINC aggregation and grouping procedures materially affect model
      reproducibility and external validation across institutions, demonstrating that
      explicit LOINC standardization should be reported as a critical data preprocessing
      step in multi-site ML studies to ensure interpretability and portability.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1093/jamiaopen/ooy063
  - id: B2AI_APP:139
    category: B2AI:Application
    name: LOINC-to-HPO Semantic Integration for Deep Phenotyping and Biomarker Discovery
    description: A widely adopted pipeline maps LOINC-coded laboratory test results
      transmitted via FHIR to Human Phenotype Ontology (HPO) terms, enabling computational
      deep phenotyping and biomarker discovery. The system manually biocurated mappings
      for 2,923 commonly used LOINC tests, handling numeric, ordinal, and nominal
      results using standardized FHIR interpretation codes, and leveraging HPO's hierarchical
      structure to aggregate heterogeneous tests with comparable interpretations.
      Validated on 15,681 UNC EHR patients with respiratory complaints and identifying
      known asthma biomarkers, the approach supports association studies, cohort analysis,
      and was released as a SMART on FHIR application for EHR integration and downstream
      ML applications requiring ontology-based phenotype embeddings.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1038/s41746-019-0110-4
  - id: B2AI_APP:140
    category: B2AI:Application
    name: N3C Unit and Value Harmonization Using LOINC Concept Grouping for ML-Ready
      Datasets
    description: The National COVID Cohort Collaborative (N3C) unit harmonization
      pipeline groups laboratory measurements by LOINC concepts, selects canonical
      units with UCUM conventions and conversion formulas, and infers missing units
      using Kolmogorov-Smirnov distributional comparisons across pooled multi-site
      data. Applied to billions of OMOP-mapped EHR lab records, the pipeline harmonized
      88.1% of values and imputed units for 78.2% of records missing units (41% of
      contributors' records), reclaiming large data fractions otherwise unavailable
      for analysis. This LOINC-based harmonization directly enables creation of ML-ready
      datasets for predictive modeling, phenotyping, and analytics across heterogeneous
      EHR sources.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1093/jamia/ocac054
  - id: B2AI_APP:141
    category: B2AI:Application
    name: High-Throughput LOINC Document Ontology Mapping Using Metadata-Based Methods
    description: A scalable pipeline maps clinical document metadata to LOINC Document
      Ontology (LOINC DO) codes at large scale using bag-of-words and vector distance
      methods applied to structured EHR fields (event titles, tags, encounter types)
      rather than full-text NLP. Applied to University of Missouri Cerner database
      (over 130 million decompressed notes), the metadata-driven approach achieved
      73.4% document coverage and mapped 132 million notes in under 2 hours, claimed
      to be an order of magnitude more efficient than NLP-based methods. This LOINC
      DO standardization supports downstream computable phenotyping, documentation
      quality assessment, and potential ML applications requiring standardized clinical
      document classification.
    used_in_bridge2ai: false
    references:
    - https://pmc.ncbi.nlm.nih.gov/articles/PMC10785913/pdf/1116.pdf
- id: B2AI_STANDARD:188
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:27
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Macromolecular Crystallographic Information File
  is_open: true
  name: mmCIF
  purpose_detail: PDBx/mmCIF became the standard PDB archive format in 2014. All PDB
    data processing and annotation will be performed using PDBx/mmCIF at all wwPDB
    sites. PDBx/mmCIF consists of categories of information represented as tables
    and keyword value pairs.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:82
  url: https://mmcif.wwpdb.org/
- id: B2AI_STANDARD:189
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:23
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Markup Components for Describing Multiple Sequence Alignments
  is_open: true
  name: MSAML
  purpose_detail: 'MSAML (Multiple Sequence Alignment Markup Language) is an XML-based
    standard designed to provide a structured, machine-readable representation of
    multiple sequence alignments (MSAs) that facilitates manipulation, extraction,
    and computational analysis of alignment information through logically defined
    components. MSAML encodes essential alignment elements including sequence identifiers
    and metadata (organism, gene name, accession numbers, database cross-references),
    aligned sequence strings with explicit gap characters and position indexing, alignment
    coordinates mapping aligned positions to original sequence coordinates, conservation
    scores and quality metrics (percent identity, consensus sequences, position-specific
    scoring matrices), secondary structure annotations, and phylogenetic tree relationships
    among sequences. By structuring MSAs as hierarchical XML documents with well-defined
    schemas, MSAML enables validation, transformation via XSLT stylesheets, querying
    with XPath/XQuery, and integration with XML-native databases and web services,
    overcoming limitations of legacy flat-file formats (FASTA, Clustal, Stockholm,
    Phylip) that lack standardized parsers and semantic markup. The XML conformance
    supports interoperability between alignment tools (Clustal Omega, MUSCLE, MAFFT,
    T-Coffee), visualization software, and downstream analysis pipelines, allowing
    automated extraction of alignment regions, sub-alignment generation for domain
    analysis, and integration with ontologies (Gene Ontology, Sequence Ontology) for
    functional annotation. For AI and machine learning applications in computational
    biology, MSAML-encoded alignments provide structured training data for protein
    structure prediction models (AlphaFold, RoseTTAFold, ESMFold consume MSA features
    as evolutionary coupling signals), enable feature engineering where conservation
    patterns, co-evolution scores, and gap distributions extracted from MSAML serve
    as inputs for supervised learning of protein function, binding site prediction,
    and mutation effect estimation, support transfer learning by facilitating alignment
    dataset curation and augmentation (sampling homologous sequences at varying identity
    thresholds, generating synthetic alignments), and enable meta-analysis across
    MSA databases (Pfam, SMART, InterPro) where MSAML standardization allows aggregation
    of alignment quality metrics, consensus annotation, and phylogenetic signal for
    training foundation models on protein families, ultimately improving reproducibility
    and automation in sequence-based AI workflows for functional genomics, evolutionary
    biology, and therapeutic target identification.'
  requires_registration: false
  url: http://xml.coverpages.org/msaml-desc-dec.html
- id: B2AI_STANDARD:190
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:3
  - B2AI_TOPIC:27
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MDL molfile Format
  is_open: true
  name: MDL
  purpose_detail: 'The MDL Molfile format (also known as MOL format or V2000/V3000
    format) is the de facto standard file format for representing chemical structures
    in computational chemistry, developed by MDL Information Systems (now BIOVIA)
    and widely adopted across cheminformatics tools, chemical databases, and drug
    discovery platforms. A Molfile encodes a molecule as a connection table specifying
    atom types, 2D or 3D coordinates, bond types (single, double, triple, aromatic),
    stereochemistry (wedge/dash bonds for chiral centers, cis/trans double bonds),
    formal charges, isotopes, and molecular properties (header block with compound
    name, program identifiers, comments; counts line with atom/bond tallies; atom
    block with element symbols and Cartesian coordinates; bond block with connectivity
    and bond orders; properties block with M CHG, M ISO, M RAD tags). The format exists
    in two versions: V2000 (limited to 999 atoms/bonds, widely compatible) and V3000
    (extended format supporting large molecules, enhanced stereochemistry, polymers,
    Markush structures used in patent databases). Molfiles are plain-text ASCII, enabling
    human inspection, version control, and parsing by cheminformatics libraries (RDKit,
    Open Babel, CDK, OEChem, ChemmineR) that convert MOL data to in-memory molecular
    objects for computational manipulation, similarity searching, substructure matching,
    and property prediction. The SDF (Structure-Data File) format extends Molfile
    by concatenating multiple MOL records with associated data fields (biological
    activity, physicochemical properties, vendor information), forming the basis for
    PubChem, ChEMBL, ZINC, and DrugBank downloads. For AI and machine learning in
    drug discovery, Molfile-encoded structures serve as primary inputs for featurization
    pipelines: molecular fingerprints (ECFP/Morgan, MACCS keys, topological torsions)
    are computed from connection tables for QSAR modeling and virtual screening; graph
    neural networks (GNNs) directly consume atom and bond tables as node/edge features
    for property prediction (solubility, toxicity, binding affinity, blood-brain barrier
    permeability); 3D conformations extracted from Molfiles train geometric deep learning
    models for structure-based drug design; and generative models (variational autoencoders,
    GANs, diffusion models) output Molfile-formatted structures for de novo molecular
    generation optimizing multi-objective criteria (potency, selectivity, ADMET properties,
    synthetic accessibility). Large-scale Molfile datasets (millions of compounds
    from commercial libraries, patent filings, bioassay repositories) enable transfer
    learning and foundation models (e.g., MolFormer, ChemBERTa pre-trained on SMILES
    derived from Molfiles) that generalize across chemical space, while standardized
    structure representation facilitates benchmarking of ML architectures on reproducible
    test sets, data augmentation through conformational sampling or stereoisomer enumeration,
    and federated learning across pharmaceutical organizations where Molfile compatibility
    ensures interoperability without revealing proprietary compound identities, ultimately
    accelerating hit identification, lead optimization, and AI-driven exploration of
    synthetically accessible chemical matter for therapeutic applications.'
  requires_registration: false
  url: https://en.wikipedia.org/wiki/Chemical_table_file
- id: B2AI_STANDARD:191
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:3
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MDL reaction Format
  is_open: true
  name: RXN
  purpose_detail: 'The MDL RXN (reaction) format is a widely adopted file format for
    representing chemical reactions with complete structural information about reactants,
    products, and reagents, developed by MDL Information Systems (now part of BIOVIA)
    as an extension of the MDL Molfile (MOL/SDF) format. An RXN file encodes a reaction
    as a collection of connection tables specifying atom coordinates, bond types,
    stereochemistry, and molecular properties for each component, organized into reactant
    and product blocks separated by delimiter lines ($RXN header, reaction counts,
    individual MOL blocks for each molecule). The format supports stoichiometric coefficients,
    reaction conditions metadata (temperature, pressure, solvents, catalysts encoded
    as comments or extended properties), atom mapping numbers that trace atom transformations
    from reactants to products (critical for reaction mechanism elucidation and retrosynthetic
    analysis), and hierarchical reaction schemes with multiple steps. RXN files are
    human-readable ASCII text, enabling version control and manual inspection, while
    remaining machine-parsable through cheminformatics toolkits (RDKit, Open Babel,
    CDK Chemistry Development Kit, OEChem) that convert RXN to internal reaction objects
    for computational manipulation. The format is essential for reaction databases
    (Reaxys, SciFinder, USPTO patent reactions, Pistachio from NextMove Software),
    electronic lab notebooks (ELNs), and synthetic route planning tools, where standardized
    reaction representation supports querying by substructure, similarity searching,
    yield prediction, and reaction classification. For AI and machine learning in
    drug discovery and synthetic chemistry, RXN-encoded reactions provide training
    data for reaction outcome prediction models (predicting major products, regioselectivity,
    stereoselectivity from reactant structures and conditions), retrosynthetic planning
    algorithms (neural network-based approaches like Molecular Transformer, graph
    neural networks decomposing target molecules into purchasable precursors via learned
    reaction templates), reaction condition optimization (ML models predicting optimal
    temperature, catalyst, solvent combinations from historical RXN datasets), and
    generative models for novel reaction discovery (variational autoencoders, reinforcement
    learning agents exploring chemical reaction space). Atom mapping in RXN files
    enables extraction of reaction templates (SMARTS patterns capturing reaction mechanisms)
    used as features or constraints in graph-based neural architectures, while reaction
    fingerprints (bit vectors encoding structural changes) derived from RXN data support
    similarity-based recommendation systems for analogous reactions. Large-scale RXN
    datasets (millions of reactions from patents and literature) power transfer learning
    strategies where models pre-trained on general organic reactions are fine-tuned
    for specialized domains (medicinal chemistry, materials synthesis, enzymatic biocatalysis),
    and RXN standardization facilitates benchmarking of ML models across reproducible
    test sets, ultimately accelerating computer-aided synthesis planning, reducing
    experimental trial-and-error, and enabling AI-driven exploration of uncharted
    regions of chemical reaction space for therapeutic discovery and sustainable manufacturing.'
  requires_registration: false
  url: https://open-babel.readthedocs.io/en/latest/FileFormats/MDL_RXN_format.html
- id: B2AI_STANDARD:192
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:6
  - B2AI_TOPIC:31
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Measuring Sex, Gender Identity, and Sexual Orientation
  is_open: true
  name: MSGISO
  publication: doi:10.17226/26424
  purpose_detail: Measuring Sex, Gender Identity, and Sexual Orientation recommends
    that the National Institutes of Health (NIH) adopt new practices for collecting
    data on sex, gender, and sexual orientation - including collecting gender data
    by default, and not conflating gender with sex as a biological variable. The report
    recommends standardized language to be used in survey questions that ask about
    a respondent's sex, gender identity, and sexual orientation. Better measurements
    will improve data quality, as well as the NIH's ability to identify LGBTQI+ populations
    and understand the challenges they face.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:60
  url: https://nap.nationalacademies.org/catalog/26424/measuring-sex-gender-identity-and-sexual-orientation
- id: B2AI_STANDARD:193
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:22
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Medical Imaging NetCDF (Minc) format
  is_open: true
  name: MNC
  publication: doi:10.3389/fninf.2016.00035
  purpose_detail: A system for flexible, self-documenting representation of neuroscientific
    imaging data with arbitrary orientation and dimensionality.
  requires_registration: false
  url: https://en.wikibooks.org/wiki/MINC/SoftwareDevelopment/MINC2.0_File_Format_Reference
  has_application:
  - id: B2AI_APP:255
    category: B2AI:Application
    name: MINC Toolkit for PET Amyloid Classification with Random Forests
    description: The MINC toolkit processes 18F-florbetapir brain PET scans to generate standardized uptake value (SUV) ratios normalized to cerebellar gray matter reference regions, producing quantitative regional features that serve as inputs to random forest machine learning classifiers for automated amyloid positivity classification. In a study of 57 baseline PET scans, MINC toolkit preprocessing and quantification produced SUV ratio data per region of interest, which together with clinical reads provided the feature set and labels for supervised learning, with data split into 37 training and 20 testing cases. The random forest model implemented in MATLAB with 10,000 trees (each tree built using randomly selected cases and feature subsets following standard RF algorithm) classified amyloid PET scans as positive versus negative, achieving 86% sensitivity (correctly identifying true positive amyloid cases), 92% specificity (correctly identifying true negative cases), and 90% overall accuracy on the held-out test set, demonstrating that MINC-derived quantitative imaging features enable robust automated classification matching expert visual interpretation. This workflow exemplifies MINC's role as a preprocessing and quantification layer converting raw PET images into tabular regional SUV features suitable for classical machine learning, where the format's self-documenting metadata (coordinate systems, acquisition parameters, processing history) ensures reproducible feature extraction across multi-center studies and the toolkit's standardized pipeline (registration to anatomical templates, ROI definition, intensity normalization) produces consistent measurements enabling pooling of data from different scanners and sites. The approach supports clinical applications including automated screening for Alzheimer's disease pathology in memory clinics (reducing variability from visual reads and enabling quantitative tracking of amyloid burden over time), research studies correlating regional amyloid deposition with cognitive decline trajectories, and drug trial enrichment strategies using ML-based amyloid classification to identify eligible participants with elevated brain amyloid, demonstrating MINC format's utility in bridging molecular imaging and machine learning for precision neurology diagnostics.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1097/rlu.0000000000002747
  - id: B2AI_APP:256
    category: B2AI:Application
    name: MINC-Based MRI Morphometry Pipeline for ALS Survival Prediction
    description: A multicentre amyotrophic lateral sclerosis (ALS) neuroimaging study implements a comprehensive preprocessing pipeline built on MINC toolkit (minc-tools) and ANTs (Advanced Normalization Tools) to process T1-weighted MRI scans and extract deformation-based morphometry (DBM) features for machine learning-based survival prediction, demonstrating MINC's role in robust, standardized neuroimaging feature engineering for prognostic modeling. The pipeline applies sequential preprocessing steps implemented via MINC tools including denoising (reducing thermal noise and acquisition artifacts), N3/N4 bias field correction (correcting intensity inhomogeneity from RF coil sensitivity profiles), histogram-matching intensity normalization (standardizing intensity distributions across scanners and acquisition protocols enabling multi-site pooling), linear registration to MNI-ICBM152-2009c standard template (aligning brain position and global scale), and high-dimensional nonlinear registration (warping individual anatomy to template space capturing local volume differences), with DBM maps computed as Jacobian determinants of the nonlinear deformation fields quantifying regional volume expansion (Jacobian > 1) or contraction (Jacobian < 1) relative to the population template. Regional mean DBM values are extracted using established brain atlases (CerebrA for 62 bilateral gray matter regions and ventricles, JHU white matter atlas for major tracts, Allen atlas for corpus callosum subregions), converted to w-scores by regressing out age, sex, and scanner site effects and scaling by control group standard deviations to produce standardized measures of regional atrophy, and used as imaging features in elastic-net Cox proportional hazards survival models (combining L1 and L2 regularization penalties to handle correlated features and perform automatic feature selection). The study employs nested cross-validation repeated 100 times to optimize hyperparameters and assess generalization, trains separate models using imaging-only features (DBM w-scores), clinical-only features (ALSFRS-R functional rating, forced vital capacity, disease duration, onset type), and combined feature sets, and evaluates performance with concordance index (C-index measuring discriminative ability to rank survival times), integrated Brier score (IBS quantifying calibration of predicted survival probabilities over time), calibration plots (comparing predicted vs. observed survival), and mean absolute error variants (MAE for survival time predictions). Results demonstrate that imaging features derived from MINC-processed MRI significantly improve survival model performance compared to clinical-only models, with regional atrophy patterns (particularly motor cortex, corticospinal tracts, frontotemporal regions) providing complementary prognostic information enabling more accurate individualized survival predictions. This application showcases MINC's critical role in multi-center neuroimaging ML where the format's flexible metadata accommodates diverse scanner types (Siemens, GE, Philips with varying field strengths and sequences), the toolkit's standardized preprocessing ensures harmonization across sites reducing batch effects, and the self-documenting provenance (embedded processing history in MINC headers) supports reproducibility and quality control, enabling aggregation of data from Canadian ALS neuroimaging consortium sites to achieve sample sizes sufficient for robust survival model development, supporting clinical applications in ALS trial design (patient stratification, enrichment for rapid progressors), individualized counseling (data-driven survival estimates incorporating imaging biomarkers), and therapeutic monitoring (detecting treatment effects on neurodegeneration patterns via longitudinal DBM changes).
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2024.10.04.24314899
  - id: B2AI_APP:257
    category: B2AI:Application
    name: MINC MRSI Metabolite Maps for Glioma Classification
    description: "Ultra-high-field 7 Tesla magnetic resonance spectroscopic imaging (MRSI) generates three-dimensional metabolite concentration maps stored in MINC (.mnc) and NIfTI (.nii) formats, which are systematically converted into machine learning-ready tabular databases for automated classification of glioma molecular subtypes (IDH mutation status) and tumor grades using classical machine learning methods including random forests and support vector machines. The workflow processes 3D MRSI metabolite maps representing spatial distributions of key brain metabolites (total creatine tCr, GABA, glutamine Gln, glutamate Glu, glycine Gly, total choline tCho, glutathione GSH, myo-inositol Ins, N-acetylaspartate NAA, N-acetylaspartylglutamate NAAG, total NAA tNAA, serine Ser, taurine Tau) acquired at 7T with high spatial resolution (typical voxel sizes ~5mm enabling detailed intratumoral metabolic heterogeneity mapping), with each metabolite represented as a separate 3D image volume. The database construction pipeline flattens each 3D metabolite map (dimensions resulting in 159,744 voxels per volume) into single-dimensional arrays and creates per-patient CSV files where each row represents one voxel with columns for three-dimensional Cartesian coordinates (X, Y, Z positions in standard space), tissue classification masks (creatine signal mask indicating metabolite detectability, white matter mask, gray matter mask derived from co-registered anatomical MRI), absolute metabolite concentrations (institutional units calibrated to tissue water reference), and computed metabolite ratios (normalized concentrations reducing inter-scanner variability: tCho/tCr, NAA/tCr, Glu/tCr, etc., commonly used clinically). This per-voxel tabular representation enables spatial pattern recognition by machine learning classifiers where intratumoral metabolic heterogeneity (varying metabolite profiles across tumor regions reflecting different degrees of malignancy, proliferation, necrosis) provides discriminative features for IDH mutation prediction (IDH-mutant gliomas showing characteristic 2-hydroxyglutarate accumulation and altered glutamate/glutamine metabolism detectable via specific metabolite ratio patterns) and grade classification (high-grade gliomas exhibiting elevated choline reflecting increased membrane turnover, reduced NAA indicating neuronal loss, lactate accumulation from anaerobic glycolysis). The machine learning pipeline trains random forest classifiers (ensemble of decision trees voting on classification, robust to high-dimensional feature spaces with correlated metabolite measurements) and support vector machines (finding optimal hyperplanes in metabolite feature space separating IDH-mutant from IDH-wildtype cases, low-grade from high-grade tumors, with kernel functions capturing nonlinear metabolite combination patterns) using features extracted from tumor voxels identified via anatomical segmentation and co-registered to metabolite maps. This approach demonstrates MINC format's utility in advanced neuroimaging ML where the format's support for multi-dimensional data (3D spatial dimensions plus spectral dimension encoding chemical shift information) and flexible metadata (storing acquisition parameters: echo time TE, repetition time TR, spectral bandwidth, number of averages essential for quality assessment and inter-study comparison) enables systematic organization of complex MRSI datasets, while conversion to CSV tabular format facilitates integration with standard ML frameworks (scikit-learn, R caret, MATLAB Statistics and ML Toolbox) and enables spatial feature engineering (extracting summary statistics from tumor subregions, computing texture metrics on metabolite maps, modeling spatial gradients between tumor core and periphery). Clinical applications include non-invasive molecular subtyping of gliomas (avoiding biopsy sampling errors by assessing entire tumor metabolic profiles, predicting IDH status with implications for prognosis and treatment selection), personalized treatment planning (identifying aggressive tumor regions via metabolic signatures for targeted biopsy or focused radiotherapy boost volumes), and treatment response monitoring (detecting metabolic changes preceding anatomical tumor size changes on conventional MRI, enabling earlier assessment of therapy effectiveness), supporting precision neuro-oncology where metabolic imaging biomarkers augment genomic profiling and anatomical imaging in guiding personalized management of brain tumors."
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.34726/hss.2023.82753
- id: B2AI_STANDARD:194
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - codesystem
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Medicare Severity Diagnosis Related Groups codes
  is_open: true
  name: MS-DRG
  purpose_detail: Medical cases in the US are classified into Medicare Severity Diagnosis
    Related Groups (MS-DRGs) for payment based on the following information reported
    by the hospital - the principal diagnosis, up to 24 additional diagnoses, and
    up to 25 procedures performed during the stay. In a small number of MS-DRGs, classification
    is also based on the age, sex, and discharge status of the patient. Effective
    October 1, 2015, the diagnosis and procedure information is reported by the hospital
    using codes from the International Classification of Diseases, Tenth Revision,
    Clinical Modification (ICD-10-CM) and the International Classification of Diseases,
    Tenth Revision, Procedure Coding System (ICD-10-PCS).
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:17
  url: https://www.cms.gov/Medicare/Medicare-Fee-for-Service-Payment/AcuteInpatientPPS/MS-DRG-Classifications-and-Software
- id: B2AI_STANDARD:195
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:11
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MEDIN Discovery Metadata Standard
  formal_specification: https://github.com/medin-marine/Discovery-Standard-public-content
  is_open: true
  name: MEDIN
  purpose_detail: MEDIN Discovery Metadata Standard is a UK marine profile of the
    UK GEMINI (GEo-spatial Metadata Interoperability iNItiative) standard that also
    complies with the EU INSPIRE Directive and ISO19115. It provides comprehensive
    guidance for creating discovery metadata that accompanies marine datasets, describing
    what the data contains, where it was collected, and how to access it. The standard
    supports both geospatial (v3.1.2) and non-spatial marine data types. MEDIN provides
    multiple tools including the web-based Discovery Metadata Editor for creating,
    validating, exporting, and publishing records to the MEDIN Data Discovery Portal,
    and the standalone Metadata Maestro desktop application with user-friendly interface
    and offline capability. The standard includes XML Schema Definition (XSD) and
    Schematron files for validation, enabling systematic quality control of metadata
    records within organizational workflows.
  requires_registration: false
  url: https://www.medin.org.uk/medin-discovery-metadata-standard
- id: B2AI_STANDARD:196
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - diagnosticinstrument
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Memorial Symptom Assessment Scale
  is_open: true
  name: MSAS
  publication: doi:10.1016/0959-8049(94)90182-1
  purpose_detail: The Memorial Symptom Assessment Scale (MSAS) is a new patient-rated
    instrument that was developed to provide multidimensional information about a
    diverse group of common symptoms.
  requires_registration: false
  url: http://www.npcrc.org/files/news/memorial_symptom_assessment_scale.pdf
- id: B2AI_STANDARD:197
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:13
  - B2AI_TOPIC:28
  - B2AI_TOPIC:34
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Meta-omics Data and Collection Objects
  is_open: true
  name: MOD-CO
  purpose_detail: "Meta-omics Data and Collection Objects (MOD-CO) schema is a comprehensive conceptual and procedural framework designed to standardize the processing, annotation, and exchange of sample data and metadata in meta-omics researchencompassing metagenomics (DNA sequencing of microbial communities), metatranscriptomics (RNA-seq of community gene expression), metaproteomics (mass spectrometry of community proteins), and metabolomics (small molecule profiling)addressing the critical challenge of integrating heterogeneous multi-omics datasets from environmental, host-associated, and synthetic microbial ecosystems where thousands of species and millions of genes generate complex data requiring systematic organization for reproducible analysis and cross-study comparison. Developed through international collaboration coordinating multiple meta-omics initiatives and published in various schema representations (UML class diagrams for conceptual modeling, XML Schema Definitions for data exchange, relational database schemas for implementation, JSON-LD for semantic web integration), MOD-CO provides a generic, technology-agnostic framework adaptable to diverse experimental workflows, sequencing platforms (Illumina, PacBio, Oxford Nanopore), sample types (soil, marine, human gut, industrial bioreactors), and analysis pipelines. The schema defines core data objects organized hierarchically starting with Study (research project with objectives, funding, publications, contacts), Investigation (experimental campaign addressing specific hypotheses), Sample Collection Event (spatial-temporal metadata capturing when, where, and how environmental or biological samples were obtained, including GPS coordinates, depth/altitude, temperature, pH, sampling methodology), Sample Processing (laboratory procedures for DNA/RNA/protein extraction, quality control metrics, storage conditions, contamination controls), through to Sequence Data Objects (raw FASTQ files with quality scores, trimmed/filtered reads, assembled contigs, annotated genes) and derived analytical results (taxonomic profiles, functional annotations, abundance matrices, diversity indices, statistical comparisons). Critical metadata categories include environmental context following MIxS (Minimum Information about any (x) Sequence) standards with environment ontology (ENVO) terms describing biome/feature/material (e.g., freshwater lake sediment with depth strata, sediment grain size, organic carbon content), host-associated metadata for microbiome studies (host taxonomy, age, sex, health status, diet, medications, anatomical sampling site), experimental treatments (perturbations applied to communities: antibiotic exposure, temperature shifts, nutrient amendments with precise concentrations and timing), technical metadata (sequencing platform, library preparation kit, barcode sequences for multiplexing, sequencing depth targets), and quality metrics (DNA concentration, 260/280 absorbance ratios, fragment size distributions, read quality distributions, contamination screening results). MOD-CO explicitly models relationships between objects enabling complex queries like 'retrieve all metatranscriptome samples from human gut with matching metagenomic and metabolomic profiles from the same subjects and timepoints' or 'find marine metagenomes from similar temperature/salinity ranges across different ocean basins,' supporting meta-analyses aggregating hundreds of studies. The schema accommodates multi-omics integration where the same biological samples undergo parallel sequencing (metagenomics revealing community composition) and functional assays (metatranscriptomics showing active metabolic pathways, metaproteomics quantifying enzyme abundances, metabolomics measuring pathway products), with explicit linkage through shared sample identifiers and temporal alignment of measurements enabling systems-level analysis of microbial community structure-function relationships. For machine learning and AI applications, MOD-CO-structured datasets enable supervised learning for microbiome-phenotype associations where taxonomic/functional features predict host disease status (inflammatory bowel disease, obesity, cancer) or environmental conditions (soil fertility, water quality, bioremediation potential), unsupervised clustering discovering novel ecological patterns and community types across global microbiome surveys, time-series modeling forecasting community dynamics and succession patterns in response to environmental change or host development, transfer learning where models trained on well-characterized systems (human gut, mouse models) generalize to understudied environments through shared functional gene representations, multi-modal learning integrating sequence-derived features (gene abundance matrices) with chemical measurements (metabolite concentrations) and environmental variables (temperature, pH, nutrients) in deep neural networks predicting ecosystem functions, causal inference methods identifying keystone taxa and metabolic interactions driving community assembly and stability through interventional experiments and longitudinal observations, and federated learning enabling privacy-preserving analysis of sensitive human microbiome data where MOD-CO-standardized metadata allows model training across institutions without sharing raw sequences, supporting precision medicine applications predicting individualized treatment responses based on microbiome composition, environmental monitoring systems detecting ecosystem disturbances through anomaly detection in microbial community trajectories, and biotechnology optimization where machine learning guides engineering of synthetic microbial consortia for industrial applications including biofuel production, bioremediation, and sustainable agriculture by learning from natural community assembly principles encoded in thousands of standardized meta-omics studies."
  requires_registration: false
  url: https://www.mod-co.net/wiki/Schema_Representations
- id: B2AI_STANDARD:198
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:33
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831'
  description: MIAME Notation in Markup Language
  has_relevant_organization:
  - B2AI_ORG:74
  is_open: true
  name: MINiML
  purpose_detail: MINiML (MIAME Notation in Markup Language, pronounced 'minimal')
    is a data exchange format optimized for microarray gene expression data, as well
    as many other types of high-throughput molecular abundance data. MINiML assumes
    only very basic relations between objects - Platform (e.g., array), Sample (e.g.,
    hybridization), and Series (experiment). MINiML captures all components of the
    MIAME checklist, as well as any additional information that the submitter wants
    to provide. MINiML uses XML Schema as syntax.
  requires_registration: false
  url: https://www.ncbi.nlm.nih.gov/geo/info/MINiML.html
- id: B2AI_STANDARD:199
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:33
  - B2AI_TOPIC:34
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MicroArray Gene Expression Markup Language
  is_open: true
  name: MAGE-ML
  purpose_detail: This document is a standard that addresses the representation of
    gene expression data and relevant annotations, as well as mechanisms for exchanging
    these data. The field of gene expression experiments has several distinct technologies
    that a standard must include (e.g., single vs. dual channel experiments, cDNA
    vs. oligonucleotides). Because of these different technologies and different types
    of gene expression experiments, it is not expected that all aspects of the standard
    will be used by all organizations. With the acceptance of XML Metadata Interchange
    as an OMG standard it is possible to specify a normative UML model using a tool
    such as Rational Rose that describes the data structures for Gene Expression
  requires_registration: false
  url: http://scgap.systemsbiology.net/standards/mage_miame.php
- id: B2AI_STANDARD:200
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:33
  - B2AI_TOPIC:34
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MicroArray Gene Expression Markup Language Tab format
  is_open: true
  name: MAGE-TAB
  purpose_detail: Sharing of microarray data within the research community has been
    greatly facilitated by the development of the disclosure and communication standards
    MIAME and MAGE-ML by the FGED Society. However, the complexity of the MAGE-ML
    format has made its use impractical for laboratories lacking dedicated bioinformatics
    support. We propose a simple tab-delimited, spreadsheet-based format, MAGE-TAB,
    which will become a part of the MAGE microarray data standard and can be used
    for annotating and communicating microarray data in a MIAME compliant fashion.
    MAGE-TAB will enable laboratories without bioinformatics experience or support
    to manage, exchange and submit well-annotated microarray data in a standard format
    using a spreadsheet. The MAGE-TAB format is self-contained, and does not require
    an understanding of MAGE-ML or XML
  requires_registration: false
  url: http://scgap.systemsbiology.net/standards/mage_miame.php
- id: B2AI_STANDARD:201
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Microbiological Common Language
  is_open: true
  name: MCL
  publication: doi:10.1016/j.resmic.2010.02.005
  purpose_detail: MCL is a data exchange standard for microbiological information.
    In short, MCL defines terms which can be used to reference and describe microorganisms.
    It is designed to form a simple and generic framework leveraging the electronical
    exchange of information about microorganisms. MCL is loosely coupled from its
    actual representation technologies and is currently used to structure XML and
    RDF files (see examples).
  requires_registration: false
  url: https://doi.org/10.1016/j.resmic.2010.02.005
- id: B2AI_STANDARD:202
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MIMIC Waveform Database Format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:9
  has_relevant_organization:
  - B2AI_ORG:57
  - B2AI_ORG:115
  is_open: true
  name: WFDB Format
  purpose_detail: Format for MIMIC Waveform Database records.
  requires_registration: false
  url: https://wfdb.io/mimic_wfdb_tutorials/mimic/formatting.html
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:39
    category: B2AI:Application
    name: MIMIC-BP Curated Dataset for Blood Pressure Estimation Benchmarking
    description: The Python wfdb package reads MIMIC-III waveform signals directly,
      converts samples to double precision, and preserves signals exactly as recorded
      for ML workflows. MIMIC-BP organizes 1,524 patients into fixed 30-second segments
      (30 segments per patient) at 125 Hz with ABP, ECG, PPG, and RESP waveforms
      stored as NumPy files, providing per-segment median SBP/DBP labels for supervised
      learning. Pre-defined per-subject train/validation/test splits prevent data
      leakage under calibration-free protocols. This WFDB-enabled standardized segmentation,
      labeling, and splitting supports reproducible benchmarking of deep learning
      (ResNet, LSTM, Transformer) and classical ML (SVR, Random Forest) models for
      cuff-less blood pressure estimation across approximately 380 hours of ICU data.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1038/s41597-024-04041-1
  - id: B2AI_APP:142
    category: B2AI:Application
    name: PulseDB Large-Scale BP Estimation Corpus Using WFDB Toolbox
    description: The WFDB toolbox retrieves and validates MIMIC-III matched subset
      records, filtering for simultaneous ECG (lead II), PPG, and ABP channels while
      removing NaN samples by extracting the longest valid intervals per record (10
      seconds to 10 hours). Signals are resampled to 125 Hz and segmented into standardized
      10-second windows with beat-level annotations and per-segment SBP/DBP labels,
      yielding 5.2 million segments. This WFDB-enabled quality control, multi-channel
      synchronization, and standardized segmentation provides a large benchmarking
      corpus for training and evaluating ML/DL cuff-less blood pressure estimators
      with cross-study comparability and reproducible preprocessing pipelines.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.3389/fdgth.2022.1090854
  - id: B2AI_APP:143
    category: B2AI:Application
    name: Intracranial Hypertension Forecasting Using Multi-Scale WFDB Waveforms
    description: MIMIC-III WFDB provides high-frequency waveforms (125 Hz) and derived
      1 Hz time series for ML early-warning systems. Specific channels (ICP, CPP,
      ABP variants, HR, PLETH, RESP, ECG) are selected via WFDB, with cohort filtering
      enforcing minimum 24-hour recording length and maximum 25% per-channel missing
      values, reducing to 123 usable segments. Preprocessing produces 1-minute blocks
      with artifact deletion, and supervised labels are derived from WFDB ICP time
      series (median ICP greater than 20 mmHg for five consecutive minutes defines
      intracranial hypertension events). This WFDB-based channel selection, quality
      filtering, segmentation, and labeling enables training of logistic regression
      and feature-based predictive models for forecasting intracranial hypertension
      up to 8 hours ahead.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1088/1361-6579/ab6360
  - id: B2AI_APP:144
    category: B2AI:Application
    name: Deep Learning ABP Waveform Imputation from WFDB Multi-Channel Signals
    description: MIMIC-III waveform records accessed via WFDB provide synchronized
      ECG (lead II), PPG, and invasive ABP for training deep learning models to impute
      continuous arterial blood pressure waveforms from non-invasive signals. Preprocessing
      includes downsampling to 100 Hz, low-pass filtering (16 Hz cutoff), robust
      per-window scaling, and cross-correlation-based clock drift correction (up to
      plus or minus 4 seconds). Sliding 32-second windows with 16-second steps are
      quality-filtered (variance and peak-count thresholds), and labels are derived
      from invasive ABP medians over 4-second windows. This WFDB-enabled preprocessing,
      segmentation, inter-signal alignment, and label derivation supports training
      U-Net-like segmentation models with transfer learning to external (UCLA) cohorts
      for generalizable waveform-to-waveform prediction.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1038/s41598-021-94913-y
  - id: B2AI_APP:145
    category: B2AI:Application
    name: Comparative Deep Learning for ABP Prediction from WFDB Physiological Signals
    description: MIMIC waveform records provide co-recorded ECG (lead V), PPG, and
      ABP for comparative evaluation of deep learning architectures including ResNet,
      WaveNet, and LSTM for blood pressure prediction. WFDB-sourced data undergoes
      artifact detection and removal (flat lines, missing heartbeats, negative BP
      values), signal filtering (8th-order Chebyshev bandpass for ECG at 2-59 Hz),
      and segmentation into 10-minute recordings. Target ABP is normalized with physiological
      bounds (15-300 mmHg) for model training using RMSE and Huber loss, requiring
      de-normalization for evaluation. This WFDB-based data selection, preprocessing,
      segmentation, and target normalization pipeline supports systematic comparison
      of CNN and RNN architectures for continuous blood pressure estimation from ICU
      waveforms.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1007/s12559-021-09910-0
- id: B2AI_STANDARD:203
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Minimal Information about a high throughput SEQuencing Experiment
  formal_specification: https://drive.google.com/file/d/1YyvWT02puzMG_UgNmfAEJwVr60-pMvIE/view?usp=sharing
  is_open: true
  name: MINSEQE
  purpose_detail: Information needed to enable the unambiguous interpretation and
    facilitate reproduction of the results of a high throughput sequencing experiment.
  requires_registration: false
  url: https://zenodo.org/record/5706412
- id: B2AI_STANDARD:204
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Minimal Information about a Self-Monitoring Experiment
  is_open: false
  name: MISME
  publication: doi:10.3233/978-1-61499-423-7-79
  purpose_detail: "Minimal Information about a Self-Monitoring Experiment (MISME) is a reporting guideline and metadata schema developed to standardize documentation of self-monitoring, personal health tracking, and quantified-self experiments where individuals collect longitudinal data about their own physiology, behavior, environment, or experiences using consumer devices, mobile apps, or manual logging, addressing the critical gap in reproducibility and data quality assessment for n-of-1 studies and personal science investigations that generate rich time-series datasets increasingly valuable for personalized medicine research, digital health validation, and machine learning model development. MISME specifies core metadata elements organized into multiple categories capturing the who (participant demographics including age, sex, health conditions, medications, baseline characteristics), what (measured variables with precise definitions: heart rate in beats per minute, sleep duration in hours, mood ratings on 1-10 scales, step counts from accelerometer thresholds, dietary intake in calories/macronutrients, blood glucose in mg/dL), when (temporal structure with measurement frequency, duration of observation period, time zones, timestamp precision, missing data patterns documenting gaps in collection due to device failures or participant non-adherence), where (geographic location at appropriate granularity respecting privacy, environmental context like indoor/outdoor settings, altitude for activity tracking), why (research question or personal goal motivating the experiment, hypotheses being tested, expected relationships between variables), and how (detailed device and methodology descriptions including manufacturer, model numbers, firmware versions, sensor specifications, calibration procedures, measurement protocols, data export formats, preprocessing steps applied before analysis). The guideline addresses unique challenges of self-experimentation including lack of blinding (participants aware of interventions and measurements, introducing potential placebo effects and confirmation bias), temporal confounding (seasonal variations, life events, concurrent behavioral changes making causal attribution difficult), device heterogeneity (consumer wearables with proprietary algorithms, varying accuracy across brands and body sites), and data ownership/ethics (consent for research use of personal data, privacy protections when sharing sensitive health information, appropriate credit for participant contributions as co-investigators rather than subjects). MISME recommends standardized reporting of experimental design elements including baseline periods (establishing individual norms before interventions), intervention descriptions (dietary changes, exercise regimens, medication adjustments, environmental modifications with precise timing and dosing), washout periods (time between sequential interventions allowing return to baseline), and randomization schemes (if self-randomizing intervention timing, documenting the randomization method and allocation concealment). Data quality metadata includes device validation studies (comparing device outputs against gold-standard measurements like research-grade polysomnography for sleep tracking or doubly-labeled water for energy expenditure), measurement reliability (test-retest correlation coefficients, intra-individual variability quantified through repeated measures), data cleaning procedures (outlier detection thresholds, imputation methods for missing values, smoothing or filtering applied to noisy sensor data), and uncertainty quantification (confidence intervals around estimates, sensitivity analyses examining robustness to methodological choices). For machine learning and AI applications, MISME-compliant datasets enable robust model training where standardized metadata facilitates transfer learning across individuals (identifying which personal characteristics and device configurations affect model generalizability), time-series forecasting of health outcomes (predicting blood glucose excursions from continuous glucose monitor data, sleep quality from activity/heart rate, mood episodes from behavioral digital phenotypes captured via smartphone sensors), anomaly detection for early disease warning (identifying deviations from individual baselines suggesting onset of infection, metabolic dysregulation, or mental health crises), causal inference from observational self-tracking data (applying difference-in-differences, interrupted time series, or instrumental variable methods to estimate intervention effects in the presence of confounding), and federated learning where privacy-sensitive personal health data remains on individual devices while contributing to population-level models learning patterns generalizable across diverse self-tracking contexts, supporting digital health applications including adaptive interventions (AI-driven just-in-time recommendations personalized to real-time physiological and behavioral states), closed-loop systems (automated insulin delivery adjusting doses based on predicted glucose trends, circadian rhythm optimization adjusting light exposure and sleep timing based on actigraphy and performance metrics), and precision diagnostics where longitudinal self-monitoring augments episodic clinical assessments, enabling earlier detection of disease progression and personalized treatment titration in conditions like diabetes, cardiovascular disease, mental health disorders, and sleep disturbances where individual trajectories vary substantially and population-average guidelines provide insufficient guidance for optimal management."
  requires_registration: false
  url: https://doi.org/10.3233/978-1-61499-423-7-79
- id: B2AI_STANDARD:205
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Minimal Information Required In the Annotation of Models
  is_open: true
  name: MIRIAM
  purpose_detail: MIRIAM (Minimum Information Required In the Annotation of biochemical
    Models) is a set of guidelines and standards developed by the computational systems
    biology community to ensure consistent annotation and curation of computational
    models in biology, particularly those encoded in SBML (Systems Biology Markup
    Language) and CellML formats. MIRIAM defines requirements for model attribution
    including reference publications, creator information, creation dates, and modification
    history to ensure proper provenance tracking and reproducibility. The guidelines
    mandate the use of controlled vocabularies and standardized identifiers from MIRIAM
    Resources (identifiers.org) to annotate biological entities such as genes, proteins,
    metabolites, reactions, and pathways with unambiguous references to external databases
    like UniProt, ChEBI, Gene Ontology, KEGG, and Reactome. MIRIAM specifies the use
    of RDF (Resource Description Framework) and qualified annotations to encode biological
    semantics and relationships between model components and biological knowledge.
    The standard promotes model reusability by requiring clear licensing information
    (Creative Commons, etc.), encoded parameter units using standardized ontologies,
    and comprehensive documentation of model assumptions and limitations. MIRIAM compliance
    is supported by tools including the MIRIAM annotation editor in systems biology
    software, libSBML annotation functions, and BioModels Database validation workflows.
    The guidelines have been widely adopted by model repositories (BioModels, CellML
    Model Repository), enhancing model discoverability, integration into larger modeling
    frameworks, and facilitating quantitative comparison of alternative models representing
    the same biological system.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:19
  url: http://co.mbine.org/standards/miriam
- id: B2AI_STANDARD:206
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:2
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Minimum Information about a Flow Cytometry Experiment
  is_open: true
  name: MIFlowCyt
  publication: doi:10.1002/cyto.a.20623
  purpose_detail: The fundamental tenet of scientific research is that the published
    results of any study have to be open to independent validation or refutation.
    The Minimum Information about a Flow Cytometry Experiment (MIFlowCyt) establishes
    criteria for recording and reporting information about the flow cytometry experiment
    overview, samples, instrumentation and data analysis. It promotes consistent annotation
    of clinical, biological and technical issues surrounding a flow cytometry experiment
    by specifying the requirements for data content and by providing a structured
    framework for capturing information.
  requires_registration: false
  url: https://isac-net.org/page/MIFlowCyt
- id: B2AI_STANDARD:207
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Minimum Information About a Proteomics Experiment
  is_open: true
  name: MIAPE
  purpose_detail: Minimum Information About a Proteomics Experiment (MIAPE) is a comprehensive
    suite of modular reporting guidelines developed by the HUPO Proteomics Standards
    Initiative (HUPO-PSI) to define the minimum set of metadata and experimental
    details required for public proteomics data repositories, ensuring that proteomics
    experiments can be evaluated, interpreted, and reproduced by the scientific community
    while addressing the context-sensitive nature of proteome data that necessitates
    capturing richer metadata than basic genetic sequence information alone. Published
    as the foundational MIAPE Principles document in Nature Biotechnology (2007,
    doi:10.1038/nbt1329), the framework recognizes that paper citations serve as
    inadequate proxies for actual experimental metadata, hindering data reassessment
    and obstructing non-standard searching (e.g., querying by liquid chromatography
    column coupling order), while varying journal requirements result in inconsistent
    reporting where critical details may be lacking or presented in esoteric formats,
    necessitating explicit documentation of both sample provenance (where samples
    came from including biological source, tissue type, growth conditions, disease
    state, treatment history) and analytical workflows (how analyses were performed
    encompassing separation methods, mass spectrometry parameters, data processing
    algorithms, statistical validation criteria). MIAPE comprises multiple modular
    components addressing different experimental stages and technologies where MIAPE-MS
    (Mass Spectrometry, v2.98) specifies reporting requirements for mass spectrometry
    instrumentation including ion source type (electrospray ionization ESI, matrix-assisted
    laser desorption/ionization MALDI, atmospheric pressure chemical ionization APCI),
    mass analyzer configuration (time-of-flight TOF, quadrupole, ion trap, Orbitrap,
    Fourier transform ion cyclotron resonance FT-ICR), fragmentation methods (collision-induced
    dissociation CID, higher-energy collisional dissociation HCD, electron-transfer
    dissociation ETD), mass accuracy and resolution specifications, scan parameters
    (m/z range, scan rate, dynamic exclusion settings), and instrument calibration
    procedures; MIAPE-MSI (Mass Spectrometry Informatics, v1.1) addresses computational
    analysis workflows documenting database searching algorithms (Mascot, SEQUEST,
    X!Tandem, Andromeda), protein sequence databases searched (UniProt, RefSeq, Ensembl
    with version and taxonomy specification), search parameters (precursor and fragment
    mass tolerances, enzyme specificity, fixed and variable modifications including
    oxidation, phosphorylation, acetylation, ubiquitination), false discovery rate
    (FDR) control methods (target-decoy approach, posterior error probability), protein
    inference algorithms handling degenerate peptide-to-protein mappings, and spectral
    library generation; MIAPE-Quant (Quantification, v1.0) covers quantitative proteomics
    approaches specifying label-free methods (spectral counting, extracted ion chromatogram
    XIC intensity-based quantification with retention time alignment and normalization
    strategies), stable isotope labeling techniques (SILAC incorporating heavy amino
    acids, iTRAQ/TMT isobaric tagging for multiplexed quantification, dimethyl labeling,
    metabolic labeling), quantification software tools (MaxQuant, Skyline, OpenMS,
    Proteome Discoverer), statistical testing for differential expression (t-tests,
    ANOVA, limma, empirical Bayes moderation), multiple testing correction (Benjamini-Hochberg
    FDR, Bonferroni), and normalization methods (median normalization, quantile normalization,
    variance stabilization transformation VST); MIAPE-GE (Gel Electrophoresis, v1.4)
    documents gel-based separation methods including 1D and 2D polyacrylamide gel
    electrophoresis (PAGE) with gel composition specifications (acrylamide percentage,
    pH gradient for isoelectric focusing IEF), running conditions (voltage, current,
    duration), staining methods (Coomassie blue, silver stain, fluorescent dyes SYPRO
    Ruby/Cy dyes), and gel imaging parameters (scanner type, pixel resolution, dynamic
    range); MIAPE-GI (Gel Informatics, v1) addresses computational analysis of gel
    images documenting spot detection algorithms, background subtraction methods,
    spot matching across gel replicates, normalization strategies (total spot volume,
    housekeeping protein normalization), statistical analysis identifying differentially
    abundant spots, and software tools (PDQuest, Progenesis, DeCyder); MIAPE-CC (Column
    Chromatography, v1.1) specifies liquid chromatography separation parameters including
    column dimensions (length, inner diameter), stationary phase chemistry (C18 reversed-phase,
    C8, strong cation exchange SCX, hydrophilic interaction HILIC, size exclusion),
    mobile phase composition (organic solvents, pH, buffer additives), gradient profiles
    (flow rate, gradient slope, column temperature), and injection volumes; MIAPE-CE
    (Capillary Electrophoresis, v0.9.3) covers capillary electrophoresis separation
    documenting capillary dimensions, buffer composition, voltage settings, temperature
    control, and detection methods; and MIMIx (Molecular Interactions, v1-1-2) addresses
    protein-protein interaction experiments specifying interaction detection methods
    (yeast two-hybrid, co-immunoprecipitation, affinity purification followed by
    mass spectrometry AP-MS, surface plasmon resonance SPR), experimental conditions
    (buffer pH, salt concentration, temperature), and confidence scores for interaction
    assignment. MIAPE integration with proteomics data repositories positions these
    guidelines as foundational standards for major public databases where PRIDE (PRoteomics
    IDEntifications database) hosted by EMBL-EBI serves as primary repository for
    mass spectrometry-based proteomics data requiring MIAPE-compliant metadata submission,
    ProteomeXchange Consortium coordinates data submission across multiple repositories
    (PRIDE, PeptideAtlas, MassIVE, jPOST, iProX) with MIAPE metadata ensuring interoperability,
    PeptideAtlas provides validated peptide and protein identifications with MIAPE-documented
    search parameters enabling reproducible analysis, and PhosphoSitePlus curates
    post-translational modifications with MIAPE-standardized mass spectrometry evidence
    supporting site localization confidence. MIAPE-compliant reporting supports diverse
    proteomics applications where biomarker discovery studies documenting clinical
    sample metadata (patient demographics, disease stage, treatment response) alongside
    MIAPE-specified analytical workflows enable cross-cohort validation and meta-analyses
    aggregating candidate biomarkers across independent studies, clinical proteomics
    implementing standardized reporting in FDA-cleared or CLIA-certified laboratory
    workflows supports regulatory compliance and quality assurance programs, multi-omics
    integration combining MIAPE-documented proteomics with transcriptomics (RNA-seq),
    metabolomics, and genomics data enables systems biology modeling where consistent
    metadata annotation facilitates cross-platform correlation analyses, and machine
    learning applications training on MIAPE-compliant datasets including supervised
    classification models predicting disease subtypes or treatment response from protein
    abundance profiles (support vector machines, random forests, neural networks
    on log-transformed abundance matrices with batch effect correction), deep learning
    architectures for spectrum identification (DeepNovo de novo sequencing, Prosit
    predicting fragment ion intensities for spectral library generation, MS2DeepScore
    calculating spectral similarity for unknown compound identification), transfer
    learning adapting models trained on well-characterized samples to novel biological
    contexts or rare diseases with limited training data, unsupervised clustering
    (hierarchical clustering, k-means, UMAP/t-SNE dimensionality reduction) discovering
    proteomic subtypes and patient stratification, pathway enrichment analysis (GSEA,
    over-representation analysis) mapping differentially abundant proteins to biological
    processes and signaling networks (Reactome, KEGG, WikiPathways), causal inference
    methods integrating perturbation proteomics (CRISPR screens, drug treatments)
    with observational data to distinguish causal drivers from correlation, federated
    learning enabling multi-institutional model training on sensitive patient proteomics
    data without sharing raw mass spectra or protein identifications, and synthetic
    data generation (GANs, VAEs creating realistic proteomic profiles for algorithm
    benchmarking, privacy-preserving data sharing, and augmenting small sample size
    studies). Integration with computational proteomics tools positions MIAPE as
    foundational metadata standard where search engines (Mascot, SEQUEST, Andromeda,
    Comet, MS-GF+) implement MIAPE-MSI specifications producing standardized output
    formats (mzIdentML, pepXML, protXML), quantification platforms (MaxQuant, Skyline,
    OpenMS, Proteome Discoverer) support MIAPE-Quant metadata annotation enabling
    reproducible quantitative workflows, spectral library tools (SpectraST, NIST,
    Prosit) incorporate MIAPE-documented acquisition parameters ensuring library applicability
    across instruments and laboratories, and quality control software (PTXQC, QuaMeter,
    iMonDB) evaluate MIAPE-specified metrics (peptide identifications per MS run,
    mass accuracy distributions, chromatographic peak widths) detecting technical
    issues and monitoring longitudinal instrument performance. MIAPE registration
    with the MIBBI Project (Minimum Information for Biological and Biomedical Investigations)
    aligns proteomics reporting standards with broader biological data standardization
    initiatives spanning genomics (MIAME for microarrays, MINSEQE for sequencing),
    metabolomics (MSI minimum reporting standards), flow cytometry (MIFlowCyt), and
    imaging (MIACME for correlative microscopy), facilitating multi-omics data integration
    and cross-platform meta-analyses where consistent metadata schemas enable federated
    queries and harmonized data models. Implementation by journals and funding agencies
    where Nature Biotechnology, Journal of Proteomics, and Molecular & Cellular Proteomics
    require MIAPE compliance for data deposition in public repositories prior to
    publication, NIH and European research councils mandate MIAPE-documented data
    sharing for proteomics grants supporting reproducibility and open science initiatives,
    and pharmaceutical industry adopts MIAPE standards for internal proteomics workflows
    ensuring consistency across contract research organizations (CROs) and enabling
    regulatory submissions with well-documented analytical methods, collectively
    positions MIAPE as essential infrastructure for reproducible, FAIR (Findable,
    Accessible, Interoperable, Reusable) proteomics data supporting biomedical discovery,
    clinical translation, and precision medicine applications spanning cancer biology,
    neurodegenerative diseases, infectious diseases, cardiovascular disorders, and
    drug development.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:41
  url: https://www.psidev.info/miape
- id: B2AI_STANDARD:208
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:33
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Minimum Information About a RNAi Experiment
  is_open: true
  name: MIARE
  purpose_detail: Minimum Information About an RNAi Experiment (MIARE) is a set of
    reporting guidelines that describes the minimum information that should be reported
    about an RNAi experiment to enable the unambiguous interpretation and reproduction
    of the results. MIARE forms part of a larger effort to develop RNAi data standards
    that include a data model, data exchange format, controlled vocabulary and supporting
    software tools.
  requires_registration: false
  url: http://miare.sourceforge.net/HomePage
- id: B2AI_STANDARD:209
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Minimum information about any sequence
  formal_specification: https://github.com/GenomicsStandardsConsortium/mixs/
  is_open: true
  name: MIxS
  publication: doi:10.1038/nbt.1823
  purpose_detail: The Minimum Information about any (x) Sequence (MIxS) is a unified
    standard developed by the Genomic Standards Consortium (GSC) for describing sequence
    data from diverse environments and organisms. MIxS provides a single point of
    entry for the scientific community to access and use GSC checklists, which include
    11 distinct sequence types covering bacterial/archaeal genomes (MigsBa), eukaryotes
    (MigsEu), organelles (MigsOrg), plasmids (MigsPl), viruses (MigsVi), marker sequences
    (MimarksC/S), metagenomes (Mims), metagenome-assembled genomes (Mimag), single
    amplified genomes (Misag), and uncultivated virus genomes (Miuvig). The standard
    is complemented by 21 environmental extensions tailored to specific sampling contexts
    including agriculture, air, built environment, food production, host-associated,
    human-associated (with specific gut, oral, skin, and vaginal extensions), hydrocarbon
    resources, microbial mat/biofilm, plant-associated, sediment, soil, symbiont-associated,
    wastewater/sludge, and water environments. The specification is maintained as
    a YAML-formatted LinkML schema serving as the authoritative source for generating
    downstream GSC artifacts.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:38
  url: https://genomicsstandardsconsortium.github.io/mixs/
- id: B2AI_STANDARD:210
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Minimum Information About Plant Phenotyping Experiments
  is_open: true
  name: MIAPPE
  purpose_detail: A reporting guideline for plant phenotyping experiments. Comprises
    a checklist, i.e., a list of attributes that may be necessary to fully describe
    an experiment so that it is understandable and replicable. Should be consulted
    by people recording and depositing the data. Covers description of the following
    aspects of plant phenotyping experiment - study, environment, experimental design,
    sample management, biosource, treatment and phenotype. To read more, please visit
    http://cropnet.pl/phenotypes
  requires_registration: false
  url: https://www.miappe.org/
- id: B2AI_STANDARD:211
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:35
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Minimum Information About Somatic Mutation
  is_open: true
  name: MIASM
  publication: doi:10.1002/humu.20832
  purpose_detail: Minimum Information About Somatic Mutation (MIASM) is a comprehensive
    reporting standard framework developed to ensure complete and consistent annotation
    of somatic variation data in cancer research, addressing the critical need for
    standardized collection and documentation of cancer-specific genetic variations
    to enable data integration across databases, facilitate meta-analyses, support
    clinical interpretation, and promote reproducible cancer genomics research. Originally
    published by Olivier et al. (2009) and subsequently updated by the original authors,
    MIASM provides structured guidelines targeted to diverse stakeholders including
    researchers conducting cancer mutation discovery studies, clinicians interpreting
    tumor sequencing results for patient care, epidemiologists investigating cancer
    genetics across populations, database curators managing mutation repositories
    (COSMIC, TCGA, ICGC), bioinformaticians developing variant annotation pipelines,
    journal editors establishing submission requirements, and funding bodies setting
    data sharing policies. The framework recognizes that correct interpretation of
    cancer-specific variation patterns requires a minimum set of information spanning
    three critical dimensions where the variation itself (genetic alteration details),
    the method of detection (technical validation and quality control), and the tumor
    sample (clinical and pathological context) must all be documented to enable meaningful
    biological and clinical insights, with this basic information mandated for inclusion
    in any report on cancer-related variations whether published manuscripts, database
    submissions, clinical reports, or regulatory filings. MIASM specifies required
    components including gene name (HUGO approved nomenclature ensuring unambiguous
    gene identification across studies and databases), reference sequence (NCBI accession
    number or GI number defining the genomic or transcript coordinate system used
    for variant calling), published/unpublished nature of data (citation or data
    source enabling provenance tracking and literature integration), study design
    (consecutive series, case-control, prospective cohort, clinical trial design
    affecting interpretation of variant frequency and associations), nature and number
    of tumor samples analyzed (sample size and tumor type composition determining
    statistical power and generalizability), geographic location or hospital names
    where samples obtained (addressing population stratification and enabling multi-center
    validation), variation detection method (Sanger sequencing, targeted NGS panels,
    whole-exome sequencing, whole-genome sequencing with depth and quality metrics
    affecting sensitivity for detecting low-frequency variants and distinguishing
    true mutations from technical artifacts), gene portion screened for variations
    (exons, intronic regions, regulatory elements defining the mutational landscape
    resolution and potential for missing mutations in unscreened regions), quality
    control procedure (requirement that variations must be confirmed by repeating
    sequencing on independent PCR product or orthogonal technology reducing false
    positive rate from PCR errors, sequencing artifacts, or contamination), assessment
    of somatic origin (documentation whether variation searched in matched blood
    or normal tissue confirming somatic vs germline status essential for distinguishing
    driver mutations from inherited variants and avoiding misinterpretation of germline
    polymorphisms as tumor-specific alterations), variation description (DNA or RNA
    position and nucleotide change with nomenclature compliant with HGVS standards
    ensuring consistent variant representation enabling database queries, literature
    searches, and automated annotation pipelines to accurately identify identical
    mutations reported across studies), sample topography (anatomical site ICD-O
    codes), morphology (histological classification), nature (fresh-frozen, FFPE,
    circulating tumor DNA affecting DNA quality and detection sensitivity), and source
    (surgical resection, biopsy, liquid biopsy determining tumor purity and cellularity),
    and patient information (country of hospital admission capturing population ancestry
    and environmental exposures relevant for mutation etiology and therapeutic response
    prediction). The MIASM criteria enable integration with established standards
    and nomenclatures including HGVS variant nomenclature (Human Genome Variation
    Society standards for unambiguous mutation description at DNA, RNA, and protein
    levels using sequence-based coordinates and standardized abbreviations for substitutions,
    deletions, insertions, duplications, inversions, and complex rearrangements),
    HUGO Gene Nomenclature Committee approved gene symbols (avoiding ambiguity from
    gene aliases, synonyms, or outdated nomenclature), ICD-O topography and morphology
    codes (International Classification of Diseases for Oncology providing hierarchical
    classification of tumor anatomical sites and histological types), NCBI Reference
    Sequences (RefSeq accessions defining genomic and transcript contexts with version
    tracking for coordinate lifting between genome builds GRCh37/hg19 and GRCh38/hg38),
    and clinical trial design terminology (consistent description of study types
    enabling appropriate interpretation of mutation frequency estimates and association
    statistics). Advanced data mining annotations recommended beyond core MIASM criteria
    include functional impact predictions (in silico tools SIFT, PolyPhen-2, CADD,
    REVEL scoring missense variants for deleteriousness), population allele frequencies
    (gnomAD, 1000 Genomes, ExAC distinguishing rare somatic mutations from common
    germline polymorphisms), mutation signatures (SBS, DBS, ID signatures from COSMIC
    reflecting underlying mutational processes like UV exposure, smoking, APOBEC
    activity, mismatch repair deficiency, POLE exonuclease domain mutations), clonality
    assessment (variant allele frequency, copy number state, phylogenetic reconstruction
    distinguishing clonal founder mutations from subclonal late events), therapeutic
    actionability (FDA-approved targeted therapies, clinical trial eligibility, resistance
    mechanisms documented in CIViC, OncoKB, ClinVar databases), pathway annotations
    (KEGG, Reactome, WikiPathways enrichment identifying perturbed biological processes),
    protein domain disruption (Pfam, InterPro mapping variants to functional domains
    and post-translational modification sites), 3D structure context (PDB coordinates,
    AlphaFold predictions visualizing spatial clustering of mutations in protein
    structures), evolutionary conservation (PhyloP, PhastCons, GERP scores indicating
    purifying selection at mutated positions), splicing predictions (MaxEntScan,
    SpliceAI identifying variants affecting splice sites or creating cryptic splice
    sites), and epigenetic context (chromatin state, histone modifications, DNA methylation
    from ENCODE, Roadmap Epigenomics characterizing regulatory regions where non-coding
    mutations may affect gene expression). MIASM-compliant reporting supports diverse
    applications in cancer genomics research where comprehensive mutation databases
    (COSMIC Catalogue of Somatic Mutations in Cancer aggregating MIASM-documented
    variations from publications and genome-wide screens, TCGA The Cancer Genome
    Atlas providing open-access multi-platform molecular profiling across 33 cancer
    types, ICGC International Cancer Genome Consortium harmonizing somatic mutation
    data from global sequencing projects) rely on standardized annotations enabling
    federated queries and cross-cohort analyses, clinical sequencing interpretation
    pipelines integrate MIASM-documented variants with knowledge bases (CIViC Clinical
    Interpretation of Variants in Cancer, OncoKB Precision Oncology Knowledge Base,
    ClinGen/ClinVar somatic variant curation) supporting molecular tumor boards and
    precision oncology treatment selection, cancer surveillance and epidemiology
    studies leverage MIASM-standardized population-based mutation data to identify
    environmental risk factors, gene-environment interactions, and disparities in
    mutation burden across demographic groups, biomarker discovery for early detection,
    prognosis, and therapeutic response prediction, and machine learning applications
    training on MIASM-compliant datasets including supervised models predicting driver
    vs passenger mutations (integrating sequence context, evolutionary conservation,
    protein structure, and pathway enrichment features), deep learning architectures
    (CNNs on sequence context, GNNs on protein-protein interaction networks, transformers
    on multi-omics profiles) classifying tumor subtypes from mutation signatures,
    transfer learning adapting models trained on well-studied cancer types (breast,
    lung, colon) to rare cancers with limited data, federated learning enabling multi-institutional
    model training on sensitive patient data without sharing raw genomic information,
    causal inference methods distinguishing driver mutations causally contributing
    to oncogenesis from passenger mutations accumulated during tumor evolution using
    dN/dS ratios, mutual exclusivity patterns, and experimental validation data,
    mutational signature deconvolution (NMF, topic modeling, Bayesian inference extracting
    COSMIC signatures from tumor mutation catalogs and attributing etiology to DNA
    repair defects, carcinogen exposures, or aging), synthetic data generation (GANs,
    VAEs creating realistic tumor mutation profiles for algorithm development, benchmarking,
    and privacy-preserving data sharing), and precision medicine recommender systems
    matching patients to clinical trials and targeted therapies based on actionable
    mutations with supporting evidence from MIASM-documented case reports and cohort
    studies. Integration with clinical workflows positions MIASM as foundational
    standard for molecular tumor profiling where standardized reporting enables interoperability
    between laboratory information systems (LIS), electronic health records (EHR),
    and genomic databases, supports regulatory submissions for companion diagnostics
    and targeted therapies requiring consistent variant nomenclature and validation
    criteria, facilitates quality assurance programs and proficiency testing for
    clinical genomics laboratories through standardized benchmarking datasets, enables
    longitudinal monitoring of tumor evolution, minimal residual disease, and acquired
    resistance mutations with consistent variant tracking across timepoints, and
    promotes equitable access to precision oncology by ensuring mutation data from
    diverse populations and healthcare settings adheres to common standards enabling
    knowledge transfer and reducing disparities in genomic medicine implementation.
  requires_registration: false
  url: http://structure.bmc.lu.se/MIASM/miasm.html
- id: B2AI_STANDARD:212
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:35
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Minimum Information for QTLs and Association Studies
  is_open: true
  name: MIQAS
  purpose_detail: The MIQAS set of rules accompanied with the standardized XML and
    tab-delimited file formats will serve two goals - to encourage research groups
    that wish to publish a QTL paper to provide and submit the necessary information
    that would make meta-analysis possible and to allow easy interchange of data between
    different QTL and association analysis databases. Databases that implement the
    standardized XML format will typically write an import and an export filter to
    read data from and dump data into that an XML file. This is the same approach
    as used for the exchange of sequences between NCBI, Ensembl and DDBJ at the early
    stages of the Human Genome Project.
  requires_registration: false
  url: http://miqas.sourceforge.net/
- id: B2AI_STANDARD:213
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Minimum information required to initiate collaborations between biobanks
  is_open: true
  name: MIABIS
  publication: doi:10.1089/bio.2015.0070
  purpose_detail: MIABIS represents the minimum information required to initiate collaborations
    between biobanks and to enable the exchange of biological samples and data. The
    aim is to facilitate the reuse of bio-resources and associated data by harmonizing
    biobanking and biomedical research.
  requires_registration: false
  url: https://doi.org/10.1089/bio.2015.0070
- id: B2AI_STANDARD:214
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Minimum information required to report the use of quantification techniques
    in a proteomics experiment
  is_open: true
  name: MIAPE-Quant
  purpose_detail: This module identifies the minimum information required to report
    the use of quantification techniques in a proteomics experiment, sufficient to
    support both the effective interpretation and assessment of the data and the potential
    recreation of the results of the data analysis.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:41
  url: https://www.psidev.info/attachments/miape-quant-091-documents
- id: B2AI_STANDARD:215
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:27
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ModelCIF
  formal_specification: http://github.com/ihmwg/ModelCIF
  is_open: true
  name: ModelCIF
  publication: doi:10.1101/2022.12.06.518550
  purpose_detail: An extension of the Protein Data Bank Exchange / macromolecular
    Crystallographic Information Framework (PDBx/mmCIF); provides an extensible data
    representation for deposition, archiving, and public dissemination of predicted
    3D models of proteins.
  requires_registration: false
  url: http://github.com/ihmwg/ModelCIF
- id: B2AI_STANDARD:216
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - multimodal
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Multilevel Healthcare Information Modeling specifications
  is_open: true
  name: MLHIM
  purpose_detail: The Multilevel Healthcare Information Modeling (MLHIM) specifications
    enables the exchange of syntactically and semantically interoperable data extracts
    between distributed, independently developed, biomedical databases and clinical
    applications, promoting syntactic and semantic integration of Translational Research
    data. The Semantic MedWeb is an implementation of a MLHIM-based database development
    platform (open source code available at https://github.com/mlhim/SemanticMedWeb)
  requires_registration: false
  url: https://mlhim-specifications.readthedocs.io/en/master/
- id: B2AI_STANDARD:217
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:23
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Multiple Alignment Format
  is_open: true
  name: MAF
  purpose_detail: The multiple alignment format stores a series of multiple alignments
    in a format that is easy to parse and relatively easy to read. This format stores
    multiple alignments at the DNA level between entire genomes. Previously used formats
    are suitable for multiple alignments of single proteins or regions of DNA without
    rearrangements, but would require considerable extension to cope with genomic
    issues such as forward and reverse strand directions, multiple pieces to the alignment,
    and so forth.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:119
  url: http://genome.ucsc.edu/FAQ/FAQformat.html#format5
- id: B2AI_STANDARD:218
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: mz5 format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:16
  is_open: true
  name: mz5
  publication: doi:10.1074/mcp.O111.011379
  purpose_detail: The mz5 format is a complete reimplementation of the mzML data
    model and ontology built on the HDF5 (Hierarchical Data Format 5) storage backend,
    designed to address performance and scalability limitations of XML-based mass
    spectrometry formats. mz5 preserves the semantic structure and controlled vocabulary
    terms of mzML while leveraging HDF5's binary format, chunked storage, and native
    compression capabilities to achieve significantly faster read/write operations
    and reduced file sizes compared to mzML. The format organizes mass spectrometry
    data into hierarchical HDF5 groups and datasets representing spectra, chromatograms,
    instrument configurations, and metadata, with support for efficient random access
    to individual spectra and parallel I/O operations. mz5 maintains compatibility
    with the Proteomics Standards Initiative controlled vocabularies and supports
    the same rich metadata and data provenance as mzML, while providing superior
    performance for large-scale proteomics and metabolomics datasets. The HDF5 foundation
    enables integration with high-performance computing workflows, supports multiple
    programming languages (C, Python, Java, R), and facilitates efficient processing
    of high-resolution and high-throughput mass spectrometry data for downstream
    analysis pipelines and machine learning applications.
  related_to:
  - B2AI_STANDARD:339
  requires_registration: false
  url: https://doi.org/10.1074/mcp.O111.011379
  has_application:
  - id: B2AI_APP:269
    category: B2AI:Application
    name: PulsPhos phosphoproteomics ML workflow with mz5
    description: "Phosphoproteomics dynamics prediction workflow converting Thermo
      .RAW files to .MZ5 format using ProteoWizard, importing .MZ5 files with custom
      Julia script to extract all raw MS1 spectra, transforming mass differences
      to absolute masses, computing intensity-weighted peak centers, applying Gaussian-shape
      filtering, generating theoretical isotopic distributions (n=0...7 Poisson model),
      filtering with 4 ppm mass and 0.75 s retention-time windows, fitting metabolic-labeling
      models with equations L(t)=1+(p exp(-t/p)- exp(-t/))/(-p) and An(t)=
      pi(t) I_{n-2i}, curating 4,058 phosphosites, engineering 945 features including
      MS evidence (number of independent detections and peaks), AlphaFold2-derived
      structure features (pLDDT, RSA, DSSP v3.0.0), codon GC/content, and turnover
      rates, training gradient boosting (XGBoost v1.5.2) models on log10-transformed
      response excluding long-lived sites above 10^3.7 minutes (~83.53 hours), clustering/splitting
      with MMseqs2 (30% identity) into 90/10 train/test sets, and discarding fits
      with negative R where R=1-(vardata). Demonstrates direct use of mz5 as the
      working raw-data container in end-to-end machine learning pipeline for predicting
      phosphosite lifetimes from mass spectrometry data integrated with structural
      and sequence features."
    references:
    - https://doi.org/10.1101/2024.07.23.604744
  - id: B2AI_APP:270
    category: B2AI:Application
    name: Toffee DIA-MS deep learning with mz5 conversion
    description: "Data-independent acquisition mass spectrometry (DIA-MS) workflow
      using mz5 format in conversion pipeline alongside deep learning proof-of-concept
      for peptide localization. Converted vendor WIFF files to multiple formats including
      mz5 via msconvert (Dockerised pwiz-skyline-i-agree-to-the-vendor-licenses:3.0.19073-85be8464136)
      with command 'wine msconvert -z --mz5 --outfile ${mz5_fname} ${wiff_fname}',
      applied OpenMS/OpenSWATH (OpenMSToffee wrapper) for DIA analysis, developed
      HDF5-based toffee format representing DIA data as triplets on Cartesian grid
      and 2D image-like slices with mass accuracy loss <5 ppm, trained single-shot
      detector (SSD) with ResNet50 backbone on Amazon SageMaker using OpenSWATH-labelled
      data for computer vision-based deep learning inference of peptide location,
      processed more than 1,200 technical replicates using analytic model G(t,m,t0,m0,a,c)=Gj
      where Gj=Fj(t,m,t0,m0,j,aj,cj)-Ij with retention time t, m/z m, spreads t/m,
      peak locations t0/m0, amplitudes a, and chemical noise c, demonstrating improved
      replicate correlation via outlier removal using t, m, t0, m0 parameters.
      Shows mz5 co-use in data conversion workflows integrating machine learning
      components with HDF5-backed formats, MIT-licensed code using h5py for tensor
      operations enabling deep learning on mass spectrometry data."
    references:
    - https://doi.org/10.1038/s41598-020-65015-y
  - id: B2AI_APP:271
    category: B2AI:Application
    name: Universal bacterial detection metabolomics with mz5
    description: "Untargeted metabolomics workflow for universal bacterial detection
      in tissues converting Thermo .raw files to .mz5 format using ProteoWizard msconvert,
      building spectral metabolic library of 233 bacterial species (n=597 bacterial
      profiles), applying univariate statistical analysis to identify 359 taxon-specific
      markers (TSMs) occurring in >90% of samples with criteria p<0.05, TPR0.80,
      FPR0.10, 3 observations, extracting peak intensities with 15 ppm mass tolerance
      (internal) or 50 ppm (external) normalized to 200-1000 m/z range, validating
      on internal set of 2,677 bacterial profiles and external set of 564 spectral
      profiles achieving median AUCs 0.997 (IQR 0.047) internal and 0.985 (IQR 0.071)
      external for classification across taxonomic levels (phylum AUC=1.0, Gram AUC=0.8),
      performing PCA on log10-transformed total-intensity normalized data (PC1=14.8%),
      detecting bacteria in 97.7% of 44 colorectal tissue samples via DESI-MS imaging,
      comparing LC-MS (107 TSMs) and DESI-MSI (143 TSMs) with 77 marker overlap and
      correlation 0.70 (p<0.001), tentatively identifying 177 of 359 variables, and
      providing GitHub code (https://github.com/jsmckenzie/bacterialTSM) with worked
      examples. Demonstrates mz5 use in modern ML-adjacent statistical biomarker discovery
      pipeline with method-agnostic detection framework supporting downstream classification
      tasks across multiple MS modalities and clinical tissue types addressing challenges
      of pathogen diversity (~1,400 pathogenic species), marker specificity, variable
      bacterial cell numbers, and non-sterile specimen commensals."
    references:
    - https://doi.org/10.1038/s41467-024-55457-7
- id: B2AI_STANDARD:219
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: mzData format
  is_open: true
  name: mzData
  purpose_detail: mzData is an XML format for representing mass spectrometry data
    in such a way as to completely describe the instrumental aspects of the experiment.
    This format is deprecated and has been superseded by mzML.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:41
  url: https://psidev.info/mass-spectrometry-workgroup#mzdata
- id: B2AI_STANDARD:220
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: mzIndentML format
  is_open: true
  name: mzIndentML
  purpose_detail: A large number of different proteomics search engines are available
    that produce output in a variety of different formats. It is intended that mzIdentML
    will provide a common format for the export of identification results from any
    search engine. The format was originally developed under the name AnalysisXML
    as a format for several types of computational analyses performed over mass spectra
    in the proteomics context.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:41
  url: https://www.psidev.info/mzidentml
- id: B2AI_STANDARD:221
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: mzML format
  formal_specification: http://www.peptideatlas.org/tmp/mzML1.1.0.html
  is_open: true
  name: mzML
  publication: doi:10.1007/978-1-60761-444-9_22
  purpose_detail: From 2005-2008 there has existed two separate XML formats for encoding
    raw spectrometer output, mzData developed by the PSI and mzXML developed at the
    Seattle Proteome Center at the Institute for Systems Biology. It was recognized
    that the existence of two separate formats for essentially the same thing generated
    confusion and required extra programming effort. Therefore the PSI, with full
    participation by ISB, has developed a new format by taking the best aspects of
    each of the precursor formats to form a single one. It is intended to replace
    the previous two formats. This new format was originally given a working name
    of dataXML. The final name is mzML.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:41
  url: https://psidev.info/mzML
- id: B2AI_STANDARD:222
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: mzQuantML format
  is_open: true
  name: mzQuantML
  purpose_detail: The mzQuantML standard format is intended to store the systematic
    description of workflows quantifying molecules (principly peptides and proteins)
    by mass spectrometry. A large number of different software packages are available
    that produce output in a variety of different formats. It is intended that mzQuantML
    will provide a common format for the export of identification results from any
    software package. The format was originally developed under the name AnalysisXML
    as a format for several types of computational analyses performed over mass spectra
    in the proteomics context. It has been decided to split development into two formats,
    mzIdentML for peptide and protein identification and mzQuantML (described here),
    covering quantitative proteomic data derived from MS. The development of mzQuantML
    is driven by some general principles, specific use cases and the goal of supporting
    specific techniques, as listed below. These were discussed and agreed at the development
    meeting in Tubingen in July 2011.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:41
  url: https://www.psidev.info/mzquantml
- id: B2AI_STANDARD:223
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - deprecated
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: mzXML format
  is_open: true
  name: mzXML
  publication: doi:10.1038/nbt1031
  purpose_detail: mzXML was a pioneering open, generic XML (extensible markup language)
    representation specifically developed for mass spectrometry (MS) data exchange
    and storage in proteomics research. As one of the first standardized formats for
    MS data, mzXML provided a vendor-neutral way to represent complex mass spectrometry
    information including mass-to-charge ratios, intensity values, scan parameters,
    and instrument metadata in a structured, machine-readable format. The format supported
    both profile and centroid data representation modes and could accommodate various
    types of MS experiments including MS/MS fragmentation spectra. Despite its historical
    significance in establishing data standardization practices in proteomics, mzXML
    is now deprecated and has been superseded by the more comprehensive mzML format,
    which offers enhanced features, better compression, controlled vocabularies, and
    broader community support. While legacy systems may still encounter mzXML files,
    current best practices recommend migration to mzML for new data processing workflows
    and long-term data preservation in mass spectrometry applications.
  requires_registration: false
  url: https://doi.org/10.1038/nbt1031
- id: B2AI_STANDARD:224
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: National Cancer Institute Enterprise Vocabulary Service
  is_open: true
  name: NCI EVS
  purpose_detail: Terminology content, tools, and services to accurately code, analyze
    and share cancer and biomedical research, clinical and public health information.
    Includes NCI Thesaurus and Metathesaurus.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:71
  url: https://evs.nci.nih.gov/
- id: B2AI_STANDARD:225
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: National Emergency Medical Services Information System
  has_relevant_organization:
  - B2AI_ORG:66
  is_open: true
  name: NEMSIS
  purpose_detail: Standard for the collection and transmission of emergency medical
    services (EMS) operations and patient care data.
  requires_registration: false
  url: https://nemsis.org/technical-resources/
- id: B2AI_STANDARD:226
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: National Patient-Centered Clinical Research Network Common Data Model
  formal_specification: https://pcornet.org/wp-content/uploads/2022/01/PCORnet-Common-Data-Model-v60-2020_10_221.pdf
  is_open: true
  name: PCORNet CDM
  purpose_detail: This guiding principle is expressed in the CDM design through prioritization
    of analytic functionality, and a parsimonious approach based upon analytic utility.
    At times, this results in decisions that are not based in relational database
    modeling principles such as normalization. The model is designed to facilitate
    routine and rapid execution of distributed complex analytics. To meet this design
    requirement, some fields are duplicated across multiple tables to support faster
    analytic operations for distributed querying. The PCORnet CDM is based on the
    FDA Mini-Sentinel CDM. This allows PCORnet to more easily leverage the large array
    of analytic tools and expertise developed for the MSCDM v4.0, including data characterization
    approaches and the various tools for complex distributed analytics.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:81
  url: http://pcornet.org/pcornet-common-data-model/
- id: B2AI_STANDARD:227
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Natural Collections Description standard
  is_open: true
  name: NCD
  purpose_detail: Natural Collections Description (NCD) (A data standard for exchanging
    data describing natural history collections) is a proposed data standard for describing
    collections of natural history materials at the collection level; one NCD record
    describes one entire collection. Collection descriptions are electronic records
    that document the holdings of an organisation as groups of items, which complement
    the more traditional item-level records such as are produced for a single specimen
    or a library book. NCD is tailored to natural history. It lies between general
    resource discovery standards such as Dublin Core (DC) and rich collection description
    standards such as the Encoded Archival Description (EAD). The NCD standard covers
    all types of natural history collections, such as specimens, original artwork,
    archives, observations, library materials, datasets, photographs or mixed collections
    such as those that result from expeditions and voyages of discovery.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:93
  url: https://www.tdwg.org/standards/ncd/
- id: B2AI_STANDARD:228
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: NCBI BioProject XML Schema
  formal_specification: https://www.ncbi.nlm.nih.gov/data_specs/schema/other/bioproject/
  has_relevant_organization:
  - B2AI_ORG:74
  is_open: true
  name: BioProject Schema
  publication: doi:10.1093/nar/gkr1163
  purpose_detail: 'The NCBI BioProject Schema is a comprehensive XML Schema Definition
    (XSD) that specifies the structure and validation rules for metadata describing
    biological research projects within NCBI''s BioProject database. A BioProject
    serves as an organizing principle for diverse experimental data types, providing
    a single accession identifier that links raw sequence reads (Sequence Read Archive),
    assembled genomes (GenBank), proteomics data, annotations, publications, and
    supplementary materials generated by a specific research initiative. The schema
    formally defines the metadata elements, data types, cardinalities, and controlled
    vocabularies required for representing project scope, organism information, experimental
    design, objectives, data types generated, participating organizations, funding
    sources, and publication references. This standardization enables consistent programmatic
    submission, retrieval, and computational analysis of project-level metadata across
    NCBI''s interconnected databases, supporting reproducible research and data reuse.
    The BioProject Schema is organized hierarchically with core elements including
    ProjectID (unique accession), ProjectDescr (descriptive metadata with title, description,
    relevance, objectives), ProjectType (classification as primary submission, umbrella,
    or peer-reviewed), Organism (taxonomic identifiers with NCBI Taxonomy ID bindings
    and strain/isolate details), ProjectTypeSubmission (methodology-specific elements
    for genome sequencing, metagenome, transcriptome, epigenome, proteome, targeted
    locus, or other experimental types), LocusTagPrefix (for genome annotation submission),
    Targets (genomic regions, genes, or variants of interest), Publications (PubMed
    ID cross-references), and ExternalLinks (resources hosted outside NCBI). Complex
    types and enumerated values enforce validation rules, ensuring submitted data
    meets minimal information standards. For example, the ''MethodType'' enumeration
    restricts sequencing methodologies to predefined categories (eSequencing, eAssembly,
    eAnnotation, etc.), while taxonomic identifiers must resolve to valid NCBI Taxonomy
    entries. The schema supports nested project relationships, enabling umbrella projects
    (coordinated multi-center initiatives or consortia) to aggregate related sub-projects
    while maintaining individual accessions and metadata granularity. Temporal elements
    capture project timelines, release dates, and data embargos, coordinating with
    NCBI''s controlled-access data policies (dbGaP) for human subject research.
    BioProject metadata plays a critical role in organizing multi-omics datasets and
    enabling integrative computational analyses. The schema''s structured representation
    of experimental intent, organism context, and data provenance facilitates federated
    searches across NCBI resources. For instance, a single BioProject accession might
    link whole-genome sequencing reads in SRA, assembled contigs in GenBank, RNA-Seq
    profiles in Gene Expression Omnibus (GEO), and variant calls in ClinVar, providing
    a unified entry point for researchers investigating the same biological system.
    NCBI''s Entrez query system indexes BioProject metadata fields, enabling complex
    queries such as "all bacterial genome projects funded by NIH from 2020-2023 with
    associated antimicrobial resistance phenotypes." Programmatic access through NCBI
    E-utilities (ESearch, EFetch, ELink) and Datasets API returns XML documents conforming
    to the BioProject Schema, which can be parsed using standard XML libraries (lxml
    in Python, xml2 in R) for downstream analysis. The schema''s formal validation
    ensures data quality at submission, reducing curation burden and improving machine-readability
    for automated pipelines.
    In AI and machine learning workflows, BioProject metadata serves multiple functions.
    The schema''s structured project descriptions, experimental design features, and
    data type annotations enable large-scale meta-analyses and dataset discovery for
    model training. Text mining applications extract organism names, methodologies,
    and objectives from BioProject titles and descriptions to classify research domains
    or predict experimental outcomes. Recommendation systems leverage project similarities
    (taxonomic overlap, methodology concordance, temporal proximity, publication co-authorship)
    to suggest relevant datasets or collaborators. Scientometric analyses use BioProject
    metadata to track research trends, funding allocation impacts, and reproducibility
    rates across domains (e.g., examining how many genome projects result in peer-reviewed
    publications or reusable annotations). Knowledge graphs integrate BioProject metadata
    with ontologies (Gene Ontology for functional annotations, Disease Ontology for
    clinical relevance), enabling semantic queries and hypothesis generation. For
    training cohort assembly in supervised learning, structured project metadata allows
    filtering by taxonomic scope (human vs model organism), data types (RNA-Seq vs
    whole-genome sequencing), experimental conditions (disease vs control), and sample
    sizes, ensuring datasets meet minimal requirements for statistical power and biological
    relevance. The schema''s temporal and versioning information supports longitudinal
    studies of data growth and model performance degradation over time. As a broadly
    adopted NCBI standard with over 1.2 million BioProject records spanning prokaryotes,
    eukaryotes, viruses, and metagenomes, it provides a rich corpus for training natural
    language processing models on scientific text and developing AI systems for automated
    experimental design and data integration in biological research.'
  requires_registration: false
  url: https://www.ncbi.nlm.nih.gov/data_specs/schema/other/bioproject/
  has_application:
  - id: B2AI_APP:199
    category: B2AI:Application
    name: XML Parsing for Thermostability Dataset Construction
    description: 'HotProtein (2022) parsed NCBI BioProject XML files to filter organisms
      by environmental metadata fields (OptimumTemperature, TemperatureRange) to construct
      a large-scale species-level thermostability dataset with 182,000 protein samples.
      The structured XML Schema enabled programmatic extraction of temperature annotations
      from BioProject records, which were used to derive lower-bound thermostability
      labels for training classification and regression models predicting protein
      thermal stability. The resulting HotProtein dataset powered deep learning models
      (ESM-1B with full-sequence training, augmentation, and structure-aware pooling)
      achieving Spearman correlations of 0.9060.010 and Pearson correlations of
      0.9230.012 on sequence-to-confidence benchmarks (HP-S2C2), with gains of 1.17%13.48%
      accuracy and 0.0160.474 Spearman correlation over baseline approaches. This
      application demonstrates how BioProject XML fields directly enable ML dataset
      curation by providing machine-readable organism-level environmental annotations
      that cannot be efficiently extracted from unstructured text sources.'
    references:
    - https://openreview.net/pdf?id=YDJRFWBMNby
    used_in_bridge2ai: false
  - id: B2AI_APP:200
    category: B2AI:Application
    name: BioProject ID Linkage for Multi-Study Dataset Assembly
    description: 'Webb et al. (2021) assembled multi-study RNA-seq training datasets
      by programmatically collecting SRA accessions grouped by BioProject identifiers,
      enabling cross-species age prediction and classification using machine learning.
      BioProject IDs served as the organizing principle for harmonizing expression
      data from independent sequencing initiatives, allowing the authors to discover
      conserved transcriptome signatures predicting chronological age across species.
      The structured project-level metadata in BioProject records (organism taxonomy,
      experimental type, data relationships) facilitated federated searches and programmatic
      batch retrieval of related SRA runs, reducing manual curation overhead and ensuring
      consistent data provenance. This workflow exemplifies how BioProject accessions
      function as stable linkage keys for assembling large-scale ML training corpora
      spanning multiple NCBI repositories (SRA, GEO, GenBank), supporting reproducible
      meta-analyses and transfer learning across biological domains where project-level
      context is critical for interpreting technical and biological variability.'
    references:
    - https://doi.org/10.5204/thesis.eprints.256582
    used_in_bridge2ai: false
  - id: B2AI_APP:201
    category: B2AI:Application
    name: NLP Topic Modeling with BioProject-Linked Publications
    description: 'Molik et al. (2021) programmatically mapped 1,380 SRA amplicon
      datasets to BioProject IDs and retrieved the associated journal articles, using
      those publication texts as the corpus for Natural Language Processing (NLP)
      workflows including Latent Dirichlet Allocation (LDA) topic modeling and Random
      Forest classification to infer pathogenenvironment associations for Cryptococcus
      neoformans. BioProject metadata served as the linkage mechanism connecting raw
      sequencing data (SRA FASTA records filtered via blastn_vdb with 98% identity
      and e-value 1e-140 thresholds) to peer-reviewed literature, enabling automated
      assembly of a text corpus for machine learning analysis. This approach leveraged
      BioProject''s role as a hub record connecting data repositories (SRA), taxonomic
      classifications, and publication references (PubMed IDs), demonstrating how
      structured project-level metadata enables large-scale literature mining for
      ecological niche prediction and scientometric analyses that would require extensive
      manual curation without programmatic BioProject access.'
    references:
    - https://doi.org/10.1371/journal.pntd.0008755
    used_in_bridge2ai: false
  - id: B2AI_APP:202
    category: B2AI:Application
    name: LLM-Assisted Metadata Curation via BioProject Submission Fields
    description: 'Zhao et al. (2023) developed the Search-based Geographic Metadata
      Curation (SGMC) pipeline that programmatically retrieves and parses BioProject
      submission fields (BP) alongside SRA and BioSample fields for 2.3 million accessions,
      extracting attributevalue pairs (e.g., submitting institution, center name)
      and using a generative AI model (ChatGPT) to refine geographic assignments (institution,
      country, coordinates) at scale. The pipeline accessed BioProject metadata via
      AWS Athena queries on s3://sra-pub-metadata-us-east-1 and web scraping of NCBI
      pages using Python (requests, BeautifulSoup, pandas), treating BioProject accessions
      as unique keys for linking submission metadata to sequencing records. SGMC outputs
      achieved high concordance with CDC manual curation, improving geographic metadata
      quality for public health surveillance and outbreak investigation. This application
      demonstrates how structured BioProject submission fields enable large-language-model-assisted
      curation workflows, combining programmatic metadata extraction with AI refinement
      to address missing or inconsistent geographic annotations in biomedical sequence
      repositories.'
    references:
    - https://doi.org/10.3389/fpubh.2023.1254976
    used_in_bridge2ai: false
  - id: B2AI_APP:203
    category: B2AI:Application
    name: NER Over BioProject Descriptions for Annotation Recovery
    description: 'Klie et al. (2021) proposed extending their deep learning Named
      Entity Recognition (NER) model, originally trained on BioSample TITLE fields,
      to BioProject descriptions as an additional free-text source for recovering
      missing sample metadata. The authors explicitly noted that BioProject descriptions
      and linked publications often contain study design and sample preparation details
      absent from BioSample records, suggesting that applying NER to BioProject free-text
      could capture annotations missing from TITLEs, verify TITLE-based predictions,
      or refine BioSample metadata. While the paper validated the model only on TITLE
      text, it identified BioProject descriptions as a logical target for extending
      NLP-based metadata enhancement pipelines, leveraging the schema''s unstructured
      text fields (ProjectDescr description, relevance, objectives) to improve metadata
      coverage via deep learning information extraction. This proposed application
      highlights how BioProject''s structured XML Schema can support hybrid curation
      approaches combining rule-based parsing of controlled fields with neural NER
      over free-text elements.'
    references:
    - https://doi.org/10.1093/database/baab021
    used_in_bridge2ai: false
- id: B2AI_STANDARD:229
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: NCPDP Formulary and Benefit Standard
  is_open: false
  name: NCPDP F&B
  purpose_detail: The NCPDP Formulary and Benefit Standard provides a standardized
    means for pharmacy benefit payers, including health plans and Pharmacy Benefit
    Managers (PBMs), to electronically communicate formulary and benefit information
    to prescribers via technology vendor systems. This standard enables real-time
    access to critical medication coverage information at the point of prescribing,
    including patient eligibility verification, product coverage status, benefit financial
    details (copayments, deductibles, out-of-pocket costs), coverage restrictions
    such as prior authorization requirements, step therapy protocols, quantity limits,
    and therapeutic alternatives when restrictions exist. By standardizing this data
    exchange, the standard reduces prescription abandonment, improves medication adherence,
    decreases prior authorization processing time, and enhances clinical decision-making
    by providing prescribers with transparent, actionable benefit information. The
    standard supports integration with Electronic Health Record (EHR) systems and
    e-prescribing platforms in accordance with HIPAA, MMA, HITECH, and Meaningful
    Use requirements, ultimately contributing to reduced healthcare costs and improved
    patient safety through more informed prescribing decisions.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:63
  url: https://standards.ncpdp.org/Access-to-Standards.aspx
- id: B2AI_STANDARD:230
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: NESTcc Data Quality Framework
  formal_specification: https://mdic.org/wp-content/uploads/2020/02/NESTcc-Data-Quality-Framework.pdf
  is_open: true
  name: NESTcc DQF
  purpose_detail: Guiding principles and a foundation for the capture and use of high-quality
    data for post-market evaluation of medical devices
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:64
  url: https://nestcc.org/data-quality-and-methods/
- id: B2AI_STANDARD:231
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Neurodata Without Borders
  formal_specification: https://github.com/NeurodataWithoutBorders
  is_open: true
  name: NWB
  publication: doi:10.1101/523035
  purpose_detail: Neurodata Without Borders (NWB) is a comprehensive data standard
    and software ecosystem for neurophysiology and behavioral data, developed by a
    collaborative team of neuroscientists and software developers to break down barriers
    to data sharing in neuroscience. NWB supports diverse neurophysiology data types
    including intracellular and extracellular electrophysiology (single-unit recordings,
    local field potentials, patch-clamp), optical physiology (calcium imaging, two-photon
    microscopy, optogenetics), behavioral tracking, stimulus presentation metadata,
    and experimental trial structures. The format is built on HDF5, providing efficient
    storage and access to large-scale datasets while maintaining human-readable metadata.
    NWB's extensibility mechanism through neurodata extensions allows researchers
    to adapt the standard for novel recording modalities and experimental paradigms
    without breaking compatibility. The ecosystem includes PyNWB and MatNWB APIs for
    data creation and access, validation tools, and integration with the DANDI Archive
    for public data sharing. NWB adoption is growing across major neuroscience initiatives
    with support from Allen Institute, HHMI, Kavli Foundation, Simons Foundation,
    and INCF, facilitating reproducible research, cross-laboratory data integration,
    and development of standardized analysis pipelines across cellular, systems, and
    computational neuroscience.
  requires_registration: false
  url: https://www.nwb.org/
- id: B2AI_STANDARD:232
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:22
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Neuroimaging Data Model
  has_relevant_organization:
  - B2AI_ORG:99
  is_open: true
  name: NIDM
  purpose_detail: The Neuroimaging Data Model (NIDM) is a collection of specification
    documents and examples that outline a domain specific extension to the W3C Provenance
    Data Model (PROV-DM) for the exchange and sharing of human brain imaging data.
    The goal of the data model is to capture data, information about the data and
    processes that generated the data (i.e. provenance). This information can be converted
    to RDF and therefore queried using SPARQL. This representation allows machine
    accessible representations of brain imaging data and will provide links to related
    resources such as publications, virtual machines, people and funding agencies.
  requires_registration: false
  url: http://nidm.nidash.org/
  has_application:
  - id: B2AI_APP:204
    category: B2AI:Application
    name: PyNIDM Direct Statistical and ML Operations on NIDM Documents
    description: 'PyNIDM provides a command-line linear regression tool (nidm_linreg)
      that operates directly on NIDM documents, supporting contrasts and regularization
      for statistical modeling before complex ML workflows. The Python toolbox enables
      researchers to query NIDM documents to find studies with comparable phenotypic
      variables, facilitating dataset harmonization, feature selection, and data pooling
      across studies for machine learning. PyNIDM''s query utilities interrogate
      datasets using PROV-based provenance and community terminologies, enabling transformation
      and combination of data that improves reproducibility and supports traceable
      feature lineage. By running high-level statistical analyses directly on standardized
      NIDM metadata and capturing computation/workflow graphs in NIDM documents, PyNIDM
      provides ML-ready inputs with complete provenance tracking suitable for integration
      into scikit-learn, PyTorch, and TensorFlow pipelines.'
    references:
    - https://pdfs.semanticscholar.org/2d12/4072e9751291590f041e24c631435859d159.pdf
    used_in_bridge2ai: false
  - id: B2AI_APP:205
    category: B2AI:Application
    name: ENIGMA-COINSTAC Federated Analysis with NIDM Output
    description: 'The COINSTAC (Collaborative Informatics and Neuroimaging Suite Toolkit
      for Anonymous Computation) platform automatically generates NIDM-formatted results
      for decentralized neuroimaging analyses, demonstrated initially for VBM (voxel-based
      morphometry) regression pipelines with efforts to generalize across arbitrary
      pipelines. COINSTAC runs local Dockerized R/Python analyses, uploads local results
      to a central node for meta-analysis without creating a centralized data repository,
      and captures rich semantic metadata and provenance in NIDM format to improve
      description of experiments, analyses, and results. The ENIGMA Organic Data Science
      platform creates a searchable index of NIDM-formatted analyses to facilitate
      method comparison and interoperability, with a planned API allowing external
      platforms to query COINSTAC datasets. This integration supports federated learning
      approaches by providing ML-ready, semantically rich outputs with complete provenance
      across distributed sites, enabling meta-analyses (e.g., Cohen''s d = 0.39,
      SE=0.18, p=0.03 for expressive deficit) and potential federated machine learning
      workflows where NIDM-formatted metadata ensures comparability.'
    references:
    - https://doi.org/10.1007/s12021-021-09559-y
    used_in_bridge2ai: false
  - id: B2AI_APP:206
    category: B2AI:Application
    name: NIDM REST API for Programmatic ML Pipeline Integration
    description: 'The NIDM API provides RESTful JSON query interfaces over NIDM documents,
      enabling programmatic access to structured neuroimaging metadata and results
      for automated integration into Python-based ML pipelines. The nidm-query executable
      downloads, validates, and executes queries identified by unique identifiers,
      returning JSON results suitable for direct ingestion into scikit-learn, PyTorch,
      TensorFlow, and other ML frameworks. A browser-based interface supports query
      creation, and Linked Open Data (LOD) capabilities enable semantic dataset discovery.
      This tooling facilitates automated dataset discovery, validation of metadata
      quality, and assembly of training cohorts with traceable provenance, allowing
      ML engineers to programmatically retrieve phenotypic variables, workflow descriptions,
      and derived results while maintaining PROV-based lineage information critical
      for reproducible AI research.'
    references:
    - https://doi.org/10.1186/s13742-016-0147-0
    used_in_bridge2ai: false
  - id: B2AI_APP:207
    category: B2AI:Application
    name: DABI AutoML Ecosystem with NIDM Metadata Standardization
    description: 'The Data Archive for the BRAIN Initiative (DABI) leverages NIDM
      for associated metadata and provenance standardization alongside BIDS (Brain
      Imaging Data Structure) to support an advanced AutoML ecosystem for supervised
      neuroimaging analyses. DABI implements H2O AutoML (R/Python) with multiple model
      types including General Linear Model, Random Forest, XGBoost, Gradient Boosting,
      Deep Learning, and Stacked Ensemble, with automated k-fold cross-validation,
      oversampling, variable importance plots, AUC measures, SHAP explainability,
      and partial dependence visualizations. NIDM-standardized metadata enables harmonized
      feature representation across studies, facilitating large-scale ML analyses
      such as predicting clinical outcomes from iEEG features (26 patients, preprocessed
      with wavelet decomposition and harmonized electrode/region features fed into
      ML models) and linear mixed-effects modeling (31 patients, PDQ8 scores decreasing
      with treatment, p=0.005). The standardized provenance and metadata framework
      allows scalable, reproducible ML workflows where NIDM documentation supports
      interpretability and regulatory compliance.'
    references:
    - https://doi.org/10.1038/s41597-023-01972-z
    - https://doi.org/10.1038/s41597-023-02614-0
    used_in_bridge2ai: false
  - id: B2AI_APP:208
    category: B2AI:Application
    name: ARIES Knowledge Graph with NIDM for ML Feature Engineering
    description: 'The Arkansas Imaging Enterprise System (ARIES) integrates semantic
      knowledge graphs with NIDM to represent neuroimaging workflows and provenance,
      explicitly supporting feature engineering for statistical and machine learning
      applications. ARIES uses NIDM to provide semantic representations of processing
      pipelines, describing software applications, processing steps, and derived neuroimaging
      results with machine-interpretable description logic definitions that support
      inference, analysis, and exploration. The knowledge graph combines multi-modal
      data with NIDM-standardized workflow metadata to enable reproducible feature
      extraction, where explicit semantics facilitate automated reasoning over processing
      lineage. ARIES stores executable processing pipelines in containers, and the
      NIDM-enhanced knowledge graph provides traceable provenance critical for regulatory-compliant
      ML research, supporting applications in precision medicine where understanding
      feature derivation paths is essential for model interpretability and validation.'
    references:
    - https://doi.org/10.3389/frai.2021.649970
    used_in_bridge2ai: false
  - id: B2AI_APP:209
    category: B2AI:Application
    name: NeuroBridge Deep Learning NLP for NIDM-Enhanced Dataset Discovery
    description: 'The NeuroBridge platform uses deep learning NLP methods to annotate
      neuroimaging articles with ontology terms for dataset discovery, with companion
      work reviewing NIDM-Experiment (NIDM-E) as part of provenance modeling supporting
      FAIR data discovery for assembling ML-ready cohorts. NeuroBridge creates embedding
      vectors for ontology terms using SapBERT (pre-trained biomedical language model)
      and compares them via ElasticSearch Cosine Similarity, matching user queries
      (e.g., "abstinent") to NeuroQuery terms (e.g., "abstinence") across ~13,000
      articles. The system extracted >4,400 assessment instrument names from the National
      Database for Autism Research (NDA) using Apache Solr textual similarity methods.
      The NeuroBridge computable provenance ontology (660 classes, 49 properties,
      3,200 axioms) includes mappings to W3C PROV and NIDM, representing the first
      computable metadata model for recent schizophrenia and substance-use neuroimaging.
      By integrating NIDM provenance standards with deep learning document annotation,
      NeuroBridge enables AI-driven discovery of long-tail neuroimaging datasets with
      rich metadata suitable for training cohort assembly and cross-study harmonization.'
    references:
    - https://doi.org/10.3389/fninf.2023.1215261
    - https://doi.org/10.3389/fninf.2023.1216443
    used_in_bridge2ai: false
  - id: B2AI_APP:210
    category: B2AI:Application
    name: Provenance-Based Recommender System for Dataset-Pipeline Compatibility
    description: 'A collaborative-filtering recommender system trained on provenance
      and execution records predicts datasetpipeline compatibility for neuroimaging
      analyses, outperforming expert predictions (AUC=0.83 vs expert AUC=0.63, chance=0.5).
      The system leverages standardized provenance frameworks like NIDM to capture
      features needed for ML-driven recommendations, including file format dependencies
      (e.g., FSL-FIRST requiring NIfTI while some datasets use MINC), pre-processing
      requirements (binary masks, external templates, prior pipeline outputs like
      bedpostx), and execution success/failure patterns. Analysis of 31% "file format
      not supported" failures, 38.5% "dataset not available" issues, and 18% Type
      D pre-processing dependencies demonstrates how NIDM-structured provenance enables
      the recommender to learn complex compatibility rules. At threshold 1.2, 8/10
      recommended pipelines successfully apply to new datasets. This application shows
      how NIDM''s machine-readable provenance representation enables AI systems to
      automate workflow planning, reduce failed executions, and improve reproducibility
      by predicting which analysis pipelines are compatible with available datasets
      based on learned patterns from historical execution traces.'
    references:
    - https://doi.org/10.1109/works54523.2021.00006
    used_in_bridge2ai: false
- id: B2AI_STANDARD:233
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:22
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Neuroimaging Informatics Technology Initiative file format
  formal_specification: https://nifti.nimh.nih.gov/nifti-2
  is_open: true
  name: NIFTI
  purpose_detail: The Neuroimaging Informatics Technology Initiative (nifti) file
    format was envisioned about a decade ago as a replacement to the then widespread,
    yet problematic, analyze 7.5 file format. The main problem with the previous format
    was perhaps the lack of adequate information about orientation in space, such
    that the stored data could not be unambiguously interpreted. Although the file
    was used by many different imaging software, the lack of adequate information
    on orientation obliged some, most notably spm, to include, for every analyze file,
    an accompanying file describing the orientation, such as a file with extension
    .mat.
  requires_registration: false
  url: https://nifti.nimh.nih.gov/
- id: B2AI_STANDARD:234
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: NeuroML
  is_open: true
  name: NeuroML
  purpose_detail: NeuroML is a model description language developed in XML (extensible
    Markup Language) that was created to facilitate data archiving, data and model
    exchange, database creation, and model publication in the neurosciences. One of
    the goals of the NeuroML project is to develop standards for model specification
    that will allow for greater simulator interoperability and model exchange.
  requires_registration: false
  url: https://neuroml.org/
- id: B2AI_STANDARD:235
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:48
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Neurophysiology Data Translation Format
  has_relevant_organization:
  - B2AI_ORG:11
  is_open: true
  name: NDF
  publication: doi:10.3389/conf.fnins.2010.13.00118
  purpose_detail: The purpose of the Neurophysiology Data Translation Format (NDF)
    is to provide a means of sharing neurophysiology experimental data and derived
    data between services and tools developed within the CARMEN project (www.carmen.org.uk).
    This document specifes the NDF. The specification supports the types of data that
    are currently used by members of the CARMEN consortium and provides a capability
    to support future data types. It is capable of accommodating external data file
    formats as well as metadata such as user defined experimental descriptions and
    the history (provenance) of derived data.
  requires_registration: false
  url: https://doi.org/10.3389/conf.fnins.2010.13.00118
- id: B2AI_STANDARD:236
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:26
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: New Hampshire eXtended Format
  is_open: true
  name: NHX
  purpose_detail: NHX is based on the New Hampshire (NH) standard (also called Newick
    tree format).
  requires_registration: false
  url: http://www.phylosoft.org/NHX/
- id: B2AI_STANDARD:237
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:26
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Newick tree Format
  is_open: true
  name: Newick
  purpose_detail: The Newick Standard for representing trees in computer-readable
    form makes use of the correspondence between trees and nested parentheses, noticed
    in 1857 by the famous English mathematician Arthur Cayley.
  requires_registration: false
  url: http://evolution.genetics.washington.edu/phylip/newicktree.html
- id: B2AI_STANDARD:238
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:25
  - B2AI_TOPIC:26
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: NeXML format
  formal_specification: https://github.com/nexml/nexml
  is_open: true
  name: NeML
  purpose_detail: To facilitate interoperability in evolutionary comparative analysis,
    we present NeXML, an XML standard (inspired by the current standard, NEXUS) that
    supports exchange of richly annotated comparative data. NeXML defines syntax for
    operational taxonomic units, character-state matrices, and phylogenetic trees
    and networks. Documents can be validated unambiguously. Importantly, any data
    element can be annotated, to an arbitrary degree of richness, using a system that
    is both flexible and rigorous. We describe how the use of NeXML by the TreeBASE
    and Phenoscape projects satisfies user needs that cannot be satisfied with other
    available file formats
  requires_registration: false
  url: https://github.com/nexml/nexml
- id: B2AI_STANDARD:239
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Nibble sequence format
  is_open: true
  name: nib
  purpose_detail: The nibble (.nib) format is a compact binary file format for storing
    DNA sequences, originally developed for the UCSC Genome Browser and BLAT alignment
    tool. The format achieves 4-fold compression by packing two nucleotide bases per
    byte using 2-bit encoding (A=00, C=01, G=10, T=11), significantly reducing storage
    requirements compared to text-based FASTA format. Each .nib file contains a single
    sequence record with a simple header specifying sequence length and format version,
    followed by the packed sequence data. The format stores sequences in a form optimized
    for rapid access by genome browsers and alignment algorithms, supporting efficient
    memory mapping for large-scale genomic analyses. While .nib files provide space-efficient
    storage, they lack the flexibility of indexed formats like 2bit (which can store
    multiple sequences) and have been largely superseded by more modern compressed
    formats. The format remains in use for legacy applications and continues to be
    supported by UCSC Genome Browser utilities including nibFrag for sequence extraction
    and faToNib/nibToFa for format conversion, primarily for maintaining compatibility
    with older genome browser implementations and analysis pipelines.
  requires_registration: false
  url: https://genomebrowser.wustl.edu/goldenPath/help/blatSpec.html
- id: B2AI_STANDARD:240
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:3
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: NMR Self-defining Text Archive and Retrieval format
  is_open: true
  name: NMR-STAR
  publication: doi:10.1007/s10858-018-0220-3
  purpose_detail: Format and ontology used to represent experiments, spectral and
    derived data, and supporting metadata.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:9
  url: https://bmrb.io/standards/
- id: B2AI_STANDARD:241
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:17
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: nmrML
  has_relevant_organization:
  - B2AI_ORG:21
  is_open: true
  name: nmrML
  purpose_detail: nmrML is an open mark-up language for NMR data. It is currently
    under heavy development and is not yet ready for public use. The development of
    this standard is coordinated by Workpackage 2 of the COSMOS - COordination Of
    Standards In MetabOlomicS Project. COSMOS is a global effort to enable free and
    open sharing of metabolomics data. Coordinated by Dr Christoph Steinbeck of the
    EMBL-European Bioinformatics Institute, COSMOS brings together European data providers
    to set and promote community standards that will make it easier to disseminate
    metabolomics data through life science e-infrastructures. This Coordination Action
    has been financed with 2 million by the European Commission's Seventh Framework
    Programme. The nmrML data standard will be approved by the Metabolomics Standards
    Initiative and was derived from an earlier nmrML that was developed by the Metabolomics
    Innovation Centre (TMIC).
  requires_registration: false
  url: https://nmrml.org/
- id: B2AI_STANDARD:242
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:25
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Observ-Tab format
  is_open: true
  name: Observ-Tab
  publication: doi:10.1002/humu.22070
  purpose_detail: Observ-Tab is a simple spreadsheet format to represent and exchange
    phenotype data.
  requires_registration: false
  url: https://doi.org/10.1002/humu.22070
- id: B2AI_STANDARD:243
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Observational Medical Outcomes Partnership Common Data Model
  formal_specification: https://github.com/OHDSI/CommonDataModel
  has_relevant_organization:
  - B2AI_ORG:115
  - B2AI_ORG:114
  has_training_resource:
  - B2AI_STANDARD:844
  is_open: true
  name: OMOP CDM
  publication: doi:10.3233/978-1-61499-564-7-574
  purpose_detail: The Observational Medical Outcomes Partnership (OMOP) Common Data
    Model is an open community data standard designed to standardize the structure
    and content of observational healthcare data from electronic health records, administrative
    claims, registries, and other sources to enable efficient, large-scale analyses
    that produce reliable evidence. OMOP CDM organizes patient-level data into standardized
    tables including person, visit, condition occurrence, drug exposure, procedure
    occurrence, measurement, observation, and device exposure, with relationships
    linking events to patients and care episodes. The model uses standardized vocabularies
    (SNOMED CT, RxNorm, LOINC, etc.) mapped through the OHDSI Vocabulary for semantic
    interoperability, ensuring consistent representation of clinical concepts across
    diverse healthcare systems. OMOP CDM supports longitudinal patient histories,
    enables reproducible network studies where the same analytic code runs on data
    from multiple institutions without sharing patient-level information, and provides
    the foundation for the OHDSI open-science community's analytical tools including
    cohort definition, characterization, population-level effect estimation, and patient-level
    prediction. The standardized schema facilitates federated learning, multi-site
    clinical research, comparative effectiveness studies, pharmacovigilance, and machine
    learning applications requiring harmonized features across heterogeneous EHR
    systems for training predictive models, phenotyping algorithms, and decision support
    tools.
  related_to:
  - B2AI_STANDARD:733
  - B2AI_STANDARD:692
  - B2AI_STANDARD:695
  - B2AI_STANDARD:698
  - B2AI_STANDARD:703
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:76
  url: https://ohdsi.github.io/CommonDataModel/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:42
    category: B2AI:Application
    name: Multi-Database Patient-Level Risk Prediction with External Validation
    description: OMOP-standardized features and the OHDSI Patient-Level Prediction
      (PLP) pipeline enabled development and extensive external validation of clinical
      risk models across international sites. A Lasso logistic regression model predicting
      symptomatic hemorrhagic transformation after ischemic stroke was developed
      on OMOP-mapped EHR and externally validated across 10 databases spanning the
      US, Europe, and Asia (internal AUC 0.75; mean external AUC approximately 0.71,
      range 0.60-0.78). OMOP's standardized covariate definitions and PLP tooling
      enabled identical feature extraction and cross-site code portability for reproducible,
      multi-database risk modeling.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1371/journal.pone.0226718
  - id: B2AI_APP:146
    category: B2AI:Application
    name: Federated Diabetes Heart Failure Risk Modeling
    description: OMOP CDM enabled federated patient-level prediction for 1-year incident
      heart failure risk in type 2 diabetes patients across five US databases. Shared
      cohort and covariate definitions in OMOP, combined with OHDSI tools, enabled
      reproducible distributed model development and validation using multiple classifiers
      (Lasso logistic regression, Random Forest, Gradient Boosting, XGBoost), achieving
      external AUCs of approximately 0.72-0.80 across validation sites. OMOP's standardization
      facilitated consistent model portability without patient-level data sharing.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1371/journal.pone.0226718
  - id: B2AI_APP:147
    category: B2AI:Application
    name: Hospital Length-of-Stay Prediction with Explainable AI
    description: OMOP v5.3-standardized features from condition, drug, procedure,
      and measurement tables supported operational length-of-stay prediction for
      planned hospital admissions using gradient-boosting methods (XGBoost, LightGBM)
      with SHAP explainability. A single-site OMOP implementation (South Korea) achieved
      internal AUROC up to 0.891, with external validation at a separate OMOP-mapped
      hospital yielding AUROC approximately 0.804. OMOP's standardized feature representation
      allowed reproducible training and external testing for operational forecasting.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2024.08.23.24311950
  - id: B2AI_APP:148
    category: B2AI:Application
    name: Portable NLP-Based Clinical Phenotyping
    description: A portable NLP phenotyping system stored NLP outputs and rule artifacts
      in OMOP tables (notes and annotation mapping) to enable cross-institutional
      reuse. The system combined rule-based extractions with statistical machine
      learning classifiers for phenotype identification (e.g., obesity and comorbidities)
      and demonstrated competitive performance on i2b2 challenge discharge summaries.
      OMOP's common schema for text-derived concepts enabled portable NLP pipelines
      and downstream machine learning for cohort discovery and trial recruitment
      across multiple sites.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1016/j.jbi.2023.104343
  - id: B2AI_APP:149
    category: B2AI:Application
    name: Clinical Knowledge Graphs for Explainable AI
    description: OMOP CDM served as the data source for constructing clinical knowledge
      graphs in FHIR RDF format (FHIR-Ontop-OMOP) to support explainable AI workflows.
      OMOP's standardized vocabularies and relational structure enabled consistent
      semantic linking and query over patient-level data, transforming OMOP tables
      into RDF knowledge graphs that provide semantic features and enable explainable
      AI by linking standardized clinical concepts through ontological relationships.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2024.08.23.24311950
  - id: B2AI_APP:150
    category: B2AI:Application
    name: Process Mining of Clinical Workflows
    description: Methods to derive event logs from OMOP tables (visit, procedure,
      measurement) enabled process mining of inpatient, outpatient, and emergency
      workflows and patient care pathways. Real-world surgical cases at a tertiary
      hospital were analyzed to construct clinical pathway models, and artificial
      neural networks were demonstrated for pathway variance prediction. OMOP CDM
      provided a reproducible source for process-aware analytics and downstream predictive
      tasks from standardized event sequences.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2024.08.23.24311950
  - id: B2AI_APP:151
    category: B2AI:Application
    name: Imaging AI Enablement via MI-CDM Extension
    description: The Medical Imaging CDM (MI-CDM) extends OMOP with imaging metadata
      and feature-provenance tables to link DICOM data and imaging-derived biomarkers
      with clinical OMOP data, enabling multimodal phenotyping and imaging AI workflows.
      A prototype use case demonstrated longitudinal CT lung nodule tracking, and
      implementations for prostate cancer research (ProCAncer-I) captured imaging
      metadata and curation processes. MI-CDM makes image-derived features computable
      within OMOP, supporting reproducible imaging AI pipelines and phenotype definitions
      that include imaging biomarkers.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1007/s10278-024-00982-6
  - id: B2AI_APP:152
    category: B2AI:Application
    name: Cross-Country Model Generalizability and Feature Selection
    description: OMOP-standardized features from EHRs mapped across the US, UK, Finland,
      and Korea enabled cross-site feature evaluation to improve external validity
      of prolonged opioid use prediction after surgery. Independent cross-site feature
      selection workflows using Lasso logistic regression improved generalizability,
      with local AUROC approximately 0.75 and averaged external AUROC approximately
      0.69 after cross-site feature selection. OMOP's consistent feature representation
      enabled generalizable machine learning across countries and healthcare systems.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2024.08.23.24311950
  - id: B2AI_APP:153
    category: B2AI:Application
    name: Oncology-Specific AI with Genomic and Imaging Vocabularies
    description: OMOP oncology extensions incorporating genomic vocabularies (ClinVar,
      CIVic, OncoKB), HemOnc chemotherapy regimen vocabularies, and radiology CDM
      (R-CDM) with RadLex-mapped imaging tables enable cancer-specific AI applications.
      Use cases include case identification from clinical notes using support vector
      machines and tree-based models, predictive modeling with the ATLAS Patient-Level
      Prediction module on genomically-enriched OMOP data, and standardized imaging-AI
      workflows. OMOP's oncology-specific vocabularies and modules facilitate AI
      model development for precision oncology across harmonized multicenter datasets.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.3390/ijms231911834
    - https://doi.org/10.1101/2024.08.23.24311950
- id: B2AI_STANDARD:244
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Observational Medical Outcomes Partnership Common Evidence Model
  formal_specification: https://github.com/OHDSI/CommonEvidenceModel
  has_training_resource:
  - B2AI_STANDARD:844
  is_open: true
  name: OMOP CEM
  publication: doi:10.1007/s40264-014-0189-0
  purpose_detail: 'The Observational Medical Outcomes Partnership (OMOP) Common Evidence
    Model (CEM) is a structured knowledge base developed by the Observational Health
    Data Sciences and Informatics (OHDSI) consortium that aggregates, harmonizes,
    and standardizes evidence about drug safety and effectiveness from multiple heterogeneous
    sources including spontaneous adverse event reporting systems (FDA FAERS, EudraVigilance),
    product labeling documents (FDA SPL, EMA product information), clinical trial
    registries (ClinicalTrials.gov), and published biomedical literature (PubMed,
    Medline). CEM normalizes terminologies using OMOP Vocabulary (RxNorm for drugs,
    SNOMED CT for conditions/outcomes, MedDRA for adverse events) and represents evidence
    as standardized relationships between exposure concepts (medications, procedures)
    and outcome concepts (diseases, adverse events, clinical findings) with provenance
    metadata capturing evidence source, publication date, study design characteristics,
    and signal strength indicators. This harmonization enables programmatic querying
    of cross-source evidence patterns via RESTful APIs (CemConnector package) that
    return "evidence exists" indicators, co-occurrence frequencies, and evidence rankings
    for concept sets expressed as SNOMED or RxNorm code combinations. CEM serves as
    critical infrastructure for automated negative control selection in observational
    studies: negative controls are exposureoutcome pairs known not to have causal
    relationships (e.g., beta-blockers and fractures), identified by querying CEM
    to find candidate pairs lacking evidence in product labels, literature, and spontaneous
    reports, then validating absence through expert review or classifier-based prediction.
    These algorithmically selected negative controls construct null distributions
    of effect estimates under the assumption of no true effect, enabling empirical
    calibration that adjusts p-values and confidence intervals to account for systematic
    error in observational study designs (propensity score matching, self-controlled
    cohort, case-control), substantially reducing false discovery rates (from ~60%
    to ~7% in large-scale studies) while preserving sensitivity for true associations.
    CEM supports high-throughput pharmacoepidemiologic workflows through integration
    with OHDSI analytic tools: cohort generation (Atlas), effect estimation (CohortMethod,
    SelfControlledCaseSeries packages), and large-scale evidence generation pipelines
    (REWARD framework) that execute thousands of exposureoutcome analyses across
    OMOP CDM-mapped claims databases with automated negative control selection, effect
    calibration, and result aggregation. For AI and machine learning applications,
    CEM enables automated workflows where ML classifiers predict unlikely exposureoutcome
    pairs for negative control curation by learning from CEM-derived features (evidence
    source counts, co-occurrence patterns, temporal relationships), programmatic APIs
    allow ML pipelines to query CEM indicators as input features or filtering criteria
    for training dataset assembly, evidence rankings support active learning strategies
    prioritizing high-uncertainty pairs for expert annotation, and standardized evidence
    representations facilitate meta-learning across pharmacovigilance signal detection
    tasks by providing consistent feature encodings for transfer learning between
    drug classes or therapeutic areas, ultimately enabling scalable, reproducible,
    and transparent generation of real-world evidence for regulatory decision-making
    (FDA Sentinel, EMA DARWIN EU), comparative effectiveness research, and clinical
    guideline development.'
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:76
  url: https://github.com/OHDSI/CommonEvidenceModel/wiki/Postprocessing-Negative-Controls
  has_application:
  - id: B2AI_APP:211
    category: B2AI:Application
    name: Automated Negative Control Selection with ML Classification
    description: 'CEM enables automated negative control selection for empirical calibration
      in large-scale observational studies by aggregating evidence from spontaneous
      reports, drug labels, clinical trials, and literature to identify and rank candidate
      negative controls (exposureoutcome pairs unlikely to have causal relationships).
      One strategy explicitly uses CEM-derived features (evidence source indicators,
      co-occurrence frequencies, temporal patterns) within a machine learning classifier
      to predict unlikely exposureoutcome pairs, supporting a human-in-the-loop curation
      workflow. Applied within the REWARD framework across four US claims databases,
      automated CEM-based selection generated thousands of controls per drug (3,522
      for sertraline, 2,921 for diclofenac) and substantially reduced false discovery
      rates after empirical calibration (mean FDR ~6.8% vs ~60.6% uncalibrated) while
      maintaining comparable performance to manually curated controls. This application
      demonstrates how CEM''s structured evidence representations enable ML-assisted
      null distribution construction for bias correction in pharmacoepidemiologic
      effect estimation at scale.'
    references:
    - https://www.ohdsi.org/wp-content/uploads/2021/09/78-james-gilbert-abstract.pdf
    - https://www.ohdsi.org/wp-content/uploads/2019/09/ERICA-VOSS_Assessing-Negative-Control-Exposure-Outcome-Pair-Selection-Strategies-on-a-Replication-Study_2019Symposium.pdf
    used_in_bridge2ai: false
  - id: B2AI_APP:212
    category: B2AI:Application
    name: CemConnector Programmatic API for ML Pipeline Integration
    description: 'CemConnector provides a RESTful API and R client library enabling
      programmatic access to OMOP CEM for machine learning and automation pipelines.
      The tool allows ML systems to query CEM and retrieve "evidence exists" indicators
      for concept set expressions (SNOMED, RxNorm), rank candidate negative controls
      based on co-occurrence frequencies in CDM-compliant databases, and access formal
      selection criteria for robust empirical calibration. CemConnector supports high-throughput
      workflows through a database interface suitable for large-scale queries and
      includes an Evidence Explorer Shiny plugin for interactive exploration. By exposing
      CEM data as consumable REST endpoints returning structured JSON, CemConnector
      enables ML pipelines to use evidence indicators as features, filters, or ranking
      signals in automated dataset assembly, feature engineering for pharmacovigilance
      models, and active learning workflows where evidence patterns guide which exposureoutcome
      pairs require expert annotation. The open-source R client (GitHub: OHDSI/CemConnector)
      integrates with OHDSI analytic packages, making CEM a standard component of
      reproducible, automated real-world evidence generation.'
    references:
    - https://www.ohdsi.org/wp-content/uploads/2021/09/63-james-gilbert-abstract.pdf
    used_in_bridge2ai: false
  - id: B2AI_APP:213
    category: B2AI:Application
    name: REWARD High-Throughput Algorithmic Effect Estimation Pipeline
    description: 'REWARD (REal World Assessment and Research of Drugs) is an open-source,
      fully automated OHDSI-based pipeline that integrates CEM for large-scale population-level
      effect estimation across thousands of exposureoutcome pairs. The system programmatically
      selects negative controls using CEM via CemConnector, executes self-controlled
      cohort (SCC) analyses with automated outcome definitions (SNOMED-based), generates
      observed effect estimates, and performs empirical calibration using CEM-derived
      null distributions. Built on R targets workflow engine with R Shiny exploration
      interfaces, REWARD executed 197,032,729 observed effect estimates across 3,384
      exposures and 66,331 outcome definitions, demonstrating machine-driven evidence
      generation at unprecedented scale. The pipeline centralizes aggregated results,
      supports flexible automated execution, and provides cohort references for reproducibility.
      REWARD exemplifies how CEM enables end-to-end algorithmic pharmacoepidemiology:
      automated control selection  high-throughput effect estimation  systematic
      bias correction  aggregated evidence synthesis, with complete GitHub availability
      (OHDSI/REWARD) supporting reproducible, transparent AI-augmented observational
      research workflows.'
    references:
    - https://www.ohdsi.org/wp-content/uploads/2022/10/15-Gilbert-REWARD-abstract.pdf
    used_in_bridge2ai: false
  - id: B2AI_APP:214
    category: B2AI:Application
    name: Large-Scale Comparator Recommendation via Cohort Similarity
    description: 'An OMOP CDM-based empirical method uses cohort similarity scoring
      to recommend and rank comparator drugs for pharmacoepidemiologic studies at
      scale, with CEM facilitating identification of inactive comparators and negative
      controls for validation. The algorithm computes cohort similarity scores by
      averaging cosine similarities across five covariate domains (demographics, medical
      history, prior medications, presentation, visit context) using prevalence vectors
      from new-user cohorts (1,000 patients, 365 days prior observation). Applied
      across multiple OMOP data sources, the method generated 487,578922,761 cohort
      pairs per database, with 9095% of target cohorts having comparators with similarity
      scores 0.950. An interactive web application (data.ohdsi.org/ComparatorSelectionExplorer)
      and open-source code (GitHub: OHDSI/ComparatorSelectionExplorer) enable programmatic
      use in automated evidence-generation workflows. While comparator ranking uses
      covariate similarity rather than direct CEM signals, CEM provides supporting
      infrastructure for validating recommendations through negative control identification,
      demonstrating how CEM integrates with broader algorithmic evidence ecosystems
      for scalable, data-driven treatment comparison identification aligned with subject-matter
      knowledge.'
    references:
    - https://doi.org/10.1101/2023.02.14.23285755
    used_in_bridge2ai: false
- id: B2AI_STANDARD:245
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Open Biomedical Ontology Flat File Format
  formal_specification: https://owlcollab.github.io/oboformat/doc/obo-syntax.html
  is_open: true
  name: OBO
  purpose_detail: The Open Biomedical Ontology (OBO) Flat File Format is a human-readable,
    line-oriented syntax for representing biomedical ontologies that originated with
    the Gene Ontology and has become a widely adopted standard within the OBO Foundry
    community. OBO format provides a simplified, tag-value structure for defining
    ontology terms, their hierarchical relationships (is_a), logical definitions,
    synonyms, cross-references, and metadata, with each term enclosed in a [Term]
    stanza containing fields like id, name, namespace, def (text definition with
    citations), and relationship tags. The format has a defined mapping to OWL (Web
    Ontology Language), allowing OBO files to be converted to OWL/RDF for semantic
    web applications while maintaining a more accessible syntax for biologists and
    curators. OBO format supports rich relationship types beyond simple hierarchies
    (part_of, regulates, develops_from), structured synonym types (exact, broad,
    narrow, related), obsoletion workflows with replaced_by and consider tags, and
    cross-references to external databases. The format is designed for version control
    systems (line-oriented changes), supports modular ontology development through
    import statements, and enables community-driven collaborative ontology construction.
    OBO format files are processed by standard tools (ROBOT, Owltools, OWL API with
    OBO parser) and underpin hundreds of biomedical ontologies including GO, Uberon,
    ChEBI, Disease Ontology, and Cell Ontology. The format balances human readability
    for manual curation with machine-parseability for computational workflows, semantic
    reasoning, data annotation, and integration into knowledge graphs supporting AI/ML
    applications in biomedicine.
  requires_registration: false
  url: https://owlcollab.github.io/oboformat/doc/GO.format.obo-1_4.html
- id: B2AI_STANDARD:246
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:18
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Open mHealth
  formal_specification: https://github.com/openmhealth
  has_relevant_organization:
  - B2AI_ORG:114
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: Open mHealth
  purpose_detail: Open mHealth (OMH) is a comprehensive ecosystem of data standards,
    schemas, and tools designed to make patient-generated health data from mobile
    devices, wearables, and sensors interoperable, meaningful, and actionable across
    healthcare and research applications. The OMH data standards provide JSON-based
    schemas for over 75 common health data types including physical activity (steps,
    distance, calories burned), vital signs (heart rate, blood pressure, body temperature),
    sleep metrics, body composition, medications, survey responses, and environmental
    data. Each schema includes metadata for data provenance (acquisition source, temporal
    relationship, user notes) enabling proper contextualization and interpretation.
    OMH tools include Shimmer for integrating data from diverse sources (Fitbit, Apple
    HealthKit, Google Fit, Withings, iHealth), libraries for data validation and storage,
    visualization components for detecting health patterns, and FHIR integration capabilities
    for aligning mobile health data with electronic health records. The platform supports
    diverse use cases including randomized controlled trials with standardized mobile
    data collection, remote patient monitoring programs, n-of-1 clinical trial analyses,
    machine learning algorithm development on normalized health datasets, and patient-reported
    outcomes integrated with biometric data. OMH is used by over 6,000 developers
    and health organizations including Cornell Tech, Kaiser Permanente, Stanford Medicine,
    UCSF, and Copenhagen Center for Health Technology, enabling research reproducibility
    and clinical care integration by transforming heterogeneous mobile health data
    into a unified, queryable format.
  requires_registration: false
  url: https://www.openmhealth.org/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:44
    category: B2AI:Application
    name: Mobile Health Data Integration for Behavioral AI
    description: Open mHealth schemas are used in AI applications for standardizing
      and integrating data from wearable devices, mobile health apps, and patient-generated
      health data streams for behavioral pattern recognition, activity classification,
      and health prediction models. Machine learning systems leverage Open mHealth's
      JSON-based data schemas to process heterogeneous data from fitness trackers,
      sleep monitors, medication adherence apps, and symptom tracking tools, enabling
      AI models for real-time health monitoring, early disease detection, and personalized
      intervention recommendations. The standardized format facilitates training of
      deep learning models on multi-modal time-series data including heart rate, physical
      activity, sleep patterns, and self-reported symptoms, supporting applications
      in chronic disease management, mental health monitoring, and precision behavioral
      medicine.
    used_in_bridge2ai: false
  - id: B2AI_APP:167
    category: B2AI:Application
    name: LSTM Time-Series Prediction for Athlete Readiness Monitoring
    references:
    - https://doi.org/10.1145/3395035.3425300
    description: Open mHealth data standards are used in sports analytics systems
      for storing and processing athlete self-reported data as training datasets for
      LSTM-based prediction models. In the PMSys elite soccer analytics platform,
      a Reporter App synchronized player self-reported "readiness" assessments to
      an OMH-compliant Data Storage Unit (DSU) as Open mHealth JSON datapoint objects,
      which served as the training dataset for an LSTM model predicting next-day athlete
      readiness. The implementation used a 4-layer LSTM architecture (input layer,
      2 hidden layers, output layer) with sequence length of 36, trained for 30 epochs
      with batch size 4 using the rmsprop optimizer. Data were collected from 19 elite
      soccer players over January-August, with two training regimes evaluated. Training
      on a single athlete's data proved insufficient for reliable predictions, while
      training on team-wide data yielded substantially more accurate predictions with
      clearly visible peaks in readiness patterns. This application demonstrates how
      OMH's standardized JSON datapoint format enables reproducible time-series machine
      learning workflows in sports science, where longitudinal self-reported assessments
      combined with team-level data aggregation support predictive modeling for performance
      optimization and injury prevention.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:247
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:19
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Open Microscopy Environment TIFF specification
  is_open: true
  name: OME-TIFF
  purpose_detail: OME-TIFF (Open Microscopy Environment TIFF) is a file format specification
    that combines standard TIFF image files with OME-XML metadata embedded in the
    TIFF header, enabling storage of complex multi-dimensional microscopy datasets
    including multi-channel fluorescence, z-stacks, time series, and multi-position
    acquisitions. Each OME-TIFF file contains one or more standard TIFF images with
    rich microscopy metadata encoded as OME-XML in the ImageDescription tag of the
    first IFD (Image File Directory), describing acquisition parameters, instrument
    settings, channel information, dimensions, physical pixel sizes, timestamps, and
    experimental context. The format supports large datasets by allowing data to span
    multiple TIFF files while maintaining metadata consistency through the OME-XML
    master file that references all constituent files. OME-TIFF balances the advantages
    of widespread TIFF support in image processing software with the semantic richness
    of OME-XML metadata, making microscopy data accessible to both OME-aware applications
    (Bio-Formats, OMERO, ImageJ/Fiji) and standard image viewers. This dual compatibility
    facilitates data sharing, long-term archiving, interoperability across microscopy
    platforms, and integration into computational workflows including machine learning
    pipelines for image analysis, segmentation, and feature extraction in biological
    imaging applications.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:77
  url: https://docs.openmicroscopy.org/ome-model/5.6.3/ome-tiff/#
- id: B2AI_STANDARD:248
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:19
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Open Microscopy Environment XML format
  is_open: true
  name: OME-XML
  publication: doi:10.1186/gb-2005-6-5-r47
  purpose_detail: OME-XML (Open Microscopy Environment XML) is a comprehensive XML-based
    file format for storing both microscopy image pixels and associated metadata using
    the OME Data Model, a rich schema that describes biological imaging experiments
    with semantic precision. The format encodes multi-dimensional image data (x, y,
    z, channel, time) as Base64-encoded binary pixel arrays within XML elements, alongside
    extensive metadata including instrument configuration (microscope, objectives,
    detectors, light sources), acquisition parameters (exposure times, wavelengths,
    filters), experimental context (annotations, regions of interest, overlays), specimen
    information, and structured annotations. OME-XML uses a controlled vocabulary
    and hierarchical schema (defined by XSD) that ensures consistent representation
    of microscopy concepts across diverse imaging modalities including widefield,
    confocal, super-resolution, high-content screening, and light-sheet microscopy.
    The format serves as the foundation for OME-TIFF and is the native format for
    Bio-Formats library, enabling interoperability across proprietary microscope file
    formats through standardized conversion. OME-XML supports FAIR principles by providing
    rich, machine-readable metadata essential for data sharing, long-term preservation,
    reproducible analysis, and integration into computational workflows including
    image processing pipelines and machine learning applications requiring comprehensive
    contextual information about imaging experiments.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:77
  url: https://docs.openmicroscopy.org/ome-model/5.6.3/ome-xml/
- id: B2AI_STANDARD:249
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:27
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Open Modeling EXchange format
  formal_specification: http://co.mbine.org/specifications/omex.version-1.pdf
  is_open: true
  name: OMEX
  purpose_detail: OMEX (Open Modeling EXchange format) is a ZIP-based container file
    format developed by the COMBINE (Computational Modeling in Biology Network) community
    for bundling all components of a computational modeling and simulation experiment
    into a single, self-contained, exchangeable archive. An OMEX archive contains
    model descriptions (SBML, CellML, NeuroML, etc.), simulation experiment specifications
    (SED-ML for defining simulation protocols, analysis steps, and visualization),
    associated data files (initial conditions, parameters, experimental observations),
    metadata (OMEX Metadata describing provenance, authorship, annotations using RDF),
    and a manifest file that catalogs all contents with their roles and formats. The
    format ensures reproducibility by packaging model structure, simulation configuration,
    analysis workflows, and contextual information together, enabling complete recreation
    of computational experiments across different simulation tools and platforms.
    OMEX supports FAIR principles for computational models through standardized packaging,
    facilitates model exchange and reuse across the systems biology and computational
    physiology communities, enables archiving in model repositories (BioModels, PMR),
    and provides the foundation for reproducible in silico experiments essential for
    model validation, parameter estimation, and integration into larger computational
    workflows including systems biology pipelines and machine learning applications
    for biological modeling.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:19
  url: https://github.com/combine-org/combine-specifications/blob/main/specifications/omex.md
- id: B2AI_STANDARD:250
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Open-Source Integrated Clinical Environment Standard
  formal_specification: https://github.com/mdpnp/mdpnp
  is_open: true
  name: OpenICE
  purpose_detail: OpenICE is an initiative to create a community implementation of
    an Integrated Clinical Environment. The initiative encompasses not only software
    implementation but also an architecture for a wider clinical ecosystem to enable
    new avenues of clinical research. OpenICE seeks to integrate an inclusive framework
    of healthcare devices and clinical applications to existing Healthcare IT ecosystems.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:54
  url: https://www.openice.info/
- id: B2AI_STANDARD:251
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: openEHR Archetype Object Model
  formal_specification: https://specifications.openehr.org/releases/AM/latest/AOM1.4.html
  is_open: true
  name: AOM14
  purpose_detail: 'The openEHR Archetype Object Model (AOM) 1.4 provides the definitive
    formal specification of archetype semantics as an object-oriented model enabling
    software to process, validate, and manipulate clinical archetypes independent
    of their serialization format (ADL, XML, JSON, or other representations). AOM
    defines constraint-based domain modeling through hierarchical structures alternating
    between object constraints (C_COMPLEX_OBJECT representing archetyped classes with
    node identifiers, occurrences, and attribute constraints) and attribute constraints
    (C_ATTRIBUTE specifying cardinality, existence, and child object constraints),
    supporting multiple reference model types (openEHR Reference Model, HL7 RIM, ISO
    13606) through abstract constraint classes. The model enables archetype specialization
    via differential pathways with explicit depth tracking and node redefinition,
    composition through archetype slots (ARCHETYPE_SLOT) with assertion-based inclusion/exclusion
    constraints evaluated against slot-filler archetypes, and primitive type constraints
    (C_DATE with pattern/range/timezone constraints, C_TIME, C_STRING with regex patterns,
    C_INTEGER/C_REAL with intervals, C_BOOLEAN, C_TERMINOLOGY_CODE with terminology
    bindings to SNOMED CT/LOINC/ICD-10) ensuring data validity at acquisition. AOM''s
    internal referencing mechanisms (ARCHETYPE_INTERNAL_REF for reusing constraint
    structures within an archetype, CONSTRAINT_REF for referencing primitive constraint
    definitions) support DRY principles, while first-order predicate logic assertions
    (ASSERTION class with boolean expressions over archetype paths) enable cross-field
    validation rules (e.g., systolic > diastolic in blood pressure archetypes). The
    terminology integration section maps archetype node identifiers to external terminologies,
    supporting multilingual natural language descriptions and enabling semantic interoperability
    across clinical systems. AOM serves as the semantic foundation for archetype-enabled
    EHR kernels, ADL parsers (including grammar validation and semantic checking),
    archetype editors (clinical modeling tools like the openEHR Clinical Knowledge
    Manager), template designers (constraining archetypes for local use cases), and
    form generators (automatically rendering data entry interfaces from archetype
    constraints). For AI and machine learning applications in healthcare, AOM-grounded
    archetypes provide machine-readable semantic constraints that structure clinical
    NLP pipelines: archetype metadata (data types, value ranges, terminology bindings,
    multilingual descriptions) guide extraction targets and validation rules for entity
    recognition, archetype-specific language models trained on archetype field descriptions
    improve disambiguation of clinical narratives, and archetype paths serve as feature
    hierarchies for structured representation learning. Archetype constraints enable
    active learning workflows where models query users only when extracted values
    violate archetype rules (e.g., impossible vital sign ranges), reducing annotation
    burden. Archetype-driven data standardization pipelines map unstructured clinical
    texts to archetype templates using NLP and rule-based mappers, creating FAIR-compliant
    repositories suitable for federated learning and cross-institutional ML studies.
    Clinical decision support systems query archetype-modeled data via AQL (Archetype
    Query Language) to supply standardized facts to inference engines, and word embedding
    models improve archetype retrieval and reuse by expanding search queries with
    learned synonyms, ultimately enabling reproducible, interoperable, and semantically
    rich AI workflows across heterogeneous EHR systems while preserving clinical meaning
    and data quality through formal constraint models.'
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:79
  url: https://specifications.openehr.org/releases/AM/latest/AOM1.4.html
  has_application:
  - id: B2AI_APP:215
    category: B2AI:Application
    name: Archetype-Driven NLP for Numeric Entity Extraction
    description: 'A proof-of-concept system uses openEHR archetypes (AOM 1.4-grounded)
      to drive machine learning for automated extraction of numerical values from
      clinical narratives through archetype-specific language models and constraint-guided
      mapping. The approach combines regex-based number detection with archetype metadata
      (data types, value constraints, multilingual descriptions, term bindings, autogenerated
      abbreviations) to identify extraction targets, and employs an active-learning
      recurrent neural network classifier with fastText word/character embeddings
      for ambiguous value-to-field assignments. Character embeddings handle synonyms,
      hard abbreviations (T for temperature), sequence patterns (120/90 after blood
      pressure title), and typos (Blod pressure). Evaluated on three openEHR archetypes
      (blood pressure, body temperature, pulse) across 75 English and 75 German clinical
      documents, the system achieved per-archetype precision up to 1.00 and recall
      up to 1.00 (blood pressure English: precision 0.90, recall 1.00; German: precision
      0.93, recall 1.00; body temperature English: precision 1.00, recall 0.96), demonstrating
      how AOM semantic constraints simplify text mining and enable sharable archetype-driven
      extraction models via openEHR Clinical Knowledge Manager.'
    references:
    - https://doi.org/10.3233/shti190820
    used_in_bridge2ai: false
  - id: B2AI_APP:216
    category: B2AI:Application
    name: Archetype-Grounded NLP Pipeline for Standardizing Unstructured Histories
    description: 'An openEHR-based NLP pipeline (LingRep) extracts and standardizes
      unstructured pediatric ICU admission histories into archetype-based structured
      representations using pattern-based components and archetype-constrained mapping.
      The pipeline uses a German expert dictionary (3,055 text marker entries, 132
      text events, 66 regular expressions, 776-entry spell correction corpus) to extract
      clinical snippets, then a mapping module assigns extracted content to specific
      items within openEHR archetypes and templates, storing outputs via openEHR REST
      API and verifying with AQL queries. The system reused 24 international openEHR
      archetypes from Clinical Knowledge Manager as target information models and
      implemented 123 mapping rules to transform NLP output into AOM-compliant representations.
      In a pilot study on 50 manually annotated pediatric ICU medical histories from
      Hannover Medical School, the prototype achieved 97% precision and 94% recall,
      demonstrating that archetype-constrained targets facilitate accurate normalization
      of free text for secondary research use and decision support through semantically
      enriched, interoperable clinical repositories.'
    references:
    - https://doi.org/10.1055/s-0040-1716403
    used_in_bridge2ai: false
  - id: B2AI_APP:217
    category: B2AI:Application
    name: Archetype-Based Clinical Decision Support for Pediatric SIRS
    description: 'A knowledge-based clinical decision support system (CDSS) for automated
      early detection of pediatric systemic inflammatory response syndrome (SIRS)
      leverages openEHR archetypes with LOINC terminology bindings and Archetype Query
      Language (AQL) to retrieve standardized, unambiguous clinical facts from an
      archetype-modeled repository (HaMSTR). The system uses internationally agreed-upon
      openEHR archetypes to represent vital signs, laboratory results, and clinical
      observations, with AQL enabling declarative queries over archetype paths to
      supply dynamic facts to an inference engine implementing SIRS detection rules.
      In a pilot study on real pediatric ICU data covering 16 randomly selected patients
      across 129 patient-days, the CDSS achieved sensitivity 1.00, specificity 0.94,
      and Cohen''s kappa 0.92 when compared to clinical expert assessments, demonstrating
      how AOM-grounded data representation and archetype-driven retrieval support
      robust, interoperable clinical inference engines in real clinical contexts with
      high accuracy and agreement.'
    references:
    - https://doi.org/10.1016/j.artmed.2018.04.012
    used_in_bridge2ai: false
  - id: B2AI_APP:218
    category: B2AI:Application
    name: Word2Vec Query Expansion for Archetype Retrieval
    description: 'To improve discoverability and reduce ambiguity in archetype/template
      retrieval from openEHR repositories, a Word2Vec-based query expansion method
      learns synonyms and substitute terms for user search queries, facilitating reuse
      of AOM-grounded archetypes across medical professionals with varying terminology
      preferences. The approach trains word embeddings on Chinese Wikipedia corpus
      and applies learned semantic similarities to expand 120 original search terms,
      generating 69,348 substitute terms for querying Chinese openEHR archetypes.
      This corpus-driven embedding method contrasts with simple external dictionary
      mapping by capturing contextual semantic relationships. Applied to archetype
      retrieval tasks representing different medical professional expertise levels,
      the query expansion improved average Precision@5 (P@5) by 0.767, with best-case
      P@5 reaching 0.975, demonstrating how NLP/ML methods directly enhance archetype
      discoverability, promote interoperability through improved archetype reuse,
      and reduce semantic ambiguity in clinical modeling workflows.'
    references:
    - https://doi.org/10.1186/s12911-021-01554-2
    used_in_bridge2ai: false
- id: B2AI_STANDARD:252
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: openEHR Architecture
  formal_specification: https://specifications.openehr.org/releases/BASE/latest/architecture_overview.html
  is_open: true
  name: openEHR
  purpose_detail: openEHR is the name of a technology for e-health, consisting of
    open specifications, clinical models and software that can be used to create standards,
    and build information and interoperability solutions for healthcare. The various
    artefacts of openEHR are produced by the openEHR community and managed by openEHR
    International, an international non-profit organisation originally established
    in 2003 and previously managed by the openEHR Foundation.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:79
  url: https://www.openehr.org/about/what_is_openehr
- id: B2AI_STANDARD:253
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Outbreak Reports and Intervention Studies Of Nosocomial infection
  formal_specification: https://www.ucl.ac.uk/drupal/site_antimicrobial-resistance/sites/antimicrobial-resistance/files/checklist_authors.pdf
  is_open: true
  name: ORION
  publication: doi:10.1016/S1473-3099(07)70082-8
  purpose_detail: The quality of research in hospital epidemiology (infection control)
    must be improved to be robust enough to influence policy and practice. In order
    to raise the standards of research and publication, a CONSORT equivalent for these
    largely quasi-experimental studies has been prepared by the authors of two relevant
    systematic reviews undertaken for the HTA and the Cochrane Collaboration. The
    statement was revised following widespread consultation with learned societies,
    editors of journals and researchers. It consists of a 22 item checklist, and a
    summary table. The emphasis is on transparency to improve the quality of reporting
    and on the use of appropriate statistical techniques.The statement has been endorsed
    and welcomed by a number of professional special interest groups and societies
    including the Association of Medical Microbiologists (AMM), Bristish Society for
    Antimicrobial Chemotherapy (BSAC) and the Infection Control Nurses' Association
    (ICNA) Research and Development Group. Like CONSORT, ORION considers itself a
    work in progress, which requires ongoing dialogue for successful promotion and
    dissemination. The statement is therefore offered for further public discussion
    and journals are encouraged to trial it as part of their reviewing and editing
    process and feedback to the authors.
  requires_registration: false
  url: https://www.ucl.ac.uk/antimicrobial-resistance/reporting-guidelines/orion-statement-consort-equivalent-infection-control-intervention-studies
- id: B2AI_STANDARD:254
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Patient-Centric Integrated Clinical Environment Standard
  formal_specification: https://www.astm.org/f2761-09r13.html
  is_open: false
  name: ICE
  purpose_detail: This standard specifies the characteristics necessary for the safe
    integration of MEDICAL DEVICES and other equipment, via an electronic interface,
    from different MANUFACTURERS into a single medical system for the care of a single
    high acuity PATIENT. This standard establishes requirements for a medical system
    that is intended to have greater error resistance and improved PATIENT safety,
    treatment efficacy and workflow efficiency than can be achieved with independently
    used MEDICAL DEVICES.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:54
  url: https://mdpnp.org/mdice.html
- id: B2AI_STANDARD:255
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:26
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Pfam / HMMER Profile file format
  formal_specification: https://www.genome.jp/tools/motif/hmmformat.htm
  is_open: true
  name: HMMER Format
  purpose_detail: The profile HMM calculated from multiple sequnce alignment data
    in this service is stored in Profile HMM save format (usually with .hmm extension).
    It is an ASCII file containing a lot of header and descriptive records followed
    by large numerical matrix which holds probabilistic model of the motif. The file
    of this format is useful to search against sequnce databases to find out other
    proteins which share the same motif. This HMM file should not be edited manually
    (especially the matrix part) because it contains consistent numerical model as
    a whole.
  requires_registration: false
  url: http://hmmer.org/
- id: B2AI_STANDARD:256
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Phenopackets schema
  formal_specification: https://github.com/phenopackets/phenopacket-schema
  has_relevant_organization:
  - B2AI_ORG:58
  is_open: true
  name: Phenopackets
  purpose_detail: Phenopackets is a schema for exchanging computable
    representations of patient clinical phenotypes, diseases, genomic information,
    and associated metadata in a standardized, machine-readable format. The schema
    uses Protocol Buffers (protobuf) and JSON to encode clinical observations as
    Human Phenotype Ontology (HPO) terms, diseases as OMIM/MONDO identifiers, age
    and temporal information, biosample details, genomic interpretations (variants,
    genes), pedigree information, measurements, medical actions, and provenance metadata.
    Phenopackets supports multiple use cases including individual patient records,
    family pedigrees, and cohort descriptions, enabling seamless exchange of phenotypic
    data between clinical systems, research databases, and diagnostic platforms. The
    format bridges clinical phenotyping with genomics by linking patient phenotypes
    to genetic variants, facilitating variant interpretation, genotype-phenotype correlation
    studies, and rare disease diagnosis. Phenopackets promotes FAIR principles for
    clinical data through standardized vocabularies (HPO, LOINC, UCUM), supports
    federated analysis across rare disease registries without sharing raw patient
    data, enables AI/ML applications for automated phenotyping and diagnostic support,
    and provides the foundation for international data sharing initiatives including
    matchmaking services that connect patients with similar phenotypes for clinical
    research and trial recruitment.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:34
  url: http://phenopackets.org/
  has_application:
  - id: B2AI_APP:46
    category: B2AI:Application
    name: ML-Ready Rare Disease Corpus for Benchmarking and Training
    description: A curated corpus of 4,916 case-level phenopackets spanning 277 Mendelian
      and chromosomal diseases was released explicitly as an analysis-ready, AI-ready
      dataset to enable machine learning analyses of clinical phenotype data. The
      corpus supports gene and disease prioritization pipelines, patient stratification
      studies, and genotype-phenotype correlation analyses by providing standardized
      HPO-encoded phenotypes linked to genomic diagnoses. Phenopackets' uniform format
      enables benchmarking of diagnostic software performance, testing and tuning
      of algorithms on standardized inputs, and evaluation of ML models for rare
      disease genomic diagnostics across diverse disease presentations.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2024.05.29.24308104
  - id: B2AI_APP:154
    category: B2AI:Application
    name: Phenotype-Driven Diagnostic Tools Integration
    description: Widely used phenotype-driven diagnostic and gene/variant prioritization
      tools including Exomiser, LIRICAL, Phen2Gene, and CADA accept Phenopackets
      as standardized input files, enabling integration of patient-level HPO-encoded
      phenotypes with genomic data for AI-enabled diagnostics. Phenopackets facilitate
      computational pipeline integration by providing a consistent format for representing
      clinical observations, supporting aggregation across sites, and enabling interoperability
      with electronic health record (EHR) systems and rare disease registries. This
      standardization allows diagnostic AI tools to process patient data from diverse
      sources using a common schema, improving diagnostic accuracy and enabling federated
      diagnostic workflows.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2021.11.27.21266944
    - https://doi.org/10.1038/s41587-022-01357-4
  - id: B2AI_APP:155
    category: B2AI:Application
    name: Clinical Annotation System with Enhanced ML Performance
    description: SAMS (Symptom Annotation Made Simple), an HPO-integrated clinical
      annotation system that imports and exports Phenopackets, demonstrated measurable
      improvements in data quality that benefit downstream AI/ML pipelines. SAMS
      reported a 10% increase in recall for scientific publication annotations and
      a 20% increase in recall for EHR-derived annotations through improved entity
      linking and standardized symptom representation. These quality improvements
      in structured phenotype data directly enhance the performance of machine learning
      models that rely on high-quality, standardized clinical phenotypes for training
      and inference in diagnostic and research applications.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1093/nar/gkad1005
  - id: B2AI_APP:156
    category: B2AI:Application
    name: Large-Scale Federated Rare Disease Data Sharing
    description: Within the Solve-RD consortium, Phenopackets support standardized
      clinical data sharing for large federated cohorts with 11,349+ individuals
      (growing to over 19,000), facilitating AI/ML-ready data integration for rare
      disease diagnostics and research. The European Joint Programme on Rare Diseases
      (EJP RD) utilizes Phenopackets for federated data discovery across rare disease
      resources while adhering to FAIR principles. Phenopackets enable distributed
      analyses, patient matchmaking services, and cohort identification across international
      sites without requiring patient-level data sharing, providing a computable
      substrate for training and validating machine learning models on multi-institutional
      rare disease datasets.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2021.11.27.21266944
  - id: B2AI_APP:157
    category: B2AI:Application
    name: Genotype-Phenotype Correlation Analytics
    description: The GPSEA (GenoPheno Statistical Evidence Assessment) framework uses
      Phenopackets to represent adverse phenotype data and compute genotype-phenotype
      correlations across 6,613 individuals spanning 85 cohorts. Phenopackets' standardized
      representation of patient-level phenotypes, variants, and biosample information
      enables systematic computational analysis of genotype-phenotype relationships,
      supporting downstream AI/ML analytics for variant interpretation, phenotype
      prediction from genotypes, and discovery of novel genotype-phenotype associations.
      The format facilitates aggregation of case-level data from diverse sources
      into analysis-ready datasets that machine learning models can use to learn
      patterns between genetic variants and clinical presentations.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2024.05.29.24308104
  - id: B2AI_APP:158
    category: B2AI:Application
    name: Patient Matchmaking and Cohort Discovery Systems
    description: Phenopackets serve as the computable representation underlying data-driven
      patient matchmaking services that connect individuals with similar phenotypes
      across international rare disease networks for clinical research, trial recruitment,
      and collaborative diagnosis. The standardized format enables AI-powered differential
      diagnosis systems and automated cohort identification tools that match patient
      phenotypes to disease profiles, identify suitable clinical trial candidates,
      and discover similar cases in distributed databases. Phenopackets' consistent
      encoding of clinical observations using HPO terms allows machine learning algorithms
      to compute phenotypic similarity scores, cluster patients by presentation,
      and support precision medicine initiatives through data-driven patient stratification
      across global rare disease registries.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.17863/cam.87963
    - https://doi.org/10.1038/s41587-022-01357-4
- id: B2AI_STANDARD:257
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: PHIN Messaging Guide for Syndromic Surveillance Emergency Department,
    Urgent Care, Inpatient and Ambulatory Care Settings
  formal_specification: https://www.cdc.gov/nssp/documents/guides/syndrsurvmessagguide2_messagingguide_phn.pdf
  has_relevant_organization:
  - B2AI_ORG:40
  is_open: true
  name: PHIN Guide
  purpose_detail: An HL7 messaging and content reference standard for national, syndromic
    surveillance electronic health record technology certification; A basis for local
    and state syndromic surveillance messaging implementation guides; A resource for
    planning for the increasing use of electronic health record technology and for
    providing details on health data elements that may become a part of future public
    health syndromic surveillance messaging requirements; Optional elements of interest
    for adding laboratory results to syndromic surveillance messages using ORU^R01
    message structure (see details in the PHIN messaging Standard, National Condition
    Reporting case Notification, ORU^R01 message Structure Specification profile,
    Version 2.1, 2014)
  requires_registration: false
  url: https://knowledgerepository.syndromicsurveillance.org/hl7-version-251-phin-messaging-guide-syndromic-surveillance-emergency-department-urgent-care-and
- id: B2AI_STANDARD:258
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  - B2AI_TOPIC:12
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: phyloXML
  is_open: true
  name: phyloXML
  publication: doi:10.1186/1471-2105-10-356
  purpose_detail: phyloXML is a specialized XML-based language for representing phylogenetic
    trees and networks, along with rich associated metadata. It provides standardized
    elements for encoding taxonomic information, gene names, sequence identifiers,
    branch lengths, support values, gene duplication and speciation events, and other
    evolutionary attributes. The extensible structure of phyloXML enables interoperability
    between evolutionary biology and comparative genomics tools, supporting both simple
    and complex tree annotations. Its schema allows for domain-specific extensions
    and integration with other bioinformatics resources, making it a widely adopted
    standard for sharing, visualizing, and analyzing phylogenetic data in research
    and database applications.
  requires_registration: false
  url: http://www.phyloxml.org/
- id: B2AI_STANDARD:259
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:18
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Physical Activity Monitoring for Fitness Wearables Step Counting
  is_open: true
  name: ANSI/CTA-2056
  purpose_detail: ANSI/CTA-2056 is a voluntary consensus standard developed by the
    Consumer Technology Association (CTA) that establishes standardized definitions,
    testing methodologies, and minimum performance criteria for measuring step counting
    accuracy on consumer wearable devices and smartphone applications used for physical
    activity monitoring and health tracking. The standard defines a "step" as a single
    stride during human locomotion and specifies validation protocols using controlled
    laboratory testing on treadmills at various speeds (slow walking 2.0 mph, normal
    walking 3.0 mph, brisk walking 4.0 mph) as well as free-living wear tests in real-world
    conditions to evaluate device performance across diverse user populations and
    activity patterns. ANSI/CTA-2056 requires manufacturers to report step counting
    accuracy as mean absolute percentage error (MAPE) and specifies acceptable error
    thresholds - devices should achieve less than 10% error for controlled walking
    tests and less than 20% error for free-living conditions to meet the standard's
    performance benchmarks. The standard addresses measurement challenges including
    false positive step detection from upper body movements, vehicle vibrations, and
    daily living activities, as well as false negative errors from slow shuffling
    gaits or irregular walking patterns. ANSI/CTA-2056 promotes transparency by requiring
    clear disclosure of test conditions, participant demographics, reference measurement
    systems (ActiGraph accelerometers, manual observation), and statistical analysis
    methods used for validation. The standard facilitates comparability across commercial
    fitness trackers, smartwatches, and pedometer applications from manufacturers
    like Fitbit, Apple Watch, Garmin, and Samsung, supporting consumer informed decision-making,
    clinical research applications requiring validated activity metrics, and workplace
    wellness programs utilizing step count goals for employee health initiatives.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:4
  url: https://webstore.ansi.org/standards/ansi/cta20562016ansi
- id: B2AI_STANDARD:260
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Portable Encapsulated Project specification
  formal_specification: https://github.com/pepkit/pepspec
  is_open: true
  name: PEP
  publication: doi:10.1093/gigascience/giab077
  purpose_detail: The Portable Encapsulated Project (PEP) specification is a standardized,
    machine-readable format for organizing and describing biological sample metadata
    and associated computational analysis configurations in data-intensive bioinformatics
    projects. PEP uses a YAML-based configuration file (project_config.yaml) that
    points to a sample annotation table (typically CSV or TSV) containing sample attributes,
    along with optional subsample tables for complex hierarchical relationships. The
    specification defines a formal structure for representing sample metadata with
    arbitrary attributes, derived attributes computed from other columns, sample modifiers
    for conditional processing, and implied attributes inferred from organizational
    context. PEP supports amendments for alternative project configurations, subanotations
    for linking multiple data files to single samples, and project-level metadata
    including descriptions, keywords, and namespace information. The format is designed
    to make projects portable across compute environments, reproducible through version-controlled
    configurations, and interoperable across analysis tools through a growing ecosystem
    of PEP-compatible software (looper, pypiper, pepr, geofetch, pephub). PEP's structured
    metadata representation enables systematic queries across sample collections, facilitates
    batch processing and parallelization of analyses, supports complex experimental
    designs with multiple data types per sample, and provides a foundation for FAIR
    data practices in genomics and multi-omics research. The specification is particularly
    valuable for managing large-scale projects with hundreds or thousands of samples,
    enabling metadata-driven workflow execution, automated data retrieval, and standardized
    documentation that supports reproducible computational research and machine learning
    applications requiring well-annotated training datasets.
  related_to:
  - B2AI_STANDARD:761
  requires_registration: false
  url: http://pep.databio.org/
- id: B2AI_STANDARD:261
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Portable Format for Bioinformatics
  formal_specification: https://github.com/uc-cdis/pypfb
  is_open: true
  name: PFB
  publication: doi:10.1101/2022.07.19.500678
  purpose_detail: A self-describing serialized format for bulk biomedical data called
    the Portable Format for Biomedical (PFB) data. The Portable Format for Biomedical
    data is based upon Avro and encapsulates a data model, a data dictionary, the
    data itself, and pointers to third party controlled vocabularies. In general,
    each data element in the data dictionary is associated with a third party controlled
    vocabulary to make it easier for applications to harmonize two or more PFB files.
  requires_registration: false
  url: https://anvilproject.org/ncpi/technologies#portable-format-for-bioinformatics-pfb
- id: B2AI_STANDARD:262
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Preferred Reporting Items for Systematic Reviews and Meta-Analyses
  is_open: true
  name: PRISMA
  publication: doi:10.1136/bmj.n71
  purpose_detail: An evidence-based minimum set of items for reporting in systematic
    reviews and meta-analyses.The aim of the PRISMA Statement is to help authors improve
    the reporting of systematic reviews and meta-analyses. We have focused on randomized
    trials, but PRISMA can also be used as a basis for reporting systematic reviews
    of other types of research, particularly evaluations of interventions. PRISMA
    may also be useful for critical appraisal of published systematic reviews, although
    it is not a quality assessment instrument to gauge the quality of a systematic
    review.
  requires_registration: false
  url: https://www.prisma-statement.org/
- id: B2AI_STANDARD:263
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:27
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Protein Data Bank Format
  has_relevant_organization:
  - B2AI_ORG:82
  is_open: true
  name: PDB
  purpose_detail: An exchange format for reporting experimentally determined three-dimensional
    structures of biological macromolecules that serves a global community of researchers,
    educators, and students. The data contained in the archive include atomic coordinates,
    bibliographic citations, primary and secondary structure, information, and crystallographic
    structure factors and NMR experimental data
  requires_registration: false
  url: https://www.cgl.ucsf.edu/chimera/docs/UsersGuide/tutorials/pdbintro.html
- id: B2AI_STANDARD:264
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:26
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Proteomics Standards Initiative Protein Affinity Reagent format
  is_open: true
  name: PSI-PAR
  publication: doi:10.1074/mcp.M900185-MCP200
  purpose_detail: The work on PSI-PAR was initiated as part of the ProteomeBinders
    project and carried out by EMBL-EBI and the PSI-MI work group. The Proteomics
    Standards Initiative (PSI) aims to define community standards for data representation
    in proteomics to facilitate data comparison, exchange and verification. For detailed
    information on all PSI activities, please see PSI Home Page. The PSI-PAR format
    is a standardized means of representing protein affinity reagent data and is designed
    to facilitate the exchange of information between different databases and/or LIMS
    systems. PSI-PAR is not a proposed database structure. The PSI-PAR format consists
    of the PSI-MI XML2.5 schema (originally designed for molecular interactions) and
    the PSI-PAR controlled vocabulary. In addition, PSI-PAR documentation and examples
    are available on this web page. The scope of PSI-PAR is PAR and target protein
    production and characterization.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:41
  url: https://www.psidev.info/psi-par
- id: B2AI_STANDARD:265
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:26
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: PSI GelML
  is_open: true
  name: PSI GelML
  purpose_detail: GelML is a data exchange format for describing the results of gel
    electrophoresis experiments. GelML is developed as a HUPO-PSI working group.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:41
  url: https://www.psidev.info/gelml/1.0
- id: B2AI_STANDARD:266
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: PSI-MI XML
  is_open: true
  name: PSI-MI XML
  purpose_detail: The PSI-MI XML 2.5 is a community standard for molecular interactions
    which has been jointly developed by major data providers (BIND, CellZome, DIP,
    GSK, HPRD, Hybrigenics, IntAct, MINT, MIPS, Serono, U. Bielefeld, U. Bordeaux,
    U. Cambridge, and others).This format is stable and used for several years now
    - published in October 2007 (Broadening the Horizon  Level 2.5 of the HUPO-PSI
    Format for Molecular Interactions; Samuel Kerrien et al. BioMed Central. 2007.),
    it has been adapted for many different usages. It can be used for storing any
    kind of molecular interaction data - complexes and binary interactions not only
    protein-protein interactions, can describe nucleic acids interactions and others
    hierarchical complexes modelling by using interactionRef in participants instead
    of an interactor Data representation in PSI-MI 2.5 XML relies heavily on the use
    of controlled vocabularies. They can be accessed easily via the Ontology Lookup
    Service, PSI-MI, PSI-MOD.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:41
  url: https://www.psidev.info/mif
- id: B2AI_STANDARD:267
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: qcML format
  is_open: true
  name: qcML
  publication: doi:10.1074/mcp.M113.035907
  purpose_detail: An XML format for quality-related data of mass spectrometry and
    other high-throughput experiments. Quality control is increasingly recognized
    as a crucial aspect of mass spectrometry based proteomics. Several recent papers
    discuss relevant parameters for quality control and present applications to extract
    these from the instrumental raw data. What has been missing, however, is a standard
    data exchange format for reporting these performance metrics. We therefore developed
    the qcML format, an XML-based standard that follows the design principles of the
    related mzML, mzIdentML, mzQuantML, and TraML standards from the HUPO-PSI (Proteomics
    Standards Initiative). In addition to the XML format, we also provide tools for
    the calculation of a wide range of quality metrics as well as a database format
    and interconversion tools, so that existing LIMS systems can easily add relational
    storage of the quality control data to their existing schema. We here describe
    the qcML specification, along with possible use cases and an illustrative example
    of the subsequent analysis possibilities. All information about qcML is available
    at http://code.google.com/p/qcml
  requires_registration: false
  url: https://doi.org/10.1074/mcp.M113.035907
- id: B2AI_STANDARD:268
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:33
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Real-time PCR Data Markup Language
  is_open: true
  name: RDML
  purpose_detail: The RDML file format is developed by the RDML consortium (http://www.rdml.org)
    and can be used free of charge. The RDML file format was created to encourage
    the exchange, publication, revision and re-analysis of raw qPCR data. The core
    of an RDML file is an experiment, not a PCR run. Therefore all the information
    is collected which is required to understand an experiment. The structure of the
    file format was inspired by a database structure. In the file are several master
    elements, which are then referred to in other parts of the file. This structure
    allows to reduce the amount of redundant information and encourages the user to
    provide useful information. The Real-time PCR Data Markup Language (RDML) is a
    structured and universal data standard for exchanging quantitative PCR (qPCR)
    data. The data standard should contain sufficient information to understand the
    experimental setup, re-analyse the data and interpret the results. The data standard
    is a compressed text file in Extensible Markup Language (XML) and enables transparent
    exchange of annotated qPCR data between instrument software and third-party data
    analysis packages, between colleagues and collaborators, and between authors,
    peer reviewers, journals and readers. To support the public acceptance of this
    standard, both an on-line RDML file generator is available for end users, as well
    as RDML software libraries to be used by software developers, enabling import
    and export of RDML data files.
  requires_registration: false
  url: http://www.rdml.org
- id: B2AI_STANDARD:269
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Reporting guidelines for randomized controlled trials for livestock
    and food safety
  is_open: true
  name: REFLECT
  publication: doi:10.4315/0362-028x-73.3.579
  purpose_detail: REFLECT (Reporting guidElines For randomized controLled trials for
    livEstoCk and food safeTy) is a specialized, evidence-based minimum set of 22
    reporting items specifically designed to improve the transparency, reproducibility,
    and quality of randomized controlled trials in livestock production, animal health,
    and food safety research. This comprehensive reporting guideline addresses the
    unique challenges and complexities inherent in agricultural and veterinary research,
    including both field trials conducted in real-world farm settings and controlled
    challenge studies in laboratory environments. REFLECT covers essential aspects
    of trial design, implementation, and reporting including study population characteristics,
    randomization procedures, intervention details, outcome measurements, statistical
    analyses, and results presentation. The guideline is specifically tailored for
    trials evaluating therapeutic or preventive interventions that impact production
    outcomes, animal health parameters, and food safety measures. Available in both
    MS Word and PDF formats, REFLECT serves as a dynamic, evolving document that is
    periodically updated as new evidence emerges, helping researchers, editors, reviewers,
    and regulatory agencies ensure that livestock and food safety trials are reported
    with sufficient detail for proper scientific evaluation and evidence-based decision
    making.
  requires_registration: false
  url: https://meridian.cvm.iastate.edu/reflect/
- id: B2AI_STANDARD:270
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Reporting recommendations for tumour Marker prognostic studies
  formal_specification: https://www.equator-network.org/wp-content/uploads/2016/10/REMARK-checklist-for-EQUATOR-website-002.docx
  is_open: true
  name: REMARK
  publication: doi:10.1038/sj.bjc.6602678
  purpose_detail: Despite years of research and hundreds of reports on tumor markers
    in oncology, the number of markers that have emerged as clinically useful is pitifully
    small. Often initially reported studies of a marker show great promise, but subsequent
    studies on the same or related markers yield inconsistent conclusions or stand
    in direct contradiction to the promising results. It is imperative that we attempt
    to understand the reasons why multiple studies of the same marker lead to differing
    conclusions. A variety of methodological problems have been cited to explain these
    discrepancies. Unfortunately, many tumor marker studies have not been reported
    in a rigorous fashion, and published articles often lack sufficient information
    to allow adequate assessment of the quality of the study or the generalizability
    of study results. The development of guidelines for the reporting of tumor marker
    studies was a major recommendation of the National Cancer Institute-European Organisation
    for Research and Treatment of Cancer (NCI-EORTC) First International Meeting on
    Cancer Diagnostics in 2000. As for the successful CONSORT initiative for randomized
    trials and for the STARD statement for diagnostic studies, we suggest guidelines
    to provide relevant information about the study design, preplanned hypotheses,
    patient and specimen characteristics, assay methods, and statistical analysis
    methods. In addition, the guidelines provide helpful suggestions on how to present
    data and important elements to include in discussions. The goal of these guidelines
    is to encourage transparent and complete reporting so that the relevant information
    will be available to others to help them to judge the usefulness of the data and
    understand the context in which the conclusions apply.
  requires_registration: false
  url: https://www.equator-network.org/reporting-guidelines/reporting-recommendations-for-tumour-marker-prognostic-studies-remark/
- id: B2AI_STANDARD:271
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:31
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ReproSchema
  formal_specification: https://github.com/ReproNim/reproschema
  is_open: true
  name: ReproSchema
  purpose_detail: A common schema that encodes how the different elements of assessment
    data and / or the metadata relate to one another.
  requires_registration: false
  url: https://www.repronim.org/reproschema/
- id: B2AI_STANDARD:272
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Sentinel Common Data Model
  formal_specification: https://dev.sentinelsystem.org/projects/SCDM/repos/sentinel_common_data_model/browse
  is_open: true
  name: SCDM
  purpose_detail: A standard data structure that allows Sentinel Data Partners to
    quickly execute distributed programs against local data.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:89
  url: https://www.sentinelinitiative.org/methods-data-tools/sentinel-common-data-model
  has_application:
  - id: B2AI_APP:47
    category: B2AI:Application
    name: FDA Sentinel System and Distributed AI for Drug Safety
    description: Sentinel Common Data Model is used in AI applications for active
      surveillance of medical product safety across distributed healthcare databases,
      enabling privacy-preserving machine learning for adverse event detection and
      risk assessment. AI systems leverage SCDM's standardization of claims data,
      electronic health records, and registries to develop models that identify safety
      signals, predict adverse events, and characterize treatment patterns across
      the FDA Sentinel System network. The common data model enables federated learning
      where AI algorithms execute locally at each data partner without sharing patient-level
      data, supporting rapid assessment of emerging safety concerns. Machine learning
      applications use SCDM to train models on massive populations for rare adverse
      event detection and comparative safety analysis.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:273
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Sequence Alignment/Map Format
  is_open: true
  name: SAM
  publication: doi:10.1093/bioinformatics/btp352
  purpose_detail: The Sequence Alignment/Map (SAM) format is a TAB-delimited text
    format for storing sequence alignments against reference sequences, developed
    as part of the samtools project and maintained by the GA4GH Large Scale Genomics
    work stream. SAM consists of an optional header section (beginning with @ symbols)
    containing metadata about reference sequences, read groups, programs, and comments,
    followed by an alignment section with one line per aligned read. Each alignment
    line contains 11 mandatory fields including query name (QNAME), bitwise FLAG encoding
    mapping properties, reference sequence name (RNAME), 1-based leftmost mapping
    position (POS), mapping quality (MAPQ), CIGAR string describing alignment operations,
    mate pair information (RNEXT, PNEXT, TLEN), sequence (SEQ), and ASCII-encoded
    base quality scores (QUAL). The format supports optional fields as TAG:TYPE:VALUE
    triplets for storing additional information such as edit distance, alternative
    alignments, and aligner-specific metadata. SAM serves as the text-based companion
    to the binary BAM and compressed CRAM formats, with bidirectional conversion tools
    (samtools view) enabling interoperability. The format accommodates various alignment
    types including mapped, unmapped, secondary, supplementary, and chimeric alignments
    with proper-pair relationships. SAM files are human-readable for debugging and
    inspection, widely supported by aligners (BWA, Bowtie2, STAR), variant callers,
    and genomics toolkits, forming the foundation for standardized sequencing data
    exchange in next-generation sequencing workflows.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:34
  url: http://samtools.sourceforge.net/
- id: B2AI_STANDARD:274
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  - B2AI_TOPIC:33
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Sequence Read Archive Metadata XML
  has_relevant_organization:
  - B2AI_ORG:74
  is_open: true
  name: SRA-XML
  publication: doi:10.1093/nar/gkq1019
  purpose_detail: SRA-XML is an XML-based metadata format used by the Sequence Read
    Archive (SRA) and the European Nucleotide Archive (ENA) to describe and exchange
    metadata for raw sequencing data submissions from next-generation sequencing platforms.
    The format captures detailed information about sequencing experiments, sample
    attributes, library preparation, instrument models, and data files, enabling standardized
    data submission, validation, and integration across international repositories.
    SRA-XML supports the reproducibility and discoverability of sequencing datasets
    by providing a structured, machine-readable representation of experimental context
    and provenance, facilitating large-scale genomics research and data sharing.
  requires_registration: false
  url: https://www.ncbi.nlm.nih.gov/sra/docs/submitmeta/
  has_application:
  - id: B2AI_APP:48
    category: B2AI:Application
    name: Sequence Data Metadata Mining and Dataset Discovery
    description: SRA-XML (Sequence Read Archive metadata format) is used in AI applications
      for automated mining of experimental metadata from genomic studies, enabling
      dataset discovery, quality assessment, and training of models that learn from
      experimental design information. Machine learning systems parse SRA metadata
      to identify relevant datasets for specific research questions, predict data
      quality issues before download, and extract experimental conditions that inform
      downstream analysis. AI applications leverage structured metadata to train models
      for automated experiment type classification, sample relationship inference,
      and detection of metadata quality issues. The format enables large-scale meta-analyses
      where AI systems integrate findings across thousands of sequencing experiments
      by understanding their experimental context.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:275
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Simple Omnibus Format in Text
  is_open: true
  name: SOFT
  purpose_detail: Simple Omnibus Format in Text (SOFT) is a line-based, plain text
    format originally developed by NCBI Gene Expression Omnibus (GEO) for batch submission
    and download of genomic data. Though GEO discontinued accepting SOFT submissions
    in early 2024, all records remain available for download in SOFT format. The format
    uses four line types distinguished by first-character markers - caret lines (^)
    indicate entity types (PLATFORM, SAMPLE, SERIES), bang lines (!) specify entity
    attributes as label-value pairs, hash lines (#) describe data table headers, and
    data lines contain tab-delimited table rows. A single SOFT file can concatenate
    multiple Platform (GPL), Sample (GSM), and Series (GSE) records with their data
    tables and descriptive metadata. Platform tables require unique row identifiers
    and trackable sequence identifiers (GenBank/RefSeq accessions, clone IDs, oligo
    sequences) with standard headers for sequences, organism source, and various accession
    types. Sample tables must include ID_REF columns matching Platform identifiers
    and VALUE columns containing normalized, comparable measurements (scaled signals
    for single-channel or log ratios for dual-channel data). The format is compatible
    with common spreadsheet and database applications and supports MIAME standards
    for comprehensive data interpretation.
  requires_registration: false
  url: https://www.ncbi.nlm.nih.gov/geo/info/soft-seq.html
- id: B2AI_STANDARD:276
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:3
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Simplified Molecular Input Line Entry Specification Format
  formal_specification: http://opensmiles.org/opensmiles.html
  is_open: true
  name: SMILES
  purpose_detail: A typographical line notation for specifying chemical structure.
  requires_registration: false
  url: http://opensmiles.org/
- id: B2AI_STANDARD:277
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Simulation Experiment Description Markup Language
  is_open: true
  name: SED-ML
  purpose_detail: SED-ML is an XML-based format for encoding simulation setups, to
    ensure exchangeability and reproducibility of simulation experiments. It follows
    the requirements defined in the MIASE guidelines.
  requires_registration: false
  url: https://sed-ml.org/
- id: B2AI_STANDARD:278
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  - B2AI_TOPIC:35
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: SPDI data model
  has_relevant_organization:
  - B2AI_ORG:74
  is_open: true
  name: SPDI
  publication: doi:10.1093/bioinformatics/btz856
  purpose_detail: Sequence variant data model. Represents all variants as a sequence
    of four operations. Start at the boundary before the first nucleotide in the sequence
    S, advance P nucleotides, delete D nucleotides, then Insert the nucleotides in
    the string.
  requires_registration: false
  url: https://doi.org/10.1093/bioinformatics/btz856
  has_application:
  - id: B2AI_APP:49
    category: B2AI:Application
    name: Unambiguous Variant Representation for Clinical AI
    description: SPDI (Sequence-Position-Deletion-Insertion) notation is used in AI
      applications for creating precise, unambiguous representations of genetic variants
      that eliminate nomenclature inconsistencies affecting model training and clinical
      interpretation. AI systems leverage SPDI's standardized four-element format
      to normalize variant representations from diverse sources (clinical labs, research
      databases, literature), enabling consistent feature engineering for machine
      learning models predicting variant pathogenicity. The notation resolves ambiguities
      in variant left-normalization and reference sequence specification that can
      cause the same biological variant to appear as different features to AI models,
      improving model accuracy and reproducibility. SPDI is particularly valuable
      for clinical AI systems that must reconcile variants reported in different formats
      across laboratories.
    used_in_bridge2ai: false
    references:
    - https://www.ncbi.nlm.nih.gov/variation/notation/
- id: B2AI_STANDARD:279
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Standard EEG Data Structure
  is_open: false
  name: SEDS
  publication: doi:10.1016/j.softx.2021.100933
  purpose_detail: A new and more flexible data structure, named the Standard EEG Data
    Structure (SEDS), was proposed to meet the needs of both small-scale EEG data
    batch processing in single-site studies and large-scale EEG data sharing and analysis
    in single-/multisite studies (especially on cloud platforms).
  requires_registration: false
  url: https://doi.org/10.1016/j.softx.2021.100933
- id: B2AI_STANDARD:280
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - codesystem
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Standard PREanalytical Code
  is_open: true
  name: SPREC
  publication: doi:10.1089/bio.2017.0109
  purpose_detail: The Standard PREanalytical Code (SPREC) is a comprehensive coding
    system developed by the ISBER Biospecimen Science Working Group to standardize
    documentation of pre-analytical variables that affect biospecimen quality. SPREC
    uses a seven-element alphanumeric code to capture critical factors including specimen
    type, primary container, time to processing (warm and cold ischemia), centrifugation
    parameters (speed and temperature), and long-term storage conditions. This systematic
    encoding enables biobanks and biorepositories to consistently track and communicate
    how specimens were collected, processed, and preserved, which directly impacts
    molecular analyte stability and experimental reproducibility. By providing a standardized
    language for pre-analytical variation, SPREC facilitates data harmonization across
    biobanks, enables quality comparisons between specimen collections, supports regulatory
    compliance, and enhances the value of biological samples for translational research.
    The code is particularly valuable for multi-center studies where specimen provenance
    must be tracked and controlled. SPREC version 3.0 covers various specimen types
    including blood, tissue, urine, and other biological fluids, with specific codes
    for derivatives like DNA, RNA, and proteins.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:48
  url: https://www.isber.org/page/SPREC
- id: B2AI_STANDARD:281
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Standards for Reporting of Diagnostic Accuracy Studies
  is_open: true
  name: STARD
  publication: doi:10.1136/bmjopen-2016-012799
  purpose_detail: The objective of the STARD initiative is to improve the accuracy
    and completeness of reporting of studies of diagnostic accuracy, to allow readers
    to assess the potential for bias in the study (internal validity) and to evaluate
    its generalisability (external validity).The STARD statement consist of a checklist
    of 25 items and recommends the use of a flow diagram which describe the design
    of the study and the flow of patients.
  requires_registration: false
  url: https://www.equator-network.org/reporting-guidelines/stard/
- id: B2AI_STANDARD:282
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:26
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Stockholm Multiple Alignment Format
  is_open: true
  name: Stockholm
  purpose_detail: The Stockholm format is a comprehensive system for marking up and
    annotating features in multiple sequence alignments, widely used by HMMER, Pfam,
    and Belvu bioinformatics tools. The format supports detailed annotation of secondary
    structure (SS), surface accessibility (SA), transmembrane regions (TM), posterior
    probability (PP), ligand binding sites (LI), active sites (AS), and intron positions
    (IN). The format includes structured headers with STOCKHOLM 1.0 identifier, sequence
    alignment blocks with name/start-end notation, and comprehensive markup capabilities
    supporting database references, organism classification, phylogenetic trees in
    New Hampshire format, and various structural and functional annotations. It provides
    flexible annotation of aligned sequences with exact one-character-per-column markup
    for positional features and free-text annotations for sequence and file-level
    metadata.
  requires_registration: false
  url: https://sonnhammer.sbc.su.se/Stockholm.html
- id: B2AI_STANDARD:283
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Strengthening the Reporting of Genetic Association studies
  is_open: true
  name: STREGA
  publication: doi:10.1002/gepi.20410
  purpose_detail: Making sense of rapidly evolving evidence on genetic associations
    is crucial to making genuine advances in human genomics and the eventual integration
    of this information in the practice of medicine and public health. Assessment
    of the strengths and weaknesses of this evidence, and hence the ability to synthesize
    it, has been limited by inadequate reporting of results. The STrengthening the
    REporting of Genetic Association studies (STREGA) initiative builds on the Strengthening
    the Reporting of Observational Studies in Epidemiology (STROBE) Statement and
    provides additions to 12 of the 22 items on the STROBE checklist. The additions
    concern population stratification, genotyping errors, modelling haplotype variation,
    Hardy-Weinberg equilibrium, replication, selection of participants, rationale
    for choice of genes and variants, treatment effects in studying quantitative traits,
    statistical methods, relatedness, reporting of descriptive and outcome data, and
    the volume of data issues that are important to consider in genetic association
    studies. The STREGA recommendations do not prescribe or dictate how a genetic
    association study should be designed but seek to enhance the transparency of its
    reporting, regardless of choices made during design, conduct, or analysis.
  requires_registration: false
  url: https://www.equator-network.org/reporting-guidelines/strobe-strega/
- id: B2AI_STANDARD:284
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Strengthening the Reporting of Observational studies in Epidemiology
  is_open: true
  name: STROBE
  publication: doi:10.1016/j.jclinepi.2007.11.008
  purpose_detail: STROBE stands for an international, collaborative initiative of
    epidemiologists, methodologists, statisticians, researchers and journal editors
    involved in the conduct and dissemination of observational studies, with the common
    aim of STrengthening the Reporting of OBservational studies in Epidemiology. The
    STROBE Statement is being endorsed by a growing number of biomedical journals.
    Incomplete and inadequate reporting of research hampers the assessment of the
    strengths and weaknesses of the studies reported in the medical literature. Readers
    need to know what was planned (and what was not), what was done, what was found,
    and what the results mean. Recommendations on the reporting of studies that are
    endorsed by leading medical journals can improve the quality of reporting.Observational
    research comprises several study designs and many topic areas. We aimed to establish
    a checklist of items that should be included in articles reporting such research
    - the STROBE Statement. We considered it reasonable to initially restrict the
    recommendations to the three main analytical designs that are used in observational
    research - cohort, case-control, and cross-sectional studies. We want to provide
    guidance on how to report observational research well. Our recommendations are
    not prescriptions for designing or conducting studies. Also, the checklist is
    not an instrument to evaluate the quality of observational research.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:91
  url: https://www.strobe-statement.org/
- id: B2AI_STANDARD:285
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:27
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Structure Data Format
  is_open: true
  name: SDF
  purpose_detail: SDF (Structure-Data File) is a widely adopted chemical file format
    developed by Molecular Design Limited (MDL, now part of BIOVIA/Dassault Systmes)
    for storing and exchanging molecular structure information along with associated
    chemical, biological, and pharmaceutical data in a single text-based file. The
    format extends the MDL Molfile specification by wrapping individual molecular
    structures with delimited data fields, enabling storage of multiple compounds
    with their properties, experimental measurements, and metadata in a concatenated
    format suitable for chemical databases and compound libraries. Each SDF record
    begins with a Molfile block containing the connection table representation of
    a molecule including atom coordinates (2D or 3D), bond connectivity, atom types,
    bond orders, stereochemistry (wedge/dash bonds, chiral centers), and charge states,
    followed by a data block with tagged fields containing arbitrary properties such
    as chemical names, CAS registry numbers, molecular weights, melting points, solubility
    measurements, biological activity values (IC50, EC50, Ki), ADMET properties, patent
    identifiers, or custom annotations. The format uses delimiter lines ($$$$) to
    separate individual compound records, allowing files to contain from single molecules
    to millions of structures in vendor catalogs, virtual screening libraries, or
    high-throughput screening datasets. SDF supports representation of organic and
    inorganic molecules, polymers, organometallics, and molecular complexes with features
    including isotopic labels, radical species, multi-component systems (salts, mixtures),
    and query structures with variable attachment points or R-groups for combinatorial
    libraries. The text-based format ensures human readability and cross-platform
    compatibility, though binary variants exist for improved storage efficiency. SDF
    files are extensively used in cheminformatics workflows for compound registration
    systems storing chemical inventory with associated metadata, structure-activity
    relationship (SAR) databases linking molecular structures to biological screening
    results, virtual screening campaigns where docking scores and predicted properties
    are stored alongside structures, chemical vendor catalogs (ChemBridge, Enamine,
    ZINC database) distributing purchasable compounds, patent chemistry databases
    documenting novel chemical matter, and chemical property prediction where computed
    descriptors (logP, TPSA, hydrogen bond donors/acceptors) are appended to structures.
    Machine learning applications in drug discovery rely heavily on SDF format for
    training datasets where molecular structures are converted to fingerprints, graph
    representations, or SMILES strings while associated SDF data fields provide activity
    labels, potency values, or ADMET endpoints for supervised learning models predicting
    bioactivity, toxicity, or pharmacokinetic properties. SDF readers and writers
    are implemented in all major cheminformatics toolkits including RDKit (Python),
    Open Babel, ChemAxon, CDK (Chemistry Development Kit), OEChem (OpenEye), and commercial
    platforms (Pipeline Pilot, KNIME, Schrodinger Suite), ensuring interoperability
    across drug discovery informatics pipelines. Limitations include lack of formal
    standardization for data field naming conventions leading to inconsistent property
    tags across different data sources, limited support for complex molecular features
    like polymers or biologics compared to newer formats, and verbosity for large
    datasets where binary or compressed formats may be preferred. Despite these limitations,
    SDF remains the de facto standard for molecular structure exchange in pharmaceutical
    research, academic chemistry, and commercial chemical databases due to its simplicity,
    extensibility, widespread tool support, and ability to bundle molecular structures
    with rich experimental and computed data in a single portable file format.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/Chemical_table_file#SDF
- id: B2AI_STANDARD:286
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: SummarizedExperiment container
  formal_specification: https://github.com/Bioconductor/SummarizedExperiment
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:40
  is_open: true
  name: SummarizedExperiment
  purpose_detail: SummarizedExperiment is a foundational S4 class in the Bioconductor
    ecosystem providing a standardized container for storing and manipulating rectangular
    matrices of experimental results (assays) alongside associated genomic coordinates
    (row data) and sample annotations (column data), enabling coordinated subsetting,
    transformation, and analysis of high-throughput biological experiments. The data
    structure organizes assay data as one or more matrix-like objects (supporting
    dense matrices, sparse matrices, HDF5-backed arrays, and DelayedArray for out-of-memory
    computation) where rows typically represent genomic features (genes, transcripts,
    exons, peaks, CpG sites, genomic ranges) and columns represent biological samples
    (patients, conditions, replicates, time points). Each SummarizedExperiment object
    contains three primary components coordinated through shared row and column names
    ensuring data integrity through guaranteed synchronization during subsetting operations.
    The assays component stores primary experimental measurements as a list of matrices
    (raw counts, normalized counts, log-transformed values, TPM, FPKM, p-values, effect
    sizes) allowing multiple representations of the same experiment in parallel. The
    rowData (or rowRanges for genomic coordinates) component stores feature-level
    metadata as a DataFrame or GRanges object including gene symbols, Ensembl IDs,
    chromosome positions, strand information, gene biotypes, functional annotations,
    statistical summaries, or quality metrics for each measured feature. The colData
    component stores sample-level metadata as a DataFrame capturing experimental design
    variables (treatment groups, batch information, sequencing depth, clinical covariates,
    patient demographics, tissue types) essential for downstream statistical analysis.
    Additional metadata slots include the metadata list for experiment-level information
    (protocol descriptions, processing parameters, references), names for assay identifiers,
    and elementMetadata for slot-specific annotations. SummarizedExperiment supports
    advanced genomic operations through integration with GenomicRanges, enabling coordinate-based
    subsetting to extract features overlapping genomic regions of interest, interval
    arithmetic for computing coverage and enrichment, and annotation with regulatory
    elements or variant positions. The design ensures memory efficiency through support
    for HDF5-backed storage via HDF5Array allowing analysis of datasets exceeding
    RAM capacity, DelayedArray for lazy evaluation deferring computation until results
    are needed, and sparse matrix formats for single-cell data with predominantly
    zero values. Extensions include RangedSummarizedExperiment for genomic coordinate-aware
    operations, SingleCellExperiment adding reduced dimension embeddings and alternative
    experiments for single-cell transcriptomics, MultiAssayExperiment for multi-omics
    integration coordinating RNA-seq, proteomics, metabolomics, and clinical data
    from the same samples, and specialized derivatives for specific assay types (SpatialExperiment
    for spatial transcriptomics, TreeSummarizedExperiment for microbiome phylogenetic
    data). The format is extensively used across Bioconductor packages with over 400
    packages building upon SummarizedExperiment for RNA-seq analysis (DESeq2, edgeR,
    limma), ChIP-seq peak calling, DNA methylation profiling, variant calling workflows,
    single-cell genomics (Seurat conversion, scater, scran), spatial transcriptomics
    (Giotto, Seurat spatial), and machine learning applications where coordinated
    feature-sample matrices with rich annotations enable supervised learning, batch
    correction, dimensionality reduction, and multi-omics integration. The standardized
    structure facilitates interoperability across analysis tools, reproducible research
    through self-contained data objects preserving experimental context, and scalable
    workflows handling datasets from bulk RNA-seq to million-cell single-cell atlases.
  requires_registration: false
  url: https://bioconductor.org/packages/release/bioc/html/SummarizedExperiment.html
- id: B2AI_STANDARD:287
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Synthetic Biology Open Language
  formal_specification: https://sbolstandard.org/#specifications
  is_open: true
  name: SBOL
  purpose_detail: The Synthetic Biology Open Language (SBOL) is a data standard for
    representing synthetic biology designs in a machine-readable format, enabling
    the electronic exchange, storage, and computational manipulation of genetic constructs,
    circuits, and engineered biological systems. SBOL provides a formal, structured
    representation of DNA components, genetic parts (promoters, ribosome binding sites,
    coding sequences, terminators), hierarchical assemblies, functional annotations,
    and design constraints using XML or RDF serialization formats with precisely defined
    semantics based on community ontologies. The standard supports description of
    both natural and synthetic genetic elements with their sequences, functional roles,
    interactions, and provenance metadata, facilitating communication between synthetic
    biologists, genetic engineers, software tools, and biofabrication facilities.
    SBOL enables key workflows including electronic submission of designs to DNA synthesis
    providers and contract manufacturers, storage and versioning of genetic constructs
    in biological design repositories, computational design space exploration and optimization,
    model-based design linking genetic circuits to mathematical models (via integration
    with SBML), and embedding machine-readable genetic designs in scientific publications
    for reproducibility. The standard encompasses multiple levels of abstraction from
    abstract functional specifications to detailed sequence-level implementations,
    supporting combinatorial design where multiple variants are systematically generated,
    modular composition of reusable genetic parts from standardized libraries (BioBricks,
    iGEM Registry), and design-build-test-learn cycles common in synthetic biology
    workflows. SBOL 3.0, the current major version, introduced significant enhancements
    including representation of combinatorial designs with template-based assembly,
    support for protein and chemical entities beyond nucleic acids, constraints and
    design rules for automated validation, measures and parameters for quantitative
    specifications, improved provenance tracking with activities and agents, and topological
    descriptors for complex genetic architectures. Software ecosystem integration
    includes design tools (Benchling, GenoCAD, Eugene, Cello for genetic circuit design),
    simulation platforms coupling SBOL designs with dynamical models, repository systems
    (SynBioHub for federated part repositories, iGEM Registry integration), and workflow
    automation platforms connecting design, synthesis ordering, and experimental validation.
    Applications span metabolic pathway engineering for biomanufacturing, genetic
    circuit design for biosensors and diagnostics, genome-scale engineering projects,
    cell-free synthetic biology systems, and educational synthetic biology where standardized
    parts facilitate teaching and reproducible student projects. SBOL supports collaborative
    research by enabling teams to share complex multi-level designs unambiguously,
    computational design automation where genetic algorithms explore design spaces,
    and quality control ensuring designs meet specifications before expensive synthesis
    and testing, making it essential infrastructure for the reproducible, scalable
    practice of modern synthetic biology.
  requires_registration: false
  url: https://sbolstandard.org/
- id: B2AI_STANDARD:288
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Systems Biology Graphical Notation
  formal_specification: https://sbgn.github.io/downloads/specifications/pd_level1_version2.pdf
  is_open: true
  name: SBGN
  publication: doi:10.1038/nbt.1558
  purpose_detail: Systems Biology Graphical Notation (SBGN) is a standardized visual
    language for representing biological processes and relationships in maps. SBGN
    defines the precise meaning of all graphical symbols through three complementary
    languages - Process Description (PD) for biochemical reactions and molecular interactions,
    Activity Flow (AF) for the flow of information between biochemical entities, and
    Entity Relationship (ER) for relationships between biological entities independent
    of temporal aspects. Each language has specific symbols, syntax rules, and semantic
    definitions to ensure unambiguous interpretation across different tools and users.
    SBGN-ML, an XML-based exchange format, enables storage and transfer of SBGN diagrams
    between software applications, supported by the standard LibSBGN library. The
    notation is widely adopted in major biological databases including Reactome, PANTHER
    Pathway, BioModels, and Pathway Commons. Multiple software tools support SBGN
    diagram creation and editing (CellDesigner, Newt Editor, VANTED/SBGN-ED, PathVisio,
    yEd), format conversion from KEGG, BioPAX, and SBML, and visualization of pathway
    models, facilitating standardized communication of complex biological knowledge
    across the systems biology community.
  requires_registration: false
  url: https://sbgn.github.io/
- id: B2AI_STANDARD:289
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Systems Biology Markup Language
  has_relevant_organization:
  - B2AI_ORG:19
  is_open: true
  name: SBML
  publication: doi:10.1515/jib-2017-0081
  purpose_detail: The Systems Biology Markup Language (SBML) is a comprehensive XML-based
    exchange format specifically designed for representing computational models of
    biological processes in a machine-readable, standardized form. Developed and maintained
    by the international SBML community, SBML enables interoperability between diverse
    systems biology software tools by providing a common model representation language
    that eliminates translation errors and ensures consistent model interpretation
    across different platforms. While SBML excels at representing biochemical reaction
    networks, metabolic pathways, gene regulatory networks, and signal transduction
    cascades, its flexible architecture supports modeling of various biological phenomena
    from molecular to cellular scales. The format includes sophisticated features
    for describing reaction kinetics, species concentrations, compartmentalization,
    parameter sensitivity, and dynamic behaviors through mathematical expressions
    and differential equations. SBML's modular design supports extensions for specialized
    modeling requirements including spatial modeling, flux balance analysis, and multi-scale
    simulations. Supported by over 270 software tools, SBML serves as the de facto
    standard for systems biology model exchange, enabling reproducible research, model
    sharing, database integration, and collaborative development in computational
    systems biology.
  related_to:
  - B2AI_STANDARD:829
  requires_registration: false
  url: http://sbml.org/
- id: B2AI_STANDARD:290
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Systems Biology Results Markup Language
  is_open: false
  name: SBRML
  purpose_detail: Systems Biology has benefited tremendously from the development
    and use of SBML, which is a markup language to specify models composed of molecular
    species, and their interactions (including reactions). SBML is a common format
    that many systems biology software understand and thus it has become the way in
    which models are shared and communicated. Despite the popularity of SBML, that
    has resulted in many models being available in electronic format, there is currently
    no standard way of communicating the results of the operations carried out with
    such models (e.g. simulations). Here we propose a new markup language which is
    complementary to SBML and which is intended to specify results from operations
    carried out on models SBRML. In fact, this markup language is useful also to communicate
    experimental data as long as it is possible to express the data in terms of a
    reference SBML model. Thus SBRML is a means of specifying quantitative results
    in the context of a systems biology model.
  requires_registration: false
  url: https://sbrml.sourceforge.net/SBRML/Welcome.html
- id: B2AI_STANDARD:291
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Tabix index file format
  formal_specification: https://samtools.github.io/hts-specs/tabix.pdf
  is_open: true
  name: Tabix
  publication: doi:10.1093/bioinformatics/btq671
  purpose_detail: A tab-delimited genome position index file format. The format can
    handle individual chromosomes up to 512 Mbp (2^29 bases) in length.
  requires_registration: false
  url: http://www.htslib.org/doc/tabix.html
- id: B2AI_STANDARD:292
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Taxonomic Concept Transfer Schema
  formal_specification: https://github.com/tdwg/tcs
  is_open: true
  name: TCS
  purpose_detail: The development of an abstract model for a taxonomic concept, which
    can capture the various models represented and understood by the various data
    providers, is central to this project. This model is presented as an XML schema
    document that is proposed as a standard to allow exchange of data between different
    data models. It aims to capture data as understood by the data owners without
    distortion, and facilitate the query of different data resources according to
    the common schema model. The TCS schema was conceived to allow the representation
    of taxonomic concepts as defined in published taxonomic classifications, revisions
    and databases. As such, it specifies the structure for XML documents to be used
    for the transfer of defined concepts. Valid transfer documents may either explicitly
    detail the defining components of taxon concepts, transfer GUIDs referring to
    defined taxon concepts (if and when these are available) or a mixture of the two.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:93
  url: http://www.tdwg.org/standards/117
  has_application:
  - id: B2AI_APP:50
    category: B2AI:Application
    name: Taxonomic Data Integration and Species Classification AI
    description: TCS (Taxonomic Concept Schema) is used in AI applications for managing
      taxonomic concepts and classifications, enabling machine learning models to
      understand species relationships, resolve taxonomic synonyms, and integrate
      biodiversity data across different classification systems. AI systems leverage
      TCS to train models that automatically map species names across taxonomies,
      predict taxonomic placement of newly discovered organisms, and reconcile conflicting
      classifications. The schema supports natural language processing applications
      that extract taxonomic information from scientific literature, automated quality
      control of species occurrence records, and machine learning approaches to phylogenetic
      inference. TCS enables AI systems to reason about taxonomic hierarchies and
      evolutionary relationships when analyzing biological data.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:293
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Taxonomic Databases Working Group Standards Documentation Standard
  is_open: true
  name: TDWG SDS
  purpose_detail: This document defines how TDWG standards should be presented. Each
    standard is a logical directory or folder containing two or more files - a cover
    page outlining basic meta data for the standard and one or more normative files
    specifying the standard itself. Rules are specified for the naming of standards
    and files. Human readable files should be in English, follow basic layout principles
    and be marked up in XHTML. The legal statements that all documents must contain
    are defined.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:93
  url: http://www.tdwg.org/standards/147
- id: B2AI_STANDARD:294
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:3
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ToxML standard
  is_open: true
  name: ToxML
  publication: doi:10.1080/1062936X.2013.783506
  purpose_detail: ToxML is an open data exchange standard that allows the representation
    and communication of toxicological and related data in a well-structured electronic
    format.
  requires_registration: false
  url: https://doi.org/10.1080/1062936X.2013.783506
- id: B2AI_STANDARD:295
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Transparent Reporting of Evaluations with Nonrandomized Designs
  formal_specification: https://www.cdc.gov/trendstatement/pdf/trendstatement_TREND_Checklist.pdf
  is_open: true
  name: TREND
  publication: doi:10.2105/ajph.94.3.361
  purpose_detail: Evidence-based public health decisions are based on evaluations
    of intervention studies with randomized and nonrandomized designs. Transparent
    reporting is crucial for assessing the validity and efficacy of these intervention
    studies, and, it facilitates synthesis of the findings for evidence-based recommendations.
    Therefore, the mission of the Transparent Reporting of Evaluations with Nonrandomized
    Designs (TREND) group is to improve the reporting standards of nonrandomized evaluations
    of behavioral and public health intervention
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:12
  url: https://www.cdc.gov/trendstatement/index.html
- id: B2AI_STANDARD:296
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Trusted Instant Messaging Plus Applicability Statement
  formal_specification: https://directtrust.app.box.com/s/p3vp3g4bv52cyi4nbpyfpdal6t7z2qdz
  has_relevant_organization:
  - B2AI_ORG:26
  is_open: true
  name: TIM+
  purpose_detail: Trusted Instant Messaging (TIM+) defines a protocol that facilitates
    real-time communication and incorporates secure messaging concepts to ensure information
    is transmitted securely between known, trusted entities both within and across
    enterprises. TIM+ will determine the availability or presence of trusted endpoints
    and support text-based communication and file transfers.
  requires_registration: false
  url: https://directtrust.app.box.com/s/p3vp3g4bv52cyi4nbpyfpdal6t7z2qdz
- id: B2AI_STANDARD:297
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: TumorML standard
  is_open: true
  name: TumorML
  publication: doi:10.1145/2544063.2544064
  purpose_detail: Originally developed as part of the FP7 Transatlantic Tumor Model
    Repositories project, TumorML has been developed as an XML-based domain-specific
    vocabulary that includes elements from existing vocabularies, to deal with storing
    and transmitting existing cancer models among research communities.
  requires_registration: false
  url: https://doi.org/10.1145/2544063.2544064
- id: B2AI_STANDARD:298
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - codesystem
  concerns_data_topic:
  - B2AI_TOPIC:3
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Unique Ingredient Identifiers
  is_open: true
  name: UNII
  purpose_detail: Unique Ingredient Identifiers (UNIIs) are alphanumeric codes generated
    by the FDA's Global Substance Registration System (GSRS) to uniquely identify
    substances based on their scientific identity characteristics using ISO 11238
    data elements. Each UNII is derived from a substance's molecular structure and/or
    descriptive information, ensuring that the same substance always receives the
    same identifier regardless of when or where it is registered in the regulatory
    lifecycle. UNIIs enable efficient and accurate exchange of substance information
    across regulatory submissions, product labeling, adverse event reporting, and
    drug databases without ambiguity from varying nomenclature or trade names. The
    system covers diverse substance types including chemical compounds, proteins,
    nucleic acids, polymers, mixtures, structurally diverse materials, and specified
    substances (salts, stereoisomers, defined mixtures). UNIIs are generated at any
    time in the regulatory process and do not imply FDA review or approval. The GSRS
    database provides searchable access to UNIIs along with associated substance names,
    codes (CAS, INN, EC), structural representations, and attribute data. UNIIs are
    extensively used in SPL (Structured Product Labeling) documents, NDC (National
    Drug Code) directory, FAERS (FDA Adverse Event Reporting System), and international
    regulatory systems, facilitating interoperability between FDA databases and harmonization
    with global substance identification standards.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:31
  url: https://precision.fda.gov/uniisearch
- id: B2AI_STANDARD:299
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  - markuplanguage
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:13
  - B2AI_TOPIC:35
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Variant Call Format
  formal_specification: https://github.com/samtools/hts-specs
  has_relevant_organization:
  - B2AI_ORG:117
  is_open: true
  name: VCF
  purpose_detail: Variant Call Format (VCF) is a text file format (most likely stored
    in a compressed manner). It contains meta-information lines, a header line, and
    then data lines each containing information about a position in the genome.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:34
  url: https://en.wikipedia.org/wiki/Variant_Call_Format
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:51
    category: B2AI:Application
    name: Variant Pathogenicity Prediction and Genomic AI
    description: VCF format is essential for AI applications in genomics, particularly
      for training machine learning models to predict variant pathogenicity, disease
      associations, and functional impacts. Deep learning models parse VCF files to
      extract genetic variants and their annotations for tasks such as rare disease
      diagnosis, cancer genome interpretation, and pharmacogenomic prediction. AI
      systems leverage VCF's structured representation of genomic variants, quality
      scores, and population frequencies to build models that integrate variant-level
      data with clinical phenotypes, enabling precision medicine applications and
      automated variant classification according to ACMG guidelines.
    used_in_bridge2ai: false
  - id: B2AI_APP:248
    category: B2AI:Application
    name: DeepVariant and Cohort Joint-Calling Pipelines
    description: DeepVariant, a convolutional neural network-based variant caller using Inception-derived image classification architecture, produces genomic Variant Call Format (gVCF) files as primary output for per-sample variant calling, with gVCFs serving as inputs to GLnexus for scalable cohort joint-calling that merges single-sample gVCFs into population-scale cohort VCF files enabling downstream phasing (Eagle2), imputation (Beagle 5.0), and population genetics analyses. DeepVariant's gVCF quantization scheme yields substantially smaller file sizes compared to GATK HaplotypeCaller gVCFs (reducing storage costs and I/O bottlenecks in large-scale sequencing projects), while maintaining high accuracy with genotype quality (GQ) scores stored in VCF FORMAT fields guiding downstream filtering and quality control. The cohort VCF outputs serve as inputs to machine-learning-based variant quality score recalibration (GATK VQSR) which applies Gaussian mixture models to distinguish true variants from technical artifacts based on annotation features extracted from VCF INFO fields (mapping quality, strand bias, depth, allele balance), demonstrating VCF as the central data structure connecting deep learning variant calling, statistical recalibration, cohort aggregation, and reference panel construction. Phased cohort VCFs are converted into reference panels for genotype imputation enabling cost-effective genome-wide association studies where low-coverage or array-genotyped samples are imputed to high-density variant catalogs, with imputation accuracy evaluated by concordance metrics comparing imputed VCF genotypes against high-coverage truth VCFs. This workflow demonstrates VCF's role as the canonical interchange format linking deep learning inference (DeepVariant CNN processing aligned reads to emit probabilistic genotype calls), scalable merging algorithms (GLnexus joint-calling across thousands of samples), ML-driven quality filtering (VQSR iterative refinement), and population genetics tooling (phasing/imputation pipelines), supporting large biobanks (UK Biobank, All of Us, gnomAD) where millions of samples require consistent VCF-based variant representation enabling cross-study meta-analyses and polygenic risk score development.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2020.02.10.942086
  - id: B2AI_APP:249
    category: B2AI:Application
    name: DeepTrio Family-Based Variant Calling with Mendelian Correction
    description: The dv-trio pipeline implements family-based (trio) variant calling using DeepVariant deep neural network outputs, consuming per-sample gVCF files produced by DeepVariant and assembling them into trio VCF files with joint genotype calls for parent-offspring trios, then applying Mendelian-error-aware correction to refine genotypes and reduce false positives by leveraging inheritance constraints. The pipeline uses VCF genotype (GT) and genotype quality (GQ) fields to compute Mendelian errors (offspring genotypes inconsistent with parental genotypes under Mendelian inheritance), filters candidate errors based on GQ thresholds, and adjusts genotype calls or marks low-confidence sites, achieving approximately 60% reduction in Mendelian error rate compared to standard DeepVariant single-sample calling without compromising sensitivity for true de novo variants. An alternative approach (dv-gatk) creates DeepVariant gVCFs for each family member and merges them using GATK4 GenotypeGVCFs to produce co-called trio VCFs, demonstrating flexibility in combining deep learning variant calling with traditional joint-calling methods via the standardized gVCF intermediate format. Benchmarking is performed using Illumina hap.py tool comparing trio VCF outputs against GIAB (Genome in a Bottle) truth sets, with precision/recall/F1 metrics computed separately for SNVs and indels, and Mendelian concordance rates reported across multiple trios (validation on Ashkenazi Jewish trio, Chinese trio, and synthetic pedigrees). The workflow highlights VCF's role as the data structure enabling integration of deep learning inference (DeepVariant CNN-based genotyping), family-based correction logic (Mendelian filtering operating on VCF genotype fields), joint-calling algorithms (GATK or GLnexus merging gVCFs), and standardized benchmarking (hap.py comparing VCF outputs to truth VCFs), supporting clinical genetics applications where accurate de novo variant detection is critical for diagnosing rare genetic disorders in parent-offspring trios and enabling trio-based research studies investigating germline mutation rates and inheritance patterns.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1093/bioinformatics/btaa116
  - id: B2AI_APP:250
    category: B2AI:Application
    name: Deep Learning Variant Callers Producing VCF Outputs
    description: Multiple deep learning architectures for variant calling from sequencing reads produce VCF files as standardized output format, enabling interoperability with downstream analysis pipelines and demonstrating VCF's role as the canonical representation for ML-predicted genetic variants. HELLO implements problem-specific deep neural networks tailored for small-variant calling (substitutions and indels) with architectures smaller than standard Inception-based models, achieving improved indel accuracy while producing VCF records with allele-specific annotations and quality scores. DeepSV applies deep convolutional neural networks to structural variant detection, specifically focusing on deletion calling, where the model visualizes aligned reads in fixed-size windows, trains CNNs to distinguish true deletions from alignment artifacts, and outputs VCF records with structural variant annotations (SVTYPE=DEL, SVLEN specifying deletion length, END position) conforming to VCF 4.2 structural variant specification, reducing false-positive rates compared to traditional read-depth and split-read callers. Clairvoyante (and its successor Clair) implements multi-task convolutional neural networks predicting variant type, zygosity, and alternate alleles simultaneously across multiple sequencing technologies (Illumina short reads, PacBio/Nanopore long reads), outputting VCF files with platform-appropriate quality annotations and supporting both small variants and complex events, with model architecture processing read pileup images and outputting per-position variant probabilities encoded as VCF genotype likelihoods (PL field). These tools collectively demonstrate convergence on VCF as the standard output format regardless of underlying neural architecture (problem-specific networks, CNNs, multi-task models), variant type (SNVs, indels, structural variants), or sequencing technology (short-read, long-read, single-molecule), with VCF's flexible INFO and FORMAT fields accommodating model-specific annotations (e.g., neural network confidence scores, alternative allele rankings, read support metrics) while maintaining compatibility with standard VCF parsers and downstream tools (variant annotation via VEP/ANNOVAR, variant filtering, genotype merging, benchmarking against truth sets), supporting the genomics community's need for consistent data interchange formats enabling fair comparison of ML-based callers, ensemble calling strategies combining predictions from multiple models, and integration into clinical sequencing pipelines requiring validated VCF outputs for diagnostic reporting.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1186/s40246-022-00396-x
  - id: B2AI_APP:251
    category: B2AI:Application
    name: DeepPVP Phenotype-Driven Variant Prioritization
    description: DeepPVP implements a deep neural network for phenotype-driven prioritization of candidate causative variants, accepting an individual's whole-genome or whole-exome VCF file together with Human Phenotype Ontology (HPO) terms describing the patient's clinical phenotype and mode of inheritance (autosomal dominant/recessive, X-linked), then ranking all variants in the VCF by predicted causality and returning a prioritized list with the top candidate(s) for diagnostic follow-up. The model was trained and evaluated using synthetic patient VCFs created by inserting known ClinVar pathogenic variants into 1000 Genomes Project population VCF backgrounds (creating artificial cases with known causative variants at various allele frequencies), with evaluation also performed on a real whole-genome VCF from the Personal Genome Project containing 4,120,185 variants, demonstrating scalability to large VCF files. Performance benchmarking shows DeepPVP processing the whole-genome VCF in approximately 85 minutes (~1.3 milliseconds per variant) compared to 189 minutes for Exomiser (without CADD precomputation) and 800 minutes for Exomiser with CADD, highlighting computational efficiency advantages of the deep learning approach. The deep neural network architecture outperforms a random forest baseline (DeepPVP-RF) and existing tools (PVP, Exomiser, CADD, DANN, GWAVA) on metrics including top-hit recovery rate, top-10 recovery rate, ROC AUC, and precision-recall AUC across different inheritance modes and variant frequencies, with detailed performance breakdowns reported for de novo, autosomal recessive homozygous/compound heterozygous, and autosomal dominant variants. The system processes VCF variant records by extracting genomic context (gene annotations, variant consequences from VEP-like predictors), integrating pathogenicity scores (CADD, conservation metrics), mapping variants to affected genes and proteins, computing phenotype-gene associations using HPO semantic similarity and disease-gene knowledge graphs, and feeding multidimensional feature vectors into the deep neural network which learns nonlinear relationships between variant properties, gene function, and phenotypic outcomes. This application demonstrates VCF serving as the primary input to ML-driven clinical diagnostics, where the standardized variant representation enables integration of sequence-level information (allele frequencies, quality scores, genotype calls), functional annotations (consequence predictions, pathogenicity scores), and phenotype-driven prioritization, supporting rare disease diagnosis workflows where clinicians upload patient VCFs and phenotype descriptions to receive ranked candidate variants for confirmatory testing and therapeutic decision-making.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1186/s12859-019-2633-8
  - id: B2AI_APP:252
    category: B2AI:Application
    name: Neural Network Variant Filtering and Classification
    description: Post-calling variant filtering and classification tools apply neural networks to VCF files for distinguishing true genetic variants from technical artifacts, processing VCF records in batch mode to evaluate call quality and emit filtered VCF outputs or annotated reports. Intelli-NGS implements artificial neural networks (ANNs) to process VCF files generated from Ion Torrent sequencing, classifying variants as high-confidence or low-confidence calls based on learned patterns in VCF annotation fields (depth of coverage, strand bias, mapping quality, base quality scores, allele balance), and providing Human Genome Variation Society (HGVS) nomenclature codes for all variants in the output, demonstrating integration of ML-based quality control with standardized variant nomenclature. GARFIELD-NGS extends this approach with deeper neural networks (DNNs) combined with multi-layer perceptrons (MLPs) for variant filtering and feature engineering, consuming VCF files from Illumina platforms and applying learned classification models to distinguish true positives from false positives based on multivariate patterns in sequencing quality metrics that traditional hard-filter approaches (fixed thresholds on individual metrics) fail to capture, with the system outputting filtered VCF files or Excel reports with variant classifications and confidence scores. Both systems operate on the principle that neural networks can learn complex, nonlinear decision boundaries in high-dimensional feature spaces defined by VCF annotation fields, capturing interactions between quality metrics that indicate systematic sequencing errors (e.g., low mapping quality correlated with high strand bias suggesting alignment artifacts, or low base quality at homopolymer runs indicating sequencing chemistry issues), and generalizing across samples to provide consistent quality filtering without manual threshold tuning for each dataset. The tools demonstrate VCF's dual role as input to ML models (where INFO and FORMAT fields provide rich feature vectors for classification) and as output format (where filtered variants are written back to VCF maintaining compatibility with downstream analysis tools), supporting clinical sequencing laboratories implementing ML-based quality control pipelines to reduce manual variant review burden while maintaining high sensitivity for true variants, research studies requiring standardized variant filtering across large cohorts to minimize batch effects, and comparative evaluations of variant calling pipelines where ML-based filters can be consistently applied to VCF outputs from different callers (GATK, FreeBayes, Strelka, DeepVariant) to harmonize quality standards and improve cross-study reproducibility.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1186/s40246-022-00396-x
    - https://doi.org/10.48048/tis.2025.9149
  - id: B2AI_APP:253
    category: B2AI:Application
    name: VariantSpark Distributed ML on VCF Genotype Matrices
    description: VariantSpark provides a scalable interface between Apache Spark distributed computing framework with MLlib machine learning library and standard VCF files, enabling population-scale machine learning on genotype data by extracting genotype matrices from multi-sample VCF files (where rows represent samples and columns represent variant sites with genotype encodings 0/1/2 for reference homozygous/heterozygous/alternate homozygous) and applying distributed ML algorithms for clustering, genome-wide association, and variant importance ranking. The system reads VCF files using Spark's distributed file I/O, parses genotype fields (GT format field with phased or unphased alleles) across thousands to millions of samples in parallel, converts categorical genotypes into numerical feature matrices suitable for ML algorithms, and distributes computation across Spark worker nodes to handle datasets exceeding single-machine memory capacity (supporting biobank-scale VCF files with 100,000+ samples and 10+ million variants). VariantSpark implements random forest-based importance analysis identifying variants most predictive of phenotypic outcomes, principal component analysis (PCA) for population structure inference detecting ancestry-related clustering in genotype space, and k-means clustering discovering subpopulations with shared genetic ancestry, all operating directly on VCF-derived genotype matrices without requiring intermediate file format conversions. The tool demonstrates VCF's role as the source format for large-scale ML feature engineering in genomics, where standardized genotype representation enables systematic extraction of predictor variables (variant genotypes) and their integration with phenotype labels (case/control status, quantitative traits, drug response outcomes) for supervised learning, while distributed processing addresses computational challenges of modern biobanks where VCF files reach terabyte scale and traditional single-machine analysis becomes infeasible. Applications include genome-wide association studies (GWAS) using ML classifiers more flexible than linear regression (capturing epistatic interactions and nonlinear genetic architectures), polygenic risk score development through feature selection identifying variants with strongest predictive power, admixture mapping in diverse populations where ML-based clustering refines ancestry assignments beyond principal components, and rare variant association testing where ML aggregation methods combine signals across multiple low-frequency variants in genes or regulatory regions, supporting precision medicine initiatives requiring integration of genomic variation (encoded in VCF) with clinical outcomes through scalable, reproducible ML pipelines operating on industry-standard big data infrastructure.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1186/s40246-022-00396-x
  - id: B2AI_APP:254
    category: B2AI:Application
    name: GWAS-VCF Format for ML-Ready Summary Statistics
    description: GWAS-VCF adapts the Variant Call Format to store genome-wide association study (GWAS) summary statistics by repurposing VCF's SAMPLE/FORMAT fields to hold per-variant association metrics (effect size ES, standard error SE, -log10 p-value LP) while using INFO fields for allele frequencies, functional annotations, and harmonized metadata, creating an indexed, queryable data container for GWAS results that supports downstream machine learning workflows requiring harmonized variant-level features and rapid genomic queries. The format uses VCF header lines to store rich study-level metadata (trait descriptions, sample size, analysis methods, population ancestry, genome build, phenotype definitions), enabling automated provenance tracking and reproducible analyses, while each variant record captures CHROM/POS/REF/ALT allele definitions with left-alignment and normalization (via vgraph allele matching) ensuring consistent variant representation across studies for meta-analysis. GWAS-VCF files are indexed with tabix for fast positional queries (extracting all associations in a genomic region in milliseconds without loading the entire file) and with rsidx for dbSNP rsID-based lookups, supporting interactive exploration and programmatic access patterns common in ML feature engineering pipelines where specific loci or candidate gene regions are repeatedly queried. The gwas2vcf Python tool provides conversion from standard GWAS summary statistic formats (tab-delimited files with varied column naming conventions) to standardized GWAS-VCF with automated allele harmonization (flipping effect directions for strand mismatches, left-aligning indels, resolving ambiguous A/T and G/C SNPs using allele frequency priors), quality control filtering (removing variants with mismatched allele frequencies, impossible effect size/standard error combinations, palindromic SNPs without clear strand assignment), and metadata validation ensuring completeness for downstream analyses. Ecosystem integration enables reading GWAS-VCF files into R (VariantAnnotation package) and Python (pysam, cyvcf2) for statistical analysis and ML modeling, command-line processing with bcftools (filtering, annotation, format conversion), GATK (variant manipulation, liftover between genome builds), bedtools (intersection with genomic features), and plink (integration with individual-level genotype data in VCF format for two-sample Mendelian randomization and colocalization analyses). For machine learning applications, GWAS-VCF provides ML-ready input data where each variant's effect size and significance serve as response variables or feature labels for polygenic score construction (selecting variants and computing weighted sums of effect sizes), trans-ethnic meta-analysis models (learning ancestry-specific effect size patterns and predicting cross-population transferability), fine-mapping algorithms (combining association statistics with linkage disequilibrium matrices from reference VCFs to identify causal variants in associated loci), and colocalization methods (assessing shared causal variants between GWAS traits or between GWAS and molecular QTLs using effect size correlation patterns). The format addresses practical ML workflow requirements including rapid feature extraction (querying association statistics for gene sets or functional annotations without full file scans), consistent variant identifiers across datasets (standardized CHROM:POS:REF:ALT notation enabling joins between GWAS-VCF files and genotype VCFs or annotation databases), and streaming I/O for cloud computing (block GZIP compression with tabix indexing enables range requests extracting chromosome-specific subsets without transferring entire files, reducing bandwidth costs in cloud-based ML pipelines analyzing hundreds of GWAS traits), supporting large-scale genomics ML projects including UK Biobank PheWAS (phenome-wide association studies correlating thousands of traits with millions of variants), polygenic risk score repositories (aggregating GWAS-VCF files across traits for multi-trait prediction models), and AI-driven drug target identification (integrating GWAS-VCF associations with expression QTLs and protein-protein interaction networks to prioritize therapeutic targets via causal inference and graph neural networks).
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1186/s13059-020-02248-0
- id: B2AI_STANDARD:300
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Variant Effect Predictor format
  has_relevant_organization:
  - B2AI_ORG:29
  is_open: true
  name: VEP
  purpose_detail: Variant Effect Predictor (VEP) format is a tab-delimited text format devised by Ensembl for representing functional annotations and predicted effects of genetic variants on genes, transcripts, proteins, and regulatory regions, serving as the primary output format for Ensembl VEPthe widely-adopted variant annotation tool that processes millions of variants from whole-genome and exome sequencing studies to predict biological consequences ranging from synonymous changes to loss-of-function mutations, splice site disruptions, and regulatory element modifications. The format consists of a header section with metadata (VEP version, command-line options, annotation sources, database versions, analysis timestamp) followed by variant records in rows with dozens of tab-separated columns, with a default minimal set including Uploaded_variation (original variant identifier from input VCF), Location (chromosome:position), Allele (alternate allele), Gene (Ensembl stable gene ID like ENSG00000139618 for BRCA2), Feature (transcript ID like ENST00000380152), Feature_type (Transcript/RegulatoryFeature/MotifFeature), Consequence (standardized Sequence Ontology terms like missense_variant, stop_gained, splice_donor_variant), cDNA_position (position in transcript cDNA), CDS_position (position in coding sequence), Protein_position (position in protein), Amino_acids (reference/alternate amino acids), Codons (reference/alternate codons), Existing_variation (dbSNP rsIDs, ClinVar/COSMIC identifiers for known variants), and Extra column containing semicolon-separated key=value pairs for additional annotations. VEP's extensibility allows augmentation with plugin-based annotations including SIFT scores (predicting whether amino acid substitutions affect protein function, threshold <0.05 indicating deleterious), PolyPhen-2 scores (predicting pathogenicity with benign/possibly damaging/probably damaging classifications), CADD scores (Combined Annotation Dependent Depletion ranking variant deleteriousness on phred-scaled 0-99 scale where >20 suggests top 1% most deleterious variants), gnomAD allele frequencies (population frequencies from 125,748 exomes and 71,702 genomes enabling filtering of common benign variants), ClinVar clinical significance (pathogenic/likely pathogenic/benign/likely benign/uncertain significance classifications from expert-curated database), conservation scores (PhyloP/PhastCons measuring evolutionary conservation across species), splicing predictions (dbscSNV/MaxEntScan/SpliceAI scores identifying cryptic splice sites and branchpoint disruptions), regulatory annotations (chromatin states, transcription factor binding sites, enhancer/promoter overlaps from ENCODE/Roadmap Epigenomics), and custom annotations from user-provided VCF/BED/GFF files or remote databases. VEP format supports multi-allelic sites where each alternate allele receives a separate row with consequences calculated independently, accommodates multiple transcript annotations per variant (showing effects across canonical transcript, MANE Select transcript, Ensembl/RefSeq transcripts with varying severities), and handles complex consequences with comma-separated lists (e.g., "missense_variant,splice_region_variant" when variant affects both coding and splicing). The format integrates with standard genomic workflows VEP input accepts VCF, JSON, or Ensembl region notation; output VEP format can be converted back to VCF with consequence annotations in INFO/CSQ field using --vcf flag, exported to JSON with --json flag for programmatic parsing, or consumed by downstream tools including Ensembl Variant Recoder, VAtools for precision oncology, OpenCRAVAT for clinical interpretation, and custom parsers in R/Python for machine learning feature extraction. For AI/ML applications, VEP format serves as rich feature representation for variant effect prediction models training supervised classifiers (random forests, gradient boosting, deep neural networks) on labeled pathogenic/benign variants using VEP consequence types, conservation scores, population frequencies, and in silico predictions as features to build ensemble models outperforming individual predictors; variant prioritization pipelines ranking candidate disease-causing variants by aggregating VEP annotation scores with inheritance patterns and phenotype matching; splice variant analysis models using VEP's splice predictions combined with RNA-seq junction reads to train deep learning models identifying cryptic splicing events; cancer driver identification algorithms integrating VEP consequences, COSMIC mutation frequencies, and protein domain annotations to distinguish driver mutations from passenger variants in tumor sequencing; and pharmacogenomics models correlating VEP-annotated variants in drug metabolism genes (CYP2D6, CYP2C19, TPMT) with clinical outcomes to predict drug response, enabling precision medicine applications where genetic variants inform treatment decisions and supporting large-scale variant interpretation in clinical genomics, population genetics, and functional genomics research where systematic annotation of millions of variants requires standardized, comprehensive, machine-readable output capturing diverse biological effects and predictive scores.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:124
  url: https://useast.ensembl.org/info/docs/tools/vep/vep_formats.html
- id: B2AI_STANDARD:301
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Variation Representation Specification
  has_relevant_organization:
  - B2AI_ORG:117
  is_open: true
  name: VRS
  publication: doi:10.1016/j.xgen.2021.100027
  purpose_detail: The Variation Representation Specification (VRS, pronounced verse)
    is a standard developed by the Global Alliance for Genomics and Health (GA4GH)
    to facilitate and improve sharing of genetic information. The Specification consists
    of a JSON Schema for representing many classes of genetic variation, conventions
    to maximize the utility of the schema, and a Python implementation that promotes
    adoption of the standard.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:34
  url: https://vrs.ga4gh.org/en/latest/index.html
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:52
    category: B2AI:Application
    name: Standardized Variant Representation for ML Interoperability
    description: VRS (Variation Representation Specification) is used in AI applications
      to create unambiguous, computationally accessible representations of genetic
      variants that enable machine learning model interoperability across different
      genomic databases and variant calling pipelines. AI systems leverage VRS to
      normalize variant representations, resolve ambiguities in variant nomenclature,
      and create consistent feature representations for training models that predict
      variant effects, interpret clinical significance, or perform variant prioritization.
      VRS's precise coordinate system and allele representation enable AI models to
      correctly match variants across different genome builds, integrate data from
      multiple sources, and generate reproducible predictions that can be shared across
      institutions and validated against standardized variant benchmarks.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:302
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:35
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: VarioML format
  formal_specification: https://github.com/VarioML/VarioML
  is_open: true
  name: VarioML
  purpose_detail: A Locus-specific Database (LSDB) describes the variants discovered
    on a single gene or members of a gene family and other related functional elements.
    LSDBs are curated by experts on their respective loci, and as such are typically
    the best resources of such information available. But LSDBs vary widely in format
    and completeness, making data integration and exchange among them difficult and
    time-consuming. To address these difficulties, the VarioML format has been developed
    for the full range of variation data use-cases, providing semantically well-defined
    components which can be easily composed to fit specific needs. Using VarioML,
    data owners can now efficiently enable the integration, federation, and exchange
    of their variant data. The discoverabiliaty, extensibility, and quality of variation
    data is immediately enhanced. Critical new avenues of research and knowledge discovery
    are opened, as data using the VarioML standard can be integrated with the global
    library of purely genetic data. VarioML is a central prerequisite for effective
    modelling of phenotype data and genotype-to-phenotype relationships. It removes
    the long-standing technical obstacles to the effective passing of variant data
    from discovery laboratories into the biomedical database world. Now all that is
    needed is the broad participation of the genotype-to-phenotype research community.
  requires_registration: false
  url: https://github.com/VarioML/VarioML
- id: B2AI_STANDARD:303
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Vocabulary Maintenance Standard
  has_relevant_organization:
  - B2AI_ORG:93
  is_open: true
  name: VMS
  purpose_detail: The Vocabulary Maintenance Standard (VMS) is a TDWG (Biodiversity
    Information Standards) specification that defines formal procedures for maintaining
    and evolving controlled vocabularies used in biodiversity informatics. Unlike
    traditional standards that remain relatively static, VMS recognizes that vocabulary
    standards must adapt incrementally to meet evolving community needs without requiring
    the full standards ratification process for every change. The specification categorizes
    types of vocabulary modifications (additions, modifications, deprecations) and
    establishes governance mechanisms for implementing these changes through designated
    vocabulary maintenance groups. VMS defines roles and responsibilities for stakeholders
    including vocabulary maintainers, Interest Groups, and the broader TDWG community,
    specifying how change proposals are submitted, reviewed, and approved. The standard
    ensures that vocabulary evolution remains transparent, traceable, and backward-compatible
    where possible, while maintaining the stability needed for production systems.
    VMS applies to major TDWG vocabularies including Darwin Core terms and is essential
    for maintaining semantic interoperability across biodiversity data systems as
    scientific understanding and data practices evolve.
  requires_registration: false
  url: https://www.tdwg.org/standards/vms/
- id: B2AI_STANDARD:304
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - diagnosticinstrument
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Voice Handicap Index
  is_open: false
  name: VHI
  publication: doi:10.1044/1058-0360.0603.66
  purpose_detail: A statistically robust Voice Handicap Index (VHI). An 85-item version
    of this instrument was administered to 65 consecutive patients seen in the Voice
    Clinic at Henry Ford Hospital. The data were subjected to measures of internal
    consistency reliability and the initial 85-item version was reduced to a 30-item
    final version. This final version was administered to 63 consecutive patients
    on two occasions in an attempt to assess test-retest stability, which proved to
    be strong. The findings of the latter analysis demonstrated that a change between
    two administrations of 18 points represents a significant shift in psychosocial
    function.
  requires_registration: false
  url: https://doi.org/10.1044/1058-0360.0603.66
- id: B2AI_STANDARD:305
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: W3C HCLS Dataset Description
  is_open: true
  name: HCLS
  purpose_detail: Access to consistent, high-quality metadata is critical to finding,
    understanding, and reusing scientific data. This document describes a consensus
    among participating stakeholders in the Health Care and the Life Sciences domain
    on the description of datasets using the Resource Description Framework (RDF).
    This specification meets key functional requirements, reuses existing vocabularies
    to the extent that it is possible, and addresses elements of data description,
    versioning, provenance, discovery, exchange, query, and retrieval.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:99
  url: https://www.w3.org/TR/hcls-dataset/
- id: B2AI_STANDARD:306
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Wiggle Format
  is_open: true
  name: WIG
  purpose_detail: Wiggle files and its bedgraph variant allow you to plot quantitative
    data as either shades of color (dense mode) or bars of varying height (full and
    pack mode) on the genome.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:119
  url: https://genome.ucsc.edu/goldenPath/help/wiggle.html
- id: B2AI_STANDARD:307
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - audiovisual
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:19
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Zeiss LSM series confocal microscope image
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:19
  is_open: false
  name: LSM
  purpose_detail: A proprietary image format based on TIFF.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:101
  url: https://openwetware.org/wiki/Dissecting_LSM_files
- id: B2AI_STANDARD:308
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  concerns_data_topic:
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Adaptive Multi-Rate Speech Codec
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:49
  is_open: true
  name: AMR
  purpose_detail: Audio data compression scheme optimized for speech coding, adopted
    in October 1998 as the standard speech codec by 3GPP (3d Generation Partnership
    Project) and now widely used in GSM (Global System for Mobile Communications).
  requires_registration: false
  url: https://www.loc.gov/preservation/digital/formats/fdd/fdd000254.shtml
- id: B2AI_STANDARD:309
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Analyst native acquisition file format
  is_open: false
  name: WIFF
  publication: doi:10.1074/mcp.R112.019695
  purpose_detail: Mass spectra output format used by AB SCIEX intstruments.
  requires_registration: false
  url: https://doi.org/10.1074/mcp.R112.019695
- id: B2AI_STANDARD:310
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Analyze file format
  formal_specification: https://analyzedirect.com/documents/AD_AnalyzeImage75_File_Format.pdf
  is_open: true
  name: HDR/IMG
  purpose_detail: Analyze 7.5 can store voxel-based volumes and consists of two files
    - One file with the actual data in a binary format with the filename extension
    .img and another file (header with filename extension .hdr) with information about
    the data such as voxel size and number of voxel in each dimension.
  requires_registration: false
  url: https://analyzedirect.com/documents/AD_AnalyzeImage75_File_Format.pdf
- id: B2AI_STANDARD:311
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Apache Arrow
  formal_specification: https://github.com/apache/arrow
  has_relevant_organization:
  - B2AI_ORG:5
  is_open: true
  name: Arrow
  purpose_detail: A language-independent columnar memory format for flat and hierarchical
    data, organized for efficient analytic operations on modern hardware like CPUs
    and GPUs. The Arrow memory format also supports zero-copy reads for lightning-fast
    data access without serialization overhead.
  requires_registration: false
  url: https://arrow.apache.org/
- id: B2AI_STANDARD:312
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Apache Avro
  formal_specification: https://github.com/apache/avro
  has_relevant_organization:
  - B2AI_ORG:5
  is_open: true
  name: Avro
  purpose_detail: Avro is a row-oriented remote procedure call and data serialization
    framework developed within Apache's Hadoop project. It uses JSON for defining
    data types and protocols, and serializes data in a compact binary format.
  requires_registration: false
  url: https://avro.apache.org/
- id: B2AI_STANDARD:313
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Asset Description Metadata Schema
  has_relevant_organization:
  - B2AI_ORG:99
  is_open: true
  name: ADMS
  purpose_detail: ADMS is a profile of DCAT, used to describe semantic assets (or
    just 'Assets'), defined as highly reusable metadata (e.g. xml schemata, generic
    data models) and reference data (e.g. code lists, taxonomies, dictionaries, vocabularies)
    that are used for eGovernment system development.
  requires_registration: false
  url: https://www.w3.org/TR/vocab-adms/
- id: B2AI_STANDARD:314
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Audio Interchange File Format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:49
  is_open: true
  name: AIFF
  purpose_detail: File format for sound that wraps various sound bitstreams, ranging
    from uncompressed waveform to MIDI.
  requires_registration: false
  url: https://www.loc.gov/preservation/digital/formats/fdd/fdd000005.shtml
- id: B2AI_STANDARD:315
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:15
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Audio Video Interleave digital multimedia container format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:49
  is_open: true
  name: AVI
  purpose_detail: AVI files can contain both audio and video data in a file container
    that allows synchronous audio-with-video playback.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/Audio_Video_Interleave
- id: B2AI_STANDARD:316
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Bitmap format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:19
  is_open: true
  name: BMP
  purpose_detail: 'BMP (Bitmap) is a raster graphics file format developed by Microsoft for storing
    device-independent bitmaps (DIBs) in Windows and OS/2 systems. The format consists
    of a file header, DIB header (with multiple versions: BITMAPCOREHEADER, BITMAPINFOHEADER,
    BITMAPV4HEADER, BITMAPV5HEADER), optional color palette, and pixel array storing
    uncompressed or RLE-compressed bitmap data. BMP supports 1, 4, 8, 16, 24, and
    32 bits per pixel, accommodating indexed color (palettes), RGB color, and alpha
    channels for transparency. The format uses little-endian byte ordering and stores
    pixel rows bottom-to-top by default, with each row padded to 4-byte boundaries.
    BMP files can contain color profiles (ICC) for color management and support various
    compression methods including BI_RGB (uncompressed), BI_RLE4/BI_RLE8 (run-length
    encoding), and BI_BITFIELDS (custom RGB bit masks). The simplicity, widespread
    familiarity, and open format specification make BMP common in Windows applications,
    image processing software, and scientific imaging. While BMP files are typically
    large due to minimal compression, they compress efficiently with lossless algorithms
    (ZIP, RAR). BMP is used in GDI (Graphics Device Interface) subsystems, icons (ICO),
    cursors (CUR), and as an intermediate format for image processing. Software support
    is extensive: Adobe Photoshop, GIMP, Microsoft Office, browsers (Chrome, Edge),
    and programming libraries across platforms. In AI/ML imaging applications, BMP
    serves as a raw, lossless format for medical imaging (preserving pixel accuracy),
    training data preparation (avoiding compression artifacts), image annotation workflows,
    and intermediate processing steps where format simplicity facilitates pixel-level
    manipulation for computer vision tasks, though typically converted to compressed
    formats for efficient storage and transmission.'
  requires_registration: false
  url: https://en.wikipedia.org/wiki/BMP_file_format
- id: B2AI_STANDARD:317
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Cap'n'Proto
  formal_specification: https://github.com/capnproto/capnproto
  is_open: true
  name: Cap'n'Proto
  purpose_detail: Capn Proto is an insanely fast data interchange format and capability-based
    RPC system. Think JSON, except binary. Or think Protocol Buffers, except faster.
  requires_registration: false
  url: https://capnproto.org/
- id: B2AI_STANDARD:318
  category: B2AI_STANDARD:DataStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CellML language
  is_open: true
  name: CellML
  publication: doi:10.1186/1471-2105-11-178
  purpose_detail: CellML language is an open standard based on the XML markup language.
    CellML is being developed by the Auckland Bioengineering Institute at the University
    of Auckland and affiliated research groups. The purpose of CellML is to store
    and exchange computer-based mathematical models. CellML allows scientists to share
    models even if they are using different model-building software. It also enables
    them to reuse components from one model in another, thus accelerating model building.
  requires_registration: false
  url: https://www.cellml.org/
- id: B2AI_STANDARD:319
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: citation-file-format
  formal_specification: https://github.com/citation-file-format/citation-file-format
  is_open: true
  name: CFF
  purpose_detail: A file format for providing citation metadata for software or datasets
    in plaintext files that are easy to read by both humans and machines.
  requires_registration: false
  url: https://citation-file-format.github.io/
- id: B2AI_STANDARD:320
  category: B2AI_STANDARD:DataStandard
  collection:
  - workflowlanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Common Workflow Language
  formal_specification: https://github.com/common-workflow-language/common-workflow-language
  is_open: true
  name: CWL
  publication: doi:10.48550/arXiv.2105.07028
  purpose_detail: specification for describing data analysis workflows and tools
  requires_registration: false
  url: https://www.commonwl.org/
- id: B2AI_STANDARD:321
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CURATE(D) Checklists
  formal_specification: https://docs.google.com/document/d/1RWt2obXOOeJRRFmVo9VAkl4h41cL33Zm5YYny3hbPZ8/edit
  is_open: true
  name: CURATE(D)
  purpose_detail: The CURATE(D) steps are a teaching and representation tool. This
    model is useful for onboarding data curators and orienting researchers preparing
    to share their data. It serves as a demonstration for the type of work involved
    in robust data curation, and was created to fit within institution-specific data
    repository workflows.
  requires_registration: false
  url: https://datacurationnetwork.org/outputs/workflows/
- id: B2AI_STANDARD:322
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DAMA Data Management Body of Knowledge
  has_relevant_organization:
  - B2AI_ORG:22
  is_open: false
  name: DAMA-DMBOK
  purpose_detail: Reference guide for processes, best practices, and principles in
    data management.
  requires_registration: true
  url: https://www.dama.org/cpages/body-of-knowledge
- id: B2AI_STANDARD:323
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Data Catalog Vocabulary
  has_relevant_organization:
  - B2AI_ORG:99
  is_open: true
  name: DCAT
  purpose_detail: An RDF vocabulary designed to facilitate interoperability between
    data catalogs published on the Web.
  requires_registration: false
  url: https://www.w3.org/TR/vocab-dcat-1/
- id: B2AI_STANDARD:324
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DataCite Metadata Schema
  formal_specification: https://schema.datacite.org/meta/kernel-4.3/metadata.xsd
  is_open: true
  name: DataCite
  purpose_detail: The DataCite Metadata Schema is a list of core metadata properties
    chosen for the accurate and consistent identification of a resource for citation
    and retrieval purposes, along with recommended use instructions. The resource
    that is being identified can be of any kind, but it is typically a dataset. We
    use the term dataset in its broadest sense. We mean it to include not only numerical
    data, but any other research data outputs.
  requires_registration: false
  url: https://schema.datacite.org/
- id: B2AI_STANDARD:325
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Dataset Cards
  is_open: true
  name: Dataset Cards
  purpose_detail: 'Dataset Cards are structured documentation templates for describing machine
    learning datasets, introduced by Google and adopted widely in the ML community,
    particularly through Hugging Face''s Datasets library. Dataset Cards provide standardized
    metadata capturing dataset purpose, structure, creation methodology, intended uses,
    limitations, and ethical considerations. The documentation includes: dataset summary
    and description; language coverage and data sources; supported ML tasks and features;
    dataset structure (splits, configurations, data fields); creation process (annotations,
    quality control, collection methodology); considerations for using the data (social
    impact, biases, privacy concerns); additional resources (papers, repositories,
    licenses). Dataset Cards promote responsible AI by making dataset characteristics
    transparent, helping practitioners assess fitness for purpose, understand potential
    biases, and evaluate dataset limitations before use. The structured format enables
    dataset discovery on ML platforms, facilitates reproducibility by documenting provenance,
    and encourages accountability in dataset creation and curation. Hugging Face hosts
    100,000+ dataset cards in their Hub, standardizing documentation for NLP, computer
    vision, audio, and multimodal datasets. Dataset Cards complement Model Cards by
    extending documentation principles to training data, addressing data quality, collection
    practices, annotation procedures, and data ethics. In AI/ML pipelines, Dataset
    Cards support informed dataset selection, bias mitigation through transparency,
    reproducible research by documenting data versions, and regulatory compliance (AI
    Act, GDPR) by clarifying data provenance, consent, and usage restrictions.'
  requires_registration: false
  url: https://huggingface.co/docs/datasets/dataset_card
- id: B2AI_STANDARD:326
  category: B2AI_STANDARD:DataStandard
  collection:
  - datasheets
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Datasheets for Datasets
  is_open: true
  name: Datasheets
  publication: https://arxiv.org/abs/1803.09010
  purpose_detail: '...we propose that every dataset be accompanied with a datasheet
    that documents its motivation, composition, collection process, recommended uses,
    and so on.'
  requires_registration: false
  url: https://arxiv.org/abs/1803.09010
- id: B2AI_STANDARD:327
  category: B2AI_STANDARD:DataStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: eXtensible Graph Markup and Modeling Language
  is_open: true
  name: XGMML
  purpose_detail: eXtensible Graph Markup and Modeling Language is an XML application
    based on GML which is used for graph description. XGMML uses tags to describe
    nodes and edges of a graph. The purpose of XGMML is to make possible the exchange
    of graphs between differents authoring and browsing tools for graphs.
  requires_registration: false
  url: http://xml.coverpages.org/xgmml.html
- id: B2AI_STANDARD:328
  category: B2AI_STANDARD:DataStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FAIR Principles for Research Software
  has_relevant_organization:
  - B2AI_ORG:83
  is_open: true
  name: FAIR4RS
  publication: doi:10.1038/s41597-022-01710-x
  purpose_detail: Simple and research software appropriate goalposts to inform those
    who publish and/or preserve research software.
  requires_registration: false
  url: https://www.rd-alliance.org/group/fair-research-software-fair4rs-wg/outcomes/fair-principles-research-software-fair4rs
- id: B2AI_STANDARD:329
  category: B2AI_STANDARD:DataStandard
  collection:
  - codesystem
  - deprecated
  concerns_data_topic:
  - B2AI_TOPIC:6
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Federal Information Processing System Codes for States and Counties
  formal_specification: https://transition.fcc.gov/oet/info/maps/census/fips/fips.txt
  is_open: true
  name: FIPS
  purpose_detail: FIPS codes are numbers which uniquely identify geographic areas.
    As of database version 55, FIPS has been merged with the Geographic Names Information
    System (GNIS).
  requires_registration: false
  url: https://transition.fcc.gov/oet/info/maps/census/fips/fips.txt
- id: B2AI_STANDARD:330
  category: B2AI_STANDARD:DataStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FieldML
  is_open: true
  name: FieldML
  publication: doi:10.1007/s11517-013-1097-7
  purpose_detail: FieldML is a declarative language for building hierarchical models
    represented by generalized mathematical fields. FieldML is developed as a data
    model and accompanying API. Find out more about the FieldML API, where to get
    the latest release and how to contribute to its development. FieldML is a declarative
    language for representing hierarchical models using generalized mathematical fields.
    FieldML can be used to represent the dynamic 3D geometry and solution fields from
    computational models of cells, tissues and organs. It enables model interchange
    for the bioengineering and general engineering analysis communities. Example uses
    are models of tissue structure, the distribution of proteins and other biochemical
    compounds, anatomical annotation, and other biological annotation.
  requires_registration: false
  url: https://doi.org/10.1007/s11517-013-1097-7
- id: B2AI_STANDARD:331
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Free Lossless Audio Codec
  formal_specification: https://xiph.org/flac/format.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:49
  is_open: true
  name: FLAC
  purpose_detail: FLAC (Free Lossless Audio Codec) is an audio coding format for lossless
    compression of digital audio.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/FLAC
- id: B2AI_STANDARD:332
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Frictionless Data Package
  formal_specification: https://github.com/frictionlessdata/specs
  is_open: true
  name: Frictionless
  purpose_detail: 'Frictionless Data Package is a specification for packaging, describing, and
    sharing datasets in a lightweight, standardized container format developed by
    Open Knowledge Foundation. A Data Package consists of a descriptor (datapackage.json)
    containing metadata about the package and its resources, along with the data files
    themselves (local or remote). The specification defines comprehensive metadata
    including: package title, description, licenses, version, sources, contributors,
    and keywords; resource information for each data file (path/URL, format, schema,
    encoding); Table Schema for tabular data defining field names, types, constraints,
    and relationships; and optional goodtables validation rules. Frictionless supports
    FAIR data principles by ensuring datasets are Findable (rich metadata), Accessible
    (standard formats, clear paths), Interoperable (JSON-based descriptor, Table Schema),
    and Reusable (license, provenance, structure documentation). The ecosystem includes
    libraries for Python, JavaScript, R, Ruby, PHP, and command-line tools for validation,
    conversion, and publishing. Frictionless integrates with data portals (CKAN, Dataverse),
    notebooks (Jupyter), and analysis tools, enabling seamless dataset exchange and
    processing. Extensions support specialized data types (geospatial, time series,
    budget data) and validation frameworks. In AI/ML workflows, Frictionless Data
    Packages standardize dataset distribution for reproducible research, provide machine-readable
    schemas for automated data validation and ingestion, enable data versioning and
    provenance tracking for ML pipelines, and facilitate dataset documentation complementing
    model training by ensuring data quality, consistency, and interpretability across
    collaborative ML projects.'
  requires_registration: false
  url: https://specs.frictionlessdata.io/data-package/
- id: B2AI_STANDARD:333
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: GA4GH serviceinfo
  has_relevant_organization:
  - B2AI_ORG:34
  is_open: true
  name: serviceinfo
  purpose_detail: Provides a way for an API to expose a set of metadata to help discovery
    and aggregation of services via computational methods.
  requires_registration: false
  url: https://github.com/ga4gh-discovery/ga4gh-service-info
- id: B2AI_STANDARD:334
  category: B2AI_STANDARD:DataStandard
  collection:
  - workflowlanguage
  concerns_data_topic:
  - B2AI_TOPIC:20
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Galaxy workflow Format 2
  formal_specification: https://github.com/galaxyproject/gxformat2
  is_open: true
  name: gxformat2
  purpose_detail: A schema moving Galaxy's workflow description language toward standards
    such as the Common Workflow Language.
  related_to:
  - B2AI_STANDARD:766
  requires_registration: false
  url: https://github.com/galaxyproject/gxformat2
- id: B2AI_STANDARD:335
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:14
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Geographic Names Information System Feature IDs
  formal_specification: https://www.usgs.gov/u.s.-board-on-geographic-names/download-gnis-data
  has_relevant_organization:
  - B2AI_ORG:98
  is_open: true
  name: GNIS ID
  purpose_detail: The Geographic Names Information System (GNIS) is the Federal and
    national standard for geographic nomenclature. The U.S. Geological Survey's National
    Geospatial Program developed the GNIS in support of the U.S. Board on Geographic
    Names as the official repository of domestic geographic names data, the official
    vehicle for geographic names use by all departments of the Federal Government,
    and the source for applying geographic names to Federal electronic and printed
    products.
  requires_registration: false
  url: https://www.usgs.gov/us-board-on-geographic-names/domestic-names
- id: B2AI_STANDARD:336
  category: B2AI_STANDARD:DataStandard
  collection:
  - codesystem
  concerns_data_topic:
  - B2AI_TOPIC:6
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Geopolitical Entities, Names, and Codes
  formal_specification: https://evs.nci.nih.gov/ftp1/GENC/
  is_open: true
  name: GENC
  purpose_detail: The GENC Standard specifies a profile of ISO 3166 codes for the
    representation of names of countries and their subdivisions.
  requires_registration: false
  url: https://www.dni.gov/index.php/who-we-are/organizations/ic-cio/ic-cio-related-menus/ic-cio-related-links/ic-technical-specifications/geopolitical-entities-names-and-codes
- id: B2AI_STANDARD:337
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Graph Markup Language
  is_open: true
  name: GraphML
  purpose_detail: An XML file format and language for describing graphs.
  related_to:
  - B2AI_STANDARD:829
  requires_registration: false
  url: http://graphml.graphdrawing.org/specification.html
- id: B2AI_STANDARD:338
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Graphics Interchange Format
  is_open: true
  name: GIF
  purpose_detail: 'Graphics Interchange Format (GIF) is a bitmap image format developed by CompuServe in 1987 that supports lossless LZW compression, up to 256 colors from a 24-bit RGB palette, transparency, and multi-frame animation with per-frame delay controls. GIF became widely adopted on the early web for its efficient compression and cross-platform compatibility, and remains ubiquitous for short animated loops and simple graphics with well-defined edges. The format supports two versions (GIF87a and GIF89a), with the latter adding animation delays, transparent backgrounds, and application-specific metadata. While the LZW patent controversy drove development of PNG as an alternative, GIF''s animation capabilities and "hot" data accessibility (frames stored sequentially without random access penalties) maintain its relevance for social media reactions, educational demonstrations, and web UI elements. Modern applications leverage GIF''s deterministic looping (via Netscape Application Block extension) and universal browser support, though video formats like WebP and MP4 increasingly replace GIF for better compression ratios. The format''s 256-color limitation makes it suitable for logos, diagrams, and pixel art but less appropriate for photographs or gradients, where dithering techniques are often applied. GIF files consist of a logical screen descriptor, optional global color table, image descriptors with optional local color tables, and LZW-encoded pixel data stored in sub-blocks. For AI/ML workflows, GIF serves as a compact format for visualizing model predictions over time series, displaying attention mechanisms frame-by-frame, and sharing animated training/validation metrics without requiring video codec dependencies.'
  requires_registration: false
  url: https://en.wikipedia.org/wiki/GIF
- id: B2AI_STANDARD:339
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HDF5 format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:16
  has_relevant_organization:
  - B2AI_ORG:116
  is_open: true
  name: HDF5
  purpose_detail: HDF5 is a data model, library, and file format for storing and managing
    data. It supports an unlimited variety of datatypes, and is designed for flexible
    and efficient I/O and for high volume and complex data. HDF5 is portable and is
    extensible, allowing applications to evolve in their use of HDF5. The HDF5 Technology
    suite includes tools and applications for managing, manipulating, viewing, and
    analyzing data in the HDF5 format.
  requires_registration: false
  url: https://www.hdfgroup.org/solutions/hdf5/
  used_in_bridge2ai: true
- id: B2AI_STANDARD:340
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  - B2AI_TOPIC:48
  contribution_date: '2025-05-29'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Hierarchical Data Modeling Framework for Modern Science Data Standards
  formal_specification: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8500680/
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:16
  is_open: true
  name: HDMF
  publication: doi:10.1109/bigdata47090.2019.9005648
  purpose_detail: 'HDMF is a hierarchical data modeling framework for modern science
    data standards. It separates data standardization into three main components:
    (1) data modeling and specification, (2) data I/O and storage, and (3) data interaction
    and data APIs. HDMF provides object mapping infrastructure to insulate and integrate
    these components, supporting flexible development of data standards and extensions,
    optimized storage backends, and data APIs. It offers advanced data I/O functionality
    for iterative data write, lazy data load, parallel I/O, and modular data storage.
    HDMF is particularly used to design NWB 2.0, a data standard for neurophysiology
    data.'
  related_to:
  - B2AI_STANDARD:339
  - B2AI_STANDARD:379
  requires_registration: false
  url: https://hdmf-common-schema.readthedocs.io/en/latest/format.html
- id: B2AI_STANDARD:341
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ISA-Tab format
  formal_specification: https://github.com/ISA-tools/ISAdatasets
  has_relevant_organization:
  - B2AI_ORG:47
  is_open: true
  name: ISA-Tab
  purpose_detail: General-purpose ISA-Tab file format - an extensible, hierarchical
    structure that focuses on the description of the experimental metadata (i.e. sample
    characteristics, technology and measurement types, sample-to-data relationships).
  requires_registration: false
  url: https://isa-specs.readthedocs.io/en/latest/isatab.html
- id: B2AI_STANDARD:342
  category: B2AI_STANDARD:DataStandard
  collection:
  - codesystem
  concerns_data_topic:
  - B2AI_TOPIC:6
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ISO 3166 Country Codes
  formal_specification: https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes
  has_relevant_organization:
  - B2AI_ORG:49
  is_open: true
  name: ISO 3166
  purpose_detail: The purpose of ISO 3166 is to define internationally recognized
    codes of letters and/or numbers that we can use when we refer to countries and
    their subdivisions. However, it does not define the names of countries  this
    information comes from United Nations sources (Terminology Bulletin Country Names
    and the Country and Region Codes for Statistical Use maintained by the United
    Nations Statistics Divisions).
  requires_registration: false
  url: https://www.iso.org/iso-3166-country-codes.html
- id: B2AI_STANDARD:343
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ISO 8601 Date and time format
  has_relevant_organization:
  - B2AI_ORG:49
  is_open: true
  name: ISO 8601
  purpose_detail: A way of presenting dates and times that is clearly defined and
    understandable to both people and machines.
  requires_registration: false
  url: https://www.iso.org/iso-8601-date-and-time-format.html
- id: B2AI_STANDARD:344
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Joint Photographic Experts Group Format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:19
  has_relevant_organization:
  - B2AI_ORG:116
  is_open: true
  name: JPEG
  purpose_detail: A method of lossy compression for digital images.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/JPEG
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:53
    category: B2AI:Application
    name: Medical Image Compression and Transfer Learning
    description: JPEG format is used in AI applications for efficient storage and
      transmission of medical images, particularly in telepathology, dermatology AI,
      and mobile diagnostic applications where bandwidth and storage constraints are
      critical. Deep learning models are trained on JPEG-compressed whole slide images,
      dermoscopic images, and fundus photographs to perform tasks such as cancer detection,
      skin lesion classification, and diabetic retinopathy screening. While DICOM
      remains the standard for radiology, JPEG enables AI deployment in resource-constrained
      settings and supports transfer learning from natural image datasets (ImageNet)
      to medical imaging domains. AI researchers must account for JPEG compression
      artifacts when training models, and recent work explores AI-optimized compression
      techniques that preserve diagnostically relevant features.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:345
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: JSON-schema
  formal_specification: https://github.com/json-schema-org/json-schema-spec
  is_open: true
  name: JSON-schema
  purpose_detail: 'JSON Schema is a vocabulary for annotating and validating JSON documents,
    providing a declarative format for describing the structure, constraints, and
    semantics of JSON data. As an IETF standard (draft specifications progressing
    toward RFC status), JSON Schema defines a JSON-based format for specifying the
    expected shape of JSON data, including data types, required properties, value
    ranges, string patterns, array constraints, and object structures. The schema
    itself is expressed in JSON, enabling meta-schema validation and recursive definitions.
    JSON Schema supports multiple vocabularies including Core, Validation, Hyper-Schema
    for hypermedia, and Format for semantic validation. Widely adopted across industries
    with 60+ million weekly downloads, it powers API documentation (OpenAPI/Swagger),
    configuration validation, code generation, form generation in UIs, and data interchange
    contracts. Major implementations exist in JavaScript, Python, Java, Go, and 40+
    other languages, with extensive tooling ecosystem including validators, generators,
    linters, and editors. JSON Schema enables confident JSON data handling through
    streamlined testing, seamless data exchange via shared understanding, and clear
    documentation for developer collaboration. In AI/ML contexts, JSON Schema validates
    training data structures, API request/response payloads, configuration files for
    ML pipelines, and ensures data quality for machine learning workflows by catching
    inconsistencies and schema violations at ingestion time.'
  related_to:
  - B2AI_STANDARD:761
  requires_registration: false
  url: https://json-schema.org/
- id: B2AI_STANDARD:346
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Knowledge Graph Exchange
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:21
  is_open: true
  name: KGX
  purpose_detail: 'Knowledge Graph Exchange (KGX) is a standardized graph-oriented data format
    for exchanging knowledge graphs, developed by the Biolink community to facilitate
    interoperability between biomedical knowledge bases. KGX provides both a formal
    specification and a Python toolkit (kgx library) for transforming, validating,
    and exchanging knowledge graphs conforming to the Biolink Model. The format represents
    graphs as nodes (entities with identifiers, categories, and properties) and edges
    (relationships with predicates, subject/object references, and provenance), serialized
    in JSON, TSV, or RDF formats. KGX ensures semantic alignment by enforcing Biolink
    Model categories (e.g., Gene, Disease, Pathway) and predicates (e.g., causes,
    treats, interacts_with), enabling consistent cross-database integration. The toolkit
    supports graph transformations (merging, filtering, mapping), format conversions
    (Neo4j, RDF, GraphML), and validation against Biolink Model constraints. KGX is
    foundational for Translator knowledge graphs, integrating data from 150+ biomedical
    sources including Monarch Initiative, NCATS Biomedical Data Translator, and Clinical
    Data Commons. In AI/ML applications, KGX-formatted knowledge graphs power link
    prediction for drug repurposing, knowledge graph embeddings for biomedical entity
    representation learning, reasoning algorithms for hypothesis generation, and multi-modal
    knowledge integration combining genomics, phenotypes, pathways, and clinical data
    to support precision medicine and systems biology research.'
  related_to:
  - B2AI_STANDARD:783
  requires_registration: false
  url: https://github.com/biolink/kgx/blob/master/specification/kgx-format.md
- id: B2AI_STANDARD:347
  category: B2AI_STANDARD:DataStandard
  collection:
  - markuplanguage
  - standards_process_maturity_development
  - implementation_maturity_pilot
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: LinkML
  formal_specification: https://github.com/linkml/linkml
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:41
  - B2AI_SUBSTRATE:6
  has_relevant_organization:
  - B2AI_ORG:116
  is_open: true
  name: LinkML
  purpose_detail: LinkML is a flexible modeling language that allows you to author
    schemas in YAML that describe the structure of your data. Additionally, it is
    a framework for working with and validating data in a variety of formats (JSON,
    RDF, TSV), with generators for compiling LinkML schemas to other frameworks.
  requires_registration: false
  url: https://linkml.io/linkml
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:54
    category: B2AI:Application
    name: AI-Ready Data Schema Design and Validation
    description: LinkML is used in AI applications to define machine-readable data
      schemas that enable automated data validation, transformation, and integration
      for machine learning pipelines. AI systems leverage LinkML schemas to ensure
      data quality and consistency across heterogeneous biomedical datasets, automatically
      generate data loaders and validators for ML frameworks, and create semantic
      mappings that allow AI models to understand relationships between data elements.
      LinkML's ability to compile to multiple formats (JSON Schema, SHACL, SQL DDL)
      makes it particularly valuable for building reproducible AI/ML workflows where
      data provenance and validation are critical.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:348
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MathML
  has_relevant_organization:
  - B2AI_ORG:99
  is_open: true
  name: MathML
  purpose_detail: A product of the W3C Math Working Group, MathML is a low-level specification
    for describing mathematics as a basis for machine to machine communication which
    provides a much needed foundation for the inclusion of mathematical expressions
    in Web pages. It is also important in publishing workflows for science and technology
    and wherever mathematics has to be handled by software.
  requires_registration: false
  url: https://www.w3.org/Math/whatIsMathML.html
- id: B2AI_STANDARD:349
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Microsoft Excel spreadsheet container file
  formal_specification: https://www.iso.org/standard/71691.html
  has_relevant_organization:
  - B2AI_ORG:56
  is_open: false
  name: xlsx
  purpose_detail: Format used by Microsoft Excel spreadsheet software.
  requires_registration: false
  url: https://www.iso.org/standard/71691.html
- id: B2AI_STANDARD:350
  category: B2AI_STANDARD:DataStandard
  collection:
  - modelcards
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Model Cards
  formal_specification: https://github.com/tensorflow/model-card-toolkit/blob/master/model_card_toolkit/schema/v0.0.2/model_card.schema.json
  has_relevant_organization:
  - B2AI_ORG:37
  is_open: true
  name: Model Cards
  publication: https://arxiv.org/abs/1810.03993
  purpose_detail: Structured documentation detailing performance characteristics of
    machine learning models.
  requires_registration: false
  url: https://modelcards.withgoogle.com/about
- id: B2AI_STANDARD:351
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MPEG-1 Audio Layer 3 | MPEG-2 Audio Layer 3
  formal_specification: https://www.iso.org/standard/26797.html
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:49
  is_open: true
  name: MP3
  purpose_detail: MP3 (formally MPEG-1 Audio Layer III or MPEG-2 Audio Layer III)
    is a coding format for digital audio.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/MP3
- id: B2AI_STANDARD:352
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:15
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MPEG-4 Part 14 digital multimedia container format
  formal_specification: https://www.loc.gov/preservation/digital/formats/fdd/fdd000155.shtml
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:49
  has_relevant_organization:
  - B2AI_ORG:49
  is_open: true
  name: MPEG-4
  purpose_detail: A digital multimedia container format most commonly used to store
    video and audio, but it can also be used to store other data such as subtitles
    and still images. Like most modern container formats, it allows streaming over
    the Internet.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/MP4_file_format
- id: B2AI_STANDARD:353
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:3
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Network Common Data Form
  formal_specification: https://www.astm.org/e1947-98r14.html
  has_relevant_organization:
  - B2AI_ORG:8
  is_open: true
  name: netCDF
  purpose_detail: A standardized format for chromatographic data representation.
  requires_registration: false
  url: https://www.unidata.ucar.edu/software/netcdf/
- id: B2AI_STANDARD:354
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Neural Network Exchange Format
  formal_specification: https://github.com/KhronosGroup/NNEF-Tools
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:27
  - B2AI_SUBSTRATE:33
  - B2AI_SUBSTRATE:42
  is_open: true
  name: NNEF
  purpose_detail: An exchange format for neural network models produced using Torch,
    Caffe, TensorFlow, Theano, Chainer, Caffe2, PyTorch, or MXNet.
  related_to:
  - B2AI_STANDARD:816
  - B2AI_STANDARD:831
  - B2AI_STANDARD:834
  requires_registration: false
  url: https://www.khronos.org/nnef
  has_application:
  - id: B2AI_APP:55
    category: B2AI:Application
    name: Neural Network Exchange for Medical Device AI
    description: NNEF (Neural Network Exchange Format) is used in biomedical AI for
      deploying models on medical devices and embedded systems where hardware-specific
      optimization and standardized model representation are critical. Medical device
      manufacturers leverage NNEF to ensure neural network models can be optimized
      for diverse hardware accelerators (DSPs, NPUs, custom ASICs) commonly used in
      portable medical equipment, bedside monitors, and point-of-care devices. The
      format enables vendor-independent model deployment, facilitates regulatory approval
      by providing clear model specifications, and supports hardware efficiency optimizations
      necessary for real-time inference in resource-constrained medical devices. NNEF
      is particularly valuable for AI-enabled medical devices where power consumption,
      latency, and deterministic behavior are critical requirements.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:355
  category: B2AI_STANDARD:DataStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: OGC and ISO Observations and Measurements standard in XML
  has_relevant_organization:
  - B2AI_ORG:49
  is_open: true
  name: OMXML
  purpose_detail: This standard specifies an XML implementation for the OGC and ISO
    Observations and Measurements (O&M) conceptual model (OGC Observations and Measurements
    v2.0 also published as ISO/DIS 19156), including a schema for Sampling Features.
    This encoding is an essential dependency for the OGC Sensor Observation Service
    (SOS) Interface Standard. More specifically, this standard defines XML schemas
    for observations, and for features involved in sampling when making observations.
    These provide document models for the exchange of information describing observation
    acts and their results, both within and between different scientific and technical
    communities.
  requires_registration: false
  url: https://www.ogc.org/standards/om
- id: B2AI_STANDARD:356
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ogg Speex Audio Format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:49
  is_open: true
  name: OGG Speex
  purpose_detail: File format and bitstream encoding for for spoken content, targeted
    at a wide range of devices other than mobile phones.
  requires_registration: false
  url: https://speex.org/docs/manual/speex-manual/node8.html
- id: B2AI_STANDARD:357
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Open Neural Network Exchange
  formal_specification: https://github.com/onnx/onnx/blob/main/docs/IR.md
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:28
  is_open: true
  name: ONNX
  purpose_detail: ONNX (Open Neural Network Exchange) is an open, cross-platform
    intermediate representation for machine learning and deep learning models, developed
    collaboratively by Microsoft, Facebook/Meta, and the Linux Foundation AI & Data
    initiative. It defines a standardized, extensible computation graph format that
    enables models trained in one frameworksuch as PyTorch, TensorFlow, Keras, scikit-learn,
    or MXNetto be exported, optimized, and deployed in another environment without
    framework-specific runtime dependencies. ONNX represents both the model architecture
    (nodes as operators, edges as tensors) and learned parameters (weights, biases)
    in a protocol-buffer-based binary format (`.onnx` files), with a human-readable
    protobuf text representation for debugging and versioning. The core specification
    includes an operator set (ONNX opset) versioned incrementally to add new operators
    and preserve backward compatibility, covering a wide range of operations including
    convolutional layers, recurrent units, attention mechanisms, normalization layers,
    activation functions, and custom operators. ONNX's intermediate representation
    decouples the training environment from the inference environment, allowing data
    scientists to train models using high-level APIs and researchers to experiment
    with novel architectures, while deployment engineers optimize and integrate those
    models into production systems using ONNX Runtime or hardware-specific runtimes
    (NVIDIA TensorRT, Intel OpenVINO, Apple Core ML, Qualcomm SNPE, AWS SageMaker
    Neo, Google Edge TPU) that perform graph-level optimizations, quantization, pruning,
    and kernel fusion for target hardware. ONNX Runtime, the reference cross-platform
    inference engine, provides optimized execution on CPUs, GPUs, and specialized
    accelerators, with extensive support for quantization (INT8, INT4, mixed-precision),
    model compression, and dynamic shape inference, making it a cornerstone of MLOps
    pipelines and production AI systems. In biomedical and clinical AI, ONNX is particularly
    valuable for deploying models across heterogeneous hospital IT infrastructures,
    medical imaging workstations, portable diagnostic devices, and federated learning
    networks where framework dependencies and model portability are critical concerns.
    Research groups use ONNX to share pretrained models (e.g., medical image segmentation,
    clinical NLP, drug discovery) via model zoos and repositories (ONNX Model Zoo,
    Hugging Face Hub, BioImage Model Zoo), ensuring reproducibility and facilitating
    transfer learning without requiring the original training codebase. Regulatory
    submissions (FDA, CE mark) benefit from ONNX's stable, versioned model representation
    that can be validated and audited independently of the training framework, with
    explicit documentation of operator semantics and numerical behavior. ONNX supports
    advanced model architectures including transformers, graph neural networks, and
    recurrent networks, as well as pre- and post-processing operations (image resizing,
    normalization, tokenization) embedded directly in the computation graph, reducing
    preprocessing discrepancies between training and inference. The ONNX ecosystem
    includes converters for major frameworks (PyTorch's `torch.onnx.export`, TensorFlow's
    `tf2onnx`, scikit-learn's `sklearn-onnx`), visualization tools (Netron for graph
    inspection), optimization tools (ONNX Simplifier, ONNX Optimizer), and validation
    utilities (ONNX Checker) for ensuring model correctness and compliance with the
    specification. ONNX's adoption in clinical AI products enables vendor-neutral
    deployment, allowing models to run on diverse medical device platforms and hospital
    PACS systems, supporting interoperability in multi-site clinical trials and federated
    learning initiatives where different institutions may use different hardware and
    software stacks.
  requires_registration: false
  url: https://onnx.ai/
  has_application:
  - id: B2AI_APP:56
    category: B2AI:Application
    name: Cross-Platform Medical AI Model Interoperability
    description: ONNX (Open Neural Network Exchange) is used in biomedical AI for
      creating framework-agnostic models that can be trained in PyTorch, TensorFlow,
      or other frameworks and deployed across diverse clinical platforms and hardware
      accelerators. Healthcare AI developers leverage ONNX to ensure vendor independence,
      enabling models to run on different hospital IT systems, edge devices, and specialized
      medical hardware regardless of training framework. The standard facilitates
      regulatory submissions by providing a stable model representation, supports
      hardware optimization through ONNX Runtime's performance tuning for CPUs, GPUs,
      and custom accelerators, and enables model sharing across research institutions
      without requiring framework dependencies. ONNX is particularly valuable for
      clinical AI products that must support multiple deployment environments.
    used_in_bridge2ai: false
    references:
    - https://github.com/microsoft/onnxruntime
  - id: B2AI_APP:193
    category: B2AI:Application
    name: Cross-Framework Model Interoperability and Export-Once-Run-Anywhere Workflows
    description: ONNX serves as the standard cross-framework interchange format enabling
      "export once, run anywhere" workflows where models trained in PyTorch, TensorFlow,
      Keras, or scikit-learn are converted to a common intermediate representation
      with a standardized operator set and extensible custom operator domains. The
      format encodes computation graphs, learned weights, and input/output specifications
      in a portable .onnx file that decouples training frameworks from inference runtimes,
      allowing data scientists to experiment with diverse architectures while deployment
      engineers optimize for production targets. PyTorch models are exported via torch.onnx.export
      with configurable options (input/output names, dynamic_axes for variable batch
      sizes, opset_version for compatibility), and the resulting graphs can be visualized
      using Netron for inspection of layer connectivity, operator types, and tensor
      shapes. ONNX's extensibility through custom operator domains enables migration
      among frameworks, compilers (Apache TVM), and hardware-specific runtimes (TensorRT,
      OpenVINO, MiGraphX) while preserving model portability and avoiding vendor lock-in.
      In biomedical AI, cross-framework interoperability enables research groups to
      share pretrained models (medical image segmentation, clinical NLP, genomics
      variant calling) via model zoos (ONNX Model Zoo, Hugging Face Hub, BioImage
      Model Zoo) without requiring recipients to install the original training framework,
      facilitating reproducibility and transfer learning across institutions. Evaluation
      toolchains like kenning integrate ONNX as the standard export format, with
      dedicated onnxconverters modules and ModelWrapper implementations requiring
      ONNX serialization support, enabling systematic benchmarking of quality and
      inference performance across frameworks. The common file format simplifies regulatory
      submissions (FDA 510(k), EU MDR) by providing stable, versioned model representations
      that can be validated independently of training code, with explicit operator
      semantics and numerical behavior documentation.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.3390/electronics14152977
    - https://lirias.kuleuven.be/retrieve/c1369a18-92c9-4ba2-9752-7001dac30e46
    - https://udspace.udel.edu/bitstreams/92c9aa1c-0e9e-456d-bd9d-9cc913fed657/download
  - id: B2AI_APP:194
    category: B2AI:Application
    name: Edge and IoT Deployment with ONNX Runtime for Low-Latency Inference
    description: ONNX Runtime is an embeddable, multi-language inference engine designed
      for efficient deployment on edge devices, mobile applications, embedded systems,
      and server APIs, providing optimized execution across CPUs, GPUs, and specialized
      accelerators without framework-specific dependencies. Benchmarking studies on
      NVIDIA Jetson AGX Orin edge platforms include ONNX Runtime (v1.17.1) alongside
      TensorRT, TVM, PyTorch, and JAX, evaluating realistic deployment metrics including
      inference time, throughput, memory usage, power consumption, and accuracy (Top-1/Top-5)
      on ImageNet datasets with both convolutional networks and transformers, demonstrating
      ONNX Runtime's suitability for production edge AI. The runtime applies automatic
      graph-level optimizations including operator fusion (Conv+BatchNorm+ReLU patterns),
      constant folding, reshape/transpose elimination, and removal of identity operations
      to reduce inference latency and memory overhead. For IoT architectures, ONNX
      models provisioned to run locally on devices (Intel Atom, ARM-based embedded
      boards) with ONNX Runtime enable decentralized serving patterns that avoid central
      HTTP endpoints, reducing cloud dependency, network latency, and data privacy
      concernscritical for medical devices performing on-device diagnostics or wearable
      health monitors processing physiological signals. ONNX Runtime supports multiple
      programming languages (Python, C++, Java, C#, JavaScript) enabling integration
      into diverse application stacks, from Python-based research prototypes to C++
      embedded firmware for real-time clinical decision support systems. The runtime's
      predictable latency, efficient memory usage, and cross-platform consistency
      make it suitable for production-scale deployments in healthcare settings where
      AI models must execute reliably on heterogeneous hardware ranging from hospital
      workstations to portable ultrasound devices and point-of-care diagnostic instruments.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.3390/electronics14152977
    - https://lirias.kuleuven.be/retrieve/c1369a18-92c9-4ba2-9752-7001dac30e46
    - https://aaltodoc.aalto.fi/bitstreams/a68b52e2-5581-43ee-89a6-0358e2963df4/download
  - id: B2AI_APP:195
    category: B2AI:Application
    name: Hardware Acceleration via Pluggable Execution Providers and Backend Optimization
    description: ONNX Runtime provides a flexible execution provider architecture
      enabling hardware-appropriate acceleration through pluggable backends spanning
      NVIDIA CUDA GPUs, TensorRT for optimized NVIDIA inference, Intel OpenVINO for
      CPU/GPU/VPU, ARM XNNPACK for mobile/edge, AMD MiGraphX (beta status), and DirectML
      for Windows devices, allowing deployment engineers to target diverse hardware
      from desktop workstations to edge SoCs without modifying model code. Execution
      providers apply vendor-specific graph optimizations, kernel fusion, and memory
      layout transformations tailored to target hardware characteristicsfor example,
      TensorRT compiles ONNX graphs into fully optimized inference engines with serialization/deserialization
      support that eliminates rebuild overhead during application startup, while OpenVINO
      optimizes for Intel architectures including CPU SIMD instructions, integrated
      GPUs, and Vision Processing Units in medical imaging workstations. Studies on
      exascale supercomputers demonstrate ONNX Runtime's support for heterogeneous
      HPC hardware including CPU clusters, NVIDIA GPUs via CUDA/TensorRT, and AMD
      GPUs via MiGraphX, with containerization recommended to manage runtime dependencies
      and backend variations when migrating between hardware vendors (NVIDIAAMD)
      or acceleration strategies (CUDATensorRT). The pluggable backend architecture
      enables biomedical AI applications to maintain portable ONNX models while leveraging
      specialized acceleratorsradiotherapy planning systems can deploy on NVIDIA
      GPUs with TensorRT optimization, pathology whole-slide image analysis can utilize
      Intel integrated GPUs via OpenVINO on diagnostic workstations, and portable
      ultrasound devices can execute models on ARM processors with XNNPACK acceleration.
      For performance-critical applications, ONNX Runtime performs operator fusion,
      constant folding, and graph pruning at the IR level, with experiments reporting
      inference speedups when models are optimized specifically for ONNX Runtime execution.
      Integration with compiler frameworks like Apache TVM enables further low-level
      optimization through operator fusion and ML-based schedule search, combining
      ONNX portability with hardware-specific kernel generation for maximum throughput.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.3390/electronics14152977
    - https://udspace.udel.edu/bitstreams/92c9aa1c-0e9e-456d-bd9d-9cc913fed657/download
    - https://lirias.kuleuven.be/retrieve/c1369a18-92c9-4ba2-9752-7001dac30e46
  - id: B2AI_APP:196
    category: B2AI:Application
    name: Post-Training Quantization and Reduced Precision for Model Compression
    description: ONNX Runtime supports Post-Training Quantization (PTQ) to INT8 precision
      for reducing model size and inference latency without requiring retraining,
      with both dynamic quantization (weights converted to INT8, activations quantized
      on-the-fly at runtime in FP32) and static quantization (both weights and activations
      quantized using calibration datasets to measure activation ranges) available
      for CPU and accelerator deployments. Quantization guidance recommends FP16 execution
      on NVIDIA GPUs including Jetson edge platforms, industrial PCs, and datacenter
      GPUs to achieve 2x throughput improvements over FP32 with minimal accuracy degradation,
      particularly valuable for real-time medical imaging applications processing
      high-resolution scans (CT, MRI, whole-slide pathology) where inference latency
      directly impacts clinical workflow efficiency. While ONNX Runtime does not support
      Quantization-Aware Training (QAT) natively, models can undergo QAT in PyTorch
      or TensorFlow training frameworks and then be exported to ONNX format for optimized
      inference, enabling workflows where simulated quantization during training produces
      models robust to reduced precision at deployment. Integration with hardware-specific
      quantization tools (Intel Neural Compressor for CPU/OpenVINO targets, NVIDIA
      TensorRT for GPU INT8 calibration) extends quantization capabilities to vendor-optimized
      implementations. For biomedical AI deployed on resource-constrained devicesportable
      diagnostic instruments, wearable health monitors, point-of-care ultrasoundPTQ
      enables execution of sophisticated deep learning models (segmentation networks,
      object detectors, classification CNNs) within power and memory budgets while
      maintaining clinical-grade accuracy. Quantization also reduces cloud inference
      costs in large-scale screening programs and population health studies where
      millions of scans are processed, with INT8 models providing 4x memory reduction
      and proportional decreases in storage and bandwidth requirements for model distribution
      to federated clinical sites. The combination of ONNX's portable model representation
      and ONNX Runtime's PTQ support enables "quantize once, deploy anywhere" workflows
      where a single quantized model artifact runs efficiently across diverse edge
      and server hardware.
    used_in_bridge2ai: false
    references:
    - https://lirias.kuleuven.be/retrieve/c1369a18-92c9-4ba2-9752-7001dac30e46
  - id: B2AI_APP:197
    category: B2AI:Application
    name: Classical Machine Learning Support via ai.onnx.ml Operator Set
    description: ONNX's ai.onnx.ml operator set extends the standard ai.onnx deep
      learning operators to represent classical machine learning models and pipelines,
      including linear regression, support vector machines (SVM), decision trees,
      random forests, gradient boosting, bagging ensembles, and dimensionality reduction
      techniques (PCA mapped to matrix operations), enabling portable inference for
      traditional ML algorithms alongside deep neural networks. ONNX Runtime was the
      first inference engine to support both ai.onnx (deep learning) and ai.onnx.ml
      (classical ML) operator sets for unified execution, allowing deployment of hybrid
      pipelines that combine classical feature engineering, ensemble models, and deep
      learning components in a single ONNX graph. Conversion tools like sklearn-onnx
      enable exporting scikit-learn pipelines (preprocessing transformers, feature
      selectors, estimators) to ONNX format for deployment on diverse hardware (CPUs,
      GPUs, edge devices) without Python/scikit-learn dependencies, valuable for embedding
      ML models in mobile applications, microservices, or embedded systems. In biomedical
      applications, classical ML remains relevant for structured clinical data (EHR-derived
      features, lab values, vital signs) where tree-based models and linear models
      often achieve performance comparable to deep learning with greater interpretability
      and lower computational requirements. ONNX's support for classical ML enables
      clinical decision support systems to deploy interpretable risk prediction models
      (logistic regression for mortality prediction, random forests for readmission
      risk) alongside deep learning models for imaging or unstructured text analysis,
      with unified ONNX Runtime inference simplifying production deployment and monitoring.
      Genomics applications leverage classical ML for variant effect prediction, gene
      expression classification, and phenotype association where gradient boosting
      and ensemble methods excel, and ONNX export enables these models to be integrated
      into automated diagnostic pipelines or shared across research institutions as
      portable artifacts. The ai.onnx.ml operator set also supports feature engineering
      operations (scaling, normalization, one-hot encoding, binning) directly within
      the ONNX graph, reducing deployment complexity by embedding preprocessing logic
      that would otherwise require separate data pipeline code.
    used_in_bridge2ai: false
    references:
    - https://aaltodoc.aalto.fi/bitstreams/a68b52e2-5581-43ee-89a6-0358e2963df4/download
    - https://doi.org/10.3390/electronics14152977
  - id: B2AI_APP:198
    category: B2AI:Application
    name: HPC and Compiler-Level Optimization for Scientific ML Workflows
    description: ONNX provides the portability layer for deploying machine learning
      models on heterogeneous supercomputer and HPC infrastructure, enabling scientific
      AI workflows to target diverse CPU clusters, NVIDIA GPU nodes (via CUDA/TensorRT),
      and AMD GPU nodes (via MiGraphX) with a single model artifact, critical for
      exascale computing environments where code must efficiently utilize mixed vendor
      hardware across compute partitions. Studies on scientific applications in exascale
      supercomputing demonstrate ONNX Runtime's inference capabilities across HPC
      backends, with experiments reporting speedups when models are optimized for
      ONNX Runtime execution and recommendations for containerization (Docker, Singularity)
      to manage runtime dependencies and backend variations when migrating between
      hardware configurations (e.g., CUDA to TensorRT transitions or NVIDIA to AMD
      vendor switches). For compiler-level optimization, ONNX models serve as input
      to Apache TVM's compilation framework, which applies operator fusion, automated
      schedule search using ML-based cost models, and target-specific kernel generation
      to produce highly optimized executables for specific hardware; the combination
      of ONNX+ONNX Runtime+TVM provides a pathway from framework-agnostic model representation
      through portable inference to hardware-specialized performance. In biomedical
      research, HPC deployments of ONNX models enable large-scale molecular dynamics
      simulations coupled with ML-based force field prediction, cryo-EM image reconstruction
      with deep learning denoising on GPU clusters, genomic variant calling pipelines
      processing population-scale whole-genome sequencing datasets, and drug discovery
      virtual screening campaigns evaluating millions of compounds using structure-based
      binding affinity prediction models. The ONNX ecosystem's support for batching,
      dynamic shape inference, and efficient memory management makes it suitable for
      throughput-oriented HPC workloads where thousands of inference requests are
      processed in parallel across distributed compute nodes. ONNX's standardized
      operator semantics and versioned opsets ensure reproducibility in scientific
      ML experiments, allowing researchers to document exact model architectures and
      weights in publications while enabling independent verification by other groups
      using different hardware or software stacks. The integration of ONNX with HPC
      resource managers (Slurm, PBS) and workflow engines (Nextflow, Snakemake) supports
      complex multi-stage scientific pipelines that alternate between data preprocessing,
      simulation, ML inference, and analysis steps.
    used_in_bridge2ai: false
    references:
    - https://udspace.udel.edu/bitstreams/92c9aa1c-0e9e-456d-bd9d-9cc913fed657/download
- id: B2AI_STANDARD:358
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: OpenAPI Specification
  formal_specification: https://github.com/OAI/OpenAPI-Specification/
  is_open: true
  name: OpenAPI
  purpose_detail: Standard for describing program interfaces.
  requires_registration: false
  url: https://spec.openapis.org/oas/latest.html
- id: B2AI_STANDARD:359
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Parquet
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:30
  has_relevant_organization:
  - B2AI_ORG:114
  is_open: true
  name: parquet
  purpose_detail: Apache Parquet is an open-source, column-oriented data file format
    designed for efficient storage and retrieval of large datasets in analytical and
    machine learning workloads, providing superior compression and encoding schemes
    compared to row-based formats while enabling selective column reading (predicate
    pushdown) that dramatically accelerates queries filtering on specific attributes
    without scanning entire datasets. Developed as part of the Apache Hadoop ecosystem
    with contributions from Cloudera, Twitter, and the Apache Arrow community, Parquet
    addresses the performance limitations of traditional row-oriented formats (CSV,
    JSON) for analytical queries that aggregate or filter on subsets of columns across
    billions of rows, which is the dominant access pattern in data science, business
    intelligence, and machine learning feature engineering. The columnar layout stores
    all values from each column together, enabling column-specific compression algorithms
    (dictionary encoding, run-length encoding, bit-packing) that achieve compression
    ratios of 10x-100x compared to uncompressed data, while maintaining compatibility
    with schema evolution as datasets grow and change over time. Parquet files embed
    rich metadata including column statistics (min/max values, null counts, distinct
    counts per row group) that enable query engines to skip irrelevant data chunks
    entirely, supporting efficient filtering and aggregation without reading unnecessary
    columns or row groups from disk or cloud storage. The format is designed for
    cross-platform, language-agnostic data interchange with implementations in Java,
    C++, Python, Go, Rust, and other languages through Apache Arrow's unified memory
    model, enabling zero-copy data sharing between processes and efficient integration
    with distributed computing frameworks (Spark, Dask, Ray) and cloud data warehouses
    (Snowflake, BigQuery, Redshift Spectrum, Athena). For machine learning workflows,
    Parquet serves as the standard serialization format for feature stores (Feast,
    Tecton, Hopsworks), data lakehouse architectures (Delta Lake, Apache Iceberg),
    and ML dataset versioning systems (DVC, Pachyderm), providing ACID-like semantics
    for time-travel queries and reproducible model training. The format's partitioning
    capabilities (by date, category, or other dimensions) enable efficient incremental
    processing and support for streaming pipelines that continuously append new data
    while maintaining queryable history. Parquet integrates seamlessly with popular
    data processing libraries including pandas, Polars, DuckDB, PyArrow, Dask, and
    Vaex, and is the preferred format for AI/ML frameworks (TensorFlow Data API,
    PyTorch DataLoader, JAX) that require fast sequential or random access to training
    examples with selective feature loading. The format supports nested and complex
    data types (structs, arrays, maps) through its Dremel-inspired encoding, allowing
    representation of hierarchical biomedical data (nested clinical observations,
    multi-level ontology annotations, structured EHR records) without flattening into
    denormalized tables. In scientific computing and biomedical research, Parquet
    is adopted for genomics datasets (variant call tables, expression matrices), clinical
    data warehouses (harmonized EHR extracts, multi-site cohort studies), imaging
    metadata catalogs (DICOM header databases, radiomics feature stores), and multi-omics
    integration where columnar access patterns (e.g., loading gene expression for
    specific genes across millions of cells) match analytical needs. Cloud platforms
    provide native Parquet support with optimized readers/writers (AWS S3 Select,
    Google Cloud Storage, Azure Blob Storage with Synapse Analytics), enabling serverless
    queries over petabyte-scale archives without ETL overhead. The format's metadata
    layer supports Bloom filters for existence checks, dictionary-encoded categorical
    columns for efficient grouping operations, and column-level encryption for compliance
    with data privacy regulations (HIPAA, GDPR) in healthcare and genomics applications,
    making Parquet the de facto standard for storing structured and semi-structured
    analytical data in modern data platforms.
  requires_registration: false
  url: https://parquet.apache.org/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:57
    category: B2AI:Application
    name: Efficient Large-Scale ML Data Storage and Processing
    description: Apache Parquet is widely adopted in AI/ML pipelines for efficient
      storage and processing of large-scale biomedical datasets, particularly for
      training deep learning models on tabular clinical data, multi-omics datasets,
      and imaging metadata. The columnar storage format enables high-performance data
      loading during model training, reduces storage costs through efficient compression,
      and supports predicate pushdown for selective feature reading. AI frameworks
      like TensorFlow, PyTorch, and scikit-learn leverage Parquet's integration with
      Apache Arrow for zero-copy data transfer, enabling faster iteration during hyperparameter
      tuning and model development. Parquet is particularly valuable for storing processed
      features from EHR data, genomic variant annotations, and large-scale biobank
      datasets where query performance and storage efficiency are critical.
    used_in_bridge2ai: false
  - id: B2AI_APP:188
    category: B2AI:Application
    name: Direct Training from Parquet via Petastorm for Healthcare ML
    description: Apache Parquet serves as a direct training data format for machine
      learning pipelines using Petastorm, a Python library enabling TensorFlow, PyTorch,
      and PySpark to stream training data directly from Parquet files without intermediate
      format conversion. In healthcare data engineering workflows with Flat FHIR datasets,
      transforming bulk FHIR data to Parquet format enables training pipelines to
      read directly from columnar storage with measured read latency of ~0.3 s/record,
      significantly faster than row-oriented JSON. Petastorm's integration allows
      ML frameworks to load and stream untransformed Parquet data into training algorithms,
      reducing preprocessing overhead and I/O bottlenecks during model training. Healthcare
      organizations transform large-scale clinical datasets (immunization records,
      patient observations, medication histories) into Parquet format to achieve superior
      read/query performance while maintaining compatibility with cloud AI platforms
      (Google Cloud AI) and major ML frameworks. Parquet's columnar layout with compression
      (1.2 bytes/record for synthetic Flat-FHIR immunization data) and efficient
      SQL query execution makes it particularly suitable for analytic and ML workloads
      where training datasets are repeatedly accessed during hyperparameter tuning,
      cross-validation, and model development. The approach enables cost savings (potentially
      millions of dollars) on cloud-hosted ML training through improved storage efficiency
      and query performance, while supporting direct data exchange between clinical
      data warehouses and ML training environments without format conversion overhead.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1093/jamia/ocz207
    - https://pmc.ncbi.nlm.nih.gov/articles/PMC7153160/pdf/3203152.pdf
  - id: B2AI_APP:189
    category: B2AI:Application
    name: Delta Lake/Delta Parquet for ML Reproducibility and Feature Stores
    description: Apache Parquet serves as the foundational columnar file format for
      Delta Lake and Delta Parquet, adding ACID transactions, time-travel capabilities,
      and versioning on top of Parquet's efficient storage and read performance. In
      Spark and Databricks-centered ML workflows, Delta Parquet enables reproducible
      machine learning operations through sophisticated time-travel features that
      allow rollback to previous dataset versions, critical for experiment reproducibility,
      model lineage tracking, and regulatory compliance in automotive and healthcare
      applications. The format excels in feature store creation where training datasets
      require frequent updates with transactional guarantees, supporting both micro-batch
      and streaming ingestion patterns with ACID consistency. Delta's integration
      with Apache Spark provides faster access to versioned data, schema enforcement,
      and streamlined data governance for large training datasets and historical analytics.
      ML practitioners leverage Delta Parquet for model training reproducibility,
      enabling controlled experimentation where specific dataset versions can be referenced
      for training, validation split repeatability across research teams. The time-travel
      capabilities support continuous model training scenarios such as autonomous
      vehicle perception systems where annotation datasets evolve with new labeled
      images, allowing teams to train on specific annotation versions and compare
      model performance across dataset evolution. Delta Parquet simplifies debugging
      ML pipelines by providing snapshot isolation for training runs, supports regulatory-compliant
      governance through immutable audit trails of dataset changes, and enables efficient
      storage management through compaction and data skipping optimizations while
      maintaining Parquet's columnar efficiency for read-intensive ML workloads.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.48550/arxiv.2508.13396
    - https://doi.org/10.47363/jaicc/2023(2)e241
  - id: B2AI_APP:190
    category: B2AI:Application
    name: Columnar Feature Filtering for Multimodal AI and NLP Pipelines
    description: Apache Parquet's columnar storage architecture enables efficient
      filtering and selection of specific features from large multimodal datasets,
      significantly accelerating preprocessing pipelines for AI models that process
      text, images, and sensor data. In sentiment analysis and NLP applications, Parquet
      has been used to store and process millions of text samples with reported significant
      reductions in data-retrieval time compared to CSV formats, enabling quicker
      iterations during model training. The columnar layout allows AI pipelines to
      load only relevant features needed for specific model architectures, lowering
      memory consumption and speeding processing by avoiding full-record deserialization.
      For multimodal healthcare AI applications handling diverse data types (clinical
      text narratives, medical images, wearable sensor streams), standardizing data
      into unified schemas using Parquet enables models to seamlessly access and process
      heterogeneous data sources with consistent preprocessing. Parquet's standardized
      metadata embedded in column headers supports reproducible preprocessing steps
      (tokenization, lemmatization, normalization) across research teams, improving
      collaboration in multi-institution NLP projects where consistent data handling
      is critical. The format's schema evolution capabilities allow models to adapt
      as feature sets expand with new clinical variables, imaging biomarkers, or sensor
      modalities without requiring complete dataset reprocessing. In production ML
      pipelines, Parquet's efficient column pruning enables real-time feature serving
      where inference requests selectively load feature subsets for specific model
      endpoints, reducing latency and compute costs compared to formats requiring
      full-record reads. The combination of compression, selective column access,
      and standardized metadata makes Parquet particularly effective for iterative
      feature engineering workflows where data scientists experiment with different
      feature combinations during model development.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.47363/jaicc/2023(2)e241
  - id: B2AI_APP:191
    category: B2AI:Application
    name: Lakehouse Bronze Layer for Cost-Efficient ML Training Data Storage
    description: Apache Parquet is recommended as the storage format for raw, immutable
      "bronze" layers in modern data lakehouse architectures, serving as the cost-efficient,
      read-optimized foundation for feature engineering and model training pipelines.
      In hybrid lakehouse designs combining Parquet and Delta Lake, Parquet handles
      stable, read-intensive analytical workloads and serves as "the backbone for
      cross-engine compatibilityideal for lightweight analytics and ML model training."
      The format's unmatched cost efficiency for read-only analytics makes it particularly
      suitable for archival training datasets that are written once and read repeatedly
      during model development, hyperparameter tuning, and ensemble training. Parquet's
      marginally higher write throughput compared to transactional formats enables
      efficient bulk loading of large-scale training corpora from batch ETL processes,
      though it requires manual version directories for maintaining multiple dataset
      snapshots, adding complexity compared to native versioning in Delta/Iceberg.
      For ML workflows, the bronze Parquet layer stores raw or minimally processed
      data (e.g., deduplicated sensor readings, parsed clinical notes, normalized
      imaging metadata) that feeds feature engineering pipelines producing curated
      "silver" and "gold" datasets. The approach optimizes cloud storage costs by
      using Parquet for high-volume, infrequently modified training data while reserving
      more expensive transactional formats for actively updated feature tables and
      serving layers. Parquet's cross-engine compatibility ensures training datasets
      remain accessible to diverse ML frameworks (TensorFlow, PyTorch, scikit-learn,
      XGBoost) and compute engines (Spark, Dask, Ray, Polars) without vendor lock-in,
      enabling organizations to experiment with different ML stacks without data migration
      overhead. The bronze layer pattern supports data lineage and reproducibility
      by preserving immutable source datasets while allowing downstream curated datasets
      to evolve independently, critical for regulated ML applications in healthcare
      and finance requiring audit trails of training data provenance.
    used_in_bridge2ai: false
    references:
    - https://irjernet.com/index.php/fecsit/article/view/237/196
  - id: B2AI_APP:192
    category: B2AI:Application
    name: Cross-Framework ML Training Data Exchange and Cloud Cost Optimization
    description: Apache Parquet serves as the standard interchange format for machine
      learning training datasets across heterogeneous frameworks (TensorFlow, PyTorch,
      PySpark) and cloud platforms (AWS, Google Cloud, Azure), enabling vendor-neutral
      data pipelines and substantial cloud cost reductions through efficient storage
      and query performance. Healthcare AI projects transforming Flat FHIR bulk data
      to Parquet achieve storage efficiency (1.2 bytes/record for immunization data)
      and fast query execution, with authors recommending Parquet for large-scale
      analytic and ML projects where read-optimized access patterns dominate and cloud
      storage costs significantly impact total cost of ownership. Parquet's compatibility
      matrix with major ML platforms (TensorFlow Yes, PyTorch Yes, PySpark Yes, Google
      Cloud AI Yes) ensures training datasets can be shared across research institutions,
      clinical sites, and cloud environments without format conversion, reducing data
      engineering overhead and enabling reproducible multi-site ML studies. The format's
      columnar compression and predicate pushdown capabilities dramatically lower
      data transfer costs in cloud environments where egress charges apply, as queries
      retrieve only necessary columns rather than full records. For federated learning
      initiatives spanning multiple hospitals or research centers, Parquet's cross-platform
      implementations in Java, C++, Python, Go, and Rust through Apache Arrow enable
      consistent data representation across diverse IT infrastructures without framework-specific
      dependencies. In production ML systems, Parquet's integration with cloud-native
      services (AWS S3 Select, BigQuery, Redshift Spectrum, Azure Synapse) enables
      serverless SQL queries over training datasets for feature exploration, data
      quality monitoring, and model performance analysis without provisioning compute
      resources. The combination of high compression ratios (10x-100x), fast selective
      reads, and broad ecosystem support positions Parquet as the de facto standard
      for cost-efficient storage and exchange of large-scale ML training corpora,
      particularly in biomedical domains where multi-modal datasets (genomics, imaging,
      clinical records) require harmonization across institutions and platforms.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1093/jamia/ocz207
    - https://pmc.ncbi.nlm.nih.gov/articles/PMC7153160/pdf/3203152.pdf
    - https://doi.org/10.47363/jaicc/2023(2)e241
- id: B2AI_STANDARD:360
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Persistent Uniform Resource Locator
  formal_specification: https://code.google.com/archive/p/persistenturls/
  has_relevant_organization:
  - B2AI_ORG:43
  is_open: true
  name: PURL
  purpose_detail: PURLs are Web addresses or Uniform Resource Locators (URLs) that
    act as permanent identifiers in the face of a dynamic and changing Web infrastructure.
    Instead of resolving directly to Web resources (documents, data, services, people,
    etc.) PURLs provide a level of indirection that allows the underlying Web addresses
    of resources to change over time without negatively affecting systems that depend
    on them. This capability provides continuity of references to network resources
    that may migrate from machine to machine for business, social or technical reasons.
  requires_registration: false
  url: https://sites.google.com/site/persistenturls/
- id: B2AI_STANDARD:361
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:15
  - B2AI_TOPIC:32
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Portable Document Format
  is_open: true
  name: PDF
  purpose_detail: A file format to present documents, including text formatting and
    images, in a manner independent of application software, hardware, and operating
    systems.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/PDF
- id: B2AI_STANDARD:362
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Portable Network Graphics
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:19
  is_open: true
  name: PNG
  purpose_detail: A raster-graphics file format that supports lossless data compression.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/Portable_Network_Graphics
- id: B2AI_STANDARD:363
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:15
  - B2AI_TOPIC:32
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Postscript Format
  is_open: true
  name: PS
  purpose_detail: "PostScript (PS) is a Turing-complete, stack-based page description
    language and programming language developed by Adobe Systems (John Warnock, Chuck
    Geschke, Doug Brotz, Ed Taft, Bill Paxton) and released in 1984, revolutionizing
    desktop publishing by providing device-independent representation of documents
    combining text, vector graphics, and raster images. PostScript uses reverse Polish
    notation and an interpreted execution model where documents are programs that,
    when executed by a PostScript interpreter (Raster Image Processor or RIP), render
    pages at the target device's resolution. The language describes graphics using
    vector primitives (straight lines, cubic Bzier curves) enabling arbitrary scaling,
    rotation, and transformation without quality loss, crucial for professional typography
    and technical illustrations. PostScript's sophisticated font system uses outline
    fonts with font hinting to maintain glyph quality at low resolutions, standardized
    through Type 1, Type 2, and Type 3 font formats that influenced modern font technologies
    like TrueType and OpenType. Three major versions exist: PostScript Level 1 (1984)
    introducing basic page description capabilities, PostScript Level 2 (1991) adding
    improved speed, image decompression (JPEG support), composite fonts, color separation,
    and form caching, and PostScript 3 (1997) providing enhanced color handling with
    up to 4096 gray levels, smooth shading operations, DeviceN color space for spot
    colors, and better filtering. PostScript powered the Apple LaserWriter (1985),
    triggering the desktop publishing revolution by enabling WYSIWYG document creation
    on Macintosh with PageMaker software. The language became the de facto standard
    for electronic prepress systems, high-end typesetters (Linotronic), and professional
    printing workflows throughout the 1980s-1990s. PostScript's imaging model directly
    influenced PDF (Portable Document Format), Adobe's 1993 successor that simplified
    PostScript for document distribution by removing general-purpose programming features
    while retaining the imaging model, making PDF documents static data structures
    rather than executable programs. PostScript remains common in high-end printers,
    professional publishing, and scientific visualization where precise vector graphics
    control is required. Open-source implementations like Ghostscript enable PostScript
    rendering on devices lacking native PostScript support. Scientific applications
    include generation of publication-quality figures from computational analysis
    software, precise technical diagrams, and device-independent archival documents."
  requires_registration: false
  url: https://en.wikipedia.org/wiki/PostScript
- id: B2AI_STANDARD:364
  category: B2AI_STANDARD:DataStandard
  collection:
  - markuplanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Predictive Model Markup Language
  formal_specification: https://dmg.org/pmml/v4-4-1/GeneralStructure.html
  is_open: true
  name: PMML
  purpose_detail: PMML (Predictive Model Markup Language) uses XML to represent mining
    models. The structure of the models is described by an XML Schema. One or more
    mining models can be contained in a PMML document. A PMML document is an XML document
    with a root element of type PMML
  requires_registration: false
  url: https://dmg.org/pmml/v4-4-1/GeneralStructure.html
- id: B2AI_STANDARD:365
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Protocol Buffers
  formal_specification: https://github.com/protocolbuffers/protobuf
  has_relevant_organization:
  - B2AI_ORG:37
  is_open: true
  name: protobuf
  purpose_detail: Protocol Buffers (protobuf) is Google's open-source, language-neutral, platform-neutral, extensible mechanism for serializing structured data, providing compact binary encoding (typically 3-10 smaller than XML/JSON), fast parsing (20-100 faster than XML), and strongly-typed schema definitions that enable backward and forward compatibility, making it the de facto standard for high-performance RPC systems (gRPC), distributed machine learning model serving (TensorFlow Serving), and large-scale data pipelines requiring efficient cross-language interoperability. Developed internally at Google in 2001 and open-sourced in 2008, protobuf defines data structures in .proto schema files using an interface definition language (IDL) specifying messages (analogous to structs or classes) with typed fields (int32, int64, float, double, string, bytes, bool, enum, nested messages, repeated fields for arrays), field numbers (integer tags used in wire format for backward compatibility), and optional/required/repeated modifiers controlling field cardinality, with schema compilation via protoc compiler generating language-specific code (C++, Java, Python, Go, C#, JavaScript, Ruby, PHP, Objective-C, Dart, Rust) providing type-safe serialization/deserialization APIs that eliminate manual parsing and reduce bugs from schema mismatches. The wire format employs tag-length-value encoding where each field is identified by its field number (1-byte to 5-byte varint tags) enabling schema evolution field numbers are permanently reserved once assigned, new fields can be added without breaking existing code (old parsers ignore unknown fields), and fields can be deprecated by marking as reserved, ensuring long-term compatibility in distributed systems where client and server versions may diverge, critical for AI/ML production environments where model inference services must handle requests from diverse client SDKs with varying feature sets. Protobuf's compact encoding uses variable-length integers (varints) for small numbers (1-byte for 0-127, scaling to 5 bytes for large values), zigzag encoding for signed integers, length-prefixed strings, and packed repeated fields (arrays stored contiguously without per-element tags), achieving space efficiency essential for mobile/edge ML applications where model metadata, feature vectors, and predictions are serialized for transmission over bandwidth-constrained networks, and for large-scale data lakes storing billions of training examples where 10 compression translates to significant storage cost reductions. Integration with gRPC (Google's high-performance RPC framework) makes protobuf the standard for microservices communication in ML serving architectures where feature extraction services, model inference endpoints, and post-processing pipelines exchange structured requests/responses with sub-millisecond latency, with protobuf schemas defining service interfaces (methods with input/output message types) compiled into client/server stubs supporting streaming RPCs (server-side streaming for batch inference, bidirectional streaming for online learning feedback loops). TensorFlow uses protobuf extensively for serializing computation graphs (GraphDef protocol buffers), model checkpoints (SavedModel protobuf format), and TensorBoard event logs, enabling cross-platform model deployment where models trained in Python are served via C++ TensorFlow Serving with Java/Go clients consuming predictions through protobuf-defined APIs, demonstrating protobuf's role in ML operationalization pipelines. Extensions and well-known types support common patterns google.protobuf.Any enables polymorphic message embedding (useful for heterogeneous training example types), google.protobuf.Duration/Timestamp provide standardized temporal representations, google.protobuf.Struct represents JSON-like dynamic structures when schema is not known at compile time (accommodating variable-length feature vectors in online learning), and oneof constructs enable tagged unions (e.g., model prediction can be classification probabilities OR regression value, serialized efficiently with single tag). Schema documentation and tooling include proto3 syntax (simplified modern variant with implicit optional fields and JSON mapping), protobuf-to-JSON converters enabling web API gateways that accept JSON and translate to binary protobuf for backend services, and rich IDE support (IntelliJ, VSCode extensions) with syntax highlighting, error checking, and schema navigation. For AI/ML applications, protobuf enables efficient feature storage where training datasets serialize as sequences of Example protobuf messages containing feature dictionaries (feature name  tensor value), model serving where prediction requests/responses are strongly typed (preventing silent errors from schema drift), distributed training where parameter server updates exchange gradient protobuf messages, and model monitoring where inference logs capture predictions and ground-truth labels in compact binary format suitable for large-scale analysis and retraining pipelines.
  requires_registration: false
  url: https://developers.google.com/protocol-buffers/
- id: B2AI_STANDARD:366
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Provenance
  has_relevant_organization:
  - B2AI_ORG:99
  is_open: true
  name: PROV
  purpose_detail: The PROV Family of Documents defines a model, corresponding serializations
    and other supporting definitions to enable the inter-operable interchange of provenance
    information in heterogeneous environments such as the Web.
  requires_registration: false
  url: https://www.w3.org/TR/prov-overview/
- id: B2AI_STANDARD:367
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Python pickle format
  formal_specification: https://github.com/python/cpython/blob/3.8/Lib/pickle.py
  is_open: true
  name: pickle
  purpose_detail: Serialization format for a Python object structure. Pickling is
    the process whereby a Python object hierarchy is converted into a byte stream,
    and unpickling is the inverse operation, whereby a byte stream (from a binary
    file or bytes-like object) is converted back into an object hierarchy.
  requires_registration: false
  url: https://docs.python.org/3.8/library/pickle.html
- id: B2AI_STANDARD:368
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Qualitative Data Exchange Schema
  formal_specification: https://dam.data-archive.ac.uk/standards/qudex_v03_01.xsd
  has_relevant_organization:
  - B2AI_ORG:97
  is_open: true
  name: QuDEx
  purpose_detail: The Qualitative Data Exchange Schema (QuDEx) allows users to discover,
    find, retrieve and cite complex qualitative data collections in context.
  requires_registration: false
  url: https://www.data-archive.ac.uk/managing-data/standards-and-procedures/metadata-standards/qudex/
- id: B2AI_STANDARD:369
  category: B2AI_STANDARD:DataStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: RDA CURE-FAIR 10 Things for Curating Reproducible and FAIR Research
  formal_specification: https://curating4reproducibility.org/10things/
  has_relevant_organization:
  - B2AI_ORG:83
  is_open: true
  name: RDA 10 Things
  purpose_detail: A framework for implementing effective curation workflows for achieving
    greater FAIR-ness and long-term usability of research data and code.
  requires_registration: false
  url: https://www.rd-alliance.org/group/cure-fair-wg/outcomes/10-things-curating-reproducible-and-fair-research
- id: B2AI_STANDARD:370
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: RDF Schema
  has_relevant_organization:
  - B2AI_ORG:99
  is_open: true
  name: RDFS
  purpose_detail: RDF Schema (RDFS) is the foundational vocabulary description language
    that extends the basic RDF vocabulary to provide essential data modeling capabilities
    for the Semantic Web and linked data applications. As a W3C Recommendation and
    semantic extension of RDF, RDFS enables the description of groups of related resources
    and relationships between them by defining classes, properties, and their hierarchical
    structures. Unlike traditional object-oriented programming models that define
    classes in terms of their instance properties, RDFS takes a property-centric approach
    where properties are described in terms of the classes they apply to through domain
    and range mechanisms. This design philosophy promotes the extensibility principle
    of the Web, allowing anyone to define additional properties for existing resources
    without requiring modification of original class definitions. RDFS provides core
    vocabulary elements including rdfs:Class, rdfs:Resource, rdfs:Property, rdfs:subClassOf,
    rdfs:subPropertyOf, rdfs:domain, rdfs:range, rdfs:label, and rdfs:comment, which
    form the basis for more sophisticated ontology languages like OWL. The schema
    supports the development of machine-readable vocabularies that can be processed
    automatically, enabling applications to discover and reason about resource relationships,
    making it an essential component of the Semantic Web infrastructure.
  requires_registration: false
  url: https://www.w3.org/TR/rdf-schema/
- id: B2AI_STANDARD:371
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Registry Interchange Format - Collections and Services schema
  has_relevant_organization:
  - B2AI_ORG:6
  is_open: true
  name: RIF-CS
  purpose_detail: The Registry Interchange Format - Collections and Services (RIF-CS)
    is an XML-based metadata schema developed by the Australian National Data Service
    (ANDS) for describing and exchanging information about research collections, services,
    parties (people and organizations), and activities. RIF-CS serves as the foundational
    data interchange format for Research Data Australia and enables institutions to
    contribute metadata about their research assets to national and international
    discovery services. The schema organizes metadata into four core entity types
    with rich relationship modeling capabilities - collections (datasets, databases,
    repositories), services (software tools, web services, facilities), parties (researchers,
    institutions, funders), and activities (projects, programs, events). Each entity
    supports comprehensive descriptive metadata including identifiers, names, descriptions,
    locations, dates, subjects, and crucially, relationships to other entities that
    create a connected graph of research infrastructure. RIF-CS enables automated
    harvesting and aggregation of research metadata across institutions, supporting
    research discovery, collaboration, and compliance with research data management
    policies.
  requires_registration: false
  url: https://services.ands.org.au/documentation/rifcs/1.2.0/guidelines/rif-cs.html
- id: B2AI_STANDARD:372
  category: B2AI_STANDARD:DataStandard
  collection:
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Research Object Crate
  formal_specification: https://www.researchobject.org/ro-crate/specification.html
  has_relevant_organization:
  - B2AI_ORG:116
  is_open: true
  name: RO-CRATE
  publication: doi:10.3233/DS-210053
  purpose_detail: "Research Object Crate (RO-Crate) is a community-developed, lightweight standard for packaging research data, code, workflows, and metadata into self-describing, FAIR-compliant digital objects using Schema.org annotations serialized as JSON-LD, making formal metadata description accessible and practical across diverse research contexts from individual datasets to large-scale computational experiments. Emerging from the Research Object movement and endorsed by communities including WorkflowHub, Australian Research Data Commons (ARDC), and European Open Science Cloud (EOSC), RO-Crate addresses the reproducibility crisis by bundling computational artifacts with their contextual metadataincluding authorship, provenance, licensing, computational environment specifications, and relationships among componentsin a machine-readable format that enables automated validation, discovery, and reuse without requiring specialized knowledge representation tools. The specification defines a structured directory containing data files, software, documentation, and a standardized ro-crate-metadata.json file at the root that describes the crate's contents using Schema.org vocabulary (Person, Organization, Dataset, SoftwareApplication, File, CreativeWork) extended with domain-specific types for computational workflows (ComputationalWorkflow, FormalParameter, WorkflowDescription Language), enabling rich representation of research artifacts as connected graphs where entities link to authors, funding agencies, publications, input/output data, and execution environments. RO-Crate supports hierarchical composition where complex research objects nest sub-cratesfor example, a parent crate describing a machine learning experiment contains child crates for training data, validation data, model checkpoints, inference code, and evaluation resultseach with its own metadata while the parent ro-crate-metadata.json aggregates relationships among components, supporting modular reuse where individual datasets or code components can be extracted and cited independently while maintaining their connection to the larger research context. The format's lightweight design requires only JSON-LD understanding (widely supported across programming languages) rather than specialized triple stores or OWL reasoners, with human-readable JSON structure enabling researchers to manually create or inspect metadata using text editors while automated tools (ro-crate-py Python library, ro-crate-js JavaScript package, ro-crate-ruby) provide programmatic creation, validation, and parsing capabilities integrating with workflow management systems (Galaxy, Nextflow, Snakemake), electronic lab notebooks (RSpace), data repositories (Zenodo, OSF, institutional repositories), and model registries (Hugging Face, MLflow). Integration with FAIR principles is explicit Findable through required persistent identifiers (DOIs, ORCIDs, ROR for organizations) and rich descriptive metadata; Accessible via standard web protocols with human-readable HTML landing pages generated from JSON-LD metadata; Interoperable through Schema.org's ubiquitous vocabulary and JSON-LD's linked data semantics enabling merging and querying across crates; Reusable through explicit licensing (Creative Commons, software licenses), versioning (schema.org/version), and detailed provenance including software versions, parameter settings, and computational environment specifications (Docker images, Conda environments, language versions). RO-Crate profiles enable domain-specific metadata requirements: Workflow RO-Crate for computational workflows with explicit input/output specifications and tool dependencies; ML RO-Crate for machine learning models bundling training data, model artifacts, performance metrics, and deployment requirements; Bioschemas profiles for life sciences data integrating biological annotations. WorkflowHub (workflowhub.eu), a major adopter, uses RO-Crate to package computational workflows from diverse systems (Galaxy, Nextflow, CWL, Snakemake) with consistent metadata enabling cross-platform workflow discovery and reuse, while FAIRSCAPE implements RO-Crate for AI-readiness packaging combining datasets, evidence graphs (EVI provenance), data dictionaries, and validation results in portable bundles supporting explainable AI requirements. For AI/ML applications, RO-Crate enables reproducible model packaging where training datasets, preprocessing scripts, model weights (Safetensors, PyTorch checkpoints), evaluation notebooks, and computational environment specifications travel together with metadata documenting intended use cases, known limitations, bias assessments, and performance benchmarks, supporting regulatory compliance in clinical AI, transparent reporting in publications, and confident model reuse by downstream researchers who can verify exact software versions, hyperparameters, and data provenance before adapting models to new domains."
  requires_registration: false
  url: https://www.researchobject.org/ro-crate/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:243
    category: B2AI:Application
    name: Workflow Run RO-Crate for ML Digital Pathology
    description: Workflow Run RO-Crate profiles are applied to capture execution provenance for machine learning workflows that train and evaluate deep learning models to detect carcinoma cells in high-resolution digital images of magnified human prostate tissue. The Process Run Crate and CPM RO-Crate profiles bundle workflow inputs (training/testing image datasets), outputs (trained models, classification results, evaluation metrics), scripts implementing preprocessing/training/evaluation steps, execution logs, and explicit parameter mappings connecting workflow-level parameters to tool-level parameters (e.g., linking workflow parameter to "extract_tissue.cwl tool"), enabling programmatic discovery of which tools are affected by workflow-level configurations and providing insight into workflow internal mechanics. The same CWL workflow can be executed with different workflow management systems (cwltool converting to Provenance Run Crate via runcrate tool, StreamFlow WMS) generating fully interoperable RO-Crates with consistent metadata structures recording four actions (workflow, two tissue extraction tool runs, tumour classification tool run) including schema.org instrument links, start/end times, inputs/outputs, and parameter value mappings, demonstrating cross-WMS provenance interoperability. Reported benefits include traceability of parameters across execution layers, improved reproducibility via properties like schema.org alternateName documenting alternative tool identifiers, integration of distributed provenance via the CPM extension, and cohesive packing of data/metadata/provenance enabling rerun of computations from recorded CWL input parameter mappings, supporting quality assessment and validation of ML-based diagnostic outputs in digital pathology research.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1371/journal.pone.0309210
  - id: B2AI_APP:244
    category: B2AI:Application
    name: MLentory Model Registry with RO-Crate and FAIR Signposting
    description: MLentory uses RO-Crate plus FAIR Signposting to implement a lightweight FAIR Digital Object registry for machine learning model metadata, treating the metadata (rather than model binaries) as the primary research object. Each model's metadataincluding name, description, provenance, licensing, associated datasets, and publication referencesis encapsulated as an RO-Crate JSON-LD document dynamically generated via server-side rendering and exposed through a backend API, with direct download links available from each model's landing page. FAIR Signposting is implemented via HTML <link> elements and HTTP Link headers connecting landing pages to RO-Crates and machine-readable metadata, with each RO-Crate assigned a resolvable persistent identifier via w3id.org following FDO Configuration Type 2 (PID resolution to landing page, which links to metadata). The system improves FAIRness of ML model metadata by making it machine-actionable (enabling automated harvesting and integration), discoverable (through structured Signposting links and Schema.org markup), reusable (with explicit licensing and provenance), and interoperable (via JSON-LD serialization supporting integration with knowledge graphs and semantic web tools). Current limitations include absence of hosted model artifact binaries (registry focuses on metadata aggregation from external repositories), reliance on simulated PIDs for some artifacts where permanent identifiers are unavailable, and RO-Crates linking to downloadable artifacts via landing pages rather than embedding permanent archives (with plans to offer temporary archive downloads), demonstrating a pragmatic approach to FAIR model metadata management that balances lightweight implementation with evolving FDO principles while improving transparency and reproducibility of ML model documentation.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.18420/inf2025_104
    - https://doi.org/10.52825/ocp.v5i.1188
  - id: B2AI_APP:245
    category: B2AI:Application
    name: FAIRSCAPE AI-Readiness Platform with RO-Crate Packaging
    description: FAIRSCAPE implements RO-Crate as the core packaging format for an AI-readiness framework targeting biomedical datasets and software, where users collect required metadata (producing JSON-LD shown in web interface), package it into ZIP files, and upload RO-Crate bundles to FAIRSCAPE server instances built on FastAPI with MinIO S3-compliant object storage for digital objects, MongoDB for metadata management, and REDIS message broker. The server receives, catalogs, indexes, extracts, and registers uploaded RO-Crate packages with their component metadata, exposing CLI and Web GUI clients for upload/view/download/publish operations. Each RO-Crate landing page displays a table summary of required JSON-LD metadata, serializations in multiple formats (JSON-LD for Schema.org interoperability, RDF/Turtle for semantic web integration), and D3 visualizations of evidence graphs (via jsonld-vis) revealing provenance relationships among datasets, computations, and software components. RO-Crates can be published directly to Dataverse repository instances for long-term preservation and institutional compliance. Near-term development targets explicit AI-readiness through planned expansion of metadata to include Bridge2AI Model Cards (documenting intended use, known limitations, bias assessments, performance benchmarks) and Datasheets for Datasets (capturing data collection methodology, annotation processes, ethical considerations), support for statistical characterization of input datasets (enabling automated data quality assessment and feature distribution analysis), integration with PEP (Portable Encapsulated Projects) for genomics workflows, and support for Bridge2AI CHoRUS challenge requirements. Reported benefits include deep provenance capture supporting explainability and auditability (critical for clinical AI regulatory compliance), FAIR-compliant metadata enabling automated discovery and integration, controlled sharing and publishing workflows (supporting collaborative research with governance controls), and pre-model explainability where data quality, feature engineering, and transformation provenance are documented before model training, supporting responsible AI development practices in biomedical research contexts.
    used_in_bridge2ai: true
    references:
    - https://doi.org/10.1101/2024.12.23.629818
  - id: B2AI_APP:246
    category: B2AI:Application
    name: HPC Provenance Capture for ML-at-Scale Workflows
    description: COMPSs runtime system implements automatic RO-Crate generation for HPC workflows, capturing provenance of large-scale computational tasks including distributed machine learning training and inference runs common in high-performance computing environments. The approach is lightweight and efficient the COMPSs runtime records file accesses during execution in dataprovenance.log files, then generates RO-Crate packages post-run via generate_COMPSs_RO-Crate.py using ro-crate-py library (version 0.6.1) to avoid runtime overheads that would degrade performance on time-critical HPC jobs, enabling scalability for large workflows with thousands of task nodes and files. Generated RO-Crates (named COMPSs_RO-Crate_[uuid]/) contain ro-crate-metadata.json with machine-readable JSON-LD adhering to Workflow RO-Crate profile, application source code files annotated with types (File, SoftwareSourceCode, ComputationalWorkflow), workflow visualization images (e.g., complete_graph.pdf showing task dependencies), command-line arguments capturing execution configuration, and input/output file lists with provenance relationships. The system integrates with WorkflowHub for workflow discovery and sharing, supporting FAIR principles by making provenance findable (through workflow registries), accessible (via standard RO-Crate format), interoperable (JSON-LD serialization compatible with semantic web tools), and reusable (bundling code, parameters, and artifacts needed for reproduction). Design goals achieved include automatic provenance registration requiring no user annotations (reducing burden on computational scientists), efficiency through post-execution generation minimizing impact on HPC resource utilization, and scalability proven on production workflows processing large scientific datasets. For AI/ML applications, this approach enables transparent documentation of distributed training runs (recording which GPUs/nodes processed which data partitions, parameter server update patterns, checkpoint schedules), model hyperparameter sweeps (capturing parameter combinations and resulting validation metrics across hundreds of parallel trials), and large-scale inference workflows (documenting model versions, input data batches, and output predictions with lineage), supporting reproducibility, debugging, and auditing requirements for production machine learning systems deployed on supercomputing infrastructure.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1109/works56498.2022.00006
  - id: B2AI_APP:247
    category: B2AI:Application
    name: Workflow Packaging and Testing Ecosystems for ML Pipelines
    description: Community-developed RO-Crate profiles for computational workflows and workflow testing, maintained by WorkflowHub and adopted by Galaxy, LifeMonitor, and other platforms, provide standardized packaging and automated quality assurance mechanisms frequently leveraged for machine learning pipelines requiring reproducibility and interoperability across heterogeneous execution environments. Workflow RO-Crate profile snapshots multi-file workflows (e.g., Common Workflow Language definitions in GitHub repositories, Galaxy workflow XML, Nextflow scripts) into single RO-Crate ZIP archives for archival preservation and assignment of persistent identifiers, with formal metadata documenting workflow structure (main workflow file, subworkflows, tool dependencies), input/output specifications with types and formats, software dependencies with versions, and authorship/licensing information following Schema.org and Bioschemas conventions. Workflow Testing RO-Crate extension adds formal testing components integrated with LifeMonitor, enabling specification of test instances (example inputs with expected outputs), automated health checks (periodic execution to verify workflow functionality), and continuous integration workflows, ensuring that packaged ML pipelines maintain correctness as underlying tools evolve. RO-Crates serve as typed, machine-actionable metadata templates enabling workflow discovery through registries (WorkflowHub search and filtering), PID resolution and FAIR Signposting (linking DOIs to workflow metadata and executable files), and CRUD operations via standardized APIs (GA4GH TRS API for programmatic workflow retrieval and execution). Profiles have been extended to encode provenance at multiple levels (researchers/authorship, computational resources/infrastructure, activities/executions, measurements/results) and combined with machine-actionable Data Management Plans, with a tailored profile existing for electronic lab notebook protocols supporting experimental workflow documentation. Reported benefits include improved machine-actionability (enabling automated workflow execution and composition in platforms like Galaxy), FAIRness through findability (registry integration), accessibility (PID resolution), interoperability (Schema.org/Bioschemas vocabularies compatible across domains), and reusability (licensing and frozen snapshots preventing version drift), portability via self-contained packaging (workflows travel with dependency specifications and example data), reproducibility through testing infrastructure (automated validation that workflows produce consistent outputs), and interoperability demonstrated by cross-platform workflow sharing (CWL workflows developed in one environment executed in Galaxy, Nextflow workflows shared via WorkflowHub consumed by diverse execution backends), supporting collaborative ML research where pipeline components are shared, validated, and reused across institutions and computational infrastructures.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.3897/rio.8.e93937
- id: B2AI_STANDARD:373
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Resource Description Framework
  has_relevant_organization:
  - B2AI_ORG:99
  is_open: true
  name: RDF
  purpose_detail: 'Resource Description Framework (RDF) is a W3C standard framework for representing
    information about resources on the Web using subject-predicate-object triples.
    RDF provides a graph-based data model where resources are identified by IRIs (Internationalized
    Resource Identifiers) and relationships form a directed, labeled graph. The framework
    enables decentralized knowledge representation, allowing anyone to make statements
    about any resource and merge distributed data seamlessly. RDF Schema (RDFS) extends
    RDF with vocabulary for describing classes, properties, domain, range, and hierarchical
    relationships, enabling semantic reasoning and inference. RDF supports multiple
    serialization formats including Turtle (human-readable), RDF/XML (verbose XML),
    JSON-LD (JSON-based), and N-Triples (line-oriented). The semantic web stack builds
    upon RDF: OWL for rich ontologies, SPARQL for querying RDF graphs, SHACL for
    shape validation. RDF powers linked open data initiatives (DBpedia, Wikidata),
    biomedical ontologies (OBO Foundry, Bio2RDF), knowledge graphs (Google Knowledge
    Graph principles), and enterprise knowledge management. The framework enables
    data integration across heterogeneous sources by providing common vocabularies
    (Dublin Core, FOAF, Schema.org) and federated querying. In AI/ML contexts, RDF
    graphs serve as structured knowledge bases for knowledge graph embeddings (TransE,
    DistMult, ComplEx), semantic reasoning for inference rules, ontology-guided feature
    engineering, and multi-relational learning where symbolic knowledge augments statistical
    learning, enabling explainable AI and knowledge-driven machine learning systems.'
  requires_registration: false
  url: https://www.w3.org/TR/rdf-schema/
- id: B2AI_STANDARD:374
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Safetensors
  formal_specification: https://github.com/huggingface/safetensors
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:42
  is_open: true
  name: Safetensors
  purpose_detail: Safetensors is a secure, simple, and fast file format for storing machine learning model weights and tensors, designed by Hugging Face to eliminate critical security vulnerabilities inherent in Python's pickle serialization while maintaining zero-copy memory-mapped loading performance essential for rapid model deployment and inference in production environments. Unlike pickle-based formats (PyTorch's .pt/.pth, TensorFlow's SavedModel with pickled components) that execute arbitrary Python code during deserializationenabling attackers to embed malicious code in model files that executes upon loading, compromising systems through remote code execution, data exfiltration, or cryptominingSafetensors provides a pure data format with no executable code, consisting of a JSON header specifying tensor metadata (names, shapes, dtypes, byte offsets) followed by raw tensor data in a flat binary layout, enabling parsers to validate file structure and extract tensors without executing untrusted code. The format achieves zero-copy loading through memory mapping where tensor data is accessed directly from disk without intermediate copying into RAM, reducing model loading time from seconds to milliseconds for large language models (LLaMA 70B, Falcon 180B with hundreds of gigabytes of weights) and enabling memory-efficient inference where only accessed tensor regions are paged into physical memory, particularly valuable for edge deployment on devices with limited RAM where traditional loading would exceed memory capacity. Safetensors supports all standard numeric data types (float32, float16, bfloat16, int8, uint8) used in modern neural networks including quantized models, with explicit endianness specification ensuring cross-platform compatibility between x86, ARM, and GPU architectures, and tensor metadata encoding shapes up to 8 dimensions with clear ordering (row-major/column-major) preventing misinterpretation that causes silent model corruption. The format provides built-in integrity verification through file structure validation and optional checksums, detecting corrupted downloads or storage errors before model loading, while supporting streaming and partial loading where specific layers or tensors can be extracted without parsing the entire file, enabling selective model component updates (e.g., replacing adapter modules, swapping attention heads) or distributed model serving where different nodes host different model shards. Integration with major deep learning frameworks through official libraries (safetensors Python package, Rust crate) enables seamless conversion from existing checkpoint formats (torch.save to safetensors.torch.save_file) with equivalent API ergonomics, while Hugging Face Hub defaults to Safetensors for all uploaded models since 2023, with automatic conversion tools for legacy pickle checkpoints and transparent fallback mechanisms ensuring backward compatibility. The format specification is framework-agnostic, supporting not only PyTorch and TensorFlow but also JAX (Flax), ONNX Runtime, and custom inference engines (llama.cpp, vLLM, TensorRT-LLM), enabling model portability across deployment stacks without format conversions that risk numerical precision degradation or metadata loss. Security benefits extend beyond code execution prevention to defense against adversarial model poisoning where malicious actors distribute trojan models disguised as benign checkpointsSafetensors' inability to execute code during loading eliminates entire classes of supply chain attacks targeting the model distribution phase, complementing runtime defenses against adversarial inputs. For model repositories and registries (Hugging Face Hub, TensorFlow Hub, PyTorch Hub), Safetensors reduces infrastructure risk by eliminating server-side vulnerabilities where automated model scanning or preview generation could trigger malicious pickle payloads, enabling safer community model sharing at scale. Performance benchmarks demonstrate loading speedups of 10-100 versus pickle for large models due to zero-copy memory mapping and elimination of Python object reconstruction overhead, with particularly dramatic improvements for quantized models (GPTQ, AWQ, GGUF-compatible formats) where 4-bit or 8-bit weights pack efficiently into Safetensors' raw binary layout without requiring custom pickle classes. The format's simplicitya JSON header plus raw tensor bytesenables easy implementation in languages beyond Python (Rust, C++, Go, JavaScript) supporting diverse deployment environments from mobile (CoreML, TensorFlow Lite conversion pipelines) to web (WASM-based inference) to embedded systems (microcontroller ML frameworks), democratizing safe model deployment across the full spectrum of AI/ML applications without platform-specific serialization lock-in.
  requires_registration: false
  url: https://github.com/huggingface/safetensors
- id: B2AI_STANDARD:375
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Scalable Vector Graphics Format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:19
  has_relevant_organization:
  - B2AI_ORG:99
  is_open: true
  name: SVG
  purpose_detail: Scalable Vector Graphics (SVG) Version 1.1, a modularized language
    for describing two-dimensional vector and mixed vector/raster graphics in XML.
  requires_registration: false
  url: https://www.w3.org/Graphics/SVG/About.html
- id: B2AI_STANDARD:376
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Shapes Constraint Language
  has_relevant_organization:
  - B2AI_ORG:99
  is_open: true
  name: SHACL
  purpose_detail: A language for validating RDF graphs against a set of conditions.
    These conditions are provided as shapes and other constructs expressed in the
    form of an RDF graph. RDF graphs that are used in this manner are called shapes
    graphs in SHACL and the RDF graphs that are validated against a shapes graph are
    called data graphs. As SHACL shape graphs are used to validate that data graphs
    satisfy a set of conditions they can also be viewed as a description of the data
    graphs that do satisfy these conditions. Such descriptions may be used for a variety
    of purposes beside validation, including user interface building, code generation
    and data integration.
  requires_registration: false
  url: https://www.w3.org/TR/shacl/
- id: B2AI_STANDARD:377
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Silicon Graphics image format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:19
  is_open: true
  name: SGI
  purpose_detail: Silicon Graphics Image (SGI or RGB) is a raster graphics file format
    developed by Silicon Graphics, Inc. for storing and displaying digital images
    on SGI workstations and UNIX systems. The format supports 8-bit, 16-bit, and 24-bit
    color depths with optional alpha channel for transparency, allowing representation
    of grayscale, RGB, and RGBA images with various bit depths per channel. SGI images
    can be stored uncompressed or with run-length encoding (RLE) compression for reduced
    file sizes while maintaining lossless quality. The format was widely used in professional
    computer graphics, visual effects, scientific visualization, and medical imaging
    during the 1980s-1990s when SGI workstations dominated high-end graphics computing.
    SGI files use .sgi, .rgb, .rgba, .bw, or .int file extensions depending on color
    configuration and bit depth. The format specifies image dimensions, number of
    channels (1 for grayscale, 3 for RGB, 4 for RGBA), compression method, and pixel
    data in a header-based structure readable by graphics software on big-endian systems.
    While largely superseded by more modern formats like PNG and TIFF for general
    use, SGI format remains relevant in legacy scientific visualization applications,
    particularly in medical imaging archives, computational fluid dynamics visualization,
    and legacy 3D rendering pipelines. ImageMagick, GIMP, and specialized scientific
    visualization software maintain SGI format support for backward compatibility with
    historical image datasets. The format's simplicity and direct pixel representation
    made it suitable for high-performance graphics rendering on SGI's proprietary hardware
    and OpenGL-based visualization systems.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/Silicon_Graphics_Image
- id: B2AI_STANDARD:378
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Simple Standard for Sharing Ontological Mappings
  formal_specification: https://github.com/mapping-commons/sssom
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:41
  - B2AI_SUBSTRATE:6
  is_open: true
  name: SSSOM
  publication: doi:10.1093/database/baac035
  purpose_detail: SSSOM is a Simple Standard for Sharing Ontological Mappings, providing
    a TSV-based representation for ontology term mappings, a comprehensive set of
    standard metadata elements to describe mappings, and a standard translation between
    the TSV and the Web Ontology Language (OWL).
  requires_registration: false
  url: https://mapping-commons.github.io/sssom/
- id: B2AI_STANDARD:379
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Statismo format
  formal_specification: https://github.com/statismo/statismo
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:16
  is_open: true
  name: statismo
  purpose_detail: Statismo defines a storage format (Statistical Image And Shape Models)
    based on HDF5, which includes all the information necessary to use the model,
    as well as meta-data about the model creation, which helps to make model building
    reproducible.
  related_to:
  - B2AI_STANDARD:339
  requires_registration: false
  url: https://github.com/statismo/statismo
- id: B2AI_STANDARD:380
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Statistical Data and Metadata eXchange standard
  has_relevant_organization:
  - B2AI_ORG:88
  is_open: true
  name: SDMX
  purpose_detail: "The Statistical Data and Metadata eXchange (SDMX) standard is an international initiative sponsored by major statistical organizations (IMF, OECD, Eurostat, World Bank, ECB, BIS, UN Statistics Division) providing ISO-standardized technical specifications (ISO 17369) for exchanging and sharing statistical data and metadata, enabling interoperable data flows among national statistical offices, international organizations, central banks, and research institutions through common information models, data structure definitions, and multiple serialization formats (SDMX-ML XML, SDMX-JSON, SDMX-CSV, SDMX-RDF). SDMX models statistical data as multidimensional cubes where observations are indexed by dimensions (time period, geographic area, indicator, sex, age group) and associated with measures (observed values, units, observation status) and attributes (confidentiality flags, estimation methods, data quality annotations), with Data Structure Definitions (DSDs) serving as schemas that specify allowed dimensions, their codelists (enumerated valid values), dimension cardinalities, and constraints, enabling automated validation and semantic interoperability where systems can interpret statistical series without manual codebook consultation. The standard encompasses an information model (conceptual entities: concepts, codelists, category schemes, data flows, organizational schemes defining data providers and recipients), structural metadata (DSDs defining cube structures, concept schemes providing semantic definitions, code lists enumerating dimension values with hierarchies), and reference metadata (methodological notes, collection procedures, quality indicators, revision policies) that together enable comprehensive statistical data documentation supporting reproducible research and informed reuse. SDMX Registry services provide centralized metadata repositories where organizations publish DSDs, codelists, and dataflow definitions with persistent identifiers, enabling consuming applications to discover available datasets, retrieve structural metadata to interpret data, and access reference metadata documenting quality and methodology, with SDMX web services (RESTful APIs and SOAP endpoints) supporting both metadata queries (retrieve DSDs for dataset X) and data queries (retrieve GDP series for OECD countries 2010-2020) with filtering by dimension values, time ranges, and aggregation levels. Common serialization formats include SDMX-ML (XML-based verbose format with full structural annotations), SDMX-JSON (lightweight format for web applications and JavaScript clients), SDMX-CSV (tabular format with embedded metadata headers for spreadsheet compatibility), and SDMX-RDF (linked data representation using W3C Data Cube vocabulary with sdmx-dimension, sdmx-measure, sdmx-attribute properties enabling SPARQL queries and integration into knowledge graphs), supporting diverse consumption patterns from web dashboards to statistical analysis tools to machine learning pipelines. Cross-domain exchange relies on SDMX Global Registry (maintained by SDMX consortia) hosting authoritative codelists (ISO 3166 country codes, ISO 4217 currency codes, ISO 639 language codes, SNA 2008 economic classifications) and cross-domain concepts (reference areas, time periods, frequencies, units of measure) enabling semantic alignment where GDP data from Eurostat, World Bank, and IMF use shared dimension definitions and codes, facilitating federated queries and comparative analysis across sources. The standard supports hierarchical and temporal codelists where dimension values nest (countries within regions, industries within sectors, months within quarters) enabling drill-down analysis and aggregation, with validity periods documenting when codes are active (e.g., currency codes before/after Euro adoption, country codes before/after political changes) ensuring correct temporal interpretation. SDMX adoption spans global statistical production with 80+ implementing organizations including all G20 statistical offices, over 150 published DSDs covering national accounts, labor statistics, prices, trade, finance, health, education, and environment domains, and thousands of registered dataflows with billions of observations exchanged annually via SDMX infrastructure, making it the de facto standard for official statistics dissemination. Machine learning and artificial intelligence applications leverage SDMX through natural language question answering systems that parse user queries and map linguistic elements to SDMX dimensions and codelists using Data Structure Definitions to identify relevant tables and dimensions, retrieve numeric values from SDMX web services, and generate responses explaining temporal trends or cross-country comparisons, with evaluation of dimension-value extraction accuracy, table ranking performance, and comparison/aggregation intent detection; programmatic dataset augmentation via SDMX APIs where ML pipelines in economics and finance (gingado library for Python/scikit-learn workflows) automatically retrieve official statistical series through SDMX protocol endpoints to enrich feature matrices with macroeconomic indicators (GDP growth, inflation, unemployment, trade balances) sourced from authoritative providers, supporting empirical testing of performance improvements when SDMX-derived features supplement domain datasets; semantic knowledge graph construction using SDMX vocabularies (sdmx-dimension, sdmx-measure, sdmx-attribute) combined with W3C Data Cube to generate RDF observations representing scientometric indicators, importing structured statistical metadata into graph databases (Neo4j) for graph analytics and explainable AI workflows that reason over semantically coherent indicator relationships; and graph neural network modeling over open statistics knowledge graphs that reuse SDMX-inspired concepts (sdmx:timePeriod for temporal dimensions, sdmx:refArea for geographic areas, sdmx:sex for demographic breakdowns) within linked data cubes, enabling GNN architectures (ChebNet, GCN, GraphSAGE) to predict outcomes (house prices) from spatiotemporal statistical patterns with explainability analysis (GNNExplainer, SHAP) identifying which SDMX-structured dimensions contribute most to predictions, demonstrating how SDMX semantic standardization facilitates AI interpretability on official statistics."
  requires_registration: false
  url: https://sdmx.org/
  has_application:
  - id: B2AI_APP:239
    category: B2AI:Application
    name: Natural Language Question Answering over SDMX Statistical Databases
    description: A natural language question answering (QA) system built to interact
      with SDMX-structured statistical databases enables users to query official statistics
      in plain English by parsing natural language questions, mapping linguistic elements
      to SDMX dimensions and codelists via Data Structure Definitions, identifying
      comparison and aggregation intents, detecting temporal and geographic constraints,
      ranking candidate SDMX datasets by query relevance, and retrieving numeric values
      from SDMX web services to generate natural language responses. Implemented over
      OECD's SDMX web service endpoints covering 12 statistical tables across 3 categories
      (labor, education, health), the QA system performs module-level AI/NLP tasks
      including question-type detection classifying queries as value requests or list
      requests (evaluated on 100 queries), time and location extraction parsing FROM/TO/THAN
      temporal expressions and geographic mentions (30 queries), comparison operator
      identification detecting greater-than/less-than/equal relationships (35 queries),
      aggregation intent recognition identifying count/min/max operations and item
      enumeration requests (30 queries), topic and table identification selecting
      relevant SDMX tables from query keywords with Top-1 accuracy 58% and Top-category
      accuracy 94% (48 queries distinguishing known vs unknown SDMX topics), and dimension-value
      extraction mapping query terms to specific SDMX dimension values such as SEX=FEMALE
      or COUNTRY=FRANCE with 78% accuracy (40 queries across 10 tables). The system
      leverages SDMX metadata structures where Data Structure Definitions list dimensions
      and codelists (allowed values) enabling semantic matching between natural language
      terms and standardized codes, and SDMX web services (SOAP/WSDL endpoints) provide
      programmatic access to multidimensional statistical arrays with cells containing
      numeric observations. Dataset ranking computes proximity scores based on keyword
      overlap between user queries and SDMX table metadata (titles, descriptions,
      dimension labels), presenting top-ranked candidates for user selection when
      ambiguity persists. Limitations include linguistic pattern coverage restricted
      to English templates, domain knowledge gaps requiring manual codelist mappings,
      and difficulty handling multi-hop reasoning across related indicators, but the
      system demonstrates concrete operational AI/NLP capabilities directly over SDMX
      infrastructure, enabling non-technical users to access complex official statistics
      through conversational interfaces without learning SDMX query syntax or navigating
      multidimensional data catalogs.
    used_in_bridge2ai: false
    references:
    - https://inria.hal.science/hal-03021075/document
  - id: B2AI_APP:240
    category: B2AI:Application
    name: Gingado ML Dataset Augmentation via SDMX Protocol
    description: Gingado is an open-source Python library for machine learning in
      economics and finance that leverages the SDMX (Statistical Data and Metadata
      eXchange) protocol to programmatically augment user-provided datasets with official
      statistical series from authoritative sources (central banks, statistical offices,
      international organizations), enabling empirical testing of whether SDMX-sourced
      macroeconomic indicators improve model performance in prediction and classification
      tasks. The library integrates with standard ML tooling including scikit-learn
      workflows, providing utilities for data augmentation where users specify target
      economic indicators (GDP growth, inflation rates, unemployment percentages,
      trade balances, interest rates, exchange rates) and gingado automatically retrieves
      corresponding time series from SDMX web service endpoints (IMF, OECD, Eurostat,
      World Bank, BIS), aligns temporal frequencies (quarterly, monthly, annual) with
      user data, handles missing values through forward-filling or interpolation,
      and merges SDMX series into feature matrices maintaining temporal ordering for
      supervised learning. SDMX protocol advantages for ML augmentation include standardized
      dimension definitions ensuring consistent interpretation of time periods, geographic
      areas, and indicator classifications across data providers; automated metadata
      retrieval specifying units of measure, seasonal adjustment status, and revision
      policies enabling appropriate preprocessing; and RESTful API access with filtering
      by date ranges and country codes reducing manual data collection overhead. Gingado
      supports benchmarking workflows where models trained on original features are
      compared against models augmented with SDMX series using cross-validation, with
      performance metrics (RMSE, MAE, R-squared for regression; accuracy, F1-score
      for classification) quantifying the marginal contribution of official statistics
      to predictive accuracy, addressing research questions such as "Do central bank
      policy rate series improve corporate default prediction?" or "Does trade balance
      data enhance exchange rate forecasting?" The library provides simulation capabilities
      for generating synthetic economic scenarios and documentation features for reproducible
      ML experiments, with SDMX-sourced provenance metadata (data provider, publication
      date, methodology links) embedded in experiment logs supporting transparent
      reporting of data sources in research publications and regulatory submissions.
      Discussions with SDMX technical experts informed library design choices regarding
      API endpoint selection, codelist mapping strategies, and handling of SDMX versioning
      where structural definitions evolve over time, ensuring robust data ingestion
      despite heterogeneity in SDMX implementations across international organizations.
    used_in_bridge2ai: false
    references:
    - https://www.bis.org/publ/work1122.pdf
  - id: B2AI_APP:241
    category: B2AI:Application
    name: SDMX Vocabularies for Scientometric Knowledge Graph Construction
    description: A scientometric indicator modeling system employs SDMX vocabularies
      (sdmx-attribute, sdmx-dimension, sdmx-measure) combined with W3C Data Cube
      ontology to generate machine-readable RDF observations representing research
      output metrics, enabling automated knowledge graph construction and graph-based
      analytics suitable for AI and explainable ML workflows. The system extends sdmx-measure:obsValue
      to represent numeric measures including publication counts, citation totals,
      researcher headcounts, and postdoctoral researcher numbers, and extends sdmx-dimension:refPeriod
      to represent temporal intervals (academic years, funding periods) by reusing
      time interval instances from UK reference data servers, ensuring semantic interoperability
      with other statistical datasets adhering to SDMX and Data Cube standards. Two
      interactive Python notebooks (one for multidimensional indicators with multiple
      grouping dimensions, one for unidimensional indicators with single time dimension)
      programmatically unpivot CSV input files containing raw scientometric data,
      generate SDMX-annotated RDF triples in Turtle serialization format, and produce
      10 output RDF files representing complete observation sets with explicit dimension
      values, measures, and attributes. Data Structure Definitions declare dimensions
      (research institution, academic department, funding source, time period), attributes
      (collection methodology, data quality flags, confidentiality levels), and measures
      (publication output, citation impact, personnel counts), following SDMX modeling
      patterns for official statistics but applied to research evaluation context.
      The generated RDF observations are imported into a Neo4j graph database using
      the n10s (Neosemantics) plugin via import.fetch commands, creating a property
      graph representation of scientometric indicators where nodes represent institutions,
      departments, and time periods connected by edges labeled with SDMX dimension
      relationships and measure values, enabling CYPHER queries for cross-institutional
      comparisons, temporal trend analysis, and correlation discovery among research
      metrics. SKOS (Simple Knowledge Organization System) concept schemes organize
      indicator taxonomies (publication types, citation impact classes, researcher
      career stages) with hierarchical broader/narrower relationships, supporting
      aggregation queries such as "total publications across all natural science departments"
      by traversing SKOS hierarchies. While the system does not explicitly demonstrate
      downstream AI/ML models in the documented work, the resulting SDMX-structured
      knowledge graph provides foundational infrastructure commonly used for graph
      neural networks, knowledge graph embeddings, link prediction, and explainable
      AI systems that require semantically consistent, machine-readable statistical
      observations with explicit provenance and quality metadata, illustrating how
      SDMX vocabularies extend beyond economic statistics to support AI-ready knowledge
      graphs in scientometrics and research evaluation domains.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1186/s40537-022-00562-x
  - id: B2AI_APP:242
    category: B2AI:Application
    name: Graph Neural Networks on SDMX-Inspired Open Statistics Knowledge Graphs
    description: Graph Neural Network (GNN) models for predictive analytics over open
      government statistics leverage SDMX-inspired concepts (sdmx:timePeriod for temporal
      dimensions, sdmx:refArea for geographic regions, sdmx:sex for demographic breakdowns)
      within linked data cubes constructed using W3C Data Cube vocabulary, demonstrating
      how SDMX semantic standardization facilitates explainable AI on spatiotemporal
      official statistics. A house price estimation system for Scotland constructs
      an open statistics knowledge graph by integrating multiple linked data sources
      (Scottish Statistics census indicators, geographic boundary datasets, temporal
      reference series) where observations are structured as RDF triples following
      Data Cube patterns with dimensions borrowed from UK Government Linked Data Working
      Group definitions that draw inspiration from SDMX guidelines, ensuring consistent
      semantic annotation of statistical dimensions across datasets. The knowledge
      graph is queried via SPARQL to extract feature vectors for machine learning,
      with each house sale record linked to spatiotemporal context through SDMX-style
      dimensions (sdmx:timePeriod connecting sales to quarterly or monthly time slices,
      sdmx:refArea linking properties to census output areas and administrative regions,
      demographic dimensions capturing population characteristics, age distributions,
      employment statistics, and deprivation indices at multiple geographic granularities).
      Three GNN architectures are evaluated Chebyshev Spectral Graph Convolution (ChebNet)
      using polynomial filters for localized spatial feature aggregation, Graph Convolutional
      Network (GCN) applying first-order approximations for efficient neighborhood
      aggregation, and GraphSAGE sampling fixed-size neighborhoods to enable scalable
      inductive learning on large graphseach trained to predict house prices by learning
      representations that encode both property-specific features and graph-structured
      relationships among geographic areas, temporal periods, and demographic indicators.
      Explainability analysis via GNNExplainer identifies which SDMX-aligned dimensions
      (time periods with specific economic conditions, geographic areas with characteristic
      demographics, interactions between temporal trends and spatial patterns) contribute
      most to price predictions for individual properties, with SHAP (SHapley Additive
      exPlanations) values quantifying marginal contributions of each dimension to
      model outputs, enabling interpretable insights such as "price increase driven
      primarily by temporal dimension indicating post-recession recovery period combined
      with refArea indicating proximity to urban employment centers." The SDMX-inspired
      semantic structure ensures that explanations reference standardized dimension
      concepts (time, geography, demographics) with clear definitions and hierarchical
      relationships (neighborhoods within districts, months within years), supporting
      transparent communication of model reasoning to non-technical stakeholders including
      policymakers, real estate professionals, and regulatory auditors. While the
      system reuses SDMX-inspired concepts rather than directly consuming SDMX APIs
      or official SDMX-ML/SDMX-JSON data streams, it demonstrates operational benefits
      of SDMX semantic modeling for AI workflows shared dimension vocabularies enable
      seamless integration of heterogeneous statistical sources, hierarchical codelists
      support multi-resolution spatial and temporal aggregation, and standardized
      metadata facilitate automated feature engineering where GNN input graphs are
      constructed programmatically from SPARQL queries over Data Cube observations
      without manual schema mapping, collectively illustrating how SDMX-aligned linked
      data infrastructure enhances reproducibility, interoperability, and explainability
      in statistical machine learning applications.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.3390/technologies12080128
- id: B2AI_STANDARD:381
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: STereoLithography File Format Family
  is_open: true
  name: STL
  purpose_detail: STL files describe only the surface geometry of a three-dimensional
    object without any representation of color, texture or other common CAD model
    attributes. The STL format specifies both ASCII and binary representations.
  requires_registration: false
  url: https://www.loc.gov/preservation/digital/formats/fdd/fdd000504.shtml
- id: B2AI_STANDARD:382
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Structured Descriptive Data
  formal_specification: https://github.com/tdwg/sdd
  has_relevant_organization:
  - B2AI_ORG:93
  is_open: true
  name: SDD
  purpose_detail: The goal of the Structured Descriptive Data (SDD) standard is to
    allow capture, transport, caching and archiving of descriptive data in all the
    forms shown above, using a platform- and application-independent, international
    standard. Such a standard is crucial to enabling lossless porting of data between
    existing and future software platforms including identification, data-mining and
    analysis tools, and federated databases.
  requires_registration: false
  url: https://www.tdwg.org/standards/sdd/
- id: B2AI_STANDARD:383
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Tagged Image File Format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:19
  has_relevant_organization:
  - B2AI_ORG:116
  is_open: true
  name: TIFF
  purpose_detail: An image file format for storing raster graphics images.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/TIFF
  used_in_bridge2ai: true
- id: B2AI_STANDARD:384
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: TAR archive file format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:52
  is_open: true
  name: TAR
  purpose_detail: A tar (tape archive) file format is an archive created by tar, a
    UNIX-based utility used to package files together for backup or distribution purposes.
    It contains multiple files (also known as a tarball) stored in an uncompressed
    format along with metadata about the archive. Tar files are not compressed archive
    files. They are often compressed with file compression utilities such as gzip
    or bzip2.
  requires_registration: false
  url: https://www.loc.gov/preservation/digital/formats/fdd/fdd000531.shtml
- id: B2AI_STANDARD:385
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: TDWG Access Protocol for Information Retrieval
  has_relevant_organization:
  - B2AI_ORG:93
  is_open: true
  name: TAPIR
  purpose_detail: The TDWG Access Protocol for Information Retrieval (TAPIR) is a
    Web Service protocol and XML schema to perform queries across distributed databases
    of varied physical and logical structure. It was originally designed to be used
    by federated networks. TAPIR is intended for communication between applications,
    using HTTP as the transport mechanism. TAPIR's flexibility makes it suitable to
    both very simple service implementations where the provider only responds to a
    set of pre-defined queries, or more advanced implementations where the provider
    software can dynamically parse complex queries referencing output models supplied
    by the client.
  requires_registration: false
  url: https://www.tdwg.org/standards/tapir/
- id: B2AI_STANDARD:386
  category: B2AI_STANDARD:DataStandard
  collection:
  - codesystem
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: The Unified Code for Units of Measure
  has_relevant_organization:
  - B2AI_ORG:84
  is_open: true
  name: UCUM
  purpose_detail: A common syntax for communication of quantities and their units.
  requires_registration: false
  url: https://unitsofmeasure.org/ucum
- id: B2AI_STANDARD:387
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Waveform Audio File Format
  formal_specification: https://sites.google.com/site/musicgapi/technical-documents/wav-file-format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:48
  - B2AI_SUBSTRATE:49
  is_open: true
  name: WAV
  purpose_detail: Waveform Audio File Format (WAVE or WAV due to its filename extension
    is an audio file format standard.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/WAV
- id: B2AI_STANDARD:388
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Web Ontology Language
  has_relevant_organization:
  - B2AI_ORG:99
  is_open: true
  name: OWL
  purpose_detail: The Web Ontology Language (OWL) is a family of knowledge representation
    languages or ontology languages for authoring ontologies or knowledge bases. The
    languages are characterized by formal semantics and RDF/XML-based serializations
    for the Semantic Web. OWL is endorsed by the World Wide Web Consortium (W3C) and
    has attracted academic, medical and commercial interest. The OWL 2 Web Ontology
    Language, informally OWL 2, is an ontology language for the Semantic Web with
    formally defined meaning. OWL 2 ontologies provide classes, properties, individuals,
    and data values and are stored as Semantic Web documents. OWL 2 ontologies can
    be used along with information written in RDF, and OWL 2 ontologies themselves
    are primarily exchanged as RDF documents.
  requires_registration: false
  url: https://www.w3.org/TR/owl-overview/
- id: B2AI_STANDARD:389
  category: B2AI_STANDARD:DataStandard
  collection:
  - workflowlanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Workflow Description Language
  formal_specification: https://github.com/openwdl/wdl
  is_open: true
  name: WDL
  purpose_detail: The Workflow Description Language (WDL) is a way to specify data
    processing workflows with a human-readable and -writeable syntax. WDL makes it
    straightforward to define analysis tasks, chain them together in workflows, and
    parallelize their execution. The language makes common patterns simple to express,
    while also admitting uncommon or complicated behavior; and strives to achieve
    portability not only across execution platforms, but also different types of users.
    Whether one is an analyst, a programmer, an operator of a production system, or
    any other sort of user, WDL should be accessible and understandable.
  requires_registration: false
  url: https://openwdl.org/
- id: B2AI_STANDARD:390
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - deprecated
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: X PixMap bitmap image format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:19
  is_open: true
  name: XBM
  purpose_detail: X PixMap (XBM) is an image file format used by the X Window System.
    Replaced by XPM.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/X_PixMap
- id: B2AI_STANDARD:391
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: X PixMap image format
  is_open: true
  name: XPM
  purpose_detail: X PixMap (XPM) is an image file format used by the X Window System.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/X_PixMap
- id: B2AI_STANDARD:392
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: xarray
  formal_specification: https://github.com/pydata/xarray
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:1
  - B2AI_SUBSTRATE:50
  is_open: true
  name: xarray
  purpose_detail: An open source project and Python package that introduces labels
    in the form of dimensions, coordinates, and attributes on top of raw NumPy-like
    arrays, which allows for more intuitive, more concise, and less error-prone user
    experience.
  requires_registration: false
  url: https://xarray.dev/
- id: B2AI_STANDARD:393
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: YAML Ain't Markup Language
  is_open: true
  name: YAML
  purpose_detail: "YAML (YAML Ain't Markup Language, originally Yet Another Markup
    Language) is a human-readable data serialization language designed for configuration
    files, data exchange between programming languages with different data structures,
    and representation of complex hierarchical data in a format that prioritizes readability
    and minimal syntax over machine parsing efficiency. YAML's design philosophy emphasizes
    whitespace-based indentation rather than brackets or braces, uses intuitive syntax
    for common data types (scalars, lists, dictionaries/mappings), supports multi-line
    strings without escape characters, allows inline comments prefixed with pound symbol, and
    provides advanced features including anchors and aliases for avoiding repetition,
    custom data type tags, and document streaming with multiple YAML documents in
    a single file separated by ---. The language is a superset of JSON, meaning any
    valid JSON document is also valid YAML, but offers more expressive syntax with
    less visual noise, making it the preferred format for human-authored configuration
    files in DevOps, continuous integration/deployment pipelines (GitHub Actions,
    GitLab CI, CircleCI, Travis CI), container orchestration (Kubernetes manifests,
    Docker Compose), infrastructure-as-code tools (Ansible playbooks, Helm charts,
    CloudFormation templates), and application configuration management. YAML parsers
    exist for virtually all programming languages (PyYAML and ruamel.yaml for Python,
    js-yaml for JavaScript, SnakeYAML for Java, yaml-cpp for C++, gopkg.in/yaml for
    Go), enabling cross-language data interchange in polyglot software systems and
    supporting serialization of native data structures from one language for consumption
    by another. In scientific computing and data science, YAML serves as a lightweight
    schema language for defining data models and metadata standards (LinkML, FAIR
    Genomes semantic model, OpenAPI specifications, JSON Schema), configuration format
    for machine learning experiment tracking (MLflow, Weights & Biases, Hydra), pipeline
    definition files for workflow engines (Snakemake, Nextflow, CWL), and package
    metadata (conda environment.yml, Python pyproject.toml's dynamic dependencies).
    YAML's human-friendliness makes it suitable for version control and collaborative
    editing, as diffs are readable and merge conflicts can be resolved without specialized
    tools, though this same flexibility introduces security risks (arbitrary code
    execution through deserialization of unsafe types) that require careful parser
    configuration and validation of untrusted input. The format is maintained by the
    YAML community with formal specification (YAML 1.2.2 as of 2021) and reference
    implementations, and has become ubiquitous in cloud-native software development,
    where entire application stacks are defined as YAML manifests that declaratively
    specify desired infrastructure state, enabling GitOps workflows where infrastructure
    changes are reviewed, version-controlled, and audited through standard software
    development practices applied to configuration files."
  requires_registration: false
  url: https://en.wikipedia.org/wiki/YAML
- id: B2AI_STANDARD:394
  category: B2AI_STANDARD:DataStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Zarr
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:1
  - B2AI_SUBSTRATE:24
  - B2AI_SUBSTRATE:51
  is_open: true
  name: Zarr
  purpose_detail: A format for storage of large N-dimensional typed arrays. Has implementations
    in multiple programming languages.
  requires_registration: false
  url: https://zarr.dev/
- id: B2AI_STANDARD:395
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ZIP compressed file format
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:52
  is_open: true
  name: ZIP
  purpose_detail: An archive file format that supports lossless data compression.
  requires_registration: false
  url: https://en.wikipedia.org/wiki/ZIP_(file_format)
- id: B2AI_STANDARD:396
  category: B2AI_STANDARD:ModelRepository
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: AdapterHub
  formal_specification: https://github.com/adapter-hub/Hub
  is_open: true
  name: AdapterHub
  publication: doi:10.48550/arXiv.2007.07779
  purpose_detail: AdapterHub is a centralized repository and framework for parameter-efficient transfer learning, enabling researchers to share, discover, and compose lightweight adapter modules that modify pre-trained transformer models for specific tasks without full fine-tuning, reducing computational costs by up to 99% while maintaining 95-98% of full fine-tuning performance across diverse NLP tasks. Adapters are compact neural network modules (typically 0.5-8% of base model parameters) inserted between transformer layers that learn task-specific transformations through bottleneck architectures (down-projection, non-linearity, up-projection) while keeping the original pre-trained weights frozen, enabling a single base model (BERT, RoBERTa, XLM-R, T5) to support hundreds of tasks through modular adapter stacking rather than maintaining separate full model copies for each application. Built as an extension to Hugging Face's transformers library, AdapterHub integrates seamlessly with existing training workflows requiring only two additional lines of code to train and share adapters (model.add_adapter("task_name") and model.train_adapter("task_name")), with the adapter-transformers library providing unified interfaces for adapter loading, composition, and configuration management across 20+ adapter architectures including Pfeiffer adapters (original bottleneck design), Houlsby adapters (parallel insertion), LoRA (Low-Rank Adaptation using rank decomposition), prefix tuning (learning soft prompts), and compacter adapters (using hypercomplex multiplication for further parameter reduction). The repository hosts over 2,000 pre-trained adapters spanning classification (sentiment analysis, topic categorization, toxicity detection), sequence labeling (named entity recognition, part-of-speech tagging, chunking), question answering (SQuAD, Natural Questions, BoolQ), semantic parsing (dependency parsing, constituency parsing), and generation tasks (summarization, translation, data-to-text), trained on standard benchmarks (GLUE, SuperGLUE, XTREME for multilingual evaluation) with documented performance metrics, training hyperparameters, and dataset characteristics enabling reproducible adapter reuse. AdapterHub supports multilingual transfer where adapters trained on high-resource languages (English, Chinese) compose with language-specific adapters for low-resource languages (Swahili, Quechua) through cross-lingual transfer, enabling zero-shot or few-shot adaptation to languages with limited labeled data by leveraging shared multilingual representations in base models like XLM-RoBERTa while learning language-specific patterns in lightweight adapter modules. Adapter composition enables modular multi-task learning where task-specific adapters (e.g., "sentiment" + "sarcasm-detection") stack or fuse to create specialized models combining complementary capabilities, with fusion mechanisms (weighted averaging, attention-based selection) determining how adapter outputs combine, supporting applications like aspect-based sentiment analysis that requires both entity recognition and opinion classification capabilities. The platform provides standardized adapter configurations (reduction factors controlling bottleneck dimensions, placement strategies specifying which layers receive adapters, activation functions) ensuring reproducibility and fair comparisons across research groups, while adapter cards document training procedures, base model requirements, evaluation results on held-out data, and intended use cases with known limitations. For enterprise and research applications, AdapterHub enables efficient model deployment where a single shared base model loads task-specific adapters on-demand based on inference requests, reducing GPU memory footprint from gigabytes to megabytes per task and enabling multi-tenant serving where different users access different task adapters simultaneously without model duplication, particularly valuable for edge deployment (mobile devices, embedded systems) where memory constraints prohibit storing multiple full models. The framework supports continual learning scenarios where new task adapters are trained incrementally without forgetting previous tasks (avoiding catastrophic forgetting) since base model weights remain frozen and each task's knowledge is encapsulated in its dedicated adapter module, enabling long-lived systems that accumulate capabilities over time. AdapterHub accelerates research by providing baseline adapters for benchmark datasets, enabling rapid prototyping where researchers can compare novel adapter architectures against established designs using consistent evaluation protocols, and supporting meta-analyses examining how adapter capacity (parameter count), placement strategies (serial vs. parallel insertion), and fusion techniques affect transfer learning efficiency across task types, model sizes, and linguistic phenomena, collectively advancing understanding of parameter-efficient transfer learning and democratizing access to task-specific model customization without requiring extensive computational resources for full fine-tuning.
  requires_registration: false
  url: https://adapterhub.ml/
- id: B2AI_STANDARD:397
  category: B2AI_STANDARD:ModelRepository
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Bioimage Model Zoo
  formal_specification: https://github.com/bioimage-io/bioimage.io
  is_open: true
  name: Bioimage
  publication: doi:10.1101/2022.06.07.495102
  purpose_detail: A community-driven, fully open resource where standardized pre-trained
    models can be shared, explored, tested, and downloaded for further adaptation
    or direct deployment in multiple end user-facing tools (e.g., ilastik, deepImageJ,
    QuPath, StarDist, ImJoy, ZeroCostDL4Mic, CSBDeep).
  requires_registration: false
  url: https://bioimage.io/#/
- id: B2AI_STANDARD:398
  category: B2AI_STANDARD:ModelRepository
  collection:
  - dataregistry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Hugging Face  Models
  formal_specification: https://github.com/huggingface/huggingface_hub
  is_open: true
  name: HF Models
  purpose_detail: Hugging Face Models (huggingface.co/models) is the world's largest open-source repository and collaboration platform for machine learning models, hosting over 500,000 pre-trained models spanning natural language processing, computer vision, audio processing, multimodal learning, reinforcement learning, and tabular data applications, with standardized model cards, inference APIs, and version control enabling reproducible AI research and democratized access to state-of-the-art architectures. Founded in 2016 and rapidly becoming the de facto standard for sharing transformer-based models, the platform provides Git-based versioning (Git-LFS for large files) allowing researchers to track model iterations, fork and modify existing models, and collaborate on model development with pull requests and issue tracking, while the huggingface_hub Python library enables programmatic model download, upload, and inference integration with popular frameworks (PyTorch, TensorFlow, JAX, ONNX). The repository encompasses language models ranging from compact distilled variants (DistilBERT, TinyBERT for edge deployment) to massive foundation models (LLaMA, Mistral, GPT-NeoX, BLOOM with billions of parameters), with specialized architectures for named entity recognition (RoBERTa-NER, SpaCy transformers), sentiment analysis (BERT-base-cased fine-tuned on SST-2), question answering (ELECTRA-QA, DeBERTa-SQuAD), translation (MarianMT covering 1,200+ language pairs, NLLB-200 for low-resource languages), summarization (BART, T5, Pegasus), and code generation (CodeGen, StarCoder, Code Llama trained on GitHub repositories). Beyond NLP, the platform hosts vision transformers (ViT, DeiT, Swin) pre-trained on ImageNet-21k enabling transfer learning for medical imaging, satellite analysis, and manufacturing quality control; object detection models (DETR, YOLOv8, Mask R-CNN) with COCO weights; image segmentation architectures (SegFormer, Mask2Former); audio models for automatic speech recognition (Whisper supporting 100+ languages, Wav2Vec2 for self-supervised learning) and audio classification (Audio Spectrogram Transformer); and multimodal models (CLIP, BLIP, Flamingo) enabling vision-language tasks including image captioning, visual question answering, and text-to-image generation (Stable Diffusion, ControlNet). Each model includes a standardized model card documenting intended use cases, training data composition with dataset sources and sizes, evaluation metrics on benchmark datasets, known limitations and biases, ethical considerations (potential misuse scenarios, fairness assessments), carbon emissions from training (kg CO2 equivalent), computational requirements (GPU memory, inference latency), and licensing terms (Apache 2.0, MIT, CreativeML for generative models), supporting informed model selection and responsible AI deployment. The Inference API provides instant model testing via web interface or curl requests without local setup, enabling rapid prototyping and model comparison, while Hugging Face Spaces hosts interactive demos (Gradio, Streamlit apps) showcasing model capabilities and allowing non-technical stakeholders to evaluate outputs before integration. Integration with the transformers library (100M+ downloads) provides unified interfaces (AutoModel, pipeline API) abstracting framework differences and enabling single-line model loading with automatic tokenizer/feature extractor selection, while optimizations including 8-bit quantization (bitsandbytes), Flash Attention 2 for memory efficiency, and ONNX Runtime conversion reduce inference costs for production deployment. The platform supports private repositories for enterprise use cases, organizations for team collaboration, gated models requiring license acceptance (LLaMA 2, BLOOM), and community-driven model evaluation through the Open LLM Leaderboard benchmarking language models on standardized tasks (MMLU, HellaSwag, TruthfulQA) with automated evaluation pipelines ensuring fair comparisons. For AI/ML practitioners, Hugging Face Models accelerates development by providing production-ready checkpoints eliminating weeks of pretraining compute (training GPT-3-scale models costs millions of dollars), enables domain adaptation through fine-tuning on specialized datasets (legal documents, biomedical literature, scientific papers) leveraging pretrained representations, and supports model serving via Inference Endpoints (managed GPU instances) or export to TensorFlow Lite/CoreML for mobile deployment, collectively reducing the barrier to deploying sophisticated AI systems from research prototypes to production applications serving millions of users.
  requires_registration: false
  url: https://huggingface.co/models
- id: B2AI_STANDARD:399
  category: B2AI_STANDARD:ModelRepository
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Model Zoo
  is_open: true
  name: modelzoo.co
  purpose_detail: 'Model Zoo (modelzoo.co) is a curated discovery platform that aggregates pre-trained machine learning models from diverse frameworks (TensorFlow, PyTorch, Caffe, MXNet, ONNX), providing a centralized index for finding models across computer vision, natural language processing, speech recognition, and reinforcement learning domains. Originally launched as a community-driven resource, Model Zoo enables practitioners to search for state-of-the-art architectures by task (object detection, semantic segmentation, translation), dataset (ImageNet, COCO, WMT), or framework compatibility. The platform serves as a model marketplace connecting researchers publishing novel architectures with practitioners seeking production-ready implementations, reducing the barrier to adopting cutting-edge techniques. Model Zoo listings typically include architecture descriptions, training configurations, performance benchmarks (accuracy, inference speed), framework-specific code repositories, and pre-trained weight files for transfer learning. Unlike framework-specific repositories (PyTorch Hub, TensorFlow Hub), Model Zoo provides cross-framework search capabilities, enabling users to discover equivalent architectures implemented in multiple ecosystems and compare performance characteristics across frameworks. The platform supports both academic research models (recent conference publications) and industry-proven architectures (ResNet, BERT variants), with community ratings and download metrics indicating model popularity and reliability. Model Zoo facilitates reproducibility by linking to original papers, training datasets, and hyperparameter configurations, while model cards provide metadata on intended use cases, known limitations, and ethical considerations. For AI/ML practitioners, Model Zoo accelerates prototyping by providing a starting point for transfer learning, enabling rapid experimentation with pre-trained models fine-tuned on domain-specific data rather than training from scratch, particularly valuable when compute resources or labeled data are limited.'
  requires_registration: false
  url: https://modelzoo.co/
- id: B2AI_STANDARD:400
  category: B2AI_STANDARD:ModelRepository
  collection:
  - dataregistry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: OpenML
  formal_specification: https://github.com/openml/OpenML
  is_open: true
  name: OpenML
  purpose_detail: OpenML is an open-source platform and collaborative ecosystem for sharing, organizing, and automating machine learning experiments, providing standardized access to over 21,000 datasets, 7,000 tasks, 100,000+ algorithm implementations (flows), and millions of experimental results (runs) with rich metadata enabling reproducible research, automated machine learning (AutoML), and meta-learning across the global ML research community. Founded in 2013 by Joaquin Vanschoren and collaborators, OpenML addresses the reproducibility crisis in machine learning by establishing a centralized repository where researchers deposit not only datasets and code but complete experimental pipelines including data preprocessing steps, algorithm hyperparameters, performance metrics, runtime statistics, and computational environment details, all indexed by unique persistent identifiers and searchable via a RESTful API and Python/R/Java client libraries. The platform defines four core entities that structure ML experiments datasets (tabular data, time series, images) with comprehensive metadata including feature types, class distributions, missing value patterns, and provenance; tasks that specify learning problems (classification, regression, clustering) with defined train/test splits, evaluation metrics, and target variables, ensuring fair comparisons across studies; flows representing algorithm implementations with specific library versions, hyperparameter schemas, and software dependencies captured via containerization or environment specifications; and runs documenting execution results linking a specific flow applied to a specific task, recording predictions, evaluation scores, confusion matrices, learning curves, computational cost, and reproducibility artifacts. OpenML's standardized task definitions enable apples-to-apples benchmarking where multiple research groups apply different algorithms to identical problems with fixed data splits and metrics, eliminating the variability introduced when each paper uses custom evaluation protocols and reporting only favorable splits, thereby providing more reliable performance estimates and accelerating identification of genuinely superior methods versus tuning artifacts. The platform's API supports programmatic experiment automation where AutoML systems (Auto-sklearn, TPOT, H2O AutoML) query OpenML for benchmark tasks, execute hyperparameter optimization across multiple algorithms, and automatically upload results with full reproducibility metadata, creating a continuously growing knowledge base of algorithm performance across diverse problem characteristics (dataset size, dimensionality, class imbalance, feature types) that informs meta-learning models predicting which algorithms will perform best on new unseen datasets based on dataset meta-features (number of instances, attributes, classes, skewness, entropy). Integration with popular ML frameworks through openml-python, openml-r, and mlr3 packages enables one-line dataset loading (openml.datasets.get_dataset(id)), task retrieval with pre-defined splits preserving reproducibility, and automatic run uploading that captures scikit-learn pipelines, XGBoost models, or deep learning architectures with complete hyperparameter configurations and performance metrics, reducing friction in contributing to community benchmarks. OpenML's metadata richness supports advanced queries identifying datasets with specific characteristics (e.g., "binary classification tasks with 1,000-10,000 instances, 50-200 features, <10% missing values") for systematic empirical studies, meta-analyses aggregating results across hundreds of datasets to assess algorithm performance in different regimes, and educational applications where students explore the relationship between dataset properties and model behavior through interactive visualizations of performance landscapes. The platform promotes FAIR principles (Findable, Accessible, Interoperable, Reusable) by assigning DOI identifiers to datasets, versioning all entities to track changes, requiring open licenses (CC BY, CC0, public domain), and providing machine-readable metadata (ARFF format, JSON schemas) that integrates with scholarly infrastructure (DataCite, Zenodo). For AI/ML research, OpenML serves as a benchmark suite for evaluating novel algorithms against established baselines across diverse tasks, a data source for meta-learning studies training models to recommend algorithms or initialize hyperparameters based on dataset characteristics, an infrastructure for collaborative competitions where participants upload solutions to shared tasks and leaderboards update automatically, and a teaching resource providing curated datasets with ground-truth labels and documented preprocessing steps suitable for ML coursework and tutorials, collectively advancing reproducible, transparent, and cumulative progress in machine learning research.
  requires_registration: true
  url: https://www.openml.org/
- id: B2AI_STANDARD:401
  category: B2AI_STANDARD:ModelRepository
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: PyTorch Hub
  formal_specification: https://github.com/pytorch/hub
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:33
  is_open: true
  name: PyTorch Hub
  purpose_detail: 'PyTorch Hub is the official model repository and distribution system for the PyTorch deep learning framework, providing programmatic access to pre-trained models through a standardized torch.hub API that enables single-line model loading with automatic dependency resolution and weight downloading. Launched by Facebook AI Research (now Meta AI) and the PyTorch Foundation, Hub hosts curated models from leading research institutions including FAIR, NVIDIA, Hugging Face, Intel, and academic labs, covering computer vision (YOLOv5, ResNet, EfficientNet), NLP (Transformers, RoBERTa), speech (Silero models, Tacotron2), and video understanding (SlowFast, X3D) domains. Each Hub model is defined by a hubconf.py file in a GitHub repository that specifies entry points, dependencies, and preprocessing pipelines, ensuring reproducible model instantiation across environments. PyTorch Hub supports both feature extraction (frozen backbone models) and fine-tuning workflows (unfrozen weights), with models returning standard torch.nn.Module objects compatible with PyTorch training loops, data loaders, and distributed training frameworks (DDP, FSDP). The platform integrates with PyTorch''s TorchScript compilation for deployment optimization, ONNX export for cross-framework compatibility, and TorchServe for production serving. Hub models include metadata specifying input/output tensor shapes, preprocessing requirements (normalization statistics, resize dimensions), and performance benchmarks (latency, throughput) across hardware configurations (CPU, GPU, mobile). The repository supports version pinning via Git commit hashes or tags, enabling deterministic model loading and reproducible research results. For researchers, PyTorch Hub accelerates experimentation by providing battle-tested implementations of recent architectures (often released alongside conference publications) with pre-trained ImageNet, COCO, or Kinetics weights, reducing training time from weeks to hours through transfer learning. In AI/ML pipelines, Hub models serve as feature extractors for downstream tasks (medical imaging, satellite analysis), few-shot learning backbones, and initialization points for domain adaptation, with the torch.hub.load() API supporting custom repositories for internal enterprise model sharing.'
  related_to:
  - B2AI_STANDARD:816
  requires_registration: false
  url: https://pytorch.org/hub/
- id: B2AI_STANDARD:402
  category: B2AI_STANDARD:ModelRepository
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Tensorflow Hub
  has_relevant_organization:
  - B2AI_ORG:37
  is_open: true
  name: TFHub
  purpose_detail: 'TensorFlow Hub (tfhub.dev) is Google''s official repository for reusable machine learning modules, providing pre-trained model components as SavedModel or TF2 format assets that integrate seamlessly into TensorFlow and Keras workflows via the tensorflow-hub library. Launched in 2018, TF Hub pioneered the concept of "transfer learning building blocks" by packaging not just model weights but complete computational graphs including preprocessing layers, embedding extractors, and prediction heads that can be composed into larger architectures. Hub modules span text embeddings (Universal Sentence Encoder, BERT, LaBSE), image feature vectors (MobileNet, EfficientNet, ResNet), object detection (SSD, Faster R-CNN), image generation (BigGAN, StyleGAN), video understanding (I3D, MoViNet), and audio processing (YAMNet, TRILL). Each module provides a consistent interface via hub.KerasLayer or hub.load(), supporting both feature extraction (trainable=False) and fine-tuning (trainable=True) modes with automatic gradient flow through module internals. TF Hub emphasizes model cards with detailed documentation on training data, performance metrics, intended use cases, and ethical considerations (bias, fairness), promoting responsible AI deployment. The platform supports multiple serving formats including TensorFlow Lite for mobile/edge deployment, TensorFlow.js for in-browser inference, and TensorFlow Serving for production APIs, with modules optimized for quantization and pruning. Hub''s standardized interface enables model composition where text embeddings feed into classification heads, or image encoders combine with text encoders for multimodal learning. For researchers, TF Hub reduces training time and computational costs by providing pre-trained representations on large-scale datasets (Wikipedia, ImageNet, YouTube-8M) that transfer effectively to specialized domains with limited data. In AI/ML production systems, Hub modules accelerate deployment by providing battle-tested, versioned components with defined input/output signatures, enabling A/B testing of different encoders and rapid iteration on model architectures without retraining entire pipelines from scratch.'
  requires_registration: false
  url: https://tfhub.dev/
- id: B2AI_STANDARD:403
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Acoustical Society of America / American National Standards Institute
    S3.20
  has_relevant_organization:
  - B2AI_ORG:7
  - B2AI_ORG:4
  is_open: false
  name: ASA/ANSI S3.20
  purpose_detail: Definitions for terms used in human bioacoustics.
  requires_registration: true
  url: https://webstore.ansi.org/Standards/ASA/ANSIASAS3202015R2020
- id: B2AI_STANDARD:404
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Alzheimer's Disease Ontology
  formal_specification: https://github.com/Fraunhofer-SCAI-Applied-Semantics/ADO
  is_open: true
  name: ADO
  purpose_detail: "The Alzheimer's Disease Ontology (ADO) is a knowledge-based ontology
    developed by Fraunhofer SCAI (Society for Applied Computer Science) that provides
    comprehensive, structured representations of concepts related to Alzheimer's
    disease, built on the Basic Formal Ontology (BFO) upper-level framework for
    consistent semantic foundations. Funded by the Alzheimer Forschung Initiative
    (AFI) and maintained by Fraunhofer SCAI with commitment for at least three years,
    ADO encompasses the complex network of interrelated entities characterizing Alzheimer's
    disease including neuropathological features (amyloid-beta plaques, neurofibrillary
    tangles with hyperphosphorylated tau protein, neuronal loss, synaptic dysfunction,
    cerebral amyloid angiopathy), cognitive and behavioral symptoms (memory impairment
    particularly episodic memory, executive dysfunction, language difficulties including
    anomia and reduced fluency, visuospatial deficits, behavioral changes including
    apathy and agitation, activities of daily living decline), genetic risk factors
    (APOE 4 allele conferring dose-dependent risk, early-onset autosomal dominant
    mutations in APP, PSEN1, PSEN2 genes, late-onset susceptibility variants in CLU,
    CR1, PICALM, BIN1, ABCA7, TREM2), biomarkers for diagnosis and staging (CSF markers
    with decreased A42 and increased total tau and phospho-tau181, PET imaging with
    amyloid tracers like Pittsburgh Compound B and florbetapir, tau PET tracers like
    flortaucipir, FDG-PET showing temporoparietal hypometabolism, structural MRI
    revealing hippocampal and entorhinal cortex atrophy, plasma biomarkers including
    p-tau217 and A42/40 ratio), disease mechanisms and pathways (amyloid cascade
    hypothesis with APP processing by -secretase and -secretase generating A peptides,
    tau pathology with microtubule destabilization and propagation, neuroinflammation
    with microglial activation and astrogliosis, oxidative stress and mitochondrial
    dysfunction, cholinergic deficit with reduced acetylcholine, synaptic failure
    preceding neuronal death, protein clearance impairment involving autophagy and
    proteasome systems), clinical stages and progression (preclinical Alzheimer's
    with biomarker positivity before symptoms, mild cognitive impairment due to Alzheimer's
    disease as prodromal stage, mild/moderate/severe dementia stages with progressive
    functional decline), diagnostic criteria and assessment tools (NIA-AA research
    framework emphasizing ATN biomarker classification with A for amyloid, T for
    tau, N for neurodegeneration, DSM-5 criteria for major and mild neurocognitive
    disorder, clinical assessments including MMSE, MoCA, CDR, ADAS-cog cognitive
    batteries), therapeutic interventions (cholinesterase inhibitors donepezil, rivastigmine,
    galantamine for symptomatic treatment, NMDA receptor antagonist memantine, anti-amyloid
    monoclonal antibodies aducanumab, lecanemab targeting early disease, investigational
    anti-tau therapies, lifestyle interventions including cognitive stimulation and
    physical activity, management of behavioral symptoms), and research domains (epidemiology
    with prevalence increasing with age affecting 10% over 65 and 32% over 85, clinical
    trials design and outcome measures, animal models including transgenic mice expressing
    mutant APP/PSEN1, cell culture models using iPSC-derived neurons, neuroimaging
    methods and analysis pipelines, omics approaches including genomics, transcriptomics,
    proteomics, metabolomics for biomarker discovery). Designed specifically for
    text mining applications, ADO enables automated extraction of Alzheimer's disease-related
    information from biomedical literature by providing standardized concept identifiers
    and semantic relationships, supporting named entity recognition systems that
    identify disease mentions, symptoms, treatments, and biomarkers in PubMed abstracts
    and full-text articles, facilitating literature-based discovery by connecting
    concepts across publications to generate hypotheses about disease mechanisms
    or repurposing opportunities, and enabling knowledge graph construction linking
    genes, proteins, pathways, drugs, and clinical phenotypes for systems-level understanding.
    The ontology supports data integration across heterogeneous Alzheimer's research
    datasets by providing common semantic framework for annotating clinical trial
    data with standardized outcomes, linking biobank samples to molecular and imaging
    data using shared terminology, integrating electronic health records with research
    cohorts through consistent diagnostic coding, and enabling federated analyses
    across multiple studies without harmonizing raw data by aligning on ontology
    terms. Available under Creative Commons CC BY 4.0 license through OBO Foundry
    with persistent URL purl.obolibrary.org/obo/ado.owl, ADO integrates with OBO
    ecosystem ontologies including BFO for upper-level structure, Gene Ontology for
    molecular functions and biological processes relevant to neurodegeneration, Uber-anatomy
    ontology for brain regions and neuroanatomical structures, Chemical Entities
    of Biological Interest (ChEBI) for therapeutic compounds and metabolites, Human
    Phenotype Ontology for clinical manifestations, and Disease Ontology for situating
    Alzheimer's within broader neurodegenerative disease classification, collectively
    supporting AI and machine learning applications in Alzheimer's research including
    clinical decision support systems predicting disease progression from baseline
    cognitive and biomarker profiles, drug repurposing screens identifying compounds
    with semantic similarity to known Alzheimer's therapeutics, patient stratification
    for precision medicine trials based on ontology-driven molecular subtyping, and
    automated literature surveillance tracking emerging concepts and therapeutic targets
    across the exponentially growing Alzheimer's research literature."
  requires_registration: false
  url: https://github.com/Fraunhofer-SCAI-Applied-Semantics/ADO
- id: B2AI_STANDARD:405
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Antibiotic Resistance Ontology
  formal_specification: https://github.com/arpcard/aro
  is_open: true
  name: ARO
  publication: doi:10.1093/nar/gkz935
  purpose_detail: Antibiotic resistance genes and mutations
  requires_registration: false
  url: https://github.com/arpcard/aro
- id: B2AI_STANDARD:406
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Apollo Structured Vocabulary
  formal_specification: https://github.com/ApolloDev/apollo-sv
  is_open: true
  name: APOLLO_SV
  purpose_detail: Terms and relations for interoperation between epidemic models and
    public health application software.
  requires_registration: false
  url: https://github.com/ApolloDev/apollo-sv
- id: B2AI_STANDARD:407
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Artificial Intelligence Ontology
  formal_specification: https://github.com/berkeleybop/artificial-intelligence-ontology
  is_open: true
  name: AIO
  purpose_detail: The Artificial Intelligence Ontology (AIO) is a comprehensive formal
    ontology that provides standardized terminology and semantic relationships for
    describing artificial intelligence systems, methods, and concepts. Developed to
    address the need for consistent AI terminology across research and applications,
    AIO models classes and relationships describing deep learning networks, their
    component layers and activation functions, machine learning algorithms, data processing
    techniques, and potential algorithmic biases. The ontology contains 443 classes
    organized in a hierarchical structure with a maximum depth of 5 levels, covering
    fundamental AI concepts from basic computational methods to complex neural architectures.
    AIO serves as a critical resource for AI researchers, practitioners, and systems
    developers who need standardized vocabularies for annotating AI models, describing
    experimental procedures, ensuring reproducibility, and enabling semantic interoperability
    between AI systems and databases. The ontology facilitates automated reasoning
    about AI systems, supports metadata annotation for AI workflows, and contributes
    to the broader goal of making artificial intelligence research more findable,
    accessible, interoperable, and reusable.
  requires_registration: false
  url: https://bioportal.bioontology.org/ontologies/AIO
  has_application:
  - id: B2AI_APP:59
    category: B2AI:Application
    name: AI/ML Ontology for Model Metadata and Pipeline Documentation
    description: AIO (Artificial Intelligence Ontology) is used in biomedical AI for
      standardizing the description of machine learning models, algorithms, and workflows,
      enabling semantic search for AI methods, automated model selection, and reproducible
      research documentation. Researchers leverage AIO to annotate AI models with
      formal descriptions of their architecture, training data requirements, and applicable
      use cases, facilitating discovery of appropriate models for specific biomedical
      tasks. The ontology supports automated reasoning about AI pipeline compatibility,
      enables knowledge graphs that link models to publications and datasets, and
      provides structured vocabulary for documenting AI experiments in compliance
      with reproducibility standards. AIO enables large language models to better
      understand and recommend AI approaches for biomedical problems.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:408
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Basic Formal Ontology
  formal_specification: https://github.com/BFO-ontology/BFO
  is_open: true
  name: BFO
  publication: doi:10.3233/AO-160164
  purpose_detail: 'Basic Formal Ontology (BFO) is a small, upper-level ontology designed for supporting
    information retrieval, analysis, and integration in scientific and other domains.
    As a genuine upper ontology, BFO provides foundational categories that are domain-neutral
    and applicable across all areas of scientific investigation, rather than containing
    domain-specific terms. BFO distinguishes between continuants (entities that endure
    through time, such as objects, qualities, and spatial regions) and occurrents
    (entities that unfold over time, such as processes and temporal regions), providing
    a rigorous framework for representing the fundamental structure of reality. The
    ontology employs formal logic (first-order logic, OWL2) to define its 39 core
    classes and relations, ensuring precise semantics and enabling automated reasoning.
    BFO is used by over 550 ontology-driven projects worldwide as the top-level framework
    for domain ontologies in biomedicine (OBO Foundry ontologies like GO, CHEBI, Uberon),
    healthcare (OGMS for disease), environmental science, manufacturing, and military
    intelligence. The ontology promotes interoperability by providing consistent upper-level
    structure, enabling ontology integration and cross-domain data federation. BFO
    has been developed through extensive international collaboration, with contributions
    from philosophers, logicians, and domain scientists, and is continuously refined
    based on practical applications. In AI/ML contexts, BFO provides foundational
    structure for knowledge graphs, enabling ontology-guided reasoning, semantic data
    integration across heterogeneous sources, knowledge representation for explainable
    AI, and principled ontology alignment, supporting knowledge-driven machine learning
    where symbolic foundations enhance statistical learning with formal semantics.'
  requires_registration: false
  url: http://basic-formal-ontology.org/
- id: B2AI_STANDARD:409
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Biological Collections Ontology
  formal_specification: https://github.com/BiodiversityOntologies/bco
  is_open: true
  name: BCO
  purpose_detail: Biodiversity data, including data on museum collections and environmental/metagenomic
    samples.
  requires_registration: false
  url: https://github.com/BiodiversityOntologies/bco
- id: B2AI_STANDARD:410
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Biological Imaging Methods Ontology
  formal_specification: https://github.com/CRBS/Biological_Imaging_Methods_Ontology/
  is_open: true
  name: FBBI
  purpose_detail: Sample preparation, visualization and imaging methods used in biomedical
    research.
  requires_registration: false
  url: http://cellimagelibrary.org/
- id: B2AI_STANDARD:411
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Biological Spatial Ontology
  formal_specification: https://github.com/obophenotype/biological-spatial-ontology
  is_open: true
  name: BSPO
  publication: doi:10.1186/2041-1480-5-34
  purpose_detail: Spatial concepts, anatomical axes, gradients, regions, planes, sides,
    and surfaces.
  requires_registration: false
  url: https://github.com/obophenotype/biological-spatial-ontology
- id: B2AI_STANDARD:412
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: BRENDA tissue / enzyme source
  formal_specification: https://github.com/BRENDA-Enzymes/BTO
  is_open: true
  name: BTO
  publication: doi:10.1093/nar/gkq968
  purpose_detail: A structured controlled vocabulary for the source of an enzyme comprising
    tissues, cell lines, cell types and cell cultures.
  requires_registration: false
  url: http://www.brenda-enzymes.org/
- id: B2AI_STANDARD:413
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: C. elegans phenotype ontology
  formal_specification: https://github.com/obophenotype/c-elegans-phenotype-ontology
  is_open: true
  name: WBPHENOTYPE
  publication: doi:10.1186/1471-2105-12-32
  purpose_detail: A structured controlled vocabulary of Caenorhabditis elegans phenotypes.
  requires_registration: false
  url: https://github.com/obophenotype/c-elegans-phenotype-ontology
- id: B2AI_STANDARD:414
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Cardiovascular Disease Ontology
  formal_specification: https://github.com/OpenLHS/CVDO
  is_open: true
  name: CVDO
  purpose_detail: Entities related to cardiovascular diseases.
  requires_registration: false
  url: https://github.com/OpenLHS/CVDO
- id: B2AI_STANDARD:415
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Cell Line Ontology
  formal_specification: https://github.com/CLO-Ontology/CLO
  is_open: true
  name: CLO
  publication: doi:10.1186/2041-1480-5-3
  purpose_detail: Standardize and integrate cell line information and to support computer-assisted
    reasoning.
  requires_registration: false
  url: https://github.com/CLO-Ontology/CLO
- id: B2AI_STANDARD:416
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Cell Ontology
  formal_specification: https://github.com/obophenotype/cell-ontology
  is_open: true
  name: CL
  publication: doi:10.1186/s13326-016-0088-7
  purpose_detail: "The Cell Ontology (CL) is an OBO Foundry ontology covering biological
    cell types with curation focused on animal cell types and interoperability with
    specialized cell type ontologies in other biological domains. CL is tightly integrated
    with the Uberon multi-species anatomy ontology (for recording cell location) and
    the Gene Ontology (GO, which uses CL as its main cell type reference and provides
    cell function annotations). Built on FAIR principles, CL enables community-driven
    curation with active embedded editors from multiple projects responsive on the
    issue tracker. The ontology is released in multiple standard formats (OWL/RDF/XML,
    OBO, JSON obographs) with multiple variants: full (all imports merged, reasoner-classified),
    base (not pre-reasoned, only CL axioms including non-CL class references), and
    simple (pre-reasoned, CL classes only), all with resolvable version IRIs for persistent
    access. CL is integrated into standard tools including Ubergraph (for logical
    queries like finding cells by location), Ontology Access Kit (OAK), and major
    browsers (OLS, Ontobee). The ontology supports major initiatives including BICCN
    cell type knowledge explorer, HubMap Human Reference Atlas, ENCODE, FANTOM5, Single
    Cell Expression Atlas, Human Cell Atlas, and CellKB. CL enables AI/ML applications
    including OnClass for automatic cell type classification, Brain Data Standards
    Ontology (BDSO) for cell type navigation and search, and provides standardized
    cell type annotations essential for single-cell omics machine learning, cross-dataset
    integration, and cell type discovery algorithms."
  requires_registration: false
  url: https://cell-ontology.github.io/
- id: B2AI_STANDARD:417
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:3
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CHEBI Integrated Role Ontology
  formal_specification: https://github.com/obophenotype/chiro
  has_relevant_organization:
  - B2AI_ORG:29
  is_open: true
  name: CHIRO
  publication: doi:10.26434/chemrxiv.12591221.v1
  purpose_detail: A distinct role hierarchy for chemicals.
  requires_registration: false
  url: https://github.com/obophenotype/chiro
- id: B2AI_STANDARD:418
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:3
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Chemical Entities of Biological Interest
  formal_specification: https://github.com/ebi-chebi/ChEBI
  has_relevant_organization:
  - B2AI_ORG:29
  is_open: true
  name: CHEBI
  publication: doi:10.1093/nar/gkv1031
  purpose_detail: "ChEBI is an open-access OBO Foundry database and ontology of chemical
    entities covering constitutionally or isotopically distinct atoms, molecules,
    ions, ion pairs, radicals, complexes, conformers, groups, chemical substances,
    and classes of molecular entities. The database contains over 195,000 entries
    of naturally occurring molecules or synthetic compounds used to intervene in biological
    processes, with macromolecules directly encoded by genomes (nucleic acids, proteins,
    peptides) excluded. ChEBI incorporates ontological classification defining relationships
    between chemical entities and their parent/child classes, enabling queries based
    on chemical class and role. The database provides comprehensive information including
    nomenclature following IUPAC and NC-IUBMB standards, molecular formulas, InChI
    and SMILES identifiers, literature citations, cross-references to other databases,
    and species data. ChEBI supports text and structure searches and is released in
    multiple formats (SDF, OBO, OWL, flat file, SQL dumps). As an ELIXIR Core Data
    Resource and Global Core Biodata Resource, ChEBI enables AI/ML applications in
    cheminformatics, drug discovery, metabolomics, systems biology, and chemical-phenotype
    association studies."
  requires_registration: false
  url: https://www.ebi.ac.uk/chebi/
- id: B2AI_STANDARD:419
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:3
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Chemical Information Ontology
  formal_specification: https://github.com/semanticchemistry/semanticchemistry
  is_open: true
  name: CHEMINF
  publication: doi:10.1371/journal.pone.0025513
  purpose_detail: Descriptors commonly used in cheminformatics software applications
    and the algorithms which generate them.
  requires_registration: false
  url: https://github.com/semanticchemistry/semanticchemistry
- id: B2AI_STANDARD:420
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:3
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Chemical Methods Ontology
  formal_specification: https://github.com/rsc-ontologies/rsc-cmo
  is_open: true
  name: CHMO
  purpose_detail: "The Chemical Methods Ontology (CHMO) provides a comprehensive,
    standardized vocabulary for describing experimental methods, techniques, and instruments
    used in chemical analysis, material synthesis, and sample preparation, maintained
    by the Royal Society of Chemistry (RSC) and aligned with OBO Foundry principles.
    CHMO encompasses three primary domains: analytical methods for data collection
    (mass spectrometry including ESI-MS, MALDI-TOF, GC-MS, LC-MS; spectroscopic techniques
    including NMR, IR, UV-Vis, Raman, X-ray spectroscopy; electron microscopy including
    SEM, TEM, STEM; and diffraction methods), separation and sample preparation techniques
    (chromatography including HPLC, GC, TLC, size-exclusion, affinity, ion-exchange;
    electrophoresis including SDS-PAGE, capillary electrophoresis, isoelectric focusing;
    sample ionization methods including electrospray, MALDI, electron impact; and
    extraction procedures), and material synthesis methods (chemical vapor deposition,
    epitaxy, sol-gel processes, crystallization, polymerization, and nanoparticle
    synthesis). The ontology also describes instruments and equipment (mass spectrometers,
    chromatography columns, detectors, vacuum systems, heating/cooling apparatus)
    and their components, operational parameters (temperature, pressure, flow rate,
    voltage, resolution), and measurement outputs (spectra, chromatograms, diffraction
    patterns, images). CHMO integrates with OBI (Ontology for Biomedical Investigations)
    for process and measurement concepts, CHEBI (Chemical Entities of Biological Interest)
    for chemical substances, and other OBO ontologies for cross-domain applications
    in metabolomics, proteomics, and materials science. The ontology provides both
    textual and formal OWL definitions enabling automated reasoning, classification
    of methods by input material types, conditions of application, and output data
    formats. CHMO supports reproducibility in chemical research by standardizing method
    descriptions in electronic laboratory notebooks, method sections of publications,
    protocols repositories, and analytical chemistry databases. Applications include
    semantic search for analytical protocols, automated method selection based on
    sample properties, FAIRification of chemical data workflows, text mining of chemical
    literature for method extraction, and quality control in analytical laboratories.
    The ontology is developed using the Ontology Development Kit (ODK) and distributed
    under CC-BY-4.0 license, with releases available in OBO, OWL, and JSON formats
    through http://purl.obolibrary.org/obo/chmo.owl."
  requires_registration: false
  url: https://github.com/rsc-ontologies/rsc-cmo
- id: B2AI_STANDARD:421
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: clinical LABoratory Ontology
  formal_specification: https://github.com/OpenLHS/LABO
  is_open: true
  name: LABO
  publication: doi:10.5281/zenodo.6522019
  purpose_detail: An ontology of informational entities formalizing clinical laboratory
    tests prescriptions and reporting documents.
  requires_registration: false
  url: https://github.com/OpenLHS/LABO
- id: B2AI_STANDARD:422
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Clinical measurement ontology
  formal_specification: https://github.com/rat-genome-database/CMO-Clinical-Measurement-Ontology
  is_open: true
  name: CMO
  publication: doi:10.1186/2041-1480-4-26
  purpose_detail: Morphological and physiological measurement records generated from
    clinical and model organism research and health programs.
  requires_registration: false
  url: https://github.com/rat-genome-database/CMO-Clinical-Measurement-Ontology
- id: B2AI_STANDARD:423
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Clinical Vaccines Administered
  has_relevant_organization:
  - B2AI_ORG:13
  is_open: true
  name: CVX
  purpose_detail: The Clinical Vaccines Administered (CVX) code set is a comprehensive
    standardized vocabulary developed and maintained by the CDC's National Center
    for Immunization and Respiratory Diseases (NCIRD) that provides unique numeric
    identifiers for all vaccines available in the United States healthcare system.
    This essential code set includes both active vaccines currently available for
    patient administration and inactive vaccines that may appear in historical immunization
    records, enabling complete tracking of vaccination history across a patient's
    lifetime. CVX codes are specifically designed for use in HL7 Version 2.3.1 and
    2.5.1 immunization messages and electronic health record systems, facilitating
    standardized communication between healthcare providers, immunization information
    systems (IIS), and public health agencies. Each CVX code entry includes detailed
    information about vaccine status (active, inactive, pending, non-US, or never
    active), last update timestamp, and clinical notes providing context about usage
    and availability. When paired with MVX (manufacturer) codes, CVX codes can precisely
    identify specific trade-named vaccine products, supporting accurate inventory
    management, adverse event reporting, vaccine safety monitoring, and public health
    surveillance activities essential for maintaining population immunity and preventing
    vaccine-preventable diseases.
  requires_registration: false
  url: https://www2a.cdc.gov/vaccines/iis/iisstandards/vaccines.asp?rpt=cvx
- id: B2AI_STANDARD:424
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Common Anatomy Reference Ontology
  formal_specification: https://github.com/obophenotype/caro/
  is_open: true
  name: CARO
  purpose_detail: An upper level ontology to facilitate interoperability between existing
    anatomy ontologies for different species.
  requires_registration: false
  url: https://github.com/obophenotype/caro/
- id: B2AI_STANDARD:425
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:4
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Common Terminology Criteria for Adverse Events
  formal_specification: https://ctep.cancer.gov/protocoldevelopment/electronic_applications/ctc.htm
  is_open: true
  name: CTCAE
  purpose_detail: Common Terminology Criteria for Adverse Events (CTCAE) is widely
    accepted throughout the oncology community as the standard classification and
    severity grading scale for adverse events in cancer therapy clinical trials and
    other oncology settings.
  requires_registration: false
  url: https://bioportal.bioontology.org/ontologies/CTCAE
- id: B2AI_STANDARD:426
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Comparative Data Analysis Ontology
  formal_specification: https://github.com/evoinfo/cdao
  is_open: true
  name: CDAO
  publication: doi:10.4137/EBO.S2320
  purpose_detail: "The Comparative Data Analysis Ontology (CDAO) is an OBO Foundry-aligned
    ontology formalized in Web Ontology Language (OWL 1.1) that provides standardized,
    computable representations of concepts and relations essential for evolutionary
    comparative analysis, developed by the National Evolutionary Synthesis Center
    (NESCent) Evolutionary Informatics working group to lower technical barriers
    for applying phylogenetic methods to biological data integration. CDAO addresses
    the core problem of phylogenetic character history inference by formalizing key
    concepts including Operational Taxonomic Units (OTUs) representing entities under
    comparison (species, genes, proteins), character-state data matrices encoding
    observations as Entity-Attribute-Value models where rows represent OTUs and columns
    represent characters with discrete or continuous states, phylogenetic trees and
    networks representing historical paths of descent-with-modification with nodes
    (terminal OTUs and internal ancestors including Most Recent Common Ancestor MRCA)
    and edges (branches with DirectedEdge for rooted trees, UndirectedEdge for unrooted
    trees), and evolutionary transitions representing inferred character-state changes
    along branches accounting for observed distributions through parsimony or probabilistic
    models. The ontology implements fine-grained class hierarchies with 122 concepts,
    67 object properties linking instances, and 10 data properties for primitive
    types, supporting diverse character types including CategoricalCharacter (nucleotide
    residues in sequence alignments with AminoAcidResidueCharacter subclass, morphological
    traits with discrete states, presence/absence binary characters), ContinuousCharacter
    (chemical inhibitor response IC50 values, expression levels), MolecularCharacter
    (DNA/RNA/protein sequences with coordinate systems), and CompoundCharacter (codons
    combining three nucleotide positions). CDAO's character-state data model represents
    CharacterStateDataMatrix with has_Character and has_TU properties linking to
    Characters and TUs, with CharacterStateDatum cells belonging_to specific TU-Character
    pairs and having states drawn from appropriate CharacterStateDomain (e.g., AminoAcidResidue
    domain with 20 canonical amino acids imported from external amino acid ontology,
    nucleotide domain with A/T/G/C states, GO term domain for cellular location characters).
    Phylogenetic tree representation uses Tree class with subclasses RootedTree (with
    has_Root property identifying root Node and DirectedEdges using has_Parent_Node/has_Child_Node
    properties), UnrootedTree (with UndirectedEdges using has_Left_Node/has_Right_Node),
    BifurcatingTree (all nodes have zero or two children), and UnresolvedTree (allowing
    polytomies), supporting network structures with Node concepts including TerminalNode
    (representing observable OTUs), InternalNode (ancestors), and MRCANode (most
    recent common ancestor of SetOfNodes collections). Evolutionary transitions are
    encoded as EdgeTransition annotations (subclass of EdgeAnnotation and CDAOAnnotation)
    linked to Characters via transition_of property and specifying state changes with
    has_Left_State/has_Right_State properties referencing CharacterStateDomain values,
    enabling representation of parsimony reconstructions, maximum likelihood ancestral
    state estimates, and Bayesian posterior state probabilities. The ontology supports
    metadata and provenance through CDAOAnnotation hierarchies including ModelDescription
    (specifying evolutionary models like GTR+Gamma for nucleotides, JTT for proteins),
    TreeAnnotation (bootstrap support, posterior probabilities, branch lengths via
    EdgeLength subclass), CharacterStateDataMatrixAnnotation, TUAnnotation with TaxonomicLink
    subclass linking OTUs to external species taxonomies (NCBI Taxonomy), and CoordinateSystem
    skeleton for ordered characters like sequences referencing external databases
    (NCBI accessions, UniProt IDs). CDAO enables semantic data transformation and
    interoperability by translating legacy formats (NEXUS character matrices and
    trees, NeXML) into RDF/XML instances of CDAO classes expressible as subject-property-object
    triples, supporting SPARQL queries for extracting information (finding TUs with
    alignment gaps, identifying subtrees defined by node sets, retrieving character
    states for specific OTUs) and enabling reasoning via description logic engines
    (Pellet, FaCT++, Racer) that infer relationships using OWL 1.1 features including
    transitive properties (has_Ancestor/has_Descendant via transitive closure of
    has_Parent), inverse properties, and property chaining. The ontology facilitates
    integration with existing biological ontologies by importing domain-specific knowledge
    rather than redefining it, exemplified by equating CDAO's AminoAcidResidue class
    with the imported AminoAcid class providing 20 canonical residues and quality
    hierarchies (ChargedAminoAcid, PolarAminoAcid), with future integration opportunities
    for Sequence Ontology (SO) sequence features, ChEBI chemical entities (L-Alanyl
    moiety CHEBI:32433 in polypeptides), Gene Ontology (GO) functional annotations
    as character states, and Phenotype and Trait Ontology (PATO) for morphological
    characters. Available under Creative Commons CC0 public domain waiver from GitHub
    repository (github.com/evoinfo/cdao) and OBO Library persistent URL (purl.obolibrary.org/obo/cdao.owl),
    CDAO supports use cases across evolutionary biology including sequence alignment
    homology assignment, phylogeny inference for classification, ancestral state reconstruction,
    biological function inference via phylogenomics (protein function assignment balancing
    similarity and phylogenetic context), gene tree/species tree reconciliation for
    duplication/loss events, codon evolution analysis, and comparative genomics integrating
    diverse data types (sequences, expression patterns, morphology, behavior). The
    ontology enables workflow automation by providing semantic layer for assembling
    web services (myGrid, BioMoby) into phylogenetic analysis pipelines, validating
    data consistency through ontology-based constraints, facilitating cross-database
    queries via shared semantics, and supporting machine learning applications where
    CDAO-structured phylogenetic features (tree topology, branch lengths, character
    state distributions, transition patterns) serve as inputs for supervised classification
    (species identification, function prediction), unsupervised clustering (gene family
    discovery), and evolutionary model parameter estimation, ultimately advancing
    reproducible, transparent, and integrative comparative analysis across molecular,
    morphological, and ecological data domains by formalizing the conceptual framework
    underlying evolutionary inference."
  requires_registration: false
  url: https://github.com/evoinfo/cdao
- id: B2AI_STANDARD:427
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Compositional Dietary Nutrition Ontology
  formal_specification: https://github.com/Southern-Cross-Plant-Science/cdno
  is_open: true
  name: CDNO
  publication: doi:10.3389/fnut.2022.928837
  purpose_detail: Nutritional attributes of material entities that contribute to human
    diet.
  requires_registration: false
  url: https://cdno.info/
- id: B2AI_STANDARD:428
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Confidence Information Ontology
  formal_specification: https://github.com/BgeeDB/confidence-information-ontology
  is_open: true
  name: CIO
  publication: doi:10.1093/database/bav043
  purpose_detail: The Confidence Information Ontology (CIO) is a specialized OBO
    Foundry ontology that provides standardized terms for expressing the confidence
    or certainty associated with biological annotations, computational predictions,
    and experimental assertions, enabling researchers and bioinformatics resources
    to capture nuanced distinctions between high-confidence findings backed by multiple
    lines of evidence and tentative conclusions requiring further validation. Developed
    by the Bgee gene expression database team and published in Database (2015), CIO
    addresses the critical need to represent not just what is annotated (e.g., a
    gene-disease association, protein-protein interaction, or gene expression pattern)
    but also the strength of evidence supporting that annotation, distinguishing between
    statements derived from manual expert curation, automated computational inference,
    high-throughput screening, or text mining with varying reliability. The ontology
    defines hierarchical terms describing confidence levels (high confidence, medium
    confidence, low confidence), evidence types contributing to confidence assessments
    (experimental evidence, computational prediction, literature-based inference,
    author statement, curator inference), and sources of uncertainty (incomplete data,
    conflicting evidence, limited reproducibility, indirect evidence). CIO terms are
    structured to integrate with evidence codes from the Evidence and Conclusion Ontology
    (ECO), allowing biocuration teams to specify both the type of evidence (e.g.,
    "inferred from mutant phenotype") and the confidence in that evidence (e.g., "supported
    by single experiment" vs. "confirmed by independent studies"). The ontology supports
    practical use cases including quality assessment of automated annotations in large-scale
    databases (UniProt, Gene Ontology, model organism databases), prioritization
    of experimental targets by filtering high-confidence predictions from computational
    pipelines, meta-analysis weighting schemes that adjust contribution of individual
    studies based on confidence scores, and transparent reporting of AI/ML model predictions
    with associated uncertainty estimates. For machine learning applications in biology,
    CIO enables construction of training datasets with explicit confidence labels
    that inform loss functions and class weighting strategies, supports active learning
    systems that prioritize low-confidence predictions for human review, and facilitates
    ensemble methods that combine multiple predictors with confidence-weighted voting.
    The ontology includes terms for time-dependent confidence (e.g., annotations marked
    as "requires periodic review" due to rapidly evolving evidence), context-specific
    confidence (e.g., "applicable to specific tissue type" vs. "generalizable across
    species"), and mechanisms for tracking provenance of confidence assessments (curator
    identity, date of assessment, version of evidence used). CIO is maintained on
    GitHub with versioned releases, integrates with OBO Foundry best practices (unique
    persistent identifiers, human-readable labels, logical definitions in OWL), and
    is used by gene expression databases (Bgee), orthology resources, pathway databases,
    and systems biology platforms that require principled representation of data quality
    and reliability to support evidence-based research decision-making and computational
    workflows that propagate uncertainty through analysis pipelines.
  requires_registration: false
  url: https://github.com/BgeeDB/confidence-information-ontology
- id: B2AI_STANDARD:429
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:16
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Contributor Role Ontology
  formal_specification: https://github.com/data2health/contributor-role-ontology
  is_open: true
  name: CRO
  purpose_detail: The Contributor Role Ontology (CRO) is an OBO Foundry-aligned ontology
    that provides standardized, machine-readable terms for describing the diverse
    roles and contributions performed by individuals in scientific research projects,
    addressing the limitations of traditional author lists that obscure the specific
    intellectual and practical contributions each person made to a published work.
    Developed by the CD2H (Center for Data to Health) consortium and aligned with
    the CRediT (Contributor Roles Taxonomy) initiative, CRO enables granular attribution
    of research activities including conceptualization, methodology development, software
    implementation, formal analysis, investigation, data curation, writing (original
    draft and review/editing), visualization, supervision, project administration,
    and funding acquisition. The ontology hierarchically organizes contributor roles
    from high-level categories (intellectual contributions, practical work, administrative
    support) to specific activities (e.g., distinguishing between "statistical analysis,"
    "computational modeling," and "experimental validation"), allowing research outputs
    to accurately represent the multi-disciplinary nature of modern science where
    teams include wet-lab scientists, computational researchers, statisticians, data
    managers, software engineers, project coordinators, and community engagement specialists.
    CRO supports interoperability with metadata standards for scholarly communication
    including ORCID profiles, DataCite metadata schema, JATS XML for journal articles,
    and research information management systems (RIMS), enabling automated population
    of contributor metadata in institutional repositories, grant management systems,
    and researcher profile pages. By providing persistent identifiers and formal definitions
    for contribution types, the ontology facilitates impact assessment that goes beyond
    simple publication counts, allowing funders and institutions to recognize diverse
    forms of scientific contribution including data stewardship, software development,
    community building, and mentorship activities that are undervalued in traditional
    citation-based metrics. CRO addresses equity issues in scientific attribution
    by making visible the contributions of early-career researchers, core facility
    staff, data scientists, and other professionals whose work is essential to research
    success but often goes unrecognized in conventional authorship models. The ontology
    includes terms for temporal aspects of contributions (e.g., "conceived original
    idea" vs. "contributed to later revisions"), degree of contribution (lead, equal,
    supporting), and collaborative dynamics (independent work, collaborative effort,
    supervisory role), enabling nuanced representation of team science. For research
    reproducibility and transparency, CRO supports documentation of who performed
    critical methodological steps, who validated results, and who has authority to
    answer questions about specific aspects of published work, addressing challenges
    when corresponding authors cannot respond to inquiries about methods they did
    not personally execute. The ontology integrates with emerging practices in open
    science including preprint servers (bioRxiv, medRxiv) that capture contributor
    roles, data repositories (Zenodo, Dryad, Figshare) that attribute dataset creators,
    and software citation initiatives (CFF, CITATION.cff files) that recognize code
    contributors. CRO is maintained on GitHub with community input, follows OBO Foundry
    principles (unique identifiers, open licensing, semantic versioning), and is being
    adopted by scholarly publishers, academic institutions, and research consortia
    to modernize attribution practices for the era of large-scale, interdisciplinary,
    data-intensive collaborative science.
  requires_registration: false
  url: https://github.com/data2health/contributor-role-ontology
- id: B2AI_STANDARD:430
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Core Ontology for Biology and Biomedicine
  formal_specification: https://github.com/OBOFoundry/COB
  has_relevant_organization:
  - B2AI_ORG:75
  is_open: true
  name: COB
  purpose_detail: The Core Ontology for Biology and Biomedicine (COB) is a foundational
    upper-level ontology developed by the OBO Foundry that provides a minimal set
    of high-level, broadly applicable terms representing fundamental biological and
    biomedical entities, serving as a unifying framework to improve interoperability
    and semantic consistency across the diverse landscape of domain-specific OBO ontologies
    spanning anatomy, development, phenotypes, diseases, chemicals, and experimental
    methods. COB addresses the challenge that different OBO ontologies historically
    defined overlapping or conflicting representations of core concepts (organism,
    cell, anatomical structure, biological process, material entity, information content
    entity), leading to integration difficulties when combining data annotated with
    multiple ontologies or reasoning across ontology boundaries. By establishing authoritative,
    consensus definitions for approximately 50 core terms that appear throughout biological
    knowledge representation, COB enables domain ontologies to import and reuse these
    shared definitions rather than creating redundant or incompatible local versions,
    facilitating automated reasoning, cross-ontology mappings, and federated queries
    across biomedical data resources. The ontology is organized as an OWL (Web Ontology
    Language) hierarchy with formal logical definitions grounded in Basic Formal Ontology
    (BFO) and Relations Ontology (RO), ensuring philosophical rigor and computational
    tractability for description logic reasoners used in semantic web applications
    and knowledge graph construction. COB terms cover fundamental categories including
    material entities (organism, cell, anatomical entity, molecule), processes (biological
    process, molecular function, behavior), qualities and attributes (phenotypic quality,
    measurement datum), and information entities (data item, objective specification,
    model representation), with careful axiomatization to support inference of implicit
    relationships and detection of logical inconsistencies in downstream ontologies.
    The development process involves extensive community consultation through OBO
    Foundry working groups, GitHub issue discussions, and workshops to achieve consensus
    on definitions that balance precision for computational applications with accessibility
    for domain scientists, addressing edge cases and philosophical debates about entity
    boundaries, parthood relations, and temporal aspects of biological entities. COB
    is designed for lightweight adoption, providing a "core" that domain ontologies
    can extend with specialized terms while maintaining interoperabilityfor example,
    Uberon (anatomy), GO (Gene Ontology), ChEBI (Chemical Entities of Biological
    Interest), and MONDO (disease ontology) can all reference COB's definitions of
    "organism" or "biological process" as shared anchor points. For data integration
    and AI/ML applications, COB enables construction of unified knowledge graphs that
    span multiple biological domains by providing consistent semantics for entity
    types and relations, supports transfer learning across datasets annotated with
    different ontologies by mapping domain-specific terms to shared COB concepts,
    and facilitates development of foundation models for biomedical NLP by establishing
    ground-truth semantic categories for entity recognition and relation extraction.
    The ontology includes provenance metadata documenting the source ontologies contributing
    each term, editorial notes explaining design decisions, and mappings to external
    resources (MeSH, SNOMED CT, NCIT) to bridge OBO Foundry ontologies with clinical
    terminologies and legacy systems. COB is maintained on GitHub with public development,
    follows OBO Foundry principles (open licensing, unique persistent identifiers,
    semantic versioning), and undergoes periodic releases coordinated with major OBO
    ontology updates to ensure ecosystem-wide compatibility as biological knowledge
    and ontology best practices evolve.
  requires_registration: false
  url: https://github.com/OBOFoundry/COB
- id: B2AI_STANDARD:431
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Coronavirus Infectious Disease Ontology
  formal_specification: https://github.com/cido-ontology/cido
  is_open: true
  name: CIDO
  publication: doi:10.1038/s41597-020-0523-6
  purpose_detail: Ontologically represent and standardize various aspects of coronavirus
    infectious.
  requires_registration: false
  url: https://github.com/cido-ontology/cido
- id: B2AI_STANDARD:432
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CTO Core Ontology of Clinical Trials
  formal_specification: https://github.com/ClinicalTrialOntology/CTO/
  has_relevant_organization:
  - B2AI_ORG:33
  is_open: true
  name: CTO
  purpose_detail: A structured resource integrating basic terms and concepts in the
    context of clinical trials.
  requires_registration: false
  url: https://github.com/ClinicalTrialOntology/CTO/
- id: B2AI_STANDARD:433
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Data Use Ontology
  formal_specification: https://github.com/EBISPOT/DUO
  has_relevant_organization:
  - B2AI_ORG:34
  is_open: true
  name: DUO
  purpose_detail: 'The Data Use Ontology (DUO) provides standardized vocabulary for describing
    data use conditions, restrictions, and requirements in biomedical and genomics
    research. Developed by the Global Alliance for Genomics and Health (GA4GH), DUO
    enables semantic tagging of datasets with consent-derived restrictions (e.g.,
    health/medical/biomedical research only, disease-specific research, non-commercial
    use, research ethics approval). The ontology supports automated data access matching,
    where algorithms determine whether a researcher''s purpose is compatible with dataset
    restrictions, enabling services like DUOS (Data Use Oversight System) and EGA
    (European Genome-phenome Archive) to streamline data sharing while respecting
    participant consent. DUO extends NIH dbGaP data use categories with hierarchical
    structure for logical inference, includes consent codes for international data
    sharing, and implements ADA-M (Automated Data Access Matrix) for granular permissions.
    Used by 60+ million weekly downloads, DUO facilitates GDPR-aware data governance,
    phenotype-driven differential diagnostics, and translational research. The ontology
    ensures that informed consent language translates into machine-readable terms,
    accelerating responsible data reuse for AI/ML training datasets, clinical phenotyping,
    and genomic diagnostics while maintaining participant privacy and ethical oversight.'
  requires_registration: false
  url: https://github.com/EBISPOT/DUO
- id: B2AI_STANDARD:434
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Devices, Experimental scaffolds and Biomaterials Ontology
  formal_specification: https://github.com/ProjectDebbie/Ontology_DEB
  is_open: true
  name: DEB
  publication: doi:10.1002/adfm.201909910
  purpose_detail: An ontology developed to facilitate information curation in the
    area of medical devices, experimental scaffolds and biomaterials.
  requires_registration: false
  url: https://bioportal.bioontology.org/ontologies/DEB
- id: B2AI_STANDARD:435
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Devops Infrastructure Ontology
  formal_specification: https://github.com/oeg-upm/devops-infra
  is_open: true
  name: devops-infra
  purpose_detail: This ontology network aims at representing the main sets of entities
    and relationships used in the context of DevOps infrastructure. It is the result
    of a collaboration between Huawei Research Ireland and the Ontology Engineering
    Group at Universidad Politcnica de Madrid. It originally started from an analysis
    of the Configuration Management Databases used by Huawei Research Ireland for
    the management of a large part of its DevOps infrastructure, and has evolved into
    an ontology that may be used as a starting point for the standardisation of the
    representation of CMDB-related data across vendors.
  requires_registration: false
  url: https://oeg-upm.github.io/devops-infra/index.html
- id: B2AI_STANDARD:436
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Disease Drivers Ontology
  formal_specification: https://github.com/DiseaseOntology/DiseaseDriversOntology
  is_open: true
  name: DISDRIV
  purpose_detail: Ontology for drivers and triggers of human diseases, built to classify
    ExO ontology exposure stressors. An application ontology.
  requires_registration: false
  url: https://github.com/DiseaseOntology/DiseaseDriversOntology
- id: B2AI_STANDARD:437
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:25
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Drosophila Phenotype Ontology
  formal_specification: https://github.com/FlyBase/drosophila-phenotype-ontology
  is_open: true
  name: DPO
  publication: doi:10.1186/2041-1480-4-30
  purpose_detail: Commonly encountered and/or high level Drosophila phenotypes.
  requires_registration: false
  url: https://github.com/FlyBase/flybase-controlled-vocabulary/wiki
- id: B2AI_STANDARD:438
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:8
  - B2AI_TOPIC:26
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Drug Target Ontology
  formal_specification: http://drugtargetontology.org/
  is_open: true
  name: DTO
  publication: doi:10.1186/s13326-017-0161-x
  purpose_detail: Drug Target Ontology (DTO) is being developed at the University
    of Miami in the research group of Stephan Schrer. DTO was developed as part of
    the Illuminating the Druggable Genome (IDG) project (https://commonfund.nih.gov/idg/overview),
    is supported by grant (IDG Knowledge Management Center, (U54CA189205). DTO is
    a novel semantic framework to formalize knowledge about drug targets and is developed
    as a reference for drug targets with the longer-term goal to create a community
    standard that will facilitate the integration of diverse drug discovery information
    from numerous heterogeneous resources.
  requires_registration: false
  url: https://bioportal.bioontology.org/ontologies/DTO
- id: B2AI_STANDARD:439
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:8
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Drug-drug Interaction and Drug-drug Interaction Evidence Ontology
  formal_specification: https://github.com/DIDEO/DIDEO
  is_open: true
  name: DIDEO
  purpose_detail: The Potential Drug-drug Interaction and Potential Drug-drug Interaction
    Evidence Ontology
  requires_registration: false
  url: https://github.com/DIDEO/DIDEO
- id: B2AI_STANDARD:440
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: EMBRACE Data And Methods Ontology
  formal_specification: https://github.com/edamontology/edamontology
  has_relevant_organization:
  - B2AI_ORG:29
  is_open: true
  name: EDAM
  publication: doi:10.1093/bioinformatics/btt113
  purpose_detail: Data types, identifiers, and formats
  requires_registration: false
  url: https://github.com/edamontology/edamontology
- id: B2AI_STANDARD:441
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Emotion Ontology
  formal_specification: https://github.com/jannahastings/emotion-ontology
  is_open: true
  name: MFOEM
  purpose_detail: Affective phenomena such as emotions, moods, appraisals and subjective
    feelings.
  requires_registration: false
  url: https://github.com/jannahastings/emotion-ontology
- id: B2AI_STANDARD:442
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:11
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Environment Ontology
  formal_specification: https://github.com/EnvironmentOntology/envo
  is_open: true
  name: ENVO
  purpose_detail: Environmental systems, components, and processes.
  requires_registration: false
  url: http://environmentontology.org/
- id: B2AI_STANDARD:443
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:11
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Environmental conditions, treatments and exposures ontology
  formal_specification: https://github.com/EnvironmentOntology/environmental-exposure-ontology
  is_open: true
  name: ECTO
  purpose_detail: Exposures to experimental treatments of plants and model organisms.
  requires_registration: false
  url: https://github.com/EnvironmentOntology/environmental-exposure-ontology
- id: B2AI_STANDARD:444
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - standards_process_maturity_draft
  - implementation_maturity_pilot
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Evidence Graph Ontology
  formal_specification: https://github.com/EvidenceGraph/EVI
  has_relevant_organization:
  - B2AI_ORG:116
  is_open: true
  name: EVI
  purpose_detail: "The Evidence Graph Ontology (EVI) extends core concepts from the W3C Provenance Ontology (PROV-O) to provide a formal semantic framework for representing evidence, provenance, and computational reproducibility in biomedical research, specifically designed to support AI-readiness requirements including explainability, auditability, training-data lineage, and transparent model development workflows. Developed within the FAIR (Findable, Accessible, Interoperable, Reusable) data ecosystem and extensively deployed in Bridge2AI projects, EVI enables machine-readable representation of complete evidence chains connecting computational results to their originating datasets, software versions, runtime parameters, execution environments, and responsible personnel, addressing the critical gap between textual descriptions of methods and formal, verifiable records of computational provenance essential for reproducible science and regulatory compliance. The ontology defines classes and properties for modeling Computations (executable processes with specific runtime configurations), Software (versioned code with persistent identifiers), Datasets (input and output data objects with checksums and format specifications), Evidence Graphs (structured provenance networks capturing relationships among these entities via predicates like uses, usedBy, generates, and wasGeneratedBy), and Evidence Objects (nodes in provenance graphs representing specific computational artifacts or data transformations). EVI supports inferential reasoning over evidence graphs through OWL2 semantics, enabling automated queries such as 'trace all datasets contributing to this machine learning model's training set,' 'identify which software versions were used to generate features for classifier X,' or 'verify that all transformation steps from raw clinical data to analysis-ready datasets are documented with checksums and responsible parties.' The ontology is serialized as JSON-LD for web compatibility and embedded within RO-Crate (Research Object Crate) metadata packages, where EVI graphs coexist with Schema.org and DataCite vocabularies to provide comprehensive dataset documentation combining human-readable descriptions with machine-interpretable provenance. EVI's integration with persistent identifier systems (Archival Resource Keys/ARKs, Digital Object Identifiers/DOIs) ensures that every node in an evidence graphwhether representing a dataset, software release, computation run, or derived resultcan be assigned a globally unique, resolvable identifier, enabling long-term auditability and cross-repository citation of computational artifacts. Applications span FAIRSCAPE (Framework for FAIR and Scalable Computation and Analytics Processing Engine), a production biomedical analytics platform that creates complete EVI evidence graphs for every computational result (Spark jobs, scripts, workflows, containers) with root graph URIs embedded in result metadata and all objects (datasets, software, computations) assigned persistent IDs resolvable to execution details, supporting machine-interpretable provenance and reproducibility; FAIRSCAPE's AI-readiness extensions that embed EVI provenance graphs within RO-Crate JSON-LD packages alongside data dictionaries, schema definitions, and feature-validation results, with ARK PIDs automatically assigned to RO-Crates, software components, datasets, and computation objects, enabling automated generation of human-readable datasheets from machine-readable provenance to support pre-model explainability (XAI) and training-data readiness assessments; Bridge2AI project guidelines that explicitly recommend EVI (or PROV-O) for machine-readable provenance to trace datasets back to ground-truth sources, document data transformation steps with links to versioned software, identify people and organizations responsible for data collection and processing, and attach checksums for verifiabilitypractices fundamental to explainable ML, model auditability, and dataset integrity validation; CM4AI (Center for Multi-modal AI) and other Bridge2AI generating consortia that integrate EVI into RO-Crate metadata to connect data files to specific sample identifiers, experiment protocols, and archived software versions (with Zenodo DOIs), enabling reproducible dataset assembly, sample-level provenance tracking, and cross-tool interoperability within ML pipelines; and ML-Ops integration frameworks where EVI evidence graphs are incorporated into ML workflow templates to support defeasible reasoning (reasoning with contestable or revisable claims about data quality and method appropriateness), automated metadata generation from computational logs to populate XAI documentation, and scalable audit trails for model development, training, evaluation, and deployment phases. EVI's defeasible reasoning capabilitiesdistinguishing between conclusive evidence and evidence subject to revision or reinterpretationare particularly valuable for AI/ML contexts where data quality assessments, feature engineering decisions, and model performance evaluations may need to be updated as new validation results or bias analyses become available, with the evidence graph maintaining a complete historical record of all revisions and their justifications. The ontology addresses practical AI/ML workflow requirements: training-data lineage tracing ensures that every feature in a training matrix can be traced through transformation pipelines back to raw instrument readings or clinical observations with documented provenance at each step; transformation tracking links each preprocessing operation (normalization, imputation, feature extraction, data augmentation) to specific software versions, parameter settings, and validation results, enabling reproducibility and error diagnosis; integrity verification embeds checksums, file formats, and validation reports in evidence graphs, supporting automated checks that training data matches expected schemas and quality thresholds before model training commences; and actor attribution records which individuals, teams, or automated processes performed each operation, supporting accountability, credit assignment, and regulatory audits. EVI enables pre-model explainability by documenting dataset characteristics, feature distributions, bias assessments, and data-quality metrics in structured provenance graphs that can be queried to answer questions like 'What geographic regions are represented in the training data?' or 'Which data transformation introduced missing values in feature X?'information essential for interpreting model predictions and identifying potential biases before models are trained. Integration with validation frameworks (Frictionless Data for schema validation, Great Expectations for data quality assertions) allows EVI graphs to incorporate validation results as evidence nodes, linking features to pass/fail status on distributional checks, outlier detection, or fairness metrics, and enabling automated gatekeeping where ML pipelines halt if provenance graphs indicate critical validation failures. The ontology's RO-Crate packaging ensures that EVI provenance travels with datasets across repositories, compute environments, and organizational boundaries, supporting federated learning scenarios where multiple institutions contribute training data and the combined evidence graph documents data sources, harmonization procedures, privacy-preserving transformations, and institutional data use agreements, all resolvable via persistent identifiers without exposing underlying sensitive records. EVI's adoption within Bridge2AI reflects its alignment with FAIR principles and AI-readiness criteria: Findable through persistent identifiers and searchable metadata repositories, Accessible via resolvable URIs and standard APIs, Interoperable through OWL2/RDF semantics and JSON-LD serialization compatible with web frameworks, and Reusable through explicit licensing, detailed provenance, and validation records that enable confident reuse of datasets and computational workflows across projects and institutions."
  requires_registration: false
  url: https://evidencegraph.github.io/EVI/index.html
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:232
    category: B2AI:Application
    name: FAIRSCAPE Computation-Level Provenance and Evidence Graphs
    description: FAIRSCAPE (Framework for FAIR and Scalable Computation and Analytics
      Processing Engine) implements the Evidence Graph Ontology (EVI) to create complete,
      machine-interpretable provenance graphs for every computational result in biomedical
      analytics workflows, embedding root evidence graph URIs in result metadata and
      assigning persistent identifiers (PIDs) to all software, datasets, and computation
      objects for long-term auditability and reproducibility. The framework generates
      EVI-compliant evidence graphs that link computations to the specific software
      versions, input datasets, runtime parameters, execution environments (operating
      system, library versions, container images), and personnel (researchers, data
      analysts, PIs) involved in producing each result, with all objects assigned
      globally unique persistent identifiers resolvable to detailed metadata including
      checksums, timestamps, licenses, and provenance relationships. FAIRSCAPE can
      execute Apache Spark jobs, Python/R scripts, workflow orchestration systems
      (Nextflow, Snakemake, CWL), or user-supplied Docker/Singularity containers,
      preserving complete provenance across heterogeneous execution modes and recording
      evidence graphs that capture computation-uses-software, computation-uses-input-dataset,
      computation-generates-output-dataset relationships following EVI's formal semantics.
      The evidence graph model supports inferential reasoning over provenance, enabling
      automated queries such as "retrieve all datasets derived from raw instrument
      file X," "identify which software versions contributed to result Y," or "verify
      that analysis Z used validated, quality-controlled input data," which are essential
      for AI/ML workflows requiring training-data lineage, reproducible feature engineering,
      and audit trails for regulatory submissions. All results are annotated with
      FAIR metadata using the EVI evidence graph model, ensuring that archived data
      and software remain accessible, validatable, reproducible, and reusable across
      projects and institutions, directly supporting explainable AI requirements where
      model predictions must be traceable to documented data sources and computational
      methods with verifiable provenance chains.
    used_in_bridge2ai: true
    references:
    - https://doi.org/10.1007/s12021-021-09529-4
  - id: B2AI_APP:233
    category: B2AI:Application
    name: FAIRSCAPE AI-Readiness Extensions with Pre-Model Explainability
    description: FAIRSCAPE's AI-readiness extensions operationalize the Evidence Graph
      Ontology (EVI) to embed deep provenance graphs, data dictionaries, schema definitions,
      and feature-validation results within RO-Crate JSON-LD packages, supporting
      pre-model explainability (XAI), training-data readiness assessments, and automated
      dataset documentation for machine learning workflows. EVI-based provenance captures
      detailed relationships among computations, software, inputs, and outputs (modeled
      as "Computation uses Software and Input, usedBy Computation generates Output")
      and is serialized as JSON-LD for inclusion in RO-Crate metadata files that
      accompany datasets through packaging, storage, sharing, and reuse across repositories
      and compute environments. The FAIRSCAPE server assigns Archival Resource Key
      (ARK) persistent identifiers to every RO-Crate package, software component,
      dataset, and computation object, creating resolvable evidence chains where each
      node in the provenance graph can be dereferenced to access detailed metadata,
      software repositories (Zenodo archives with DOIs), data dictionaries defining
      feature semantics and measurement units, and validation reports documenting
      data quality checks (via Frictionless Data JSON Schema generation and Great
      Expectations rule evaluation). Human-readable datasheets are automatically generated
      from machine-readable JSON-LD provenance graphs, providing AI/ML practitioners
      with comprehensive dataset documentation including data sources, transformation
      steps, responsible parties, quality metrics, known limitations, and intended
      use casesinformation required for informed model development, bias identification,
      and regulatory compliance. Integration with validation tooling ensures that
      EVI graphs incorporate feature-level validation results as evidence nodes, linking
      each feature to pass/fail status on distributional checks (normality tests,
      outlier detection, range constraints) and enabling automated gatekeeping where
      ML pipelines halt training if provenance indicates critical validation failures
      or incomplete data lineage. The combined RO-Crate-plus-EVI approach supports
      pre-model explainability by documenting dataset characteristics, sample demographics,
      temporal coverage, missing-data patterns, and known biases before models are
      trained, enabling practitioners to assess fitness-for-purpose, identify potential
      fairness issues, and make informed decisions about data preprocessing, feature
      engineering, and model architecture choices based on documented evidence rather
      than assumptions.
    used_in_bridge2ai: true
    references:
    - https://doi.org/10.1101/2024.12.23.629818
  - id: B2AI_APP:234
    category: B2AI:Application
    name: Bridge2AI Training-Data Lineage and Transformation Tracking
    description: Bridge2AI AI-readiness recommendations explicitly specify using the
      Evidence Graph Ontology (EVI) or W3C PROV-O for machine-readable provenance
      to trace datasets back to ground-truth sources, document data transformation
      steps with links to versioned software, identify responsible people and organizations,
      and attach verifiability measures (checksums, validation reports) that support
      explainable ML, model auditability, and dataset integrity validation across
      collaborative biomedical research projects. The guidelines recommend encoding
      complete data lineage showing how analysis-ready training datasets derive from
      raw clinical observations, instrument readings, imaging studies, or omics assays,
      with each transformation step (quality filtering, normalization, feature extraction,
      imputation, harmonization across sites) linked to specific software releases
      archived in sustainable repositories (Zenodo, Software Heritage) with persistent
      identifiers (DOIs, SWHIDs) and documented parameter settings, enabling reproducibility
      and error diagnosis when model performance deviates from expectations. Machine-readable
      provenance formats like EVI enable automated queries over provenance graphs
      to answer critical questions for AI/ML workflows such as "Which preprocessing
      operations were applied to this feature?", "What software version performed
      the data harmonization?", "Which institution contributed samples with characteristic
      X?", and "Are all transformation steps validated and checksummed?", supporting
      training-data readiness assessments, model explainability analyses, and regulatory
      audits. Identification of responsible actors (data collectors, curators, analysts,
      PIs, institutional data stewards) within provenance graphs supports accountability,
      credit assignment via scholarly citation of datasets and software, and compliance
      with data governance policies requiring documentation of who accessed or modified
      data at each processing stage. Verifiability measures embedded in EVI graphsincluding
      cryptographic checksums (SHA-256) for raw and processed data files, validation
      reports from schema checkers (Frictionless Data) and quality frameworks (Great
      Expectations), and data quality metrics (completeness percentages, outlier counts,
      distributional statistics)enable automated integrity checks where ML pipelines
      verify that input data matches expected characteristics before commencing training,
      and audit systems detect unauthorized modifications or data corruption by comparing
      current checksums to provenance-recorded values. The integration of EVI-based
      provenance with dataset documentation standards (Datasheets for Datasets, Healthsheets,
      Data Cards) ensures that training data for AI models arrives with comprehensive
      metadata covering data sources, collection methods, preprocessing steps, known
      limitations, potential biases, privacy protections, and intended use casesinformation
      essential for responsible AI development, fairness assessments, and transparent
      reporting of model development to regulators, ethics boards, and end-users.
    used_in_bridge2ai: true
    references:
    - https://doi.org/10.1101/2024.10.23.619844
  - id: B2AI_APP:235
    category: B2AI:Application
    name: CM4AI Dataset Packaging with Sample-Level Provenance and Archived Software
    description: The Center for Multi-modal AI (CM4AI) functional genomics generating
      consortium integrates the Evidence Graph Ontology (EVI) into RO-Crate JSON-LD
      metadata packages alongside Schema.org and DataCite vocabularies to provide
      sample-level provenance linking data files to specific experiment identifiers,
      biological sample IDs, and archived software versions with DOIs, enabling reproducible
      dataset assembly, traceable sample-to-data relationships, and cross-tool interoperability
      within ML pipelines for genomic analysis. FAIRSCAPE clients employed by CM4AI
      researchers generate RO-Crate packages that embed dataset schemas (defining
      features, data types, measurement units, controlled vocabularies), data dictionaries
      (mapping column names to biological concepts and ontology terms), and EVI-modeled
      provenance graphs capturing the complete computational lineage of processed
      datasets from raw sequencing reads or imaging data through quality control,
      alignment, feature extraction, and normalization steps to analysis-ready matrices
      suitable for machine learning. Provenance graphs explicitly connect data files
      to specific biological contextfor example, linking chromatin accessibility
      peak files to the cell line sample IDs (ENCODE accessions, Cellosaurus IDs),
      experimental protocols (ATAC-seq, ChIP-seq with specific antibody targets),
      and sequencing platforms used to generate the dataenabling queries such as
      "retrieve all datasets from HepG2 cells treated with compound X" or "identify
      replicates with consistent quality metrics across batch Y." Software components
      used in data processing workflows are archived to Zenodo to obtain Digital Object
      Identifiers and DataCite metadata records, with ARK persistent identifiers assigned
      by the FAIRSCAPE server linking provenance graph nodes to specific Zenodo-archived
      software releases (tagged versions with release notes, documentation, and test
      datasets), ensuring that every computational step in a dataset's lineage is
      traceable to a citable, versioned, archived software artifact accessible for
      inspection, reuse, or validation. The combined EVI-RO-Crate-DataCite approach
      supports reproducible dataset assembly for ML model development by documenting
      exactly which samples contributed to training, validation, and test partitions;
      which preprocessing pipelines and parameter settings were applied to each partition;
      and which software versions generated derived features or predicted labels,
      enabling transparent reporting of dataset provenance in publications, regulatory
      submissions, and data-sharing agreements while facilitating cross-tool interoperability
      where downstream ML frameworks (scikit-learn, PyTorch, TensorFlow) can parse
      RO-Crate metadata to automatically configure data loaders, validate feature
      schemas, and log dataset provenance in experiment tracking systems (MLflow,
      Weights & Biases).
    used_in_bridge2ai: true
    references:
    - https://doi.org/10.48550/arxiv.2509.10432
  - id: B2AI_APP:236
    category: B2AI:Application
    name: ML-Ops Integration with Automated Metadata and Defeasible Reasoning
    description: Bridge2AI standards and ML-Ops frameworks integrate the Evidence
      Graph Ontology (EVI) into machine learning operations pipelines to support defeasible
      reasoning (reasoning with contestable or revisable evidence about data quality,
      method appropriateness, and model performance), automated metadata generation
      from computational logs, and scalable audit trails linking model development,
      training, evaluation, and deployment phases to documented datasets, software,
      and responsible parties. EVI's support for defeasible reasoning enables encoding
      of provisional or contested claims about data and methodsfor example, marking
      a feature as "likely biased based on preliminary fairness analysis, pending
      expert review" or documenting that "model performance on subgroup X is acceptable
      under assumption A but requires validation under alternative assumption B"with
      evidence graphs maintaining complete revision histories showing when assessments
      were updated, which evidence prompted revisions, and who authorized changes,
      supporting transparent documentation of model development decisions and enabling
      retrospective audits of how data quality concerns or bias findings influenced
      final model specifications. Integration with ML-Ops templates (MLflow Projects,
      TensorFlow Extended pipelines, Kubeflow workflows) allows EVI evidence graphs
      to be automatically generated from execution logs, with nodes representing data
      ingestion steps, preprocessing operations, hyperparameter tuning runs, training
      epochs, evaluation metrics, and deployment configurations, and edges capturing
      dependencies and data flow among these pipeline stages, providing a complete
      audit trail of the ML development lifecycle without requiring manual documentation
      by data scientists. Automated metadata generation leverages EVI's structured
      provenance model to parse workflow logs and extract key informationsoftware
      versions, dataset identifiers with checksums, hyperparameter settings, compute
      resources used, training duration, convergence metrics, validation set performanceand
      populate human-readable datasheets, model cards, and XAI documentation templates
      that can be automatically updated whenever pipelines are rerun with new data
      or modified configurations, reducing documentation burden and ensuring that provenance
      records remain synchronized with actual computational artifacts. The combination
      of EVI provenance with ML-Ops experiment tracking systems enables queries across
      development iterations such as "compare validation accuracy across all runs
      using dataset version 2.1 versus 2.2," "identify which preprocessing variant
      introduced the feature distribution shift detected in model M," or "trace model
      N back to the specific training samples that contributed most to its decision
      boundary," supporting systematic optimization of ML pipelines, root-cause analysis
      of performance regressions, and evidence-based justification of modeling choices
      in regulatory submissions or peer-reviewed publications.
    used_in_bridge2ai: true
    references:
    - https://doi.org/10.48550/arxiv.2509.10432
  - id: B2AI_APP:237
    category: B2AI:Application
    name: Clinical Metadata Provenance for AI-Ready Health Data
    description: Clinical and health data applications of the Evidence Graph Ontology
      (EVI) within Bridge2AI projects focus on capturing protocol-level provenance,
      linking clinical measurements to controlled terminologies and common data models,
      and establishing governance-compliant audit trails that support AI-readiness
      requirements for explainability, computability, and bias identification in multi-site
      clinical datasets. EVI-based provenance records document measurement protocols
      (sample collection procedures, assay platforms, instrument calibration records,
      software versions for clinical analyzers), diagnostic criteria (ICD-10/SNOMED-CT
      codes with version timestamps), and medication coding (RxNorm identifiers with
      dosage, frequency, and administration route), linking each recorded observation
      to the specific protocol, code system version, and responsible clinician or
      laboratory that generated the data, enabling downstream AI applications to account
      for measurement heterogeneity across sites, temporal changes in diagnostic criteria
      (e.g., DSM-5 versus DSM-5-TR), and differences in medication formulations when
      training models on aggregated clinical data. Integration with common data models
      (OMOP CDM, FHIR) and controlled terminologies (LOINC for lab tests, SNOMED-CT
      for diagnoses, RxNorm for medications) is documented in EVI provenance graphs
      showing which terminology mapping pipelines and concept-set definitions were
      applied to harmonize heterogeneous source data into standardized representations,
      supporting reproducible queries across institutions and transparent reporting
      of cohort definitions, inclusion/exclusion criteria, and feature engineering
      steps in multi-site ML studies. Governance features enabled by EVI provenanceincluding
      audit trails documenting who accessed or modified records (with timestamps and
      justifications), anomaly monitoring detecting unexpected changes in data distributions
      or missingness patterns that might indicate quality issues or unauthorized modifications,
      and version-controlled documentation of data use agreements, institutional review
      board approvals, and informed consent proceduressupport compliance with HIPAA,
      GDPR, and institutional data governance policies while enabling retrospective
      analysis of how dataset composition or processing decisions affected model performance
      or fairness across demographic subgroups. Privacy-preserving practices documented
      in EVI provenance graphssuch as tiered access controls limiting sensitive fields
      to authorized researchers, geographic generalization replacing precise locations
      with census tracts or zip code prefixes, and date shifting applying consistent
      random offsets to preserve temporal relationships while preventing re-identificationare
      explicitly recorded as transformation steps with links to responsible data custodians
      and institutional policies, enabling AI model developers to understand which
      privacy protections are in effect, assess their impact on model performance
      (e.g., does date shifting affect longitudinal disease progression models?),
      and document privacy-preserving data handling in ethics applications and regulatory
      submissions. Common data quality risks in clinical datasetsincluding incorrect
      timestamps from timezone inconsistencies or daylight saving transitions, inconsistent
      units mixing metric and imperial measurements, and duplicate records from multiple
      data feedsare detected through automated quality checks (Great Expectations
      rules, Frictionless Data schema validation) with results embedded as evidence
      nodes in EVI graphs, enabling gating mechanisms where ML pipelines refuse to
      train on data failing critical quality thresholds and alerting data curators
      to issues requiring manual review or systematic correction across the data warehouse.
    used_in_bridge2ai: true
    references:
    - https://doi.org/10.48550/arxiv.2509.10432
  - id: B2AI_APP:238
    category: B2AI:Application
    name: Evidence Graph Structures for Scientific Claim Verification and LLM Augmentation
    description: Graph-based evidence representations inspired by the Evidence Graph
      paradigm are applied to scientific claim verification tasks, where sentence-level
      evidence graphs structure rationales, capture stance relationships, and support
      retrieval-augmented generation (RAG) pipelines that improve large language model
      (LLM) reasoning by providing explicit, pruned evidence chains linking claims
      to supporting or refuting sentences from scientific literature. In claim verification
      workflows, evidence graphs are constructed with nodes representing sentences
      extracted from scientific abstracts or full-text articles and edges capturing
      contextual relationships (same paragraph, same section, citation links), hierarchical
      document structure (sentence within section within article), or learned semantic
      similarity (embedding-based nearest neighbors in evidence space), enabling graph-based
      retrieval (GraphRAG) that selectively retrieves multi-hop evidence paths more
      effectively than flat vector similarity search by leveraging document structure
      and inter-sentence dependencies. The verification pipeline operates in three
      stages enhanced by evidence graph representations first, evidence-paragraph
      retrieval uses BM25 or dense retrieval to identify candidate documents containing
      relevant information; second, rationale-sentence selection employs graph-updated
      sentence embeddings (where node representations are refined via graph neural
      networks incorporating edge information from the evidence graph) to identify
      the most salient sentences supporting or refuting the claim; third, stance classification
      predicts whether each rationale supports, refutes, or provides neutral evidence
      for the claim, with graph structure enabling multi-rationale aggregation where
      multiple supporting sentences collectively determine overall stance. Pruning
      strategies address the risk that fully connected evidence graphs introduce noise
      and fail to reflect meaningful document structure by retaining only high-confidence
      edges (sentence pairs with cosine similarity above threshold, adjacent sentences
      in key sections, citation-linked sentences across papers) and removing spurious
      connections, resulting in sparser graphs that improve downstream model accuracy
      by focusing attention on relevant evidence while filtering distractors. Evidence
      graphs serve as external structured knowledge to augment LLMs through two mechanisms
      in retrieval-augmented generation (RAG), relevant subgraphs (paths from claim
      node to evidence nodes with annotated stances) are serialized and inserted into
      LLM prompts, providing explicit rationale chains that improve reasoning accuracy
      and enable citation of specific supporting sentences; in fine-tuning or in-context
      learning, evidence graphs provide training supervision where models learn to
      follow graph-structured reasoning paths rather than relying solely on semantic
      encoding in pretrained weights, improving performance on domain-specific scientific
      verification tasks (SciFact dataset) where specialized knowledge and explicit
      reasoning chains outperform pure retrieval or generation approaches. The graph-structured
      rationale approach enhances explainability and auditability by providing interpretable
      traces for model decisionseach predicted stance can be traced to specific sentence
      nodes in the evidence graph, which in turn link to source documents with section
      and paragraph provenanceenabling human reviewers to verify whether model predictions
      align with cited evidence, identify when models misinterpret context or miss
      key counterevidence, and assess whether training data contained biased or incomplete
      evidence graphs that might propagate systematic errors into model predictions.
    used_in_bridge2ai: false
    references:
    - https://openreview.net/pdf?id=cYAFwjY2bY
- id: B2AI_STANDARD:445
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Evidence ontology
  formal_specification: https://github.com/evidenceontology/evidenceontology
  is_open: true
  name: ECO
  publication: doi:10.1093/nar/gkab1025
  purpose_detail: An ontology for experimental and other evidence statements.
  requires_registration: false
  url: https://www.evidenceontology.org/
- id: B2AI_STANDARD:446
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Experimental condition ontology
  formal_specification: https://github.com/rat-genome-database/XCO-experimental-condition-ontology
  is_open: true
  name: XCO
  publication: doi:10.1186/2041-1480-4-26
  purpose_detail: Conditions under which physiological and morphological measurements
    are made both in the clinic and in studies involving humans or model organisms.
  requires_registration: false
  url: https://rgd.mcw.edu/rgdweb/ontology/view.html?acc_id=XCO:0000000
- id: B2AI_STANDARD:447
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:11
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Exposure ontology
  formal_specification: https://github.com/CTDbase/exposure-ontology
  is_open: true
  name: EXO
  purpose_detail: Vocabularies for describing exposure data to inform understanding
    of environmental health.
  requires_registration: false
  url: https://github.com/CTDbase/exposure-ontology
- id: B2AI_STANDARD:448
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:25
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Fission Yeast Phenotype Ontology
  formal_specification: https://github.com/pombase/fypo
  is_open: true
  name: FYPO
  publication: doi:10.1093/bioinformatics/btt266
  purpose_detail: Phenotypes observed in fission yeast.
  requires_registration: false
  url: https://github.com/pombase/fypo
- id: B2AI_STANDARD:449
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:8
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Food Interactions with Drugs Evidence Ontology
  formal_specification: https://github.com/getbordea/fideo/
  is_open: true
  name: FIDEO
  purpose_detail: Food-Drug interactions automatically extracted from scientific literature.
  requires_registration: false
  url: https://gitub.u-bordeaux.fr/erias/fideo
- id: B2AI_STANDARD:450
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Food Ontology
  formal_specification: https://github.com/FoodOntology/foodon/
  is_open: true
  name: FOODON
  publication: doi:10.1038/s41538-018-0032-6
  purpose_detail: A broadly scoped ontology representing entities which bear a food
    role. It encompasses materials in natural ecosystems and agriculture tha...
  requires_registration: false
  url: https://foodon.org/
- id: B2AI_STANDARD:451
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Food-Biomarker Ontology
  formal_specification: https://github.com/pcastellanoescuder/FoodBiomarkerOntology
  is_open: true
  name: FOBI
  publication: doi:10.1093/bioinformatics/btab626
  purpose_detail: Represent food intake data and associate it with metabolomic data
  requires_registration: false
  url: https://github.com/pcastellanoescuder/FoodBiomarkerOntology
- id: B2AI_STANDARD:452
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:25
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FuTRES Ontology of Vertebrate Traits
  formal_specification: https://github.com/futres/fovt
  is_open: true
  name: FOVT
  purpose_detail: Application ontology used to convert vertebrate trait data in spreadsheets
    to triples.
  requires_registration: false
  url: https://futres.org/
- id: B2AI_STANDARD:453
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:6
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Gender, Sex, and Sexual Orientation Ontology
  formal_specification: https://github.com/Superraptor/GSSO
  is_open: true
  name: GSSO
  purpose_detail: Terms for annotating interdisciplinary information concerning gender,
    sex, and sexual orientation.
  requires_registration: false
  url: https://gsso.research.cchmc.org/
- id: B2AI_STANDARD:454
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:12
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Gene Ontology
  formal_specification: https://github.com/geneontology/go-ontology
  has_relevant_organization:
  - B2AI_ORG:36
  is_open: true
  name: GO
  publication: doi:10.1093/nar/gkaa1113
  purpose_detail: Function of genes and gene products.
  requires_registration: false
  url: http://geneontology.org/
- id: B2AI_STANDARD:455
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Genomic Epidemiology Ontology
  formal_specification: https://github.com/GenEpiO/genepio
  is_open: true
  name: GENEPIO
  purpose_detail: Vocabulary necessary to identify, document and research foodborne
    pathogens.
  requires_registration: false
  url: http://genepio.org/
- id: B2AI_STANDARD:456
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:4
  - B2AI_TOPIC:6
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Genomics Cohorts Knowledge Ontology
  formal_specification: https://github.com/IHCC-cohorts/GECKO
  is_open: true
  name: GECKO
  purpose_detail: "The Genomics Cohorts Knowledge Ontology (GECKO) provides a standardized
    vocabulary for describing attributes of genomics cohorts and individual-level
    data in population-based research studies. Developed by the CINECA (Common Infrastructure
    for National Cohorts in Europe, Canada, and Africa) project and maintained for
    the International HundredK+ Cohorts Consortium (IHCC), GECKO enables harmonized
    representation of cohort metadata across diverse genomics studies, biobanks, and
    research consortia. The ontology encompasses five major categories: cohort design
    characteristics (prospective/retrospective, longitudinal/cross-sectional, case-control,
    family-based), participant demographics and socioeconomic attributes, phenotypic
    data collection methods (questionnaires, clinical assessments, biospecimen types),
    genomics data types (whole genome sequencing, exome sequencing, genome-wide association
    studies, RNA-seq, methylation arrays), and data access policies with consent frameworks.
    GECKO standardizes terminology for cohort recruitment strategies, inclusion/exclusion
    criteria, sample sizes, age ranges, ancestry populations, geographic locations,
    and follow-up durations critical for cohort discovery and meta-analysis planning.
    The ontology integrates with other OBO Foundry ontologies including BFO (Basic
    Formal Ontology) as top-level and OBI (Ontology for Biomedical Investigations)
    as mid-level, ensuring semantic interoperability. GECKO provides two products:
    the OBO-compliant gecko.owl for formal ontology applications, and ihcc-gecko.owl
    tailored for IHCC cohort cataloging with specialized browser labels and categorization.
    Automated tools based on JSON schema mapping files enable generation of harmonized
    data dictionaries and FAIRified metadata for cohort studies. Applications include
    cohort discovery portals enabling researchers to identify suitable cohorts for
    collaborative studies, standardized phenotype harmonization across studies for
    meta-GWAS, ethical data sharing frameworks through consent ontology terms, and
    FAIR data principles implementation in population genomics repositories. GECKO
    facilitates interoperability between cohort catalogs like BBMRI-ERIC, Maelstrom
    Research, dbGaP, and EGA by providing common terminology for cohort characteristics,
    enabling federated queries across international biobanks. The ontology is distributed
    under Creative Commons Attribution 4.0 International License, ensuring open access
    for academic and commercial research."
  requires_registration: false
  url: https://github.com/IHCC-cohorts/GECKO
- id: B2AI_STANDARD:457
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Genotype Ontology
  formal_specification: https://github.com/monarch-initiative/GENO-ontology
  has_relevant_organization:
  - B2AI_ORG:58
  is_open: true
  name: GENO
  purpose_detail: GENO is an OWL2 OBO Foundry ontology representing levels of genetic
    variation specified in genotypes to support genotype-to-phenotype (G2P) data aggregation
    and analysis across diverse research communities and sources. The core model is
    a graph decomposing genotypes into smaller components of variation, from complete
    genotypes specifying sequence variation across entire genomes down to specific
    allelic variants and sequence alterations. This partonomy structure enables integrated
    analysis of G2P data where phenotype annotations are made at different granularity
    levels. GENO describes genotype attributes including zygosity, genomic position,
    expression, dominance, and functional dependencies or consequences of variants.
    Beyond heritable genomic sequence variation, GENO represents transient variation
    in gene expression from knockdown reagents or overexpression constructs, representing
    this variation in terms of targeted genes to parallel sequence variation representation.
    GENO models G2P associations focusing on genotype-phenotype-environment interplay
    and uses the Scientific Evidence and Provenance Information Ontology (SEPIO) for
    provenance and experimental evidence. The ontology is orthogonal to but integrates
    with the Sequence Ontology (SO), Human Phenotype Ontology (HPO), Feature Annotation
    Location Description Ontology (FALDO), and Variation Ontology (VariO), supporting
    AI/ML applications in variant effect prediction, phenotype association analysis,
    and precision medicine.
  requires_registration: false
  url: https://github.com/monarch-initiative/GENO-ontology
- id: B2AI_STANDARD:458
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:14
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Geographical Entity Ontology
  formal_specification: https://github.com/ufbmi/geographical-entity-ontology/
  has_relevant_organization:
  - B2AI_ORG:96
  is_open: true
  name: GEO
  purpose_detail: The Geographical Entity Ontology (GEO) is a domain ontology developed
    by the University of Florida Biomedical Informatics group and aligned with OBO
    Foundry principles to provide standardized, computationally tractable representations
    of geographical entities, spatial relationships, and location-based concepts essential
    for biomedical research, epidemiology, environmental health studies, and clinical
    data integration where geographic context influences disease patterns, healthcare
    access, and population health outcomes. GEO defines a hierarchical classification
    of geographical entities ranging from large-scale political and administrative
    units (countries, states, provinces, counties, municipalities) through natural
    geographic features (mountains, rivers, lakes, coastlines, watersheds) to human-constructed
    spatial divisions (postal codes, census tracts, health service areas, urban/rural
    classifications) and institutional locations (hospitals, clinics, research facilities,
    universities). The ontology encodes spatial relationships using formal logic and
    topological predicates (part_of, located_in, adjacent_to, overlaps_with, contains)
    that enable computational reasoning about geographic containment hierarchies,
    proximity patterns, and spatial aggregations necessary for multi-level epidemiological
    analyses and geospatial data integration. GEO integrates with the Basic Formal
    Ontology (BFO) upper-level ontology and coordinates with complementary domain
    ontologies including the Environment Ontology (ENVO) for environmental features,
    the Population and Community Ontology (PCO) for population groups defined by
    geography, and the Units of Measurement Ontology (UO) for representing spatial
    dimensions, areas, and distances with standardized units. For public health surveillance
    and disease tracking, GEO provides the semantic foundation for georeferencing
    clinical observations, outbreak investigations, and environmental exposure assessments,
    enabling queries that aggregate cases by administrative boundaries while accounting
    for population denominators and adjusting for geographic mobility patterns. The
    ontology supports One Health applications by linking human disease surveillance
    with animal health monitoring, vector ecology, and environmental conditions across
    shared geographic spaces, facilitating detection of zoonotic disease emergence,
    vectorborne disease risk mapping, and foodborne outbreak source attribution that
    require integration of human, veterinary, and environmental data streams referenced
    to common geographic entities. In health disparities research and social determinants
    of health studies, GEO enables standardized representation of neighborhood-level
    contextual variables (socioeconomic status, built environment, food access, healthcare
    facility distribution) linked to census geographies, supporting multilevel regression
    models that quantify how place-based factors influence individual health outcomes
    while properly accounting for geographic clustering and spatial autocorrelation.
    Clinical trial site selection and patient recruitment benefit from GEO's representation
    of healthcare facility catchment areas, travel time isochrones, and residential
    locations, enabling optimization of site networks to ensure diverse geographic
    representation and minimize patient travel burden while meeting enrollment targets.
    For environmental health research examining air quality, water contamination,
    climate exposures, or built environment features, GEO provides the semantic framework
    for linking environmental measurements (monitoring station data, remote sensing
    observations, land use classifications) to population exposures through spatial
    joins and proximity analyses that account for geographic boundaries and mobility
    patterns. GEO's computational tractability supports automated geocoding pipelines
    that standardize free-text location descriptions into ontology terms, quality
    control workflows that validate geographic coordinates fall within appropriate
    administrative boundaries, and data harmonization processes that reconcile geographic
    identifiers across datasets using different geographic classification schemes
    (ZIP codes versus census tracts, health districts versus counties). The ontology
    facilitates multi-site research collaborations and federated data networks by
    providing common geographic reference terms that enable privacy-preserving spatial
    analyses where exact addresses are obscured but coarse geographic regions (e.g.,
    county, metropolitan area) are shared for population-level inference. Machine
    learning applications in spatial epidemiology and disease prediction models leverage
    GEO terms as features encoding geographic context, enabling models to learn associations
    between location characteristics and health outcomes while generalizing across
    geographic scales through the ontology's hierarchical structure. GEO is maintained
    through community curation with updates reflecting changes in political boundaries,
    new administrative divisions, evolving urban/rural classifications, and refinements
    to spatial relationship definitions informed by use cases in biomedical research
    and public health practice.
  requires_registration: false
  url: https://github.com/ufbmi/geographical-entity-ontology/
- id: B2AI_STANDARD:459
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Global Medical Device Nomenclature
  has_relevant_organization:
  - B2AI_ORG:35
  is_open: false
  name: GMDN
  purpose_detail: The Global Medical Device Nomenclature (GMDN) is a comprehensive
    set of terms, within a structured category hierarchy, which name and group ALL
    medical device products including implantables, medical equipment, consumables,
    and diagnostic devices.
  requires_registration: true
  url: https://www.gmdnagency.org/
- id: B2AI_STANDARD:460
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Glycan Naming and Subsumption Ontology
  formal_specification: https://github.com/glygen-glycan-data/GNOme
  is_open: true
  name: GNO
  publication: doi:10.5281/zenodo.6678279
  purpose_detail: The Glycan Naming and Subsumption Ontology (GNO) is a comprehensive,
    computationally tractable framework for representing glycan structures at multiple
    levels of compositional and structural detail, developed by the GlyGen project
    and aligned with OBO Foundry principles to support standardized glycan data integration,
    querying, and semantic reasoning across glycobiology databases, mass spectrometry
    workflows, and computational glycoscience applications. GNO addresses the challenge
    that glycans can be characterized at varying degrees of structural resolutionfrom
    monosaccharide composition (no linkage or topology information) through partially
    defined topologies (some linkages known, others ambiguous) to fully defined structures
    (complete linkage, anomericity, and stereochemistry)by providing a hierarchical
    ontology where each glycan structure subsumes all less-specific representations,
    enabling queries that retrieve glycans matching a composition or partial structure
    regardless of how completely they were characterized in the original data source.
    The ontology encodes glycan structures using GlycoCT condensed format as the canonical
    representation, with systematic translation to IUPAC condensed nomenclature, WURCS
    (Web3 Unique Representation of Carbohydrate Structures), and GlyTouCan accession
    identifiers for cross-referencing with international glycan repositories. GNO's
    subsumption hierarchy allows compositional queries (e.g., "all glycans containing
    2 N-acetylglucosamine, 3 mannose, 3 galactose") to return both the composition
    node and all fully characterized structures matching that composition, supporting
    mass spectrometry data interpretation where structural details may be ambiguous
    but composition is reliably determined. The ontology integrates with GlycoRDF
    to provide RDF/OWL representations suitable for SPARQL querying and semantic web
    applications, enabling federated queries across GlyGen, UniCarbKB, GlyTouCan,
    and other glycomics resources. GNO includes structural classifications based on
    biosynthetic pathways (N-glycans, O-glycans, glycosphingolipids, glycosaminoglycans),
    functional roles (blood group antigens, selectin ligands, pathogen recognition
    motifs), and taxonomic distributions (mammalian-specific structures, plant polysaccharides,
    bacterial capsular antigens), facilitating biological interpretation of glycan
    datasets. For machine learning applications in glycoscience, GNO provides standardized
    feature representations where glycan structures are encoded as ontology terms
    with subsumption relationships, enabling training of models that predict glycosyltransferase
    activity, binding specificity of glycan-binding proteins (lectins, antibodies,
    viral hemagglutinins), or disease-associated glycosylation changes from glycan
    structural features and compositional patterns. The ontology supports integration
    of glycomics data with proteomics by linking glycan structures to glycosylation
    sites on proteins, enabling systems-level analysis of glycoprotein heterogeneity,
    site occupancy, and glycoform distributions relevant to therapeutic protein development
    (monoclonal antibody glycosylation quality control, biosimilar characterization).
    GNO's computational tractability stems from its explicit encoding of structural
    rules (e.g., allowable linkage positions for each monosaccharide, constraints
    on branching patterns) that enable automated reasoning about glycan biosynthesis
    pathways and structural feasibility, supporting quality control in glycan structure
    databases and automated error detection in manually curated entries. The ontology
    facilitates meta-analysis of glycomics studies by providing consistent terminology
    for glycan structural motifs (high-mannose, complex-type, hybrid N-glycans; core
    1/2/3/4 O-glycans; sulfated/sialylated epitopes), enabling aggregation of findings
    across datasets generated by different analytical techniques (MALDI-TOF MS, LC-MS/MS,
    lectin arrays, NMR spectroscopy). GNO is maintained through collaborative curation
    by the glycobiology community with continuous updates to incorporate newly discovered
    glycan structures, refined biosynthetic pathway knowledge, and emerging structural
    motifs associated with disease states or developmental stages, ensuring the ontology
    remains current with advances in glycan analysis technologies and functional glycomics
    research.
  requires_registration: false
  url: https://gnome.glyomics.org/
- id: B2AI_STANDARD:461
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Health Surveillance Ontology
  formal_specification: https://github.com/SVA-SE/HSO
  has_relevant_organization:
  - B2AI_ORG:92
  is_open: true
  name: HSO
  purpose_detail: The Health Surveillance Ontology (HSO) provides a comprehensive
    framework for standardizing terminology and concepts related to health surveillance
    systems, disease monitoring programs, and public health data collection across
    human and veterinary medicine. Developed by the Swedish Veterinary Agency (SVA)
    and aligned with OBO Foundry principles, HSO enables interoperability between
    surveillance databases, epidemiological studies, and public health information
    systems by providing consistent vocabulary for surveillance activities, case definitions,
    reporting requirements, and data quality metrics. The ontology encompasses surveillance
    system architectures (passive surveillance, active surveillance, sentinel surveillance,
    syndromic surveillance), data collection methodologies (laboratory-based surveillance,
    clinical reporting, population surveys, environmental monitoring), case classification
    criteria (confirmed cases, probable cases, suspect cases based on laboratory/clinical/epidemiological
    evidence), temporal and spatial granularity specifications (reporting periods,
    geographic resolution, population denominators), and data quality indicators (completeness,
    timeliness, representativeness, sensitivity, specificity). HSO supports One Health
    approaches by bridging human, animal, and environmental health surveillance domains,
    facilitating detection of zoonotic disease emergence, antimicrobial resistance
    tracking, and foodborne outbreak investigations. Applications include standardization
    of surveillance system metadata for interoperability between national and international
    health agencies (WHO, ECDC, OIE), automated validation of surveillance data submissions,
    harmonization of case definitions across jurisdictions, and machine-readable representation
    of surveillance protocols for reproducibility. HSO integrates with disease ontologies
    (DO, DOID), pathogen ontologies (IDO, NCBITaxon), and geographic ontologies (GAZ)
    to provide comprehensive semantic framework for epidemiological data. The ontology
    enables FAIR principles implementation in public health surveillance by making
    surveillance system documentation findable, accessible, interoperable, and reusable.
  requires_registration: false
  url: https://w3id.org/hso
- id: B2AI_STANDARD:462
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HL7 Vocabulary
  has_relevant_organization:
  - B2AI_ORG:40
  is_open: true
  name: HL7 Vocabulary
  purpose_detail: An index to the HL7-supported Code Systems.
  requires_registration: false
  url: https://www.hl7.org/documentcenter/public/standards/vocabulary/vocabulary_tables/infrastructure/vocabulary/vocabulary.html#voc-systems
- id: B2AI_STANDARD:463
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Homology Ontology
  formal_specification: https://github.com/BgeeDB/homology-ontology
  is_open: true
  name: HOM
  publication: doi:10.1016/j.tig.2009.12.012
  purpose_detail: Concepts related to homology, as well as other concepts used to
    describe similarity and non-homology.
  requires_registration: false
  url: https://github.com/BgeeDB/homology-ontology
- id: B2AI_STANDARD:464
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:25
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HPO - ORDO Ontological Module
  formal_specification: https://www.orphadata.com/ontologies/
  has_relevant_organization:
  - B2AI_ORG:80
  is_open: true
  name: HOOM
  purpose_detail: Orphanet provides phenotypic annotations of the rare diseases in
    the Orphanet nomenclature using the Human Phenotype Ontology (HPO). HOOM is a
    module that qualifies the annotation between a clinical entity and phenotypic
    abnormalities according to a frequency and by integrating the notion of diagnostic
    criterion.
  requires_registration: false
  url: https://bioportal.bioontology.org/ontologies/HOOM
- id: B2AI_STANDARD:465
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Human Ancestry Ontology
  formal_specification: https://github.com/EBISPOT/ancestro
  is_open: true
  name: HANCESTRO
  publication: doi:10.1186/s13059-018-1396-2
  purpose_detail: A systematic description of the ancestry concepts used in the NHGRI-EBI
    Catalog
  requires_registration: false
  url: https://github.com/EBISPOT/ancestro
- id: B2AI_STANDARD:466
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Human Developmental Stages
  formal_specification: https://github.com/obophenotype/developmental-stage-ontologies
  is_open: true
  name: HSAPDV
  purpose_detail: The Human Developmental Stages ontology (HsapDv) provides standardized
    terminology for describing human lifecycle stages from conception through senescence,
    developed by the Bgee group in collaboration with Uberon and EHDAA2 (Human Developmental
    Anatomy Ontology) developers to enable precise temporal annotation of biological
    data across developmental biology, clinical research, and population studies.
    HsapDv encompasses both embryonic and postnatal stages, utilizing Carnegie staging
    system for prenatal development (Carnegie stages 1-23 covering days 1-56 post-fertilization,
    approximately embryonic weeks 1-8) which provides morphology-based developmental
    milestones independent of gestational age variations. Embryonic stages capture
    critical developmental events including fertilization, cleavage, blastocyst formation,
    gastrulation, neurulation, organogenesis, and fetal development through birth.
    Postnatal stages include neonatal period (birth to 28 days), infancy (1 month
    to 2 years), early childhood (2-6 years), middle childhood (6-12 years), adolescence
    (12-18 years encompassing puberty), young adulthood (18-40 years), middle adulthood
    (40-65 years), and late adulthood/senescence (65+ years) with subdivisions for
    geriatric populations. Each stage is formally defined with temporal boundaries,
    morphological characteristics, physiological milestones (motor skills, cognitive
    development, hormonal changes), and relationships to other developmental stages
    through "immediately_preceded_by" and "part_of" relations. HsapDv integrates
    with Uberon for anatomical structure development timing, enabling queries like
    "when does the cerebral cortex develop" or "which genes are expressed in neural
    tube during neurulation." Applications span developmental biology research (temporal
    annotation of gene expression atlases, single-cell RNA-seq developmental trajectories,
    epigenetic modification timelines), clinical medicine (prenatal diagnosis, developmental
    delay assessment, age-appropriate clinical reference ranges), teratology studies
    (critical periods for teratogen exposure), pharmacology (age-specific drug metabolism
    and dosing), and epidemiology (age-stratified disease incidence). HsapDv enables
    cross-species developmental comparisons through alignment with other species-specific
    developmental ontologies (mouse MmusDv, zebrafish ZFS), facilitating translational
    research and comparative embryology. The ontology is distributed in OBO and OWL
    formats through http://purl.obolibrary.org/obo/hsapdv.owl and browsable via OBO
    Foundry portals, supporting reproducible temporal annotation in biomedical databases,
    developmental atlases, and clinical decision support systems.
  requires_registration: false
  url: https://github.com/obophenotype/developmental-stage-ontologies/wiki/HsapDv
- id: B2AI_STANDARD:467
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Human Disease Ontology
  formal_specification: https://github.com/DiseaseOntology/HumanDiseaseOntology
  is_open: true
  name: DOID
  purpose_detail: The Human Disease Ontology (DOID) is a comprehensive, standardized
    ontology that provides a hierarchical classification system for human diseases
    organized primarily by etiology (underlying cause). Developed as part of the Open
    Biomedical Ontologies (OBO) Foundry, DOID serves as a cornerstone reference for
    disease terminology in biomedical research, clinical informatics, and healthcare
    applications. The ontology integrates multiple disease classification systems
    including ICD, SNOMED CT, UMLS, and MeSH, providing extensive cross-references
    that enable interoperability between different medical coding systems. DOID structures
    diseases into logical hierarchies based on disease mechanisms, affected anatomical
    systems, and causal agents, enabling both broad categorical searches and precise
    disease identification. Each disease concept includes standardized names, definitions,
    synonyms, and relationships to parent and child terms, creating a rich semantic
    network that supports computational analysis of disease data. The ontology is
    extensively used in genomics databases, electronic health records, biomedical
    literature annotation, drug discovery pipelines, and epidemiological studies where
    consistent disease terminology is essential for data integration and comparative
    analysis.
  requires_registration: false
  url: http://www.disease-ontology.org
- id: B2AI_STANDARD:468
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:25
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Human Phenotype Ontology
  formal_specification: https://github.com/obophenotype/human-phenotype-ontology
  is_open: true
  name: HPO
  purpose_detail: 'The Human Phenotype Ontology (HPO) provides standardized vocabulary for phenotypic
    abnormalities encountered in human disease, containing over 18,000 terms and 156,000+
    annotations to hereditary diseases. Each HPO term describes a specific phenotypic
    feature (e.g., "Atrial septal defect" HP:0001631) organized in a hierarchical
    structure from general to specific findings. Developed using medical literature,
    Orphanet, DECIPHER, and OMIM, HPO enables precise phenotype-driven differential
    diagnostics, genomic variant interpretation, and translational research. The ontology
    integrates with major biomedical resources and powers phenotype matching algorithms
    that rank diseases by clinical feature similarity. HPO is foundational for rare
    disease diagnosis tools (Exomiser, Phenomizer, PhenoTips), electronic health record
    phenotyping, and clinical decision support systems. As a Monarch Initiative flagship
    product and GA4GH driver project, HPO enables semantic integration across species,
    connecting human phenotypes to model organism phenotypes for translational research.
    The ontology supports deep phenotyping in genomics studies, electronic health record
    phenotype extraction, natural language processing for clinical notes, and phenotype-driven
    gene prioritization. HPO annotations link phenotypes to genes, diseases, and publications,
    facilitating genotype-phenotype correlation studies. In AI/ML applications, HPO
    powers phenotype-based similarity learning for rare disease diagnosis, automated
    phenotype extraction from clinical narratives using NLP, ontology-guided feature
    engineering for predictive models, knowledge graph embeddings for disease gene
    discovery, and multi-modal patient representation learning combining genomics,
    phenotypes, and clinical data to support precision medicine and clinical genomics.'
  related_to:
  - B2AI_STANDARD:784
  requires_registration: false
  url: https://hpo.jax.org/
- id: B2AI_STANDARD:469
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HUPO-PSI cross-linking and derivatization reagents controlled vocabulary
  has_relevant_organization:
  - B2AI_ORG:41
  is_open: true
  name: XLMOD
  purpose_detail: A structured controlled vocabulary for cross-linking reagents used
    with proteomics mass spectrometry.
  requires_registration: false
  url: https://www.psidev.info/groups/controlled-vocabularies
- id: B2AI_STANDARD:470
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Hypertension Ontology
  formal_specification: https://github.com/aellenhicks/htn_owl
  is_open: true
  name: HTN
  purpose_detail: An ontology for representing clinical data about hypertension.
  requires_registration: false
  url: https://github.com/aellenhicks/htn_owl
- id: B2AI_STANDARD:471
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Infectious Disease Ontology
  formal_specification: https://github.com/infectious-disease-ontology/infectious-disease-ontology
  is_open: true
  name: IDO
  purpose_detail: A set of interoperable ontologies that will together provide coverage
    of the infectious disease domain. IDO core is the upper-level ontology...
  requires_registration: false
  url: https://github.com/infectious-disease-ontology/infectious-disease-ontology
- id: B2AI_STANDARD:472
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Information Artifact Ontology
  formal_specification: https://github.com/information-artifact-ontology/IAO
  is_open: true
  name: IAO
  purpose_detail: An ontology of information entities.
  requires_registration: false
  url: https://github.com/information-artifact-ontology/IAO
- id: B2AI_STANDARD:473
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Informed Consent Ontology
  formal_specification: https://github.com/ICO-ontology/ICO
  is_open: true
  name: ICO
  purpose_detail: An ontology of clinical informed consents
  requires_registration: false
  url: https://github.com/ICO-ontology/ICO
- id: B2AI_STANDARD:474
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Integrative and Conjugative Element Ontology
  formal_specification: https://github.com/ontoice/ICEO
  is_open: true
  name: ICEO
  publication: doi:10.1038/s41597-021-01112-5
  purpose_detail: An integrated biological ontology for the description of bacterial
    integrative and conjugative elements (ICEs).
  requires_registration: false
  url: http://db-mml.sjtu.edu.cn/ICEberg/
- id: B2AI_STANDARD:475
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Intelligence Task Ontology
  formal_specification: https://github.com/OpenBioLink/ITO
  has_relevant_organization:
  - B2AI_ORG:87
  is_open: true
  name: ITO
  purpose_detail: Comprehensive, curated and interlinked data of artificial intelligence
    tasks, benchmarks, AI performance metrics, benchmark results and research papers.
  requires_registration: false
  url: https://openbiolink.github.io/ITOExplorer/
- id: B2AI_STANDARD:476
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Interaction Network Ontology
  formal_specification: https://github.com/INO-ontology/ino
  is_open: true
  name: INO
  publication: doi:10.1186/2041-1480-6-2
  purpose_detail: An ontology of interactions and interaction networks.
  requires_registration: false
  url: https://github.com/INO-ontology/ino
- id: B2AI_STANDARD:477
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Interlinking Ontology for Biological Concepts
  formal_specification: https://github.com/kushidat/IOBC
  is_open: true
  name: IOBC
  publication: doi:10.1007/s00354-019-00074-y
  purpose_detail: biological, biomedical, and related concepts
  requires_registration: false
  url: https://github.com/kushidat/IOBC
- id: B2AI_STANDARD:478
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Kinetic Simulation Algorithm Ontology
  formal_specification: https://github.com/SED-ML/KiSAO
  is_open: true
  name: KISAO
  purpose_detail: Algorithms for simulating biology, their parameters, and their outputs.
  requires_registration: false
  url: http://co.mbine.org/standards/kisao
- id: B2AI_STANDARD:479
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:25
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Mammalian Phenotype Ontology
  formal_specification: https://github.com/mgijax/mammalian-phenotype-ontology
  is_open: true
  name: MP
  publication: doi:10.1007/s00335-012-9421-3
  purpose_detail: Standard terms for annotating mammalian phenotypic data.
  requires_registration: false
  url: https://github.com/mgijax/mammalian-phenotype-ontology
- id: B2AI_STANDARD:480
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Manufacturers of Vaccines
  has_relevant_organization:
  - B2AI_ORG:13
  is_open: true
  name: MVX
  purpose_detail: Code set for active and inactive manufacturers of vaccines in the
    US.
  requires_registration: false
  url: https://www2a.cdc.gov/vaccines/iis/iisstandards/vaccines.asp?rpt=mvx
- id: B2AI_STANDARD:481
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Mass spectrometry ontology
  formal_specification: https://github.com/HUPO-PSI/psi-ms-CV
  has_relevant_organization:
  - B2AI_ORG:41
  is_open: true
  name: MS
  publication: doi:10.1093/database/bat009
  purpose_detail: A structured controlled vocabulary for the annotation of experiments
    concerned with proteomics mass spectrometry.
  requires_registration: false
  url: http://www.psidev.info/groups/controlled-vocabularies
- id: B2AI_STANDARD:482
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Measurement method ontology
  formal_specification: https://github.com/rat-genome-database/MMO-Measurement-Method-Ontology/
  is_open: true
  name: MMO
  publication: doi:10.1186/2041-1480-4-26
  purpose_detail: A representation of the variety of methods used to make clinical
    and phenotype measurements.
  requires_registration: false
  url: https://rgd.mcw.edu/rgdweb/ontology/view.html?acc_id=MMO:0000000
- id: B2AI_STANDARD:483
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Medaka Developmental Stages
  formal_specification: https://github.com/obophenotype/developmental-stage-ontologies
  is_open: true
  name: OLATDV
  purpose_detail: The Medaka Developmental Stages ontology (OlatDv) provides standardized
    terminology for describing developmental stages of medaka (Oryzias latipes, Japanese
    rice fish), a key teleost model organism in developmental biology, genetics, toxicology,
    and comparative vertebrate research, based on Iwamatsu staging system and developed
    from the original Medaka Fish Ontology (MFO) by Thorsten Henrich. Medaka serves
    as a powerful model organism complementing zebrafish and mouse due to its transparent
    embryos enabling live imaging, short generation time (2-3 months), small size
    suitable for laboratory culture, fully sequenced genome with conserved vertebrate
    gene organization, and established genetic tools including transgenesis, CRISPR/Cas9
    editing, and mutant libraries. OlatDv encompasses embryonic and larval stages
    from fertilization through sexual maturity, currently focusing on pre-adult development.
    Embryonic stages follow Iwamatsu's morphological staging system (stages 1-40)
    covering fertilization (stage 1), cleavage (stages 2-7), blastula (stages 8-10),
    gastrulation (stages 11-17), neurulation (stages 18-20), organogenesis (stages
    21-30), and pre-hatching development (stages 31-40 leading to hatching at approximately
    day 7-10 post-fertilization at 26C). Each stage is defined by specific morphological
    landmarks including somite numbers, heart development, pigmentation patterns, fin
    bud appearance, eye development, and gill filament formation. Post-hatching larval
    stages capture metamorphosis, scale formation, sex differentiation, and juvenile
    maturation through first reproduction. OlatDv provides temporal annotations crucial
    for comparative developmental biology studies examining vertebrate evolution, particularly
    teleost-specific genome duplication events and developmental innovations. Applications
    include temporal annotation of gene expression databases (Medaka Expression Database),
    developmental toxicology studies (OECD fish embryo toxicity tests using medaka
    as alternative to zebrafish), endocrine disruption research (sex determination
    mechanisms), carcinogenesis studies (medaka exhibits spontaneous tumor formation),
    and aging research (short lifespan enables longitudinal studies). Integration
    with Uberon anatomical ontology enables queries linking developmental stage to
    organ system maturation, essential for understanding tissue-specific gene expression
    changes during development. OlatDv facilitates cross-species developmental comparisons
    through alignment with zebrafish ZFS, frog XAO, and mammalian developmental ontologies,
    supporting evolutionary developmental biology (evo-devo) research and identification
    of conserved versus lineage-specific developmental programs across vertebrates.
    The ontology is distributed through OBO Foundry as olatdv.obo and olatdv.owl formats.
  requires_registration: false
  url: https://github.com/obophenotype/developmental-stage-ontologies/wiki/OlatDv
- id: B2AI_STANDARD:484
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Medical Action Ontology
  formal_specification: https://github.com/monarch-initiative/MAxO
  has_relevant_organization:
  - B2AI_ORG:58
  is_open: true
  name: MAXO
  purpose_detail: Terms for medical procedures, interventions, therapies, treatments,
    and recommendations.
  requires_registration: false
  url: https://github.com/monarch-initiative/MAxO
- id: B2AI_STANDARD:485
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - codesystem
  concerns_data_topic:
  - B2AI_TOPIC:8
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Medication Reference Terminology
  has_relevant_organization:
  - B2AI_ORG:74
  is_open: true
  name: MED-RT
  purpose_detail: Formal ontological representations of medication terminology, pharmacologic
    classifications, and asserted authoritative relationships between them. Replaces
    NDF-RT. Provided through UMLS.
  requires_registration: true
  url: https://evs.nci.nih.gov/ftp1/NDF-RT/Introduction%20to%20MED-RT.pdf
- id: B2AI_STANDARD:486
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Mental Disease Ontology
  formal_specification: https://github.com/jannahastings/mental-functioning-ontology
  is_open: true
  name: MFOMD
  purpose_detail: Mental diseases such as schizophrenia, annotated with DSM-IV and
    ICD codes where applicable.
  requires_registration: false
  url: https://github.com/jannahastings/mental-functioning-ontology
- id: B2AI_STANDARD:487
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Mental Functioning Ontology
  formal_specification: https://github.com/jannahastings/mental-functioning-ontology
  is_open: true
  name: MF
  purpose_detail: The Mental Functioning Ontology (MF) is an OBO Foundry ontology
    providing structured terminology for mental processes, cognitive functions, emotional
    states, and behavioral phenomena to support standardized annotation of neuroscience
    data, psychiatric research, psychological assessments, and mental health informatics.
    Developed by Janna Hastings and collaborators, MF extends the Basic Formal Ontology
    (BFO) upper-level framework and integrates with domain ontologies including the
    Mental Disease Ontology (MD) for psychiatric disorders, the Cognitive Atlas for
    cognitive processes, the Emotion Ontology for affective states, and the Gene
    Ontology (GO) for molecular underpinnings of neural function. The ontology comprehensively
    represents cognitive domains including perception (visual, auditory, somatosensory,
    chemosensory), attention (selective, divided, sustained), memory (working, episodic,
    semantic, procedural), executive functions (planning, inhibition, cognitive flexibility,
    decision-making), language processing, and reasoning, alongside emotional and
    motivational constructs such as valence, arousal, mood states, personality traits,
    and social cognition (theory of mind, empathy, social perception). MF captures
    temporal dynamics of mental processes (onset, duration, termination), intensity
    dimensions, and contextual dependencies, enabling nuanced representation of psychological
    phenomena. Integration with clinical terminologies like DSM-5, ICD-11, and RDoC
    (Research Domain Criteria) facilitates translational psychiatry linking basic
    neuroscience to clinical phenotypes. Applications include standardized annotation
    of neuroimaging studies identifying brain regions associated with specific mental
    functions, computational psychiatry modeling mental disorders as disruptions in
    cognitive and emotional processes, natural language processing extracting mental
    state descriptions from clinical notes, AI-based mental health assessment systems,
    meta-analysis of psychological experiments through unified terminology, and personalized
    medicine approaches tailoring psychiatric treatments to individual cognitive profiles.
    MF follows OBO Foundry principles with open-source development, logical consistency
    checking, and community-driven refinement, essential for computational neuroscience,
    digital mental health platforms, cognitive science research, and integrative brain
    databases.
  requires_registration: false
  url: https://github.com/jannahastings/mental-functioning-ontology
- id: B2AI_STANDARD:488
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Metadata vocabulary for Ontology Description and Publication
  formal_specification: https://github.com/sifrproject/MOD-Ontology
  is_open: true
  name: MOD
  publication: doi:10.1007/978-3-319-70863-8_17
  purpose_detail: 'The Metadata for Ontology Description and Publication (MOD) is
    an OWL-based ontology and application profile developed by the Semantic and Social
    Web Technologies for Biomedical Research (SIFR) project to systematically capture,
    represent, and publish comprehensive metadata about ontologies, controlled vocabularies,
    and semantic resources. MOD addresses the critical need for standardized ontology
    metadata by defining a rich set of properties describing provenance (authors,
    contributors, organizations, funding sources), versioning (release dates, version
    identifiers, change logs, deprecated versions), scope and coverage (domain, purpose,
    competency questions, use cases), technical characteristics (serialization formats
    like RDF/XML, OWL/Turtle, Manchester syntax; reasoner compatibility; expressivity
    metrics using OWL 2 profiles such as EL, QL, RL), quality indicators (validation
    status, review processes, testing methodologies, known issues), licensing and
    reuse rights (Creative Commons, open-source licenses, attribution requirements),
    relationships to other ontologies (imports, alignments, mappings, dependencies),
    usage statistics (downloads, citations, implementation instances), and community
    engagement (mailing lists, issue trackers, contribution guidelines). Built on
    Dublin Core, DCAT (Data Catalog Vocabulary), PROV-O (provenance ontology), and
    OMV (Ontology Metadata Vocabulary) foundations, MOD extends these standards with
    biomedical and life sciences-specific requirements, enabling machine-readable
    metadata suitable for automated ontology discovery, selection, integration, and
    maintenance workflows. The ontology supports semantic queries over ontology registries
    and repositories (BioPortal, Ontology Lookup Service, AberOWL), facilitating
    questions such as "find all OBO Foundry ontologies covering protein functions
    updated within the last year with Creative Commons licenses" or "identify ontologies
    compatible with ELK reasoner that import the Gene Ontology." For AI and machine
    learning applications, MOD metadata enables automated dataset annotation quality
    assessment by tracking which ontology versions were used for feature encoding,
    supports reproducible semantic embedding generation by documenting ontology preprocessing
    steps and alignment methods, facilitates knowledge graph construction by providing
    explicit import dependencies and modular decomposition information, enables meta-learning
    across ontology-annotated corpora by standardizing domain scope descriptions,
    and supports automated ontology recommendation systems that match dataset characteristics
    to appropriate semantic resources based on coverage, granularity, and maintenance
    activity, ultimately improving the discoverability, interoperability, and responsible
    reuse of ontological knowledge in data-driven biomedical research and AI model
    development.'
  requires_registration: false
  url: https://github.com/sifrproject/MOD-Ontology
- id: B2AI_STANDARD:489
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MHC Restriction Ontology
  formal_specification: https://github.com/IEDB/MRO
  is_open: true
  name: MRO
  purpose_detail: The Major Histocompatibility Complex (MHC) Restriction Ontology
    (MRO) is a specialized ontology developed by the Immune Epitope Database (IEDB)
    to provide standardized terminology for describing MHC restriction phenomena in
    immunological experiments and data. MHC restriction refers to the biological process
    by which T-cell recognition of antigens is limited to peptides presented by specific
    MHC molecules that are compatible with the T-cell's own MHC background. MRO systematically
    organizes the complex relationships between MHC alleles, T-cell responses, and
    antigen presentation contexts that are crucial for understanding adaptive immune
    responses, vaccine development, and transplantation immunology. The ontology enables
    precise annotation of immunological experiments by providing controlled vocabulary
    terms for MHC class I and class II molecules, their allelic variants, restriction
    patterns, and associated experimental conditions. This standardization is essential
    for comparative immunology studies, epitope mapping projects, and the development
    of personalized immunotherapies where accurate description of MHC-restricted immune
    responses is critical for data interpretation and clinical translation.
  requires_registration: false
  url: https://github.com/IEDB/MRO
- id: B2AI_STANDARD:490
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MIAPA Ontology
  formal_specification: https://github.com/evoinfo/miapa/
  is_open: true
  name: MIAPA
  publication: doi:10.1089/omi.2006.10.231
  purpose_detail: An application ontology to formalize annotation of phylogenetic
    data.
  requires_registration: false
  url: https://www.evoio.org/wiki/MIAPA
- id: B2AI_STANDARD:491
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Minimum PDDI Information Ontology
  formal_specification: https://github.com/MPIO-Developers/MPIO
  is_open: true
  name: MPIO
  purpose_detail: Minimum information regarding potential drug-drug interaction information.
  requires_registration: false
  url: https://github.com/MPIO-Developers/MPIO
- id: B2AI_STANDARD:492
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - modelcards
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Model Card Report Ontology
  formal_specification: https://github.com/UTHealth-Ontology/MCRO
  is_open: true
  name: MCRO
  publication: doi:10.1186/s12859-022-04797-6
  purpose_detail: An OWL2-based artifact that represents and formalizes model card
    report information. The current release of this ontology utilizes standard concepts
    and properties from OBO Foundry ontologies.
  requires_registration: false
  url: https://github.com/UTHealth-Ontology/MCRO
- id: B2AI_STANDARD:493
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:20
  - B2AI_TOPIC:26
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Molecular Interactions Controlled Vocabulary
  formal_specification: https://github.com/HUPO-PSI/psi-mi-CV
  has_relevant_organization:
  - B2AI_ORG:41
  is_open: true
  name: MI
  purpose_detail: Vocabulary for the annotation of experiments concerned with protein-protein
    interactions.
  requires_registration: false
  url: https://github.com/HUPO-PSI/psi-mi-CV
- id: B2AI_STANDARD:494
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Molecular Process Ontology
  formal_specification: https://github.com/rsc-ontologies/rxno
  has_relevant_organization:
  - B2AI_ORG:29
  is_open: true
  name: MOP
  purpose_detail: The Molecular Process Ontology (MOP) provides standardized vocabulary
    for describing molecular-level processes, transformations, and mechanisms that
    occur in chemical and biochemical systems, maintained by the Royal Society of
    Chemistry as part of the RSC ontology ecosystem. MOP encompasses reaction mechanisms
    (nucleophilic substitution, electrophilic addition, radical reactions, pericyclic
    reactions), molecular interactions (hydrogen bonding, van der Waals forces, -
    stacking, hydrophobic interactions, electrostatic interactions), conformational
    changes (protein folding, ligand-induced conformational shifts, allosteric transitions),
    energy transfer processes (fluorescence resonance energy transfer FRET, photoinduced
    electron transfer, vibrational energy relaxation), and transport phenomena (diffusion,
    membrane permeation, active transport, facilitated diffusion). The ontology provides
    detailed mechanistic descriptions including activation energies, transition states,
    reaction intermediates, rate-determining steps, and catalytic cycles essential
    for understanding chemical reactivity and biological function at the molecular
    scale. MOP integrates with the Chemical Methods Ontology (CHMO) for experimental
    techniques, CHEBI for chemical entities, and the Gene Ontology (GO) for biological
    processes, enabling comprehensive annotation of molecular transformations from
    pure chemistry through biochemistry to systems biology. Applications include annotation
    of reaction databases for synthetic chemistry planning, mechanistic modeling of
    enzymatic catalysis, drug-target interaction mechanisms for rational drug design,
    metabolic pathway analysis with detailed reaction mechanisms, and computational
    chemistry workflow documentation. MOP supports reproducibility in mechanistic studies
    by standardizing descriptions of reaction conditions, stereochemical outcomes,
    regioselectivity, and stereoselectivity patterns. The ontology enables semantic
    searches for reactions by mechanism type, facilitating discovery of analogous
    transformations across different chemical contexts and supporting retrosynthetic
    analysis in computer-aided synthesis planning tools.
  requires_registration: false
  url: https://www.ebi.ac.uk/ols/ontologies/mop
- id: B2AI_STANDARD:495
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Mondo Disease Ontology
  formal_specification: https://github.com/monarch-initiative/mondo
  has_relevant_organization:
  - B2AI_ORG:58
  is_open: true
  name: MONDO
  publication: doi:10.1093/nar/gkw1128
  purpose_detail: "MONDO is an OBO Foundry ontology providing a unified disease terminology
    that harmonizes disease definitions across multiple resources including HPO,
    OMIM, SNOMED CT, ICD, ORDO, DO, MedGen, GARD, and others. It addresses the proliferation
    of inconsistent disease mappings by providing logic-based structure with precise
    1:1 equivalence axioms connecting to other resources, validated by OWL reasoning.
    MONDO contains over 25,880 diseases including 22,919 human diseases (4,727 cancers,
    1,074 infectious diseases, 11,601 Mendelian diseases, 15,857 rare diseases) and
    2,960 non-human diseases, with 129,785 database cross-references and 108,076
    synonyms (exact, narrow, broad, and related). The ontology provides hierarchical
    classification for disease grouping and rolling up and uses precise semantic
    annotations for each mapping rather than loose cross-references. MONDO is released
    in three formats: mondo-with-equivalents.owl (with OWL equivalence axioms and
    inter-ontology axiomatization using CL, Uberon, GO, HP, RO, NCBITaxon), mondo.obo
    (simplified with xrefs), and mondo-with-equivalents.json. Coordinated with the
    Human Phenotype Ontology (HPO) which describes phenotypic features, MONDO supports
    AI/ML applications in disease classification, phenotype-disease association, rare
    disease diagnosis, and cross-resource knowledge integration."
  requires_registration: false
  url: https://mondo.monarchinitiative.org/
- id: B2AI_STANDARD:496
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Mouse Developmental Anatomy Ontology
  formal_specification: https://github.com/obophenotype/mouse-anatomy-ontology
  is_open: true
  name: EMAPA
  purpose_detail: Mouse anatomy covering embryonic development and postnatal stages.
  requires_registration: false
  url: http://www.informatics.jax.org/expression.shtml
- id: B2AI_STANDARD:497
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Mouse pathology ontology
  formal_specification: https://github.com/PaulNSchofield/mpath
  is_open: true
  name: MPATH
  purpose_detail: A structured controlled vocabulary of mutant and transgenic mouse
    pathology phenotypes.
  requires_registration: false
  url: http://www.pathbase.net/
- id: B2AI_STANDARD:498
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:3
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Name Reaction Ontology
  formal_specification: https://github.com/rsc-ontologies/rxno
  is_open: true
  name: RXNO
  purpose_detail: Connects organic name reactions to their roles in an organic synthesis
    and to processes in MOP
  requires_registration: false
  url: https://github.com/rsc-ontologies/rxno
- id: B2AI_STANDARD:499
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:8
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: National Drug Code
  has_relevant_organization:
  - B2AI_ORG:31
  is_open: true
  name: NDC
  purpose_detail: Information about finished drug products, unfinished drugs and compounded
    drug products
  requires_registration: false
  url: https://www.accessdata.fda.gov/scripts/cder/ndc/index.cfm
- id: B2AI_STANDARD:500
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: NCBI organismal classification
  formal_specification: https://github.com/obophenotype/ncbitaxon
  has_relevant_organization:
  - B2AI_ORG:74
  is_open: true
  name: NCBITAXON
  purpose_detail: "The NCBI Taxonomy Database (NCBITAXON) provides a comprehensive hierarchical classification of over 2 million organisms spanning all domains of life (Bacteria, Archaea, Eukarya, Viruses), maintained by the National Center for Biotechnology Information as the authoritative taxonomic backbone for GenBank, RefSeq, and all NCBI molecular sequence databases, with each taxon assigned a unique stable numerical identifier (NCBI Taxonomy ID) enabling consistent cross-referencing across genomic, proteomic, and literature resources worldwide. Originally developed in the 1990s to organize nucleotide sequence submissions, NCBI Taxonomy has evolved into the de facto standard for organismal classification in bioinformatics, with taxonomic IDs embedded in FASTA headers (e.g., 'gi|123456|ref|NP_001234.1| protein [Homo sapiens]' implicitly links to taxid:9606), sequence records (organism field), GenBank features (db_xref='taxon:9606'), UniProt entries (OX line with NCBI taxid), and publication metadata in PubMed Central, creating a universal identifier system that links molecular data to organismal context across millions of database records. The taxonomy encompasses named ranks following Linnaean hierarchy (superkingdom, kingdom, phylum, class, order, family, genus, species) plus unranked intermediate nodes ('clade', 'no rank') to capture phylogenetic relationships not fitting classical ranks, with each taxon record including scientific name (binomial nomenclature for species following ICZN/ICBN rules), authority (author and year), common names in multiple languages, synonyms capturing nomenclatural changes, genetic code specifications (nuclear and mitochondrial translation tables essential for correct ORF prediction), lineage strings showing complete paths from root to taxon, and cross-references to external databases (ITIS, GBIF, WoRMS, Index Fungorum). Updates occur continuously as new species are described in literature, taxonomic revisions are published (e.g., bacterial taxonomy shifts from Cavalier-Smith to GTDB-informed classifications), and sequences are deposited for previously unsequenced organisms, with monthly public releases ensuring synchronization across the bioinformatics ecosystem. The OBO Foundry NCBITAXON ontology represents this database in OWL/OBO formats with taxon IDs as URIs (NCBITaxon:9606 for Homo sapiens), taxonomic ranks as relationship types (rdfs:subClassOf chains encoding 'is_a' relationships from species through genus/family/order up to superkingdom), and semantic annotations enabling reasoning, SPARQL queries, and integration with other OBO ontologies (Gene Ontology, Uberon anatomy, Environment Ontology). NCBI Taxonomy's scale and coverage present unique challenges: the database includes organisms with minimal molecular data (single barcode sequences), extinct species when historical sequences exist, environmental samples and metagenome-assembled genomes (MAGs) with placeholder taxonomic assignments (e.g., 'uncultured bacterium', 'environmental samples' subtree with taxid:12908), hybrid organisms, and viral/phage taxonomy reflecting Baltimore classification alongside host relationships, requiring disambiguation strategies in downstream applications. Critical applications span genomics databases (Ensembl, UCSC Genome Browser, NCBI Genome using taxids to organize genome assemblies and enable taxon-specific BLAST), biodiversity informatics platforms (GBIF, iNaturalist, Encyclopedia of Life, Catalogue of Life using NCBI Taxonomy as reconciliation backbone or primary classification source), metagenomics and microbiome studies (QIIME2, mothur, Kraken2, MetaPhlAn assigning amplicon sequence variants and shotgun reads to NCBI taxa and reporting community composition as taxid frequency tables), phylogenetic databases (TimeTree, Open Tree of Life, TreeBASE mapping phylogenetic trees to NCBI taxa for comparative analyses), and literature mining systems (Europe PMC, PubTator Central extracting and normalizing species mentions to NCBI taxids enabling queries like 'find all papers mentioning Mycobacterium tuberculosis taxid:1773 and gene BCG_0001'). The taxonomy enables comparative genomics workflows identifying orthologous gene families across taxa, synteny analyses within defined clades, horizontal gene transfer detection requiring taxonomic context to identify donor-recipient relationships, and evolutionary rate calculations dependent on accurate phylogenetic distances encoded in taxonomic hierarchy. Machine learning and artificial intelligence applications leverage NCBI Taxonomy's identifiers, names, and hierarchical structure across natural language processing for species entity recognition and normalization (biomedical NER systems using NCBI taxids as canonical concept identifiers with neural re-ranking models selecting correct taxids from candidate lists generated via NCBI name dictionaries, improving linking accuracy on standard corpora like LINNAEUS and S800), species-to-gene assignment in literature mining (sequence-labeling models using NCBI taxids as structured labels to assign correct species to each gene mention, raising assignment accuracy from 65.8% to 81.3% compared to rule-based baselines, and addressing challenges of 16+ million species names in NCBI Taxonomy scale), metagenomics and microbiome analysis with deep learning classifiers for taxonomic profiling (convolutional and recurrent neural networks trained on 16S rRNA reads using species and genus labels derived from NCBI-aligned reference databases, achieving near-perfect read-level species classification and robust genus assignments for held-out species compared to k-mer alignment and binning baselines, trained on over 13,000 species with taxonomic labels as core supervision), machine learning classification of metagenome-assembled genomes (alignment-free k-mer classifiers outputting complete or partial taxonomic paths with rank-specific confidences, using NCBI taxonomy attached to public genomes as hierarchical labels for training and evaluation, enabling interpretable partial classifications when full species assignment is uncertain), deep learning read and contig classifiers (end-to-end neural networks trained to assign sequencing reads and assembled contigs to NCBI taxonomic references, reporting high species and genus performance with taxonomy providing label backbone and evaluation framework), taxonomy-aware neural network architectures and training objectives (hierarchical softmax models structured by phylogenetic and taxonomic trees using NCBI hierarchy to define output layer clusters, yielding order-of-magnitude training speedups and accuracy improvements on large-class datasets with approximately 5,000 taxonomic classes, contrasting with frequency-based hierarchical softmax approaches), taxonomic entity linking and normalization in ecological and evolutionary literature (dictionary-based NER systems built from NCBI Taxonomy databases enabling entity normalization to canonical names and identifiers, providing high precision but limited recall for vernacular names and newly described taxa, motivating hybrid machine learning augmentation), and taxonomy-informed embeddings and multiple instance learning for microbiome analysis (binning reads by species using NCBI taxonomy to create taxon-level embeddings via methods like GloVe, treating metagenome samples as bags of taxon embeddings for multiple instance learning classifiers including DeepSets and Set Transformers, using amplicon sequence variants aligned to NCBI taxa as embedding matrix rows, and injecting taxonomy trees into neural architectures and loss functions for hierarchical classification with evolutionary context)."
  requires_registration: false
  url: http://www.ncbi.nlm.nih.gov/taxonomy
  has_application:
  - id: B2AI_APP:224
    category: B2AI:Application
    name: Neural Species Normalization with NCBI Taxonomy as Target Ontology
    description: A bi-encoder species normalization pipeline uses NCBI Taxonomy as
      the canonical target ontology for linking species mentions in biomedical text
      to standardized taxon identifiers. The system constructs a comprehensive dictionary
      from NCBI Taxonomy names (scientific names, synonyms, common names) and employs
      BM25 lexical retrieval to generate top-10 candidate NCBI Taxonomy IDs for each
      detected species mention, addressing the challenge of matching ambiguous or
      variant textual mentions to over 2 million unique species and 16+ million species
      names in the NCBI database. A BERT-based neural re-ranker (using bert-base-uncased
      and BioBERT variants) scores and ranks the candidate taxonomy IDs, with the
      highest-scoring candidate selected as the normalized entity link. Evaluation
      on standard species NER corpora (LINNAEUS and S800) demonstrates that BM25+BioBERT
      achieves superior accuracy compared to BM25 alone and baseline tools (OrganismTagger,
      ORGANISMS), with the re-ranker correcting semantic and lexical mismatches (e.g.,
      disambiguating "children" to Homo sapiens taxid:9606 rather than bacterial
      genus Childrena taxid:525814 based on context). Performance is constrained
      by candidate generation coverage, as only mentions with at least one correct
      candidate in the BM25 top-10 can be correctly linked, highlighting NCBI Taxonomy's
      role as both the source corpus for candidate generation and the structured label
      space for supervised training and evaluation of neural normalization models.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.48550/arxiv.2310.14366
  - id: B2AI_APP:225
    category: B2AI:Application
    name: Species-to-Gene Assignment via Sequence Labeling with NCBI Taxon IDs
    description: A hybrid dictionary and machine learning system assigns species information
      to gene mentions in biomedical literature by treating NCBI Taxonomy IDs as
      explicit structured labels for sequence-labeling models. The pipeline first
      performs species named entity recognition using a dictionary-based tagger that
      maps textual mentions to NCBI Taxonomy IDs (achieving 94.3% F-measure), producing
      concept identifiers that serve as training and evaluation targets. The species-to-gene
      assignment task is then framed as sequence labeling rather than pairwise binary
      classification, reducing computational complexity (avoiding quadratic candidate
      pairs) while improving accuracy from 65.8% to 81.3% by applying biomedical
      pre-trained language models (PubMedBERT, Bioformer) that predict which NCBI
      taxon ID should be assigned to each gene mention based on surrounding context.
      The approach addresses the scale challenge of NCBI Taxonomy (more than 2 million
      unique species with over 16 million species names) by combining high-coverage
      dictionary-based recognition with contextual ML models that resolve ambiguity
      when multiple species are mentioned in the same sentence or paragraph. The system
      integrates with gene normalization pipelines (GNormPlus) by embedding species-assignment
      rules (SR4GN), using NCBI taxids to link genes to their organism of origin
      and enable organism-specific database queries, demonstrating NCBI Taxonomy as
      a critical semantic layer connecting biomedical entities across literature mining
      workflows.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.48550/arxiv.2205.03853
  - id: B2AI_APP:226
    category: B2AI:Application
    name: Dictionary-Based Taxonomic NER with NCBI Taxonomy Gazetteers
    description: Taxonomic named entity recognition (NER) systems for ecological and
      evolutionary literature commonly employ dictionary-based approaches built from
      NCBI Taxonomy and other taxonomic databases to detect and normalize organism
      mentions to canonical identifiers. NCBI Taxonomy serves as a primary source
      for constructing gazetteers that map textual mentions (scientific binomials,
      vernacular names, synonyms) to standardized taxon IDs and hierarchical classifications,
      enabling entity normalization that links recognized mentions to formal nomenclature,
      phylogenetic context, and cross-referenced molecular data. Dictionary-based
      systems leveraging NCBI resources (such as those described by Gerner et al.
      2010 and Pafilis et al. 2013) provide high precision and straightforward mapping
      to canonical names, IDs, and taxonomic ranks, but face recall limitations when
      encountering newly described taxa not yet in NCBI Taxonomy, vernacular or regional
      common names absent from NCBI synonym lists, and historical nomenclature predating
      current taxonomic conventions. These coverage gaps motivate hybrid approaches
      augmenting dictionary matching with machine learning classifiers (such as TaxoNERD's
      deep neural models) that can generalize beyond exact dictionary entries while
      still grounding predictions in NCBI Taxonomy structure. The integration of NCBI
      Taxonomy dictionaries with ML-based sequence labeling or contextual embedding
      models enables robust taxonomic entity extraction across diverse text sources,
      balancing the precision and standardization of curated taxonomic resources with
      the flexibility and recall of learned pattern recognition.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1111/2041-210x.13778
  - id: B2AI_APP:227
    category: B2AI:Application
    name: Deep Neural Networks for 16S rRNA Taxonomic Classification
    description: A deep learning approach to pattern recognition for short DNA sequences
      applies convolutional and recurrent neural networks to predict species and genus
      labels directly from 16S ribosomal RNA reads, using taxonomic assignments derived
      from NCBI-aligned reference databases as supervised training labels. The models
      are trained on 16S sequences spanning over 13,000 distinct species, with each
      read labeled at species and genus ranks according to reference database taxonomy
      that follows NCBI Taxonomy conventions for organismal classification. The deep
      neural network achieves near-perfect read-level species classification accuracy
      on held-in species and produces more accurate genus-level assignments for reads
      from held-out species compared to traditional k-mer, alignment-based, and taxonomic
      binning baselines (including tools like Kraken). Taxonomic labels serve multiple
      roles in this workflow as per-read supervision for end-to-end training, as hierarchical
      evaluation targets enabling separate assessment at species versus genus levels,
      as ground-truth derived from curated sequence-taxonomy mappings in reference
      databases, and as the basis for robustness testing via held-out species evaluation
      that mimics real-world scenarios where query organisms are not represented in
      training data. The work demonstrates that supervised deep learning on taxonomically
      labeled amplicon sequences can match or exceed the performance of established
      reference-based taxonomic assignment methods, with NCBI-style taxonomy providing
      the structured label space and hierarchical evaluation framework essential for
      training and validating neural taxonomic classifiers.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/353474
  - id: B2AI_APP:228
    category: B2AI:Application
    name: MT-MAG Machine Learning for Hierarchical Genome Taxonomic Assignment
    description: MT-MAG (Machine learning-based Taxonomic assignment of Metagenome-Assembled
      Genomes) employs alignment-free k-mer frequency features (k=7) and machine learning
      classifiers to provide complete or partial hierarchical taxonomic classifications
      with rank-specific confidence scores for MAGs and isolate genomes. The tool
      leverages NCBI Taxonomy as a primary source of hierarchical labels attached
      to public reference genomes, using these taxonomic paths (from species through
      genus, family, order, class, phylum to domain) as ground-truth for training
      multi-rank classifiers and as evaluation targets for assessing classification
      accuracy at each taxonomic level. MT-MAG is positioned within the broader landscape
      of genome taxonomic assignment tools that rely on NCBI Taxonomy or alternative
      systems like GTDB (Genome Taxonomy Database), with the paper noting that many
      public genomes carry "attached National Center for Biotechnology Information
      (NCBI) taxonomy" and contrasting alignment-based tools (GTDB-Tk) with marker-based
      (IDTAXA) and ML-based approaches (Kraken2, BERTax). NCBI taxonomic ranks structure
      the model's output space, enabling interpretable partial classifications that
      report "confident down to family level" when species or genus assignment is
      uncertain, and confidence scores at all ranks guide users in determining classification
      reliability. The explicit use of hierarchical NCBI taxonomy as training labels
      and evaluation criteria demonstrates how standardized taxonomic resources enable
      supervised learning for genome classification tasks, providing the semantic
      framework for multi-level predictions and facilitating comparison of ML-based
      assignments against curated reference taxonomies.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1371/journal.pone.0283536
  - id: B2AI_APP:229
    category: B2AI:Application
    name: Deep Learning Read Classifiers with NCBI Taxonomy Label Backbone
    description: Deep learning-based taxonomic assignment classifiers for metagenomic
      reads and contigs (such as DL-TODA and related architectures) are trained to
      classify sequencing data against reference databases organized by NCBI Taxonomy,
      using taxon identifiers and hierarchical relationships as the structured label
      space for end-to-end neural network models. These classifiers learn to map DNA
      sequence k-mer patterns, nucleotide composition features, and sequence embeddings
      to NCBI taxonomic categories at multiple ranks (species, genus, family), with
      NCBI Taxonomy providing the canonical reference framework that defines output
      classes, training targets, and evaluation metrics. Models report high accuracy
      at species and genus levels on benchmark datasets where ground-truth labels
      are derived from known NCBI taxonomic assignments of reference genomes, and
      taxonomic hierarchy embedded in NCBI classifications enables error analysis
      distinguishing between closely related taxa (e.g., misclassifications within
      the same genus or family) versus distant taxa. The use of NCBI Taxonomy as the
      label backbone ensures compatibility with standard bioinformatics pipelines
      (QIIME2, mothur, Kraken2) that report community composition as NCBI taxon ID
      frequency tables, facilitates integration with genomic databases indexed by
      taxids, and enables direct comparison of deep learning classifier predictions
      against established alignment-based and k-mer-based taxonomic profiling methods
      that also use NCBI Taxonomy as the reference classification system, demonstrating
      how standardized taxonomic resources serve as essential infrastructure for supervised
      learning in metagenomics.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2023.01.27.525929
  - id: B2AI_APP:230
    category: B2AI:Application
    name: Phylogenetic Hierarchical Softmax for Scalable Taxonomic Classification
    description: Phylo-HS (Phylogenetic Hierarchical Softmax) addresses the computational
      bottleneck of large-output softmax layers in neural network taxonomic classifiers
      by structuring the output layer according to the phylogenetic and taxonomic
      tree derived from NCBI Taxonomy, decomposing single-step classification over
      thousands of taxa into a series of hierarchical binary decisions that follow
      the taxonomic hierarchy. Traditional hierarchical softmax methods group classes
      by frequency or learned similarity, but Phylo-HS explicitly uses the NCBI taxonomic
      tree (reflecting evolutionary relationships and Linnaean ranks) to define hierarchical
      clusters, predicting first at higher taxonomic ranks (e.g., phylum, class) before
      refining predictions to lower ranks (genus, species). On metagenomic read classification
      tasks with approximately 5,000 taxonomic classes, Phylo-HS achieves roughly
      10-fold training speedup and improved accuracy compared to frequency-based hierarchical
      softmax baselines, demonstrating that taxonomy-aware output structure enhances
      both computational efficiency and predictive performance. The approach leverages
      NCBI Taxonomy's hierarchical organization (superkingdom, phylum, class, order,
      family, genus, species) as an inductive bias that aligns with the true structure
      of biological diversity, enabling gradient flow and parameter sharing across
      related taxa during training and providing interpretable intermediate predictions
      at multiple taxonomic ranks. This application exemplifies how NCBI Taxonomy
      serves not only as a label space but as an architectural prior that can be embedded
      directly into neural network training objectives and loss functions to improve
      scalability and accuracy in large-class taxonomic classification problems.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1101/2025.01.27.634943
  - id: B2AI_APP:231
    category: B2AI:Application
    name: Taxonomy-Informed Embeddings and Multiple Instance Learning for Microbiomes
    description: Metagenomic deep learning workflows leverage NCBI Taxonomy to structure
      sample-level representations through taxonomy-aligned embeddings and multiple
      instance learning (MIL) frameworks that treat microbiome samples as bags of
      taxonomic units. Methods such as GMEmbeddings align amplicon sequence variants
      (ASVs) or operational taxonomic units (OTUs) to NCBI taxonomic references and
      construct pretrained embedding matrices (using techniques like GloVe) where
      each row corresponds to an NCBI taxon and learned vectors capture co-occurrence
      patterns and ecological relationships across samples, enabling transfer learning
      and dimensionality reduction for downstream classification tasks. Metagenome2Vec
      bins metagenomic reads to species-level taxonomic assignments using NCBI Taxonomy
      (via tools like fastDNA) and represents each sample as a bag of taxon embeddings,
      applying multiple instance learning models (DeepSets, Set Transformer, MIL-VAE)
      that aggregate taxon-level features into sample-level predictions for phenotype
      classification (e.g., disease status, environmental conditions). IDMIL and related
      approaches extend this paradigm by treating taxonomic composition vectors (where
      each dimension corresponds to an NCBI taxid and values represent read counts
      or relative abundances) as input to set-based neural architectures that are
      permutation-invariant with respect to taxon ordering. Taxonomy-aware learning
      methods inject NCBI hierarchical structure into neural network architectures
      by incorporating taxonomy trees as graph neural network scaffolds, hierarchical
      attention mechanisms that weight closely related taxa similarly, and taxonomy-guided
      regularization losses that penalize misclassifications of phylogenetically distant
      taxa more heavily than closely related taxa. These approaches demonstrate how
      NCBI Taxonomy serves as a semantic framework for constructing biologically meaningful
      feature representations, enabling models to leverage evolutionary relationships
      and taxonomic structure to improve generalization, interpretability, and data
      efficiency in machine learning applied to microbiome and metagenomic datasets.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1099/mgen.0.001231
- id: B2AI_STANDARD:501
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: NCI Thesaurus OBO Edition
  formal_specification: https://github.com/NCI-Thesaurus/thesaurus-obo-edition
  has_relevant_organization:
  - B2AI_ORG:74
  is_open: true
  name: NCIT
  purpose_detail: A reference terminology that includes broad coverage of the cancer
    domain, including cancer related diseases.
  requires_registration: false
  url: https://github.com/NCI-Thesaurus/thesaurus-obo-edition
- id: B2AI_STANDARD:502
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Neuro Behavior Ontology
  formal_specification: https://github.com/obo-behavior/behavior-ontology/
  is_open: true
  name: NBO
  purpose_detail: Human and animal behaviours and behavioural phenotypes
  requires_registration: false
  url: https://github.com/obo-behavior/behavior-ontology/
- id: B2AI_STANDARD:503
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:22
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Neuronames Brain Hierarchy
  is_open: true
  name: NeuroNames
  publication: doi:10.1007/s12021-011-9128-8
  purpose_detail: A Comprehensive Hierarchical Nomenclature for Structures of the
    Primate Brain (human and macaque)
  requires_registration: false
  url: http://braininfo.rprc.washington.edu/aboutBrainInfo.aspx#NeuroNames
- id: B2AI_STANDARD:504
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Nomenclatural ontology for biological names
  formal_specification: https://github.com/SpeciesFileGroup/nomen
  is_open: true
  name: NOMEN
  purpose_detail: A nomenclatural ontology for biological names (not concepts). It
    encodes the goverened rules of nomenclature.
  requires_registration: false
  url: https://github.com/SpeciesFileGroup/nomen
- id: B2AI_STANDARD:505
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:33
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Non-Coding RNA Ontology
  formal_specification: https://github.com/OmniSearch/ncro
  is_open: true
  name: NCRO
  purpose_detail: An ontology for non-coding RNA, both of biological origin, and engineered.
  requires_registration: false
  url: https://github.com/OmniSearch/ncro
- id: B2AI_STANDARD:506
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Nutrition Care Process Terminology
  is_open: false
  name: NCPT
  purpose_detail: Terms for nutrition assessment, diagnosis, intervention, and monitoring/evaluation.
  requires_registration: true
  url: https://www.ncpro.org/
- id: B2AI_STANDARD:507
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: OBO Metadata Ontology
  formal_specification: https://github.com/information-artifact-ontology/ontology-metadata
  is_open: true
  name: OMO
  purpose_detail: Terms that are used to annotate ontology terms for all OBO ontologies.
  requires_registration: false
  url: https://github.com/information-artifact-ontology/ontology-metadata
- id: B2AI_STANDARD:508
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Obstetric and Neonatal Ontology
  formal_specification: https://github.com/ontoneo-project/Ontoneo
  is_open: true
  name: ONTONEO
  purpose_detail: A structured controlled vocabulary to provide a representation of
    the data from electronic health records involved in the care of pregnancy.
  requires_registration: false
  url: https://github.com/ontoneo-project/Ontoneo
- id: B2AI_STANDARD:509
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: OntoAvida ontology for Avida digital evolution platform
  formal_specification: https://gitlab.com/fortunalab/ontoavida
  is_open: true
  name: ONTOAVIDA
  purpose_detail: Vocabulary for the description of the most widely-used computational
    approach for studying digital evolution.
  requires_registration: false
  url: https://gitlab.com/fortunalab/ontoavida
- id: B2AI_STANDARD:510
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology for Biobanking
  formal_specification: https://github.com/biobanking/biobanking
  is_open: true
  name: OBIB
  purpose_detail: Annotation and modeling of biobank repository and biobanking administration.
  requires_registration: false
  url: https://github.com/biobanking/biobanking
- id: B2AI_STANDARD:511
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology for Biomedical Investigations
  formal_specification: https://github.com/obi-ontology/obi
  is_open: true
  name: OBI
  publication: doi:10.1371/journal.pone.0154556
  purpose_detail: Description of life-science and clinical investigations.
  requires_registration: false
  url: http://obi-ontology.org
- id: B2AI_STANDARD:512
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:4
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology for General Medical Science
  formal_specification: https://github.com/OGMS/ogms
  is_open: true
  name: OGMS
  purpose_detail: Treatment of disease and diagnosis and on carcinomas and other pathological
    entities.
  requires_registration: false
  url: https://github.com/OGMS/ogms
- id: B2AI_STANDARD:513
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology for MIRNA Target
  formal_specification: https://github.com/OmniSearch/omit
  is_open: true
  name: OMIT
  purpose_detail: Data exchange standards and common data elements in the microRNA
    (miR) domain.
  requires_registration: false
  url: https://github.com/OmniSearch/omit
- id: B2AI_STANDARD:514
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology for Nutritional Epidemiology
  formal_specification: https://github.com/cyang0128/Nutritional-epidemiologic-ontologies
  is_open: true
  name: ONE
  publication: doi:10.3390/nu11061300
  purpose_detail: Research output of nutritional epidemiologic studies.
  requires_registration: false
  url: https://github.com/cyang0128/Nutritional-epidemiologic-ontologies
- id: B2AI_STANDARD:515
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology for Nutritional Studies
  formal_specification: https://github.com/enpadasi/Ontology-for-Nutritional-Studies
  is_open: true
  name: ONS
  publication: doi:10.1186/s12263-018-0601-y
  purpose_detail: Description of concepts in the nutritional studies domain.
  requires_registration: false
  url: https://github.com/enpadasi/Ontology-for-Nutritional-Studies
- id: B2AI_STANDARD:516
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:4
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology of Adverse Events
  formal_specification: https://github.com/OAE-ontology/OAE/
  is_open: true
  name: OAE
  purpose_detail: 'Ontology of Adverse Events (OAE) is a community-developed biomedical ontology following OBO Foundry principles that provides standardized representation and classification of adverse events resulting from medical interventions including drug administration, vaccination, medical devices, procedures, and dietary supplements, enabling systematic analysis of safety data across clinical trials, pharmacovigilance systems, and electronic health records. OAE structures adverse events hierarchically under upper-level classes from the Basic Formal Ontology (BFO), integrating with domain ontologies including OGMS (disease processes), VO (vaccine components), DRON (drug products), and UBERON (anatomical structures) to capture mechanistic relationships between interventions, biological processes, and observed adverse outcomes. The ontology distinguishes between adverse events (any untoward medical occurrence temporally associated with intervention use) and adverse drug reactions (events with causal relationship to intervention), incorporating causality assessment frameworks (Naranjo scale, WHO-UMC criteria) as logical axioms that infer ADR status based on evidence patterns. OAE represents clinical manifestations (symptoms, signs, laboratory abnormalities), severity grades (CTCAE scales from mild to life-threatening), temporal patterns (immediate hypersensitivity, delayed reactions, cumulative toxicity), and anatomical localizations, supporting detailed phenotyping of safety profiles. The ontology enables cross-study aggregation of adverse event data by providing standardized terms that harmonize heterogeneous reporting formats from FDA FAERS, EMA EudraVigilance, WHO VigiBase, and clinical trial databases, facilitating meta-analyses of intervention safety and identification of rare adverse events invisible in individual studies. OAE supports pharmacovigilance signal detection by structuring adverse event hierarchies that enable mining of parent-child term relationships, discovering drug-event associations through disproportionality analysis (ROR, IC025), and prioritizing signals for regulatory review. In vaccine safety surveillance, OAE terms annotate Brighton Collaboration case definitions for standardized adverse event reporting post-vaccination (AEFI), supporting global safety monitoring networks coordinated by WHO. For AI/ML applications, OAE provides structured labels for training natural language processing models to extract adverse event mentions from clinical notes, social media posts, and regulatory documents, enables knowledge graph construction linking drugs, vaccines, adverse events, and patient characteristics for predictive safety modeling, and supports explainable AI systems that generate human-interpretable safety signals by reasoning over ontology-encoded mechanistic pathways connecting interventions to adverse outcomes, ultimately enhancing patient safety through earlier detection and characterization of intervention-related harms.'
  requires_registration: false
  url: https://github.com/OAE-ontology/OAE/
- id: B2AI_STANDARD:517
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology of Biological and Clinical Statistics
  formal_specification: https://github.com/obcs/obcs
  is_open: true
  name: OBCS
  purpose_detail: Biological and clinical statistics.
  requires_registration: false
  url: https://github.com/obcs/obcs
- id: B2AI_STANDARD:518
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology of Biological Attributes
  formal_specification: https://github.com/obophenotype/bio-attribute-ontology
  is_open: true
  name: OBA
  purpose_detail: A collection of biological attributes (traits) covering all kingdoms
    of life.
  requires_registration: false
  url: https://wiki.geneontology.org/index.php/Extensions/x-attribute
- id: B2AI_STANDARD:519
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology of Genetic Susceptibility Factor
  formal_specification: https://github.com/linikujp/OGSF
  is_open: true
  name: OGSF
  purpose_detail: An application ontology to represent genetic susceptibility to a
    specific disease, adverse event, or a pathological process.
  requires_registration: false
  url: https://github.com/linikujp/OGSF
- id: B2AI_STANDARD:520
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology of Host Pathogen Interactions
  formal_specification: https://github.com/OHPI/ohpi
  is_open: true
  name: OHPI
  publication: doi:10.1093/nar/gky999
  purpose_detail: Host-pathogen interactions and virulence factors.
  requires_registration: false
  url: https://github.com/OHPI/ohpi
- id: B2AI_STANDARD:521
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology of Host-Microbiome Interactions
  formal_specification: https://github.com/ohmi-ontology/ohmi
  is_open: true
  name: OHMI
  purpose_detail: Entities and relations related to microbiomes, microbiome host organisms
    (e.g., human and mouse), and the interactions between the hosts and microbiomes
    at different conditions.
  requires_registration: false
  url: https://github.com/ohmi-ontology/ohmi
- id: B2AI_STANDARD:522
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology of Medically Related Social Entities
  formal_specification: https://github.com/ufbmi/OMRSE
  is_open: true
  name: OMRSE
  publication: doi:10.1186/s13326-016-0087-8
  purpose_detail: This ontology covers the domain of social entities that are related
    to health care, such as demographic information and the roles of various...
  requires_registration: false
  url: https://github.com/ufbmi/OMRSE/wiki/OMRSE-Overview
- id: B2AI_STANDARD:523
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology of Organizational Structures of Trauma centers and Trauma
    systems
  formal_specification: https://github.com/OOSTT/OOSTT
  has_relevant_organization:
  - B2AI_ORG:96
  is_open: true
  name: OOSTT
  purpose_detail: Organizational components of trauma centers and trauma systems.
  requires_registration: false
  url: https://github.com/OOSTT/OOSTT
- id: B2AI_STANDARD:524
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology of Precision Medicine and Investigation
  formal_specification: https://github.com/OPMI/opmi
  is_open: true
  name: OPMI
  purpose_detail: Entities and relations associated with precision medicine and related
    investigations at different conditions.
  requires_registration: false
  url: https://github.com/OPMI/opmi
- id: B2AI_STANDARD:525
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:33
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology of RNA Sequencing
  formal_specification: https://github.com/safisher/ornaseq
  is_open: true
  name: ORNASEQ
  purpose_detail: An application ontology designed to annotate next-generation sequencing
    experiments performed on RNA.
  requires_registration: false
  url: https://github.com/safisher/ornaseq
- id: B2AI_STANDARD:526
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:4
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology of Vaccine Adverse Events
  formal_specification: https://github.com/OVAE-Ontology/ovae
  is_open: true
  name: OVAE
  purpose_detail: A biomedical ontology in the domain of vaccine adverse events.
  requires_registration: false
  url: https://github.com/OVAE-Ontology/ovae
- id: B2AI_STANDARD:527
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Oral Health and Disease Ontology
  formal_specification: https://github.com/oral-health-and-disease-ontologies/ohd-ontology
  is_open: true
  name: OHD
  publication: doi:10.1186/s13326-020-00222-0
  purpose_detail: Content of dental practice health records.
  requires_registration: false
  url: https://github.com/oral-health-and-disease-ontologies/ohd-ontology
- id: B2AI_STANDARD:528
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Orphanet Rare Disease Ontology
  formal_specification: https://www.orphadata.com/ontologies/
  has_relevant_organization:
  - B2AI_ORG:80
  is_open: true
  name: ORDO
  purpose_detail: The Orphanet Rare Disease ontology (ORDO) is jointly developed by
    Orphanet and the EBI to provide a structured vocabulary for rare diseases capturing
    relationships between diseases, genes and other relevant features which will form
    a useful resource for the computational analysis of rare diseases.
  requires_registration: false
  url: https://bioportal.bioontology.org/ontologies/ORDO
- id: B2AI_STANDARD:529
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Pathogen Host Interaction Phenotype Ontology
  formal_specification: https://github.com/PHI-base/phipo
  is_open: true
  name: PHIPO
  publication: doi:10.1093/nar/gkab1037
  purpose_detail: Species-neutral phenotypes observed in pathogen-host interactions.
  requires_registration: false
  url: https://github.com/PHI-base/phipo
- id: B2AI_STANDARD:530
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Pathogen Transmission Ontology
  formal_specification: https://github.com/DiseaseOntology/PathogenTransmissionOntology
  is_open: true
  name: TRANS
  publication: doi:10.1093/nar/gkp832
  purpose_detail: An ontology representing the disease transmission process during
    which the pathogen is transmitted directly or indirectly.
  requires_registration: false
  url: https://github.com/DiseaseOntology/PathogenTransmissionOntology
- id: B2AI_STANDARD:531
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Pathway ontology
  formal_specification: https://github.com/rat-genome-database/PW-Pathway-Ontology
  is_open: true
  name: PW
  publication: doi:10.1186/2041-1480-5-7
  purpose_detail: A controlled vocabulary for annotating gene products to pathways.
  requires_registration: false
  url: http://rgd.mcw.edu/rgdweb/ontology/search.html
- id: B2AI_STANDARD:532
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Performance Summary Display Ontology
  formal_specification: https://github.com/Display-Lab/psdo
  is_open: true
  name: PSDO
  purpose_detail: Ontology to reproducibly study visualizations of clinical performance
  requires_registration: false
  url: https://github.com/Display-Lab/psdo
- id: B2AI_STANDARD:533
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:3
  - B2AI_TOPIC:7
  - B2AI_TOPIC:12
  - B2AI_TOPIC:21
  - B2AI_TOPIC:25
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Phenomics Integrated Ontology
  formal_specification: https://github.com/monarch-initiative/phenio
  has_relevant_organization:
  - B2AI_ORG:58
  is_open: true
  name: PHENIO
  purpose_detail: An application ontology for accessing and comparing knowledge concerning
    phenotypes across species and genetic backgrounds.
  requires_registration: false
  url: https://github.com/monarch-initiative/phenio
- id: B2AI_STANDARD:534
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:25
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Phenotype And Trait Ontology
  formal_specification: https://github.com/pato-ontology/pato
  is_open: true
  name: PATO
  purpose_detail: Phenotypic qualities (properties, attributes or characteristics).
  requires_registration: false
  url: https://github.com/pato-ontology/pato
- id: B2AI_STANDARD:535
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Population and Community Ontology
  formal_specification: https://github.com/PopulationAndCommunityOntology/pco
  is_open: true
  name: PCO
  purpose_detail: Groups of interacting organisms such as populations and communities.
  requires_registration: false
  url: https://github.com/PopulationAndCommunityOntology/pco
- id: B2AI_STANDARD:536
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:3
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Process Chemistry Ontology
  formal_specification: https://github.com/proco-ontology/PROCO
  is_open: true
  name: PROCO
  purpose_detail: Process chemistry, the chemical field concerned with scaling up
    laboratory syntheses to commercially viable processes.
  requires_registration: false
  url: https://github.com/proco-ontology/PROCO
- id: B2AI_STANDARD:537
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:26
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Protein modification
  formal_specification: https://github.com/HUPO-PSI/psi-mod-CV
  has_relevant_organization:
  - B2AI_ORG:41
  is_open: true
  name: PSI-MOD
  publication: doi:10.1038/nbt0808-864
  purpose_detail: PSI-MOD is an ontology consisting of terms that describe protein
    chemical modifications
  requires_registration: false
  url: https://www.psidev.info/groups/protein-modifications
- id: B2AI_STANDARD:538
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:26
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: PRotein Ontology
  formal_specification: https://github.com/PROconsortium/PRoteinOntology/
  is_open: true
  name: PR
  purpose_detail: The Protein Ontology (PR) is a comprehensive formal ontology that
    provides standardized terminology and semantic relationships for describing protein-related
    entities across the complete spectrum of protein science. Developed by the PRotein
    Ontology consortium, PR serves as a unifying framework that integrates protein
    sequence, structure, and functional information into a coherent knowledge representation
    system. The ontology encompasses multiple levels of protein organization including
    protein families and complexes, individual protein molecules, protein domains
    and regions, post-translational modifications, and protein isoforms generated
    through alternative splicing or processing. PR maintains extensive cross-references
    to major protein databases including UniProt, NCBI, and Ensembl, enabling seamless
    integration with existing protein annotation resources. The ontology supports
    advanced protein function annotation by providing precise vocabulary for describing
    enzymatic activities, binding sites, regulatory mechanisms, and cellular localization
    patterns. PR is essential for proteomics data standardization, comparative protein
    analysis, functional genomics research, and systems biology applications where
    consistent protein terminology facilitates data integration, automated reasoning,
    and knowledge discovery across diverse experimental platforms.
  requires_registration: false
  url: https://github.com/PROconsortium/PRoteinOntology/
- id: B2AI_STANDARD:539
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:2
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Provisional Cell Ontology
  formal_specification: https://github.com/obophenotype/provisional_cell_ontology
  is_open: true
  name: PCL
  purpose_detail: Cell types that are provisionally defined by experimental techniques
    such as single cell or single nucleus transcriptomics.
  requires_registration: false
  url: https://github.com/obophenotype/provisional_cell_ontology
- id: B2AI_STANDARD:540
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:1
  - B2AI_TOPIC:11
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Radiation Biology Ontology
  formal_specification: https://github.com/Radiobiology-Informatics-Consortium/RBO
  is_open: true
  name: RBO
  purpose_detail: Effects of radiation on biota in terrestrial and space environments.
  requires_registration: false
  url: https://github.com/Radiobiology-Informatics-Consortium/RBO
- id: B2AI_STANDARD:541
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: RadLex radiology lexicon
  has_relevant_organization:
  - B2AI_ORG:85
  is_open: true
  name: RadLex
  purpose_detail: A comprehensive set of radiology terms for use in radiology reporting,
    decision support, data mining, data registries, education and research.
  requires_registration: true
  url: https://www.rsna.org/practice-tools/data-tools-and-standards/radlex-radiology-lexicon
- id: B2AI_STANDARD:542
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Rat Strain Ontology
  formal_specification: https://github.com/rat-genome-database/RS-Rat-Strain-Ontology
  is_open: true
  name: RS
  publication: doi:10.1186/2041-1480-4-36
  purpose_detail: 'Rat Strain Ontology (RS) is a structured vocabulary maintained by the Rat Genome Database (RGD) at the Medical College of Wisconsin, providing standardized nomenclature and hierarchical classification for laboratory rat strains, wild-derived strains, mutant lines, consomic/congenic strains, and transgenic/knockout rat models used in biomedical research, with integration into the OBO Foundry ecosystem supporting cross-species comparative genomics and phenotype studies. RS catalogs 5,000+ rat strains spanning inbred strains with defined genetic backgrounds (Wistar, Sprague-Dawley, Fischer 344, Brown Norway), outbred stocks with genetic heterogeneity, recombinant inbred lines for QTL mapping, consomic strains where complete chromosomes are substituted between strains, and gene-edited models (CRISPR knockouts, transgenic insertions) targeting specific disease mechanisms. The ontology structures strain relationships through "derived from" properties linking parent-progeny strains, "has genetic background" properties specifying founding strains for congenic lines, and "model of" properties connecting strains to disease phenotypes (hypertension, diabetes, cancer susceptibility), enabling navigation of complex breeding schemes and genetic derivations. RS terms include strain-specific metadata on phenotypic characteristics (coat color, obesity, behavioral traits), genetic markers (microsatellites, SNPs), tissue/cell sources, husbandry requirements, and availability from repositories (Rat Resource and Research Center, Charles River), facilitating experimental planning and reproducibility. The ontology integrates with RGD''s comprehensive annotations linking strains to quantitative trait loci (QTL), genes, pathways, diseases, and phenotypes (Mammalian Phenotype Ontology terms), supporting genome-wide association studies (GWAS), eQTL mapping, and systems genetics analyses that connect genetic variation to physiological endpoints. RS enables translational research by mapping rat models to human disease orthologs, facilitating target validation for drug discovery where rat pharmacology and toxicology data inform human clinical trial design, particularly for cardiovascular disease, neurodegeneration, and metabolic disorders where rat models recapitulate human pathophysiology better than mouse models. In AI/ML applications for precision medicine, RS provides structured metadata for training genomic prediction models that associate strain genotypes with phenotypic outcomes, supports knowledge graph construction linking rat genetic data to human disease mechanisms for cross-species inference, and enables meta-analyses aggregating phenotypic data across studies by standardizing strain identifiers, ultimately accelerating translational discoveries by leveraging the rat''s role as a premier mammalian model for human disease research.'
  requires_registration: false
  url: http://rgd.mcw.edu/rgdweb/search/strains.html
- id: B2AI_STANDARD:543
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Relation Ontology
  formal_specification: https://github.com/oborel/obo-relations
  is_open: true
  name: RO
  purpose_detail: Relationship types shared across multiple ontologies.
  requires_registration: false
  url: https://oborel.github.io/
- id: B2AI_STANDARD:544
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - codesystem
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:8
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: RxNorm
  has_relevant_organization:
  - B2AI_ORG:74
  is_open: true
  name: RxNorm
  purpose_detail: Medication terminology. Provided through UMLS.
  requires_registration: true
  url: https://www.nlm.nih.gov/research/umls/rxnorm/index.html
- id: B2AI_STANDARD:545
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Scientific Evidence and Provenance Information Ontology
  formal_specification: https://sepio-framework.github.io/sepio-linkml/
  is_open: true
  name: SEPIO
  publication: doi:10.5281/zenodo.5214269
  purpose_detail: The Scientific Evidence and Provenance Information Ontology (SEPIO)
    provides a structured framework for representing scientific evidence and provenance
    information supporting knowledge claims. It supports rich, computable representations
    of the evidence and provenance behind scientific assertions, particularly for
    genetic variants and their clinical interpretations.
  related_to:
  - B2AI_STANDARD:256
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:58
  url: https://github.com/monarch-initiative/SEPIO-ontology
- id: B2AI_STANDARD:546
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Semantic Sensor Network Ontology
  formal_specification: https://github.com/w3c/sdw
  has_relevant_organization:
  - B2AI_ORG:99
  is_open: true
  name: SSN
  purpose_detail: An ontology for describing sensors and their observations, the involved
    procedures, the studied features of interest, the samples used to do so, and the
    observed properties, as well as actuators.
  requires_registration: false
  url: https://www.w3.org/TR/vocab-ssn/
- id: B2AI_STANDARD:547
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Sequence types and features ontology
  formal_specification: https://github.com/The-Sequence-Ontology/SO-Ontologies
  is_open: true
  name: SO
  publication: doi:10.1016/j.jbi.2010.03.002
  purpose_detail: A structured controlled vocabulary for sequence annotation, for
    the exchange of annotation data and for the description of sequence objects.
  requires_registration: false
  url: http://www.sequenceontology.org/
- id: B2AI_STANDARD:548
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Sickle Cell Disease Ontology
  formal_specification: https://github.com/scdodev/scdo-ontology
  is_open: true
  name: SCDO
  purpose_detail: The Sickle Cell Disease Ontology (SCDO) provides comprehensive
    standardized terminology for describing all aspects of sickle cell disease (SCD),
    a group of inherited hemoglobin disorders affecting millions globally, particularly
    populations of African, Mediterranean, Middle Eastern, and Indian ancestry. Developed
    by the H3ABioNet consortium and aligned with OBO Foundry principles, SCDO encompasses
    genetic variants (HbS, HbC, HbE, beta-thalassemia mutations), disease phenotypes
    (sickle cell anemia HbSS, HbSC disease, HbS-beta thalassemia, sickle cell trait),
    clinical manifestations (vaso-occlusive crises, acute chest syndrome, stroke,
    priapism, splenic sequestration, chronic organ damage), laboratory findings (hemoglobin
    electrophoresis patterns, reticulocyte counts, bilirubin levels, fetal hemoglobin
    percentages), complications (pulmonary hypertension, renal dysfunction, avascular
    necrosis, leg ulcers, retinopathy), treatment modalities (hydroxyurea, blood transfusions,
    hematopoietic stem cell transplantation, gene therapy, pain management), and patient
    outcomes (quality of life measures, hospitalization rates, mortality). The ontology
    integrates genetic, clinical, laboratory, and treatment concepts to support comprehensive
    SCD patient data management and research. SCDO enables standardized phenotyping
    for genotype-phenotype correlation studies identifying disease modifiers (fetal
    hemoglobin levels, alpha-thalassemia co-inheritance, genetic polymorphisms affecting
    disease severity), facilitates clinical trial recruitment through precise inclusion/exclusion
    criteria specification, and supports electronic health record integration for
    automated SCD surveillance and quality improvement initiatives. Applications include
    natural history studies characterizing disease progression patterns, pharmacogenomics
    research examining hydroxyurea response variability, health disparities research
    documenting access to disease-modifying therapies, and global SCD registries enabling
    cross-population comparisons. SCDO facilitates data harmonization across international
    SCD cohorts, enabling meta-analyses and collaborative research essential for rare
    disease studies where no single center has sufficient patient numbers.
  requires_registration: false
  url: https://scdontology.h3abionet.org/
- id: B2AI_STANDARD:549
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Software ontology
  formal_specification: https://github.com/allysonlister/swo
  is_open: true
  name: SWO
  publication: doi:10.1186/2041-1480-5-25
  purpose_detail: Software tools, their types, tasks, versions, provenance and associated
    data.
  requires_registration: false
  url: https://github.com/allysonlister/swo
- id: B2AI_STANDARD:550
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Standard Current Procedural Terminology Consumer Friendly Descriptors
  is_open: false
  name: CFDs
  purpose_detail: Translate each code descriptor from the official CPT code set into
    language that is easily understood by the average patient and/or his or her caregiver.
    The objective is to simplify the highly technical CPT code descriptors into something
    more patient-focused and patient-friendly.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:3
  url: https://commerce.ama-assn.org/catalog/media/Consumer-and-Clinician-Descriptors-in-CPT-Data-Files.pdf
- id: B2AI_STANDARD:551
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Statistics Ontology
  formal_specification: https://github.com/ISA-tools/stato
  has_relevant_organization:
  - B2AI_ORG:47
  is_open: true
  name: STATO
  purpose_detail: "The Statistics Ontology (STATO) provides comprehensive standardized
    vocabulary for statistical methods, experimental design concepts, hypothesis testing
    procedures, and statistical measures used across life sciences, biomedical research,
    and data-intensive scientific domains, developed by the ISA-tools community to
    support reproducible research and transparent reporting of statistical analyses.
    STATO encompasses five major categories: statistical tests (parametric tests including
    t-tests, ANOVA, ANCOVA, linear regression, mixed models; non-parametric tests
    including Mann-Whitney U, Kruskal-Wallis, Wilcoxon signed-rank, Spearman correlation;
    and specialized methods like survival analysis, multivariate analysis, time series
    analysis), probability distributions (normal, binomial, Poisson, exponential,
    chi-square, t-distribution, F-distribution) essential for understanding test assumptions,
    descriptive statistics (measures of central tendency including mean, median, mode;
    measures of dispersion including standard deviation, variance, interquartile range;
    and measures of shape including skewness, kurtosis), data types and variables
    (categorical/nominal, ordinal, continuous, discrete, dependent/independent variables,
    covariates, confounding variables), and experimental design concepts (randomization,
    blocking, replication, control groups, factorial designs, crossover designs, longitudinal
    studies). STATO provides formal OWL definitions enabling automated reasoning about
    test conditions of application, linking test selection to data characteristics
    (e.g., use Mann-Whitney U when comparing two independent groups with non-normal
    distributions), and capturing assumptions (normality, homoscedasticity, independence)
    that must be verified before test application. Each statistical method includes
    textual definitions for human understanding, formal logical definitions for machine
    reasoning, associated R code snippets via 'R-command' annotations enabling direct
    implementation, and documentation of appropriate use cases and interpretation guidelines.
    STATO integrates with BFO (Basic Formal Ontology) as upper-level ontology and
    OBI (Ontology for Biomedical Investigations) for process definitions, ensuring
    interoperability across biomedical ontologies. Applications include annotation
    of analysis methods in ISA-Tab metadata files for omics studies, standardized
    reporting of statistical procedures in publications to meet journal guidelines
    (CONSORT, STROBE, ARRIVE), automated validation of statistical analysis workflows
    in computational notebooks, education and training through formal definitions
    of statistical concepts with R implementation examples, and text mining of scientific
    literature to extract statistical method usage patterns. STATO supports reproducibility
    by precisely specifying analysis procedures, capturing multiple testing correction
    methods (Bonferroni, Benjamini-Hochberg FDR, permutation tests), effect size measures
    (Cohen's d, odds ratios, hazard ratios), and confidence interval calculations.
    The ontology facilitates peer review by enabling reviewers to verify appropriate
    test selection and assists researchers in choosing correct statistical methods
    based on study design and data characteristics."
  requires_registration: false
  url: http://stato-ontology.org/
- id: B2AI_STANDARD:552
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Symptom Ontology
  formal_specification: https://github.com/DiseaseOntology/SymptomOntology
  is_open: true
  name: SYMP
  publication: doi:10.1093/nar/gkab1063
  purpose_detail: Disease symptoms, with symptoms encompasing perceived changes in
    function, sensations or appearance reported by a patient.
  requires_registration: false
  url: http://symptomontologywiki.igs.umaryland.edu/mediawiki/index.php/Main_Page
- id: B2AI_STANDARD:553
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - codesystem
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Systematized Nomenclature of Medicine - Clinical Terms
  has_relevant_organization:
  - B2AI_ORG:74
  is_open: true
  name: SNOMED CT
  purpose_detail: Standard for electronic exchange of clinical health information.
    Provided through UMLS.
  related_to:
  - B2AI_STANDARD:769
  requires_registration: true
  url: https://www.nlm.nih.gov/healthit/snomedct/index.html
- id: B2AI_STANDARD:554
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Systematized Nomenclature of Medicine, International Version
  has_relevant_organization:
  - B2AI_ORG:74
  is_open: true
  name: SNMI
  purpose_detail: 'Systematized Nomenclature of Medicine International (SNMI) is a historical medical
    terminology and classification system that preceded SNOMED CT. Developed in the
    1960s-1970s by the College of American Pathologists, SNMI represented one of the
    earliest attempts to create a comprehensive, multi-axial medical nomenclature covering
    topography (anatomy), morphology (structural changes), etiology (causes), and function
    (physiological processes). SNMI employed a systematic coding structure where concepts
    were represented by alphanumeric codes and could be combined using post-coordination
    to express complex clinical findings (e.g., combining topography codes with morphology
    codes to describe disease locations and characteristics). The terminology was designed
    primarily for pathology and clinical documentation, providing structured vocabulary
    for diagnoses, procedures, and laboratory findings. SNMI evolved through several
    versions including SNOMED (Systematized Nomenclature of Medicine), SNOMED II, and
    SNOMED III before being integrated into SNOMED CT (Clinical Terms) in 2002 through
    merger with UK''s Clinical Terms Version 3 (Read Codes). While SNMI itself is
    now deprecated and replaced by SNOMED CT for current clinical use, it is historically
    significant as a foundational medical terminology that pioneered multi-axial compositional
    approaches to medical concept representation. Legacy SNMI data may still exist
    in historical medical records and research databases, requiring mapping to modern
    terminologies for interoperability. Understanding SNMI''s structure provides context
    for SNOMED CT''s architecture and the evolution of standardized medical terminologies
    used in electronic health records, clinical research, and healthcare data analytics.'
  requires_registration: false
  url: https://bioportal.bioontology.org/ontologies/SNMI
- id: B2AI_STANDARD:555
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Systems Biology Ontology
  formal_specification: https://github.com/EBI-BioModels/SBO
  has_relevant_organization:
  - B2AI_ORG:29
  is_open: true
  name: SBO
  purpose_detail: Terms commonly used in Systems Biology and computational modeling.
  requires_registration: false
  url: https://github.com/EBI-BioModels/SBO
- id: B2AI_STANDARD:556
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Taxonomic rank vocabulary
  formal_specification: https://github.com/phenoscape/taxrank
  is_open: true
  name: TAXRANK
  publication: doi:10.1186/2041-1480-4-34
  purpose_detail: A vocabulary of taxonomic ranks (species, family, phylum, etc).
  requires_registration: false
  url: https://github.com/phenoscape/taxrank
- id: B2AI_STANDARD:557
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: terms4FAIRskills
  formal_specification: https://github.com/terms4fairskills/FAIRterminology
  is_open: true
  name: T4FS
  publication: doi:10.5281/zenodo.4772741
  purpose_detail: A terminology for the skills necessary to make data FAIR and to
    keep it FAIR.
  requires_registration: false
  url: https://obofoundry.org/ontology/t4fs.html
- id: B2AI_STANDARD:558
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:8
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: The Drug Ontology
  formal_specification: https://github.com/ufbmi/dron
  has_relevant_organization:
  - B2AI_ORG:96
  is_open: true
  name: DRON
  publication: doi:10.1186/s13326-017-0121-5
  purpose_detail: An ontology to support comparative effectiveness researchers studying
    claims data.
  requires_registration: false
  url: https://github.com/ufbmi/dron
- id: B2AI_STANDARD:559
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: The Extensible Observation Ontology
  formal_specification: https://github.com/NCEAS/oboe/
  has_relevant_organization:
  - B2AI_ORG:62
  is_open: true
  name: OBOE
  purpose_detail: The Extensible Observation Ontology (OBOE) is a formal ontology
    for capturing the semantics of scientific observation and measurement.
  requires_registration: false
  url: https://bioportal.bioontology.org/ontologies/OBOE
- id: B2AI_STANDARD:560
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: The Ontology of Genes and Genomes
  formal_specification: https://bitbucket.org/hegroup/ogg/src/master/
  is_open: true
  name: OGG
  purpose_detail: A formal ontology of genes and genomes of biological organisms.
  requires_registration: false
  url: https://bitbucket.org/hegroup/ogg/src/master/
- id: B2AI_STANDARD:561
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:8
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: The Prescription of Drugs Ontology
  formal_specification: https://github.com/OpenLHS/PDRO
  is_open: true
  name: PDRO
  publication: doi:10.3390/ijerph182212025
  purpose_detail: An ontology to describe entities related to prescription of drugs
  requires_registration: false
  url: https://github.com/OpenLHS/PDRO
- id: B2AI_STANDARD:562
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Toxic Process Ontology
  formal_specification: https://github.com/txpo-ontology/TXPO/
  is_open: true
  name: TXPO
  purpose_detail: Terms involving toxicity courses and processes.
  requires_registration: false
  url: https://toxpilot.nibiohn.go.jp/
- id: B2AI_STANDARD:563
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Uberon
  formal_specification: https://github.com/obophenotype/uberon
  is_open: true
  name: UBERON
  publication: doi:10.1186/gb-2012-13-1-r5
  purpose_detail: UBERON is an OBO Foundry ontology providing an integrated cross-species
    anatomy ontology covering anatomical structures in animals, with a focus on multi-species
    interoperability. The ontology enables semantic annotation of anatomical entities
    across diverse species, supporting comparative anatomy research, phenotype studies,
    and integration with specialized anatomies. UBERON is tightly integrated with
    the Gene Ontology (GO) and the Cell Ontology (CL), using formal ontology design
    patterns to represent anatomical locations and relationships. It follows FAIR
    principles and is released in standard formats (OWL, OBO, JSON obographs) with
    resolvable version IRIs. UBERON is integrated into standard tools including Ubergraph
    for logical queries (e.g., finding cell types by location), the Ontology Access
    Kit (OAK), and major browsers (OLS, Ontobee, BioPortal). The ontology supports
    AI/ML applications by providing standardized anatomical annotations for training
    data across species and enabling cross-species knowledge transfer in biomedical
    machine learning models.
  requires_registration: false
  url: https://obophenotype.github.io/uberon/
- id: B2AI_STANDARD:564
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: UMLS Metathesaurus
  has_relevant_organization:
  - B2AI_ORG:74
  - B2AI_ORG:115
  is_open: true
  name: Metathesaurus
  purpose_detail: Biomedical terminology and hierarchical relationships between concepts.
  requires_registration: true
  url: https://www.nlm.nih.gov/research/umls/knowledge_sources/metathesaurus/index.html
  used_in_bridge2ai: true
- id: B2AI_STANDARD:565
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:25
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Unified phenotype ontology
  formal_specification: https://github.com/obophenotype/upheno
  is_open: true
  name: UPHENO
  purpose_detail: Integrates multiple phenotype ontologies into a unified cross-species
    phenotype ontology.
  requires_registration: false
  url: https://github.com/obophenotype/upheno
- id: B2AI_STANDARD:566
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Units of measure ontology
  is_open: true
  name: UO
  publication: doi:10.1093/database/bas033
  purpose_detail: The Units Ontology - a tool for integrating units of measurement
    in science
  requires_registration: false
  url: https://obofoundry.org/ontology/uo.html
- id: B2AI_STANDARD:567
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Units of measurement ontology
  formal_specification: https://github.com/bio-ontology-research-group/unit-ontology
  is_open: true
  name: UO
  purpose_detail: Metrical units for use in conjunction with PATO.
  requires_registration: false
  url: https://github.com/bio-ontology-research-group/unit-ontology
- id: B2AI_STANDARD:568
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Universal Medical Device Nomenclature System
  has_relevant_organization:
  - B2AI_ORG:27
  is_open: false
  name: UMDNS
  purpose_detail: Universal Medical Device Nomenclature System (UMDNS) is a nomenclature
    that has been officially adopted by many nations. UMDNS facilitates identifying,
    processing, filing, storing, retrieving, transferring, and communicating data
    about medical devices. The nomenclature is used in applications ranging from hospital
    inventory and work-order controls to national agency medical device regulatory
    systems.
  requires_registration: true
  url: https://www.ecri.org/solutions/umdns
- id: B2AI_STANDARD:569
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Vaccine Ontology
  formal_specification: https://github.com/vaccineontology/VO
  is_open: true
  name: VO
  publication: doi:10.1186/2041-1480-3-17
  purpose_detail: The Vaccine Ontology (VO) is an OBO Foundry community-based biomedical
    ontology that systematically represents vaccine components, vaccine types, vaccination
    procedures, vaccine-induced immune responses, and vaccine-preventable infectious
    diseases in a standardized machine-readable format to support vaccine research,
    immunization program management, and vaccinomics data integration. Developed through
    international collaboration coordinated by the University of Michigan Medical
    School, VO encompasses comprehensive coverage of licensed human and veterinary
    vaccines including live-attenuated vaccines, inactivated vaccines, subunit vaccines,
    toxoid vaccines, mRNA vaccines, viral vector vaccines, and conjugate vaccines,
    representing their antigenic components, adjuvants, preservatives, manufacturing
    processes, and delivery routes. The ontology integrates with multiple biomedical
    ontologies including the Infectious Disease Ontology (IDO) for pathogen-host
    interactions, the Ontology for Biomedical Investigations (OBI) for vaccination
    protocols, the Protein Ontology (PRO) for vaccine antigens, and SNOMED CT and
    ICD codes for clinical documentation, enabling semantic interoperability across
    vaccine databases and immunization information systems. VO formally represents
    immunization schedules (primary series, booster doses, catch-up schedules), adverse
    events following immunization (AEFI) including local reactions and systemic effects,
    contraindications and precautions for specific populations, vaccine efficacy and
    effectiveness measures, and herd immunity thresholds. Applications include standardized
    annotation of vaccine clinical trials data, integration of vaccine safety surveillance
    systems (VAERS, Vaccine Safety Datalink), comparative vaccine effectiveness studies
    across populations and time periods, semantic queries of vaccine literature through
    PubMed and clinical databases, machine learning models predicting vaccine immunogenicity
    or reactogenicity from vaccine composition, and global immunization data harmonization
    supporting WHO vaccination coverage monitoring and pandemic preparedness. VO follows
    OBO Foundry principles with open-source development, versioned releases, and persistent
    URIs, making it essential infrastructure for computational vaccinology and evidence-based
    immunization policy.
  requires_registration: false
  url: https://github.com/vaccineontology/VO
- id: B2AI_STANDARD:570
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Vertebrate Breed Ontology
  formal_specification: https://github.com/monarch-initiative/vertebrate-breed-ontology
  has_relevant_organization:
  - B2AI_ORG:58
  is_open: true
  name: VBO
  purpose_detail: 'Vertebrate Breed Ontology (VBO) is a comprehensive ontology providing standardized nomenclature and hierarchical classification for vertebrate animal breeds across 38 species including cattle, sheep, goats, pigs, horses, chickens, dogs, and cats, developed collaboratively by the Monarch Initiative, Online Mendelian Inheritance in Animals (OMIA), and FAO''s Domestic Animal Diversity Information System (DAD-IS). VBO structures breed information using OBO Foundry principles, capturing breed names, synonyms (alternate spellings, historical names, local language variants), geographical origins, breed characteristics (size, coat color, production traits), and relationships to parent breeds or breed groups. The ontology integrates with DAD-IS''s 15,000+ national breed populations representing 8,800+ breeds maintained by 182 countries'' National Coordinators, ensuring global coverage and continuous updates reflecting new breed registrations and naming conventions. VBO supports cross-species breed queries by organizing breeds under species-specific classes (bovine breeds, equine breeds) while maintaining inter-breed relationships such as "derived from" for composite breeds and "related to" for breeds sharing genetic heritage. The ontology enables precise phenotype-genotype associations in animal genetics research by providing stable breed identifiers (VBO IDs) that link to genomic datasets, genetic variant databases, and phenotype repositories, facilitating genome-wide association studies (GWAS) and quantitative trait locus (QTL) mapping in livestock. VBO integrates with other biomedical ontologies including the Livestock Breed Ontology (LBO), Mammalian Phenotype Ontology (MP), and UBERON anatomy ontology to support complex queries across breed characteristics, anatomical features, and disease susceptibilities. For agricultural genomics, VBO enables tracking of breed-specific genetic diversity, conservation status of rare breeds (endangered, critical, vulnerable), and lineage documentation for breeding programs aimed at preserving genetic resources. In AI/ML applications, VBO provides structured metadata for training computer vision models on breed classification, natural language processing systems for extracting breed information from veterinary records, and knowledge graphs connecting breed genetics to production traits (milk yield, growth rate, disease resistance) and animal welfare indicators, supporting precision livestock farming and genomic selection programs.'
  requires_registration: false
  url: https://github.com/monarch-initiative/vertebrate-breed-ontology
- id: B2AI_STANDARD:571
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Vertebrate Taxonomy Ontology
  formal_specification: https://github.com/phenoscape/vertebrate-taxonomy-ontology
  is_open: true
  name: VTO
  purpose_detail: >-
    "The Vertebrate Taxonomy Ontology (VTO) provides a comprehensive unified taxonomic
    hierarchy encompassing both extinct and extant vertebrate taxa, integrating authoritative
    sources to serve comparative biology, evolutionary research, paleontology, biodiversity
    informatics, and model organism databases requiring consistent taxonomic classification
    across deep time. VTO addresses the fundamental challenge that different biological
    disciplines maintain separate taxonomic frameworksNCBI Taxonomy focuses on species
    with genetic sequence data deposited in GenBank/RefSeq (inherently biased toward extant,
    well-studied organisms), while paleontological databases like the Paleobiology Database
    (PaleoDB) capture fossil taxa spanning 540 million years of vertebrate evolution but
    lack integration with molecular resources. VTO bridges this divide by maintaining a
    backbone hierarchy based on NCBI Taxonomy for extant taxa (ensuring compatibility with
    genomics/proteomics databases indexing sequences by taxon identifiers), while
    incorporating PaleoDB taxonomic information to represent extinct lineages including
    stem-group vertebrates, armored fishes (placoderms, acanthodians), early tetrapods,
    and extinct clades within extant groups (e.g., non-avian dinosaurs, pterosaurs,
    ichthyosaurs, plesiosaurs, mammalian stem groups). The ontology incorporates more
    authoritative hierarchies for specific vertebrate groups through integration with the
    Teleost Taxonomy Ontology (TTO) maintained by the Zebrafish Model Organism Database,
    providing detailed classification for the 30,000+ teleost fish species (comprising more
    than half of extant vertebrate diversity), and AmphibiaWeb taxonomy for the 8,500+
    amphibian species with emphasis on recently described taxa and refined phylogenetic
    relationships. Each taxon term includes standardized nomenclature following ICZN
    (International Code of Zoological Nomenclature) and ICNP (International Code of
    Nomenclature for Prokaryotes) rules, synonyms capturing taxonomic revisions and
    historical names, temporal ranges for extinct taxa (e.g., 'Late Cretaceous,
    100.5-66.0 Ma'), and cross-references to NCBI Taxonomy IDs, PaleoDB taxon numbers,
    and Encyclopedia of Life pages. VTO supports the Phenoscape Knowledgebase (KB),
    a semantic database linking evolutionary phenotypes annotated from 200+ phylogenetic
    comparative studies with developmental biology and genetics data from model organisms,
    enabling queries like 'find genes associated with fin-to-limb morphological transitions'
    by traversing ontology relationships connecting fossil taxa exhibiting transitional
    phenotypes (e.g., Tiktaalik, Acanthostega) to homologous structures in zebrafish,
    Xenopus, and mouse annotated with genetic perturbation data. Applications span
    biodiversity informatics platforms (GBIF, iNaturalist, EOL) requiring standardized
    taxonomic backbones for aggregating species occurrence data and range maps, museum
    specimen databases linking physical collections to phylogenetic context, conservation
    genomics identifying evolutionarily significant units (ESUs) and management units
    within endangered species complexes based on phylogenetic position, and ecological
    modeling incorporating phylogenetic diversity metrics (Faith's PD, phylogenetic
    endemism) for conservation prioritization. VTO enables cross-species phenotype
    comparisons essential for understanding trait evolution and developmental constraints,
    supporting evo-devo research connecting genetic regulatory networks across vertebrate
    phylogeny. Machine learning applications leverage VTO's hierarchical structure for
    taxonomic prediction from morphological imaging data, where classifiers trained on
    extant species can incorporate extinct taxa's phenotypic annotations to inform
    predictions about fossil specimens, phylogenetic imputation methods predicting
    missing trait values based on taxonomic proximity weighted by VTO relationships,
    comparative genomics studies identifying taxon-specific gene family expansions and
    losses by mapping genomic features to VTO taxa and inferring evolutionary events at
    internal nodes representing common ancestors, and meta-analyses of phenotype-genotype
    associations across model organisms where VTO provides the semantic framework for
    translating findings between zebrafish cardiovascular mutants, Xenopus neural crest
    development, chicken limb patterning, and mouse craniofacial genetics by reasoning
    over homologous anatomical structures and shared ancestry. Integration with other
    OBO Foundry ontologies including Uberon (cross-species anatomy), Gene Ontology
    (biological processes and molecular functions), and PATO (phenotypic qualities)
    enables complex semantic queries spanning taxonomy, anatomy, and genetics,
    such as 'retrieve all vertebrate taxa with documented neural crest-derived
    pharyngeal arch skeletal elements exhibiting heterochronic developmental
    timing shifts relative to ancestral condition' which requires coordinating VTO
    taxon relationships, Uberon anatomical part-of hierarchies, and GO developmental
    process annotations. VTO's CC0 public domain license and OBO Foundry compliance
    ensure interoperability with global biodiversity and biomedical data ecosystems,
    with monthly releases synchronized to NCBI Taxonomy updates, PaleoDB taxonomic
    revisions, and community-contributed nomenclatural corrections."
  requires_registration: false
  url: https://github.com/phenoscape/vertebrate-taxonomy-ontology
- id: B2AI_STANDARD:572
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:25
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Vertebrate trait ontology
  formal_specification: https://github.com/AnimalGenome/vertebrate-trait-ontology
  is_open: true
  name: VT
  purpose_detail: The Vertebrate Trait Ontology (VT) provides comprehensive standardized
    terminology for describing measurable or observable phenotypic characteristics
    across vertebrate species, maintained by the AnimalGenome organization to support
    agricultural genomics, comparative biology, quantitative genetics, and translational
    research linking animal models to human health. VT encompasses broad trait categories
    including morphological traits (body size, organ dimensions, skeletal measurements,
    integument characteristics), physiological traits (metabolic parameters, cardiovascular
    function, respiratory capacity, immune response, reproductive performance), behavioral
    traits (temperament, learning ability, social interactions, feeding behavior),
    production traits critical for livestock industries (growth rate, feed efficiency,
    milk yield, egg production, meat quality, wool production), disease resistance
    traits (pathogen susceptibility, vaccine response, parasite load), and life history
    traits (longevity, fertility, developmental timing). Each trait is formally defined
    with measurement methodologies, units, biological context, and relationships to
    anatomical structures (via Uberon) and biological processes (via Gene Ontology).
    VT serves as the primary phenotype ontology for agricultural animal genomics databases
    including AnimalQTLdb (quantitative trait loci database for cattle, pig, chicken,
    sheep, horse, rainbow trout), supporting annotation of QTL mapping studies that
    identify genomic regions influencing economically important traits. The ontology
    enables cross-species phenotype comparisons essential for translational research,
    allowing researchers to leverage livestock as biomedical models (e.g., pig cardiovascular
    traits for human heart disease research, sheep bone traits for osteoporosis studies).
    Applications span livestock breeding programs through genomic selection where VT
    standardizes trait definitions for estimated breeding values (EBVs), wildlife
    conservation genetics for monitoring population health indicators, aquaculture
    for trait improvement in fish and shellfish, and comparative physiology studies
    examining adaptive evolution of traits across vertebrate lineages. VT facilitates
    GWAS (genome-wide association studies) meta-analyses by harmonizing trait definitions
    across studies, enables phenotype-driven queries in genomics databases ("find
    all QTL affecting milk fat percentage in dairy cattle"), and supports machine
    learning models predicting complex traits from genotype data by providing structured
    phenotype representations. Integration with other ontologies including Mammalian
    Phenotype Ontology (MP), Human Phenotype Ontology (HPO), and Clinical Measurement
    Ontology (CMO) enables bidirectional translation between agricultural animal research
    and human biomedicine. VT is distributed under CC-BY-NC 4.0 license through http://purl.obolibrary.org/obo/vt.owl
    with releases synchronized to OBO Foundry and AgroPortal.
  requires_registration: false
  url: https://github.com/AnimalGenome/vertebrate-trait-ontology
- id: B2AI_STANDARD:573
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: VEuPathDB ontology
  formal_specification: https://github.com/VEuPathDB-ontology/VEuPathDB-ontology
  is_open: true
  name: EUPATH
  publication: doi:10.5281/zenodo.6685957
  purpose_detail: Support ontology for the Eukaryotic Pathogen, Host & Vector Genomics
    Resource (VEuPathDB; https://veupathdb.org).
  requires_registration: false
  url: https://github.com/VEuPathDB-ontology/VEuPathDB-ontology
- id: B2AI_STANDARD:574
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:25
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Xenopus Phenotype Ontology
  formal_specification: https://github.com/obophenotype/xenopus-phenotype-ontology
  is_open: true
  name: XPO
  publication: doi:10.1186/s12859-022-04636-8
  purpose_detail: The Xenopus Phenotype Ontology (XPO) is a formal ontology for representing
    anatomical, cellular, and gene function phenotypes observed throughout the development
    of the African frogs Xenopus laevis and Xenopus tropicalis. XPO enables standardized
    annotation of phenotypes in Xenopus research, supporting integration with genotype,
    phenotype, and disease data across species. The ontology is used by Xenbase and
    other resources to facilitate cross-species comparisons, data sharing, and computational
    analysis of developmental and functional phenotypes in model organism research.
  requires_registration: false
  url: https://github.com/obophenotype/xenopus-phenotype-ontology
- id: B2AI_STANDARD:575
  category: B2AI_STANDARD:OntologyOrVocabulary
  collection:
  - obofoundry
  concerns_data_topic:
  - B2AI_TOPIC:25
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Zebrafish Phenotype Ontology
  formal_specification: https://github.com/obophenotype/zebrafish-phenotype-ontology
  is_open: true
  name: ZP
  purpose_detail: All phenotypes of the Zebrafish model organism.
  requires_registration: false
  url: https://github.com/obophenotype/zebrafish-phenotype-ontology
- id: B2AI_STANDARD:576
  category: B2AI_STANDARD:DataStandardOrTool
  concerns_data_topic:
  - B2AI_TOPIC:6
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDC Race and Ethnicity Code Set
  formal_specification: https://phinvads.cdc.gov/vads/ViewValueSet.action?id=B246B692-6DF8-E111-B875-001A4BE7FA90
  has_relevant_organization:
  - B2AI_ORG:12
  - B2AI_ORG:40
  is_open: true
  name: PHVS_Race_HL7_2x
  purpose_detail: A code set for use in coding race and ethnicity data.
  requires_registration: false
  url: https://www.cdc.gov/phin/resources/vocabulary/documents/cdc-race--ethnicity-background-and-purpose.pdf
- id: B2AI_STANDARD:577
  category: B2AI_STANDARD:DataStandardOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  - B2AI_TOPIC:6
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: IETF Request for Comment 5646 Tags for Identifying Languages
  has_relevant_organization:
  - B2AI_ORG:45
  is_open: true
  name: RFC 5646
  purpose_detail: Best practices for the structure, content, construction, and semantics
    of language tags for use in cases where it is desirable to indicate the language
    used in an information object.
  requires_registration: false
  url: https://www.rfc-editor.org/rfc/rfc5646
- id: B2AI_STANDARD:578
  category: B2AI_STANDARD:DataStandardOrTool
  concerns_data_topic:
  - B2AI_TOPIC:31
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: International Telecommunication Union E.123 Notation for national and
    international telephone numbers, e-mail addresses and web addresses
  has_relevant_organization:
  - B2AI_ORG:50
  is_open: true
  name: ITUT E.123
  purpose_detail: Standard notation for printing telephone numbers, E-mail addresses
    and Web addresses.
  requires_registration: false
  url: https://www.itu.int/rec/T-REC-E.123-200102-I/en
- id: B2AI_STANDARD:579
  category: B2AI_STANDARD:DataStandardOrTool
  concerns_data_topic:
  - B2AI_TOPIC:31
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: International Telecommunication Union E.164 The international public
    telecommunication numbering plan
  has_relevant_organization:
  - B2AI_ORG:50
  is_open: true
  name: ITUT E.164
  purpose_detail: Number structure and functionality for the five categories of numbers
    used for international public telecommunication - geographic areas, global services,
    Networks, groups of countries (GoC) and resources for trials.
  requires_registration: false
  url: https://www.itu.int/rec/T-REC-E.164-201011-I/en
- id: B2AI_STANDARD:704
  category: B2AI_STANDARD:Registry
  collection:
  - dataregistry
  - softwareregistry
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: bio.tools
  formal_specification: https://github.com/bio-tools/biotoolsregistry/
  is_open: true
  name: bio.tools
  purpose_detail: 'bio.tools is a comprehensive, community-driven registry and portal
    for bioinformatics software tools, databases, and services developed under the
    ELIXIR Tools Platform, providing standardized descriptions and metadata for over
    27,000 resources spanning genomics, proteomics, structural biology, systems biology,
    clinical informatics, and computational biology workflows. Each registered tool
    entry follows the biotoolsSchema specification defining structured metadata fields
    including tool name, description, homepage URL, version information, programming
    language, operating system compatibility, license type, publication citations,
    and crucially, semantic annotations using EDAM ontology (EDAM - ontology of bioinformatics
    operations, data types, formats, and topics) terms that classify tools by their
    function (e.g., "Sequence alignment", "Variant calling", "Phylogenetic tree generation"),
    input/output data types (e.g., "Protein sequence", "Gene expression matrix", "Pathway
    diagram"), supported file formats (e.g., "FASTA", "VCF", "BAM/SAM"), and scientific
    application topics (e.g., "Transcriptomics", "Metagenomics", "Drug discovery"),
    enabling precise semantic search and automated tool discovery. The registry provides
    RESTful API access allowing programmatic queries to retrieve tool metadata, filter
    by EDAM annotations, search by keywords or identifiers (biotools IDs, DOIs, PMIDs),
    and integrate bio.tools data into external systems including workflow management
    platforms (Galaxy, Nextflow, Snakemake), package managers (Bioconda, BioContainers),
    and institutional tool catalogs. bio.tools implements quality metrics for entries
    including information completeness scores, citation counts, version currency indicators,
    and community ratings, with curation workflows supported by community moderators
    who review submissions, validate metadata accuracy, resolve duplicate entries,
    and maintain EDAM annotation consistency. The platform serves as the authoritative
    tool registry for ELIXIR infrastructure connecting 23 European research organizations,
    integrating with ELIXIR Core Data Resources (UniProt, ENA, PDB, EMBL-EBI services),
    supporting findability through schema.org markup for search engine indexing, and
    providing training materials and documentation for tool developers to register
    and maintain their resources. bio.tools facilitates reproducibility in computational
    biology by providing persistent identifiers (biotools IDs like biotools:blast)
    for citing tools in publications and methods sections, versioning information
    to document exact tool versions used in analyses, and links to containerized deployments
    (Docker, Singularity) ensuring long-term availability. For AI and machine learning
    applications in bioinformatics, bio.tools enables systematic tool landscape analysis
    where researchers can use API queries and EDAM annotations to identify all available
    tools for specific tasks (e.g., retrieving the 200+ tools annotated with "Machine
    learning" operation, filtering by input data types to find ML tools accepting
    gene expression matrices), supports automated workflow composition where workflow
    engines can discover compatible tools by matching output data types of one tool
    to input requirements of downstream tools using EDAM semantic relationships, facilitates
    meta-research on bioinformatics software ecosystems through network analysis of
    tool relationships (citation networks, input/output compatibility graphs, co-usage
    patterns from workflow repositories), enables recommendation systems that suggest
    relevant tools to users based on their data types and analysis objectives by learning
    from historical tool usage patterns and EDAM ontology structure, supports benchmarking
    studies comparing multiple tools for the same task where bio.tools metadata provides
    the tool universe and EDAM annotations ensure fair comparisons across functionally
    equivalent implementations, and allows training of natural language processing
    models to automatically extract tool mentions from scientific literature and map
    them to canonical bio.tools entries using the rich metadata and publication linkage,
    creating comprehensive maps of which computational methods are actually used across
    different research domains and facilitating evidence-based tool selection in automated
    analysis pipelines.'
  requires_registration: false
  url: https://bio.tools/
- id: B2AI_STANDARD:705
  category: B2AI_STANDARD:Registry
  collection:
  - softwareregistry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Bioconductor
  formal_specification: https://github.com/Bioconductor/BiocManager
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:40
  is_open: true
  name: Bioconductor
  purpose_detail: The mission of the Bioconductor project is to develop, support,
    and disseminate free open source software that facilitates rigorous and reproducible
    analysis of data from current and emerging biological assays. We are dedicated
    to building a diverse, collaborative, and welcoming community of developers and
    data scientists.
  requires_registration: false
  url: https://www.bioconductor.org/
- id: B2AI_STANDARD:706
  category: B2AI_STANDARD:Registry
  collection:
  - ontologyregistry
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: BioPortal
  formal_specification: https://github.com/ncbo
  is_open: true
  name: BioPortal
  purpose_detail: 'BioPortal is a comprehensive, open-access ontology repository and collaborative
    platform developed by the National Center for Biomedical Ontology (NCBO) at Stanford,
    hosting over 1,000 biomedical ontologies, terminologies, and vocabularies. The
    platform provides centralized access to diverse biomedical knowledge organization
    systems including OBO Foundry ontologies (GO, Uberon, CHEBI, HPO), medical terminologies
    (SNOMED CT, ICD, RxNorm, LOINC), domain-specific vocabularies, and community-developed
    ontologies. BioPortal offers rich functionality including: ontology browsing with
    hierarchical visualization and concept lookup; powerful search across all ontologies
    with autocomplete and recommendations; mapping services for finding correspondences
    between ontologies; annotator services for identifying ontology concepts in free
    text; versioning and change tracking for ontology evolution; widgets and web services
    (REST APIs) for programmatic access; and community features for commenting, discussions,
    and ontology reviews. The platform serves as infrastructure for ontology developers
    (submission, hosting, versioning), data curators (annotation, mapping, validation),
    and application developers (APIs, widgets, SPARQL endpoints). BioPortal enables
    semantic annotation of biomedical data, cross-resource data integration through
    ontology mappings, and standardized vocabularies for clinical research, genomics,
    drug discovery, and translational medicine. In AI/ML applications, BioPortal supports
    ontology-based feature engineering for predictive models, semantic search for literature
    and dataset discovery, text mining and NLP by providing concept recognizers, knowledge
    graph construction by linking data to standardized ontologies, and explainable
    AI through ontological reasoning and structured domain knowledge integration.'
  requires_registration: false
  url: https://bioportal.bioontology.org/
- id: B2AI_STANDARD:707
  category: B2AI_STANDARD:Registry
  collection:
  - standardsregistry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Bioregistry
  formal_specification: https://github.com/biopragmatics/bioregistry
  is_open: true
  name: Bioregistry
  publication: doi:10.1101/2022.07.08.499378
  purpose_detail: The Bioregistry is an open-source, community-curated registry and
    meta-registry that catalogs prefixes, identifier formats, and metadata for biomedical
    ontologies, databases, and controlled vocabularies. It integrates and harmonizes
    information from multiple registries (e.g., OBO Foundry, Identifiers.org, OLS),
    providing a unified resource for resolving compact URIs (CURIEs) and supporting
    semantic interoperability. The Bioregistry also functions as a resolver, mapping
    CURIEs to web resources, and is governed by transparent contribution and review
    processes. It is widely used for data integration, annotation, and knowledge graph
    construction in the life sciences.
  requires_registration: false
  url: https://bioregistry.io/
- id: B2AI_STANDARD:708
  category: B2AI_STANDARD:Registry
  collection:
  - standardsregistry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Bridge to Artificial Intelligence Registry
  formal_specification: https://github.com/bridge2ai/b2ai-standards-registry
  is_open: true
  name: Bridge2AI registry
  purpose_detail: Standards, tools, reference implementations, and related resources.
  requires_registration: false
  url: https://github.com/bridge2ai/b2ai-standards-registry
- id: B2AI_STANDARD:709
  category: B2AI_STANDARD:Registry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDISC Shared Health And Research Electronic library
  has_relevant_organization:
  - B2AI_ORG:15
  is_open: true
  name: CDISC SHARE
  publication: PUBMED:29888049
  purpose_detail: CDISC launched the CDISC Shared Health And Research Electronic library
    (SHARE) to provide the standards metadata in machine-readable formats to facilitate
    the automated management and implementation of the standards.
  requires_registration: true
  url: https://www.cdisc.org/faq/share/what-cdisc-share
- id: B2AI_STANDARD:710
  category: B2AI_STANDARD:Registry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Database Commons
  is_open: true
  name: Database Commons
  publication: doi:10.1016/j.gpb.2022.12.004
  purpose_detail: A catalog of worldwide biological databases maintained by the China
    National Center for Bioinformation,
  requires_registration: false
  url: https://ngdc.cncb.ac.cn/databasecommons/
- id: B2AI_STANDARD:711
  category: B2AI_STANDARD:Registry
  collection:
  - softwareregistry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Dockstore
  formal_specification: https://github.com/dockstore/dockstore
  has_relevant_organization:
  - B2AI_ORG:34
  is_open: true
  name: Dockstore
  purpose_detail: Dockstore is a comprehensive, open-source platform developed by
    the Ontario Institute for Cancer Research (OICR) and other collaborators that
    serves as a central registry for sharing, discovering, and executing containerized
    bioinformatics tools and computational workflows. Built on modern container technologies
    including Docker and Singularity, Dockstore enables researchers to package their
    analytical pipelines with all dependencies and configurations, ensuring reproducibility
    across different computing environments. The platform supports multiple workflow
    languages including Common Workflow Language (CWL), Workflow Description Language
    (WDL), and Nextflow, providing flexibility for diverse computational approaches
    in genomics, proteomics, and systems biology. Dockstore integrates with popular
    code repositories like GitHub, GitLab, and Bitbucket, enabling version-controlled
    development and automated testing of computational tools. The platform facilitates
    scientific collaboration by allowing researchers to discover validated, ready-to-use
    analytical workflows, reducing duplication of effort and accelerating research
    discovery. With built-in execution engines and cloud integration capabilities,
    Dockstore supports scalable workflow execution on local clusters, cloud platforms,
    and high-performance computing systems, making advanced bioinformatics accessible
    to researchers regardless of their computational expertise.
  requires_registration: false
  url: https://dockstore.org/
- id: B2AI_STANDARD:712
  category: B2AI_STANDARD:Registry
  collection:
  - standardsregistry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Fairsharing
  formal_specification: https://github.com/FAIRsharing
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:9
  is_open: true
  name: Fairsharing
  purpose_detail: 'FAIRsharing (fairsharing.org) is a comprehensive, curated registry of research data standards, databases, repositories, and policies that promotes Findable, Accessible, Interoperable, and Reusable (FAIR) data practices across life sciences, environmental sciences, social sciences, and humanities domains. Established by the UK BBSRC, ELIXIR, and Oxford e-Research Centre, FAIRsharing catalogs 2,000+ data standards (ontologies, terminologies, formats, models), 1,800+ databases and repositories (discipline-specific archives, institutional repositories, generalist repositories), and 800+ data policies from funders, journals, and institutions, providing a one-stop resource for discovering community-endorsed resources that facilitate data reuse and reproducibility. Each FAIRsharing record includes structured metadata describing the resource''s scope, governance (community-driven, institutional, commercial), licenses, interoperability capabilities, and relationships to other standards/databases, with persistent identifiers (FAIRsharing DOIs) enabling stable citations in data management plans and publications. The registry employs a domain-tagging system covering biomedical sciences (genomics, proteomics, metabolomics), environmental sciences (climate, biodiversity, geosciences), social sciences, and interdisciplinary fields, with cross-links to related resources illustrating standard-database-policy connections within research ecosystems. FAIRsharing supports researchers in selecting appropriate standards for data annotation (MIAME for microarrays, STROBE for epidemiology) and repositories for data deposition (GEO for gene expression, PDB for protein structures), facilitating compliance with journal and funder mandates for open data sharing. The platform integrates with research infrastructure projects (ELIXIR, RDA, CODATA) and provides APIs for embedding FAIRsharing recommendations into data management planning tools, electronic lab notebooks, and repository submission interfaces. For data stewards, FAIRsharing enables discovery of domain-specific controlled vocabularies (ontologies, taxonomies) that enhance dataset interoperability, semantic search capabilities, and cross-study integration in meta-analyses. In AI/ML research, FAIRsharing guides selection of training data repositories with rich metadata, standardized formats that minimize preprocessing overhead, and permissive licenses enabling model training, while the registry''s coverage of ML-specific resources (model registries, benchmark datasets, evaluation metrics) supports reproducible AI research aligned with FAIR principles for algorithms and models (FAIR4ML).'
  requires_registration: false
  url: https://fairsharing.org/
- id: B2AI_STANDARD:713
  category: B2AI_STANDARD:DataStandardOrTool
  collection:
  - standardsregistry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: LinkML schema registry
  formal_specification: https://github.com/linkml/linkml-registry/
  is_open: true
  name: LinkML registry
  purpose_detail: schemas and machine-actionable standards
  requires_registration: false
  url: https://linkml.io/linkml-registry/registry/
- id: B2AI_STANDARD:714
  category: B2AI_STANDARD:Registry
  collection:
  - standardsregistry
  concerns_data_topic:
  - B2AI_TOPIC:4
  - B2AI_TOPIC:7
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: National Cancer Institute Cancer Data Standards Repository
  formal_specification: https://cdebrowser.nci.nih.gov/cdebrowserClient/cdeBrowser.html#/search
  has_relevant_organization:
  - B2AI_ORG:71
  is_open: true
  name: NCI caDSR
  purpose_detail: Registry and repository for oncology research common data elements
    and forms.
  requires_registration: false
  url: https://datascience.cancer.gov/resources/metadata
- id: B2AI_STANDARD:715
  category: B2AI_STANDARD:Registry
  collection:
  - dataregistry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: National Radiology Data Registry
  is_open: false
  name: NRDR
  publication: doi:10.1016/j.jacr.2011.05.014
  purpose_detail: The primary purpose of NRDR is to aid facilities with their quality
    improvement programs and efforts to improve patient care by comparing facility
    data to that of their region and the nation. A practice or facility may choose
    to participate in any or all registries as appropriate for their practice. When
    a facility joins more than one registry, the warehouse allows information to be
    shared across registries within the facility.
  requires_registration: true
  url: https://nrdr.acr.org/Portal/Nrdr/Main/page.aspx
- id: B2AI_STANDARD:716
  category: B2AI_STANDARD:Registry
  collection:
  - ontologyregistry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Open Biological and Biomedical Ontology Foundry
  has_relevant_organization:
  - B2AI_ORG:75
  is_open: true
  name: OBO Foundry
  publication: doi:10.1093/database/baab069
  purpose_detail: The OBO Foundry is a collaborative community initiative for developing
    interoperable ontologies for the biological and biomedical sciences. It establishes
    and maintains a set of principles for ontology development to ensure quality,
    consistency, and interoperability across its library of domain ontologies. The
    OBO Foundry principles address key requirements including open availability, common
    syntax and semantics, clearly defined scope and content, use of well-documented
    collaborative procedures, orthogonality with other ontologies, provision of unique
    identifiers, and adherence to established naming conventions. The Foundry provides
    comprehensive community resources including the OBO tutorial, ontology browsers
    and tools, operations committees and working groups, and communication channels
    (mailing list, Slack workspace). The OBO Library registry provides standardized
    access to member ontologies in multiple formats (YAML, JSON-LD, RDF/Turtle) and
    includes domain-specific ontologies covering anatomy (UBERON), cell types (CL),
    diseases (MONDO), chemicals (ChEBI), genotypes (GENO), phenotypes (HPO), and many
    others. The OBO Foundry infrastructure includes consistent use of permanent URLs
    (PURLs), version control via GitHub, automated quality checks, and standardized
    release workflows. By ensuring semantic interoperability through shared upper-level
    ontologies and design patterns, OBO Foundry enables cross-domain data integration
    essential for AI/ML applications in biomedical knowledge graphs, automated reasoning,
    data harmonization, and multi-modal machine learning across diverse biological
    datasets.
  requires_registration: false
  url: https://obofoundry.org/
- id: B2AI_STANDARD:717
  category: B2AI_STANDARD:Registry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Papers With Code
  formal_specification: https://github.com/paperswithcode
  is_open: true
  name: PwC
  purpose_detail: A free and open resource with Machine Learning papers, code, datasets,
    methods and evaluation tables.
  requires_registration: false
  url: https://paperswithcode.com/
- id: B2AI_STANDARD:718
  category: B2AI_STANDARD:DataStandardOrTool
  collection:
  - dataregistry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Registry of Research Data Repositories
  is_open: true
  name: r3data
  publication: doi:10.5281/zenodo.6697943
  purpose_detail: re3data is a global registry of research data repositories. The
    registry covers research data repositories from different academic disciplines.
    re3data presents repositories for the permanent storage and access of data sets
    to researchers, funding bodies, publishers and scholarly institutions. re3data
    aims to promote a culture of sharing, increased access and better visibility of
    research data.
  requires_registration: false
  url: https://www.re3data.org/
- id: B2AI_STANDARD:719
  category: B2AI_STANDARD:Registry
  collection:
  - dataregistry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: RSNA-ACR 3D Printing Registry
  has_relevant_organization:
  - B2AI_ORG:85
  is_open: false
  name: 3DP Registry
  purpose_detail: The joint RSNA and American College of Radiology (ACR) 3D printing
    clinical data registry collects 3D printing data at the point of clinical care.
    With the goal of improving both patient care and characterizing resource utilization,
    the brand-new registry collects anonymized 3D printing case information, clinical
    indications and intended uses for printed models, source imaging, model construction
    techniques and effort, 3D printing techniques and effort, and the clinical impact
    of the models.
  requires_registration: true
  url: https://www.rsna.org/practice-tools/RSNA-ACR-3D-printing-registry
- id: B2AI_STANDARD:720
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: A corpus-driven standardization framework for encoding clinical problems
    with HL7 FHIR
  formal_specification: https://github.com/OHNLP/clinical-problem-standardization
  has_relevant_organization:
  - B2AI_ORG:40
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: clinical-problem-standardization
  publication: doi:10.1016/j.jbi.2020.103541
  purpose_detail: A framework for transforming free-text problem descriptions into
    standardized Health Level 7 (HL7) Fast Healthcare Interoperability Resources (FHIR)
    models.
  related_to:
  - B2AI_STANDARD:109
  requires_registration: false
  url: https://github.com/OHNLP/clinical-problem-standardization
- id: B2AI_STANDARD:721
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: A Research Exploration System
  formal_specification: https://github.com/OHDSI/Ares
  has_relevant_organization:
  - B2AI_ORG:76
  is_open: true
  name: ARES
  purpose_detail: A Research Exploration System designed to improved the transparency
    of observational data research. ARES is an opinionated framework that delineates
    three levels of observational data assessment.
  requires_registration: false
  url: https://github.com/OHDSI/Ares
- id: B2AI_STANDARD:722
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - machinelearningframework
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Aesara
  formal_specification: https://github.com/aesara-devs/aesara
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:1
  is_open: true
  name: Aesara
  purpose_detail: Aesara is a Python library that allows one to define, optimize,
    and efficiently evaluate mathematical expressions involving multi-dimensional
    arrays.
  requires_registration: false
  url: https://github.com/aesara-devs/aesara
  has_application:
  - id: B2AI_APP:63
    category: B2AI:Application
    name: Probabilistic Modeling and Bayesian Inference for Biomedicine
    description: Aesara (successor to Theano) is used in biomedical AI for implementing
      probabilistic graphical models, Bayesian neural networks, and statistical inference
      algorithms that quantify uncertainty in clinical predictions. Researchers leverage
      Aesara's symbolic math capabilities and automatic differentiation to build sophisticated
      probabilistic models for tasks like uncertainty-aware disease risk prediction,
      Bayesian optimization of drug dosing regimens, and hierarchical models for multi-center
      clinical studies. The library enables AI applications that provide calibrated
      confidence intervals for predictions, perform approximate Bayesian inference
      through variational methods, and integrate domain knowledge through informative
      priors. Aesara is particularly valuable when prediction uncertainty quantification
      is critical for clinical decision-making.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:723
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - cloudservice
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Amazon Web Services
  is_open: false
  name: AWS
  purpose_detail: 'Amazon Web Services (AWS) is the world''s most comprehensive and broadly adopted cloud computing platform, providing 200+ fully managed services spanning compute (EC2, Lambda serverless), storage (S3 object storage, EBS block storage, EFS file systems), databases (RDS, DynamoDB, Redshift), networking (VPC, CloudFront CDN), machine learning (SageMaker, Bedrock, Rekognition), analytics (EMR, Athena, Kinesis), security (IAM, KMS, CloudHSM), and application integration (SQS, SNS, EventBridge) across 38 geographic regions with 120 availability zones globally. Launched in 2006, AWS pioneered the Infrastructure-as-a-Service (IaaS) model, transforming IT economics by offering pay-as-you-go pricing, elastic scalability, and eliminating upfront capital expenditures for hardware, enabling organizations from startups to enterprises to access enterprise-grade infrastructure previously available only to large corporations. AWS''s compute offerings range from general-purpose EC2 instances for traditional workloads to specialized instance types optimized for memory-intensive applications (R-series), compute-intensive HPC (C-series, Hpc7a), GPU-accelerated deep learning (P5, G5, Trn1 with AWS Trainium/Inferentia chips), and serverless Lambda functions billed per millisecond execution time. The platform''s storage hierarchy includes S3 for durable object storage with 99.999999999% durability, S3 Glacier for archival with retrieval SLAs from minutes to hours, EBS for low-latency block storage attached to EC2 instances, and EFS for scalable shared file systems supporting concurrent access across thousands of compute nodes. AWS''s managed database services eliminate operational overhead of patching, backups, and replication, offering relational databases (Amazon RDS for MySQL, PostgreSQL, Oracle, SQL Server; Amazon Aurora with MySQL/PostgreSQL compatibility), NoSQL databases (DynamoDB for key-value, DocumentDB for MongoDB workloads), graph databases (Neptune), time-series databases (Timestream), and data warehouses (Redshift) with columnar storage for OLAP queries on petabyte-scale datasets. For AI/ML workloads, AWS SageMaker provides end-to-end ML lifecycle management with built-in algorithms, Jupyter notebooks, distributed training across GPU clusters, hyperparameter tuning, model deployment to real-time/batch inference endpoints, and MLOps capabilities (model registry, monitoring, CI/CD pipelines). AWS Bedrock offers access to foundation models from AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon Titan via unified API with retrieval-augmented generation (RAG) capabilities connecting models to enterprise data in S3 or knowledge bases. The platform''s security model includes IAM for fine-grained access control with role-based policies, KMS for encryption key management, CloudHSM for FIPS 140-2 Level 3 hardware security modules, AWS Shield for DDoS protection, GuardDuty for threat detection, and comprehensive compliance certifications (SOC 1/2/3, PCI-DSS, HIPAA, FedRAMP, ISO 27001) enabling deployment of regulated workloads. AWS supports hybrid cloud architectures through AWS Outposts (on-premises hardware running AWS services), AWS Direct Connect (dedicated network connections bypassing public internet), and Storage Gateway (seamless cloud-extension of on-premises storage), enabling gradual cloud migrations and latency-sensitive edge computing scenarios. For AI/ML infrastructure in Bridge2AI and biomedical research, AWS provides scalable compute for genome sequencing analysis (AWS Batch, EC2 Spot instances), secure storage for HIPAA-compliant patient data (S3 with server-side encryption, VPC endpoints preventing internet exposure), managed databases for clinical trial data (RDS with automated backups, multi-AZ high availability), federated learning frameworks (AWS HealthLake for FHIR-based interoperability), and specialized AI services for medical imaging analysis (AWS HealthImaging, Amazon Rekognition Custom Labels), accelerating research workflows while maintaining data privacy, auditability, and compliance with healthcare regulations.'
  requires_registration: true
  url: https://aws.amazon.com/
  used_in_bridge2ai: true
- id: B2AI_STANDARD:724
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Amundsen
  formal_specification: https://github.com/amundsen-io/amundsen
  is_open: true
  name: Amundsen
  purpose_detail: Amundsen is a data discovery and metadata engine for improving the
    productivity of data analysts, data scientists and engineers when interacting
    with data. It does that today by indexing data resources (tables, dashboards,
    streams, etc.) and powering a page-rank style search based on usage patterns (e.g.
    highly queried tables show up earlier than less queried tables).
  requires_registration: false
  url: https://www.amundsen.io/
- id: B2AI_STANDARD:725
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - toolkit
  concerns_data_topic:
  - B2AI_TOPIC:20
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Anduril
  formal_specification: https://bitbucket.org/anduril-dev/anduril/src/stable/
  is_open: true
  name: Anduril
  publication: doi:10.1093/bioinformatics/btz133
  purpose_detail: Anduril is a workflow platform for analyzing large data sets. Anduril
    provides facilities for analyzing high-thoughput data in biomedical research,
    and the platform is fully extensible by third parties.
  requires_registration: false
  url: https://anduril.org/site/
- id: B2AI_STANDARD:726
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Apache Atlas
  formal_specification: https://github.com/apache/atlas
  has_relevant_organization:
  - B2AI_ORG:5
  is_open: true
  name: Apache Atlas
  purpose_detail: Apache Atlas provides open metadata management and governance capabilities
    for organizations to build a catalog of their data assets, classify and govern
    these assets and provide collaboration capabilities around these data assets for
    data scientists, analysts and the data governance team.
  requires_registration: false
  url: https://atlas.apache.org/
- id: B2AI_STANDARD:727
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Apache Spark
  formal_specification: https://github.com/apache/spark
  has_relevant_organization:
  - B2AI_ORG:5
  is_open: true
  name: Spark
  purpose_detail: Apache Spark is a unified analytics engine for large-scale data
    processing. It provides high-level APIs in Java, Scala, Python and R, and an optimized
    engine that supports general execution graphs.
  requires_registration: false
  url: https://spark.apache.org/docs/latest/index.html
- id: B2AI_STANDARD:728
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - deprecated
  - workflowlanguage
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Apache Taverna
  is_open: true
  name: Taverna
  purpose_detail: Taverna is a domain-independent suite of tools used to design and
    execute data-driven workflows.
  requires_registration: false
  url: https://incubator.apache.org/projects/taverna.html
- id: B2AI_STANDARD:729
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - datavisualization
  - notebookplatform
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Appyters
  formal_specification: https://github.com/MaayanLab/appyter-catalog
  is_open: true
  name: Appyters
  publication: doi:10.1016/j.patter.2021.100213
  purpose_detail: Appyters extend the Jupyter Notebook language to support external,
    end-user configurable variables. Appyters can be considered a meta Jupyter Notebook
    language that is compatible with standard Jupyter Notebook execution.
  requires_registration: false
  url: https://appyters.maayanlab.cloud/
- id: B2AI_STANDARD:730
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ATHENA
  formal_specification: https://github.com/OHDSI/Athena
  has_relevant_organization:
  - B2AI_ORG:76
  has_training_resource:
  - B2AI_STANDARD:844
  is_open: true
  name: ATHENA
  purpose_detail: A resource of searchable and loadable standardized vocabularies.
  requires_registration: false
  url: https://athena.ohdsi.org/search-terms/terms
  used_in_bridge2ai: true
- id: B2AI_STANDARD:731
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ATLAS
  formal_specification: https://github.com/OHDSI/Atlas
  has_relevant_organization:
  - B2AI_ORG:76
  has_training_resource:
  - B2AI_STANDARD:844
  is_open: true
  name: ATLAS
  purpose_detail: An open source software tool for researchers to conduct scientific
    analyses on standardized observational data converted to the OMOP Common Data
    Model V5. Researchers can create cohorts by defining groups of people based on
    an exposure to a drug or diagnosis of a particular condition using healthcare
    claims data.
  requires_registration: false
  url: https://atlas-demo.ohdsi.org/#/home
- id: B2AI_STANDARD:732
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:4
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: AtriumDB
  has_relevant_organization:
  - B2AI_ORG:117
  is_open: false
  name: AtriumDB
  publication: doi:10.1088/1361-6579/ab7cb5
  purpose_detail: A database of continuously-recorded physiological waveform data
    and other associated clinical and medical device data. Also the platform for storage
    and retrieval of clinical waveform data.
  requires_registration: true
  url: https://laussenlabs.ca/atriumdb/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:64
    category: B2AI:Application
    name: High-Frequency Physiological Waveform Analysis
    description: AtriumDB is used in AI applications for managing and analyzing high-frequency
      physiological waveform data at scale, enabling deep learning models for intensive
      care monitoring, perioperative risk prediction, and continuous vital sign analysis.
      The platform's efficient storage and query capabilities support training of
      neural networks on bedside monitor data including ECG, arterial blood pressure,
      and other high-resolution physiological signals collected over extended periods.
      AI systems leverage AtriumDB to access synchronized multi-parameter waveforms
      for developing early warning systems, detecting subtle physiological deterioration,
      and predicting adverse events in critically ill patients. The database's time-series
      optimization enables real-time AI inference on streaming waveform data.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:733
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Automated Characterization of Health Information at Large-scale Longitudinal
    Evidence System
  formal_specification: https://github.com/OHDSI/Achilles
  has_relevant_organization:
  - B2AI_ORG:76
  has_training_resource:
  - B2AI_STANDARD:844
  is_open: true
  name: ACHILLES
  purpose_detail: Provides descriptive statistics on an OMOP CDM database.
  related_to:
  - B2AI_STANDARD:243
  requires_registration: false
  url: https://ohdsi.github.io/TheBookOfOhdsi/DataQuality.html#data-quality-checks
- id: B2AI_STANDARD:734
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: balance package
  formal_specification: https://github.com/facebookresearch/balance
  has_relevant_organization:
  - B2AI_ORG:55
  is_open: true
  name: balance
  purpose_detail: A Python package for balancing biased data samples.
  requires_registration: false
  url: https://import-balance.org/
- id: B2AI_STANDARD:735
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: BigQuery
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:4
  has_relevant_organization:
  - B2AI_ORG:37
  is_open: false
  name: BigQuery
  purpose_detail: A fully managed, serverless data warehouse that enables scalable
    analysis over petabytes of data. It is a Platform as a Service (PaaS) that supports
    querying using ANSI SQL.
  requires_registration: true
  url: https://cloud.google.com/bigquery
- id: B2AI_STANDARD:736
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - toolkit
  concerns_data_topic:
  - B2AI_TOPIC:1
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Biopython
  formal_specification: https://github.com/biopython/biopython
  is_open: true
  name: Biopython
  purpose_detail: Biopython is a set of freely available tools for biological computation
    written in Python by an international team of developers. It is a distributed
    collaborative effort to develop Python libraries and applications which address
    the needs of current and future work in bioinformatics. The source code is made
    available under the Biopython License, which is extremely liberal and compatible
    with almost every license in the world.
  requires_registration: false
  url: https://biopython.org/
- id: B2AI_STANDARD:737
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - toolkit
  concerns_data_topic:
  - B2AI_TOPIC:20
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Biotite
  formal_specification: https://github.com/biotite-dev/biotite
  is_open: true
  name: Biotite
  purpose_detail: This package bundles popular tasks in computational molecular biology
    into a uniform Python library.
  requires_registration: false
  url: https://www.biotite-python.org/
- id: B2AI_STANDARD:738
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:22
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: BrainStat
  formal_specification: https://github.com/MICA-MNI/BrainStat
  is_open: true
  name: BrainStat
  publication: doi:10.1016/j.neuroimage.2022.119807
  purpose_detail: A toolbox for the statistical analysis and context decoding of neuroimaging
    data. It implements both univariate and multivariate linear models and interfaces
    with the BigBrain Atlas, Allen Human Brain Atlas and Nimare databases.
  requires_registration: false
  url: https://brainstat.readthedocs.io/
- id: B2AI_STANDARD:739
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Cavatica data analysis platform
  has_relevant_organization:
  - B2AI_ORG:1
  is_open: false
  name: Cavatica
  purpose_detail: A data analysis and sharing platform designed to accelerate discovery
    in a scalable, cloud-based compute environment where data, results, and workflows
    are shared among the world's research community. Developed by Seven Bridges and
    funded in-part by a grant from the National Institutes of Health (NIH) Common
    Fund, CAVATICA is continuously updated with new tools and datasets.
  requires_registration: true
  url: https://www.cavatica.org/
- id: B2AI_STANDARD:740
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Celery - Distributed Task Queue
  formal_specification: https://github.com/celery/celery
  is_open: true
  name: Celery
  purpose_detail: Celery is a simple, flexible, and reliable distributed system to
    process vast amounts of messages, while providing operations with the tools required
    to maintain such a system. Its a task queue with focus on real-time processing,
    while also supporting task scheduling.
  requires_registration: false
  url: https://docs.celeryq.dev/en/latest/
- id: B2AI_STANDARD:741
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Clique eXtracted Ontology
  formal_specification: https://github.com/fanzheng10/CliXO-1.0
  is_open: true
  name: CLiXO
  publication: doi:10.1126/science.abf3067
  purpose_detail: An updated version of the CliXO (Clique eXtracted Ontology) algorithm
    for inferring gene ontology terms from pairwise gene similarity data.
  requires_registration: false
  url: https://github.com/fanzheng10/CliXO-1.0
- id: B2AI_STANDARD:742
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Comet
  formal_specification: https://github.com/comet-ml
  is_open: false
  name: Comet
  purpose_detail: 'Comet is a cloud-based and self-hosted machine learning experimentation
    platform that provides comprehensive experiment tracking, model management, hyperparameter
    optimization, and collaboration capabilities for data science teams working across
    the ML lifecycle. The platform automatically logs and organizes key experimental
    artifacts including hyperparameters, metrics (training/validation loss, accuracy,
    AUC, custom metrics), system metrics (CPU/GPU utilization, memory consumption,
    network I/O), code versions (Git commits, diffs, dependencies), environment configurations
    (Python packages, CUDA versions, Docker images), datasets (checksums, versioning,
    lineage), model checkpoints, confusion matrices, feature importance plots, and
    interactive visualizations (embedding projections, activation maps, attention
    heatmaps). Comet integrates seamlessly with popular ML frameworks through lightweight
    SDKs (Python, R, Java, JavaScript) supporting TensorFlow, PyTorch, Keras, scikit-learn,
    XGBoost, LightGBM, Hugging Face Transformers, JAX, and others via framework-specific
    auto-logging hooks and manual logging APIs. The platform''s experiment comparison
    interface enables side-by-side evaluation of model runs with interactive diff
    views, parallel coordinate plots for hyperparameter analysis, statistical significance
    testing, and customizable dashboards for real-time monitoring of distributed training
    jobs. Model registry functionality provides versioning, staging workflows (development,
    staging, production), A/B testing support, model cards with metadata documentation
    (intended use, training data characteristics, performance metrics, ethical considerations,
    limitations), and integration with deployment platforms (AWS SageMaker, Google
    Vertex AI, Azure ML, Kubernetes). Comet Reports enable reproducible, shareable
    documentation of ML projects with embedded live metrics, interactive charts, markdown
    narratives, and code snippets, facilitating collaboration between researchers,
    stakeholders, and regulatory reviewers. For biomedical and healthcare AI applications,
    Comet supports compliance requirements through audit trails capturing complete
    experimental provenance (who trained which model, when, with what data, achieving
    what performance), role-based access control for PHI-containing experiments, and
    integration with data governance frameworks, while enabling meta-analyses across
    studies by standardizing metric reporting, facilitating transfer learning through
    organized model artifact repositories, supporting federated learning workflows
    by tracking distributed experiments across institutions, and accelerating reproducibility
    in clinical ML research through comprehensive environment and dependency tracking
    aligned with journal requirements and regulatory submission standards (FDA premarket
    approval, EU AI Act).'
  requires_registration: true
  url: https://www.comet.com/
- id: B2AI_STANDARD:743
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Community Detection APplication and Service
  formal_specification: https://github.com/cytoscape/cy-community-detection
  is_open: true
  name: CDAPS
  publication: doi:10.1371/journal.pcbi.1008239
  purpose_detail: Performs multiscale community detection and functional enrichment
    for network analysis through a service-oriented architecture. These features are
    provided by integrating popular community detection algorithms and enrichment
    tools. All the algorithms and tools run remotely on a dedicated server.
  requires_registration: false
  url: https://cdaps.readthedocs.io/
- id: B2AI_STANDARD:744
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:12
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Community Detection CliXO
  formal_specification: https://github.com/idekerlab/cdclixo
  is_open: true
  name: CD-CLiXO
  purpose_detail: 'CD-CLiXO (Community Detection - Clique eXtraction using Overlap)
    is a containerized implementation of the CliXO algorithm packaged as a Docker
    image compatible with the Community Detection Analysis Pipeline System (CDAPS),
    enabling scalable, reproducible hierarchical community detection in large-scale
    biological and molecular networks. CliXO employs a parameter-free, ontology-based
    approach to identify overlapping community structures by first detecting maximal
    cliques (fully connected subgraphs) as seed communities, then merging cliques
    based on shared membership and topological overlap to build a hierarchical organization
    of nested communities at multiple scales. The algorithm produces a Directed Acyclic
    Graph (DAG) representation where nodes represent communities, edges indicate parent-child
    hierarchical relationships, and each community is annotated with statistical significance
    scores, member identities, and functional enrichment results when integrated with
    Gene Ontology, pathway databases, or disease ontologies. CD-CLiXO is particularly
    effective for protein-protein interaction networks, gene co-expression networks,
    metabolic networks, and disease-gene association networks where communities correspond
    to functional modules, protein complexes, biological pathways, or disease mechanisms.
    The CDAPS-compatible Docker containerization ensures computational reproducibility
    by encapsulating all dependencies (Python libraries, network analysis tools, statistical
    packages), enables cloud-based execution on AWS, Google Cloud, or HPC clusters
    via Singularity, and facilitates integration into automated analysis pipelines
    with standardized input/output formats (edge lists, node attributes, hierarchical
    JSON/CX network exchange). For AI and machine learning applications, CD-CLiXO-derived
    community structures serve as biologically informed features for supervised learning
    tasks (predicting gene function, disease associations, drug targets), reduce dimensionality
    of high-dimensional omics data by aggregating measurements at the community level
    (module eigengene approaches), provide structured priors for graph neural networks
    (GNNs) by encoding hierarchical inductive biases, enable transfer learning across
    networks by aligning conserved community structures between species or experimental
    conditions, support network-based interpretability by mapping deep learning predictions
    to known functional modules, and facilitate active learning by prioritizing experimental
    validation of genes in communities with high prediction uncertainty, ultimately
    bridging data-driven network analysis with mechanistic biological knowledge in
    systems biology and precision medicine workflows.'
  requires_registration: false
  url: https://github.com/idekerlab/cdclixo
- id: B2AI_STANDARD:745
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Contextual Ontology-based Repository Analysis Library
  formal_specification: https://github.com/jmchandonia/CORAL
  is_open: true
  name: CORAL
  publication: doi:10.1093/gigascience/giac089
  purpose_detail: A framework for rigorous self-validated data modeling and integrative,
    reproducible data analysis
  requires_registration: false
  url: https://github.com/jmchandonia/CORAL
- id: B2AI_STANDARD:746
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Continuous Machine Learning
  formal_specification: https://github.com/iterative/cml
  is_open: true
  name: CML library
  purpose_detail: Continuous Machine Learning (CML) is an open-source library for
    implementing continuous integration & delivery (CI/CD) in machine learning projects.
    Use it to automate parts of your development workflow, including model training
    and evaluation, comparing ML experiments across your project history, and monitoring
    changing datasets.
  requires_registration: false
  url: https://cml.dev/
- id: B2AI_STANDARD:747
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Cromwell Workflow Management System
  formal_specification: https://github.com/broadinstitute/cromwell
  is_open: true
  name: Cromwell
  publication: doi:10.7490/f1000research.1114634.1
  purpose_detail: Cromwell is an open-source Workflow Management System for bioinformatics.
  requires_registration: false
  url: https://cromwell.readthedocs.io/en/stable/
- id: B2AI_STANDARD:748
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Cytoscape software
  formal_specification: https://github.com/cytoscape/cytoscape
  is_open: true
  name: Cytoscape
  purpose_detail: An open source software platform for visualizing complex networks
    and integrating these with any type of attribute data.
  requires_registration: false
  url: https://cytoscape.org/
- id: B2AI_STANDARD:749
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Dagster
  formal_specification: https://github.com/dagster-io/dagster
  is_open: false
  name: Dagster
  purpose_detail: Open source orchestration platform for the development, production,
    and observation of data assets.
  requires_registration: true
  url: https://dagster.io/
- id: B2AI_STANDARD:750
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - scrnaseqanalysis
  concerns_data_topic:
  - B2AI_TOPIC:34
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DANCE platform
  formal_specification: https://github.com/OmicsML/dance
  is_open: true
  name: DANCE
  purpose_detail: A Python toolkit to support deep learning models for analyzing single-cell
    gene expression at scale.
  requires_registration: false
  url: https://omicsml.ai/
- id: B2AI_STANDARD:751
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Data Repository Service
  has_relevant_organization:
  - B2AI_ORG:34
  is_open: true
  name: DRS
  purpose_detail: The Data Repository Service (DRS) is a standard for describing and
    accessing data objects in a data repository. The DRS provides a RESTful API for
    accessing data objects, as well as a set of metadata fields for describing the
    data objects. The DRS is designed to be used in conjunction with the GA4GH Data
    Repository Service API, which provides a standard way to access data objects in
    a data repository.
  requires_registration: false
  url: https://ga4gh.github.io/data-repository-service-schemas/preview/release/drs-1.2.0/docs/
- id: B2AI_STANDARD:752
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Data Version Control
  formal_specification: https://github.com/iterative/dvc
  is_open: true
  name: DVC
  purpose_detail: Data Version Control (DVC) is an open-source version control system
    for data science and machine learning projects. It is designed to help data scientists
    and machine learning engineers manage their data, models, and experiments in a
    reproducible and collaborative way.
  requires_registration: false
  url: https://dvc.org/
- id: B2AI_STANDARD:753
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DataHub
  formal_specification: https://github.com/linkedin/datahub
  is_open: true
  name: DataHub
  purpose_detail: DataHub is a metadata platform. It provides a central place to manage
    and discover data assets, including datasets, dashboards, and machine learning
    models. DataHub is designed to be extensible and customizable.
  requires_registration: false
  url: https://datahubproject.io/
- id: B2AI_STANDARD:754
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Datasette
  formal_specification: https://github.com/simonw/datasette
  is_open: true
  name: Datasette
  purpose_detail: 'Datasette is a comprehensive open-source tool ecosystem designed
    for exploring, analyzing, and publishing data through an intuitive web interface
    and accompanying JSON API. Created by Simon Willison, Datasette transforms data
    of any shape into interactive websites that enable users to perform exploratory
    data analysis, create visualizations, and share findings with colleagues without
    requiring extensive technical expertise. The platform excels in three primary
    use cases: exploratory data analysis (automatically detecting patterns in CSV,
    JSON, and database data), instant data publishing (using the `datasette publish`
    command to deploy data to cloud hosting providers like Google Cloud Run, Heroku,
    or Vercel), and rapid prototyping (spinning up JSON APIs in minutes for proof-of-concept
    development). Datasette is part of a broader ecosystem of 44 related tools and
    154 plugins that enhance productivity when working with structured data. The tool
    supports multiple data import formats, provides built-in security features for
    data access control, and offers extensive customization options through its plugin
    architecture. Datasette serves data journalists, museum curators, archivists,
    local governments, scientists, researchers, and anyone seeking to make their data
    more accessible and discoverable through web-based interfaces.'
  requires_registration: false
  url: https://datasette.io/
- id: B2AI_STANDARD:755
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - scrnaseqanalysis
  concerns_data_topic:
  - B2AI_TOPIC:34
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DENDRO
  formal_specification: https://github.com/zhouzilu/DENDRO
  is_open: true
  name: DENDRO
  publication: doi:10.1186/s13059-019-1922-x
  purpose_detail: An analysis method for scRNA-seq data that clusters single cells
    into genetically distinct subclones and reconstructs the phylogenetic tree relating
    the subclones.
  requires_registration: false
  url: https://github.com/zhouzilu/DENDRO
- id: B2AI_STANDARD:756
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - machinelearningframework
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Determined
  formal_specification: https://github.com/determined-ai/determined
  is_open: true
  name: Determined
  purpose_detail: Determined is an open-source deep learning training platform.
  requires_registration: false
  url: https://www.determined.ai/
  has_application:
  - id: B2AI_APP:66
    category: B2AI:Application
    name: Scalable ML Training Platform for Healthcare Research
    description: Determined is used in biomedical AI for managing large-scale deep
      learning experiments, hyperparameter tuning, and distributed training across
      GPU clusters in research and healthcare organizations. AI teams leverage Determined's
      automated experiment tracking, resource scheduling, and hyperparameter optimization
      to efficiently develop medical imaging models, genomic prediction algorithms,
      and clinical NLP systems. The platform provides reproducible experiment management
      with automatic checkpointing, fault tolerance for long-running medical imaging
      training jobs, and collaborative features for multi-institutional research teams.
      Determined's integration with healthcare security requirements and support for
      diverse model frameworks makes it valuable for organizations scaling from research
      to production clinical AI.
    used_in_bridge2ai: false
    references:
    - https://docs.determined.ai/
- id: B2AI_STANDARD:757
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Dicoogle Picture Archiving and Communications System
  formal_specification: https://github.com/bioinformatics-ua/dicoogle
  is_open: true
  name: Dicoogle
  publication: doi:10.1109/ISCC50000.2020.9219545
  purpose_detail: Dicoogle is an open source Picture Archiving and Communications
    System (PACS) archive. Its modular architecture allows the quick development of
    new functionalities, due the availability of a Software Development Kit (SDK).
  requires_registration: false
  url: https://dicoogle.com/
- id: B2AI_STANDARD:758
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - cloudservice
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DigitalOcean
  is_open: false
  name: DigitalOcean
  purpose_detail: DigitalOcean is a comprehensive cloud infrastructure platform that
    provides scalable computing resources designed to simplify cloud deployment for
    developers, startups, and enterprises. The platform offers a complete suite of
    cloud services including Droplets (virtual machines), Kubernetes clusters, App
    Platform for application deployment, managed databases, object storage (Spaces),
    load balancers, and specialized AI/ML infrastructure including GPU-powered Droplets
    with NVIDIA H100, H200, and AMD MI325X accelerators. DigitalOcean distinguishes
    itself through developer-friendly interfaces, predictable pricing without bandwidth
    overage charges, and extensive community-contributed tutorials and documentation.
    The platform serves over 600,000 customers ranging from individual developers
    to large enterprises, offering both self-service infrastructure management and
    managed services with premium support options. DigitalOcean's global data center
    network provides 99.99% uptime SLAs and supports diverse use cases including web
    hosting, application development, AI inference workloads, container orchestration,
    and data processing pipelines, making it particularly attractive for cost-conscious
    organizations seeking reliable cloud infrastructure without the complexity of
    larger cloud providers.
  requires_registration: true
  url: https://www.digitalocean.com/
- id: B2AI_STANDARD:759
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - scrnaseqanalysis
  concerns_data_topic:
  - B2AI_TOPIC:34
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DoubletDecon
  formal_specification: https://github.com/EDePasquale/DoubletDecon
  is_open: true
  name: DoubletDecon
  publication: doi:10.1016/j.celrep.2019.09.082
  purpose_detail: An approach that detects doublet cell capture artifacts in scRNA-seq
    data with a combination of deconvolution analyses and the identification of unique
    cell-state gene expression.
  requires_registration: false
  url: https://github.com/EDePasquale/DoubletDecon
- id: B2AI_STANDARD:760
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Egeria
  formal_specification: https://github.com/odpi/egeria
  is_open: true
  name: Egeria
  purpose_detail: Open metadata and governance for enterprises - automatically capturing,
    managing and exchanging metadata between tools and platforms, no matter the vendor.
  requires_registration: false
  url: https://egeria-project.org/
- id: B2AI_STANDARD:761
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Eido
  formal_specification: https://github.com/pepkit/eido
  is_open: true
  name: Eido
  publication: doi:10.1093/gigascience/giab077
  purpose_detail: Eido is used to 1) validate or 2) convert format of sample metadata.
    Sample metadata is stored according to the standard PEP specification. For validation,
    eido is based on JSON Schema and extends it with new features, like required input
    files.
  related_to:
  - B2AI_STANDARD:260
  - B2AI_STANDARD:345
  requires_registration: false
  url: http://eido.databio.org/
- id: B2AI_STANDARD:762
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - scrnaseqanalysis
  concerns_data_topic:
  - B2AI_TOPIC:34
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Expression denoising heuristic using aggregation of neighbors and principal
    component extraction
  formal_specification: https://github.com/yanailab/enhance
  is_open: true
  name: ENHANCE
  publication: doi:10.1101/655365
  purpose_detail: ENHANCE is a computational method for accurate denoising of single-cell
    RNA-sequencing data that addresses technical noise while preserving biological
    signal. The algorithm uses k-nearest neighbor aggregation to identify similar
    cells, followed by principal component analysis (PCA) to remove noise components
    while retaining significant biological variation. The method automatically determines
    the optimal number of neighbors (k) based on target transcript counts and identifies
    significant principal components using variance fold-thresholding against simulated
    noise datasets. ENHANCE processes UMI-count matrices in tab-separated format,
    supports gzip compression, and provides configurable parameters for neighbor selection,
    PC threshold determination, and precision control. The tool enhances downstream
    analyses such as clustering, trajectory inference, and differential expression
    by reducing technical artifacts while maintaining cell type distinctions and biological
    relationships in single-cell transcriptomic data.
  requires_registration: false
  url: https://github.com/yanailab/enhance
- id: B2AI_STANDARD:763
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FAIR Data Point
  formal_specification: https://github.com/FAIRDataTeam/FAIRDataPoint
  is_open: true
  name: FDP
  publication: doi:10.1162/dint_a_00160
  purpose_detail: FAIR Data Point (FDP) is a REST API for creating, storing, and serving
    FAIR metadata. This FDP implementation also presents a Web-based graphical user
    interface (GUI). The metadata contents are generated semi-automatically according
    to the FAIR Data Point software specification document.
  requires_registration: false
  url: https://github.com/FAIRDataTeam/FAIRDataPoint
- id: B2AI_STANDARD:764
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - implementation_maturity_pilot
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FAIRSCAPE digital commons framework
  formal_specification: https://github.com/fairscape/fairscape
  has_relevant_organization:
  - B2AI_ORG:116
  is_open: true
  name: FAIRSCAPE
  publication: doi:10.1007/s12021-021-09529-4
  purpose_detail: A reusable computational framework, enabling simplified access to
    modern scalable cloud-based components. FAIRSCAPE fully implements the FAIR data
    principles and extends them to provide fully FAIR Evidence, including machine-interpretable
    provenance of datasets, software and computations, as metadata for all computed
    results.
  requires_registration: false
  url: https://fairscape.github.io/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:67
    category: B2AI:Application
    name: FAIR ML Workflows and Computational Reproducibility
    description: FAIRSCAPE is used in AI applications to create Findable, Accessible,
      Interoperable, and Reusable machine learning workflows with comprehensive provenance
      tracking and metadata management. AI researchers leverage FAIRSCAPE to package
      complete ML pipelines including data preprocessing, model training, validation,
      and deployment with detailed computational provenance, enabling reproducible
      AI research and regulatory compliance. The framework supports automated metadata
      capture during model development, versioning of datasets and models, and generation
      of computational notebooks that document the entire ML lifecycle. FAIRSCAPE
      is particularly valuable for multi-institutional AI studies where workflow transparency,
      data lineage, and reproducibility are essential for scientific validity and
      clinical translation.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:765
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - machinelearningframework
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FastAI
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:33
  is_open: true
  name: FastAI
  purpose_detail: fastai is a deep learning library which provides practitioners with
    high-level components that can quickly and easily provide state-of-the-art results
    in standard deep learning domains, and provides researchers with low-level components
    that can be mixed and matched to build new approaches. It aims to do both things
    without substantial compromises in ease of use, flexibility, or performance. This
    is possible thanks to a carefully layered architecture, which expresses common
    underlying patterns of many deep learning and data processing techniques in terms
    of decoupled abstractions. These abstractions can be expressed concisely and clearly
    by leveraging the dynamism of the underlying Python language and the flexibility
    of the PyTorch library.
  related_to:
  - B2AI_STANDARD:816
  requires_registration: false
  url: https://github.com/fastai/fastai
  has_application:
  - id: B2AI_APP:68
    category: B2AI:Application
    name: Rapid Medical AI Prototyping and Transfer Learning
    description: FastAI is used in biomedical research for rapid prototyping and training
      of deep learning models with minimal code, particularly valuable for researchers
      without extensive ML engineering backgrounds. Healthcare researchers leverage
      FastAI's high-level APIs, best-practice defaults, and powerful transfer learning
      capabilities to quickly develop models for medical image classification, clinical
      text analysis, and tabular clinical data prediction. The library's sophisticated
      learning rate scheduling, progressive resizing, and mixed precision training
      enable efficient use of computational resources, while its extensive documentation
      and educational materials democratize AI development in medicine. FastAI is
      particularly popular for exploratory research, kaggle-style medical AI competitions,
      and educational settings teaching clinical AI.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:766
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - toolkit
  concerns_data_topic:
  - B2AI_TOPIC:20
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Galaxy
  formal_specification: https://github.com/galaxyproject/galaxy
  is_open: true
  name: Galaxy
  publication: doi:10.1093/nar/gky379
  purpose_detail: Galaxy is an open source, web-based platform for data intensive
    biomedical research.
  related_to:
  - B2AI_STANDARD:334
  requires_registration: false
  url: https://usegalaxy.org/
- id: B2AI_STANDARD:767
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - cloudservice
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Google Cloud Platform
  has_relevant_organization:
  - B2AI_ORG:37
  - B2AI_ORG:115
  is_open: false
  name: GCP
  purpose_detail: Google Cloud Platform (GCP) is a comprehensive cloud computing
    platform offering over 200+ services spanning compute, storage, databases, networking,
    AI/machine learning, data analytics, and developer tools across Google's global
    infrastructure. Core compute offerings include Compute Engine VMs, App Engine
    for platform-as-a-service deployment, Google Kubernetes Engine for container
    orchestration, and Cloud Functions for serverless computing. For AI and machine
    learning, GCP provides Vertex AI as a unified platform for building, deploying,
    and scaling ML models, including access to Google's Gemini large language models,
    AutoML capabilities, and specialized hardware like Tensor Processing Units (TPUs)
    for accelerated training. Storage solutions include Cloud Storage for object storage,
    persistent disks for block storage, and Filestore for managed file storage. Database
    services encompass Cloud SQL for managed relational databases (MySQL, PostgreSQL,
    SQL Server), Cloud Spanner for globally distributed relational databases, BigQuery
    for serverless data warehousing and analytics, Firestore for NoSQL document databases,
    and Bigtable for wide-column NoSQL. Healthcare and life sciences researchers benefit
    from specialized services like Healthcare API for FHIR-based health data exchange,
    Life Sciences API for genomics pipeline execution, and Cloud Healthcare Console
    for managing sensitive patient data with HIPAA and HITRUST compliance. BigQuery
    ML enables researchers to create and execute machine learning models using SQL
    queries directly on large biomedical datasets. GCP is explicitly used in Bridge2AI
    projects for scalable biomedical data processing, AI model training and deployment,
    and collaborative research infrastructure. The platform emphasizes security with
    encryption at rest and in transit, IAM for access control, VPC for network isolation,
    and compliance certifications including HIPAA, FedRAMP, ISO 27001, and SOC 2.
    GCP's global network spans 40+ regions and 120+ edge locations, providing low-latency
    access and data residency options for international research collaborations. Integration
    with Google Workspace, Looker for business intelligence, and open-source frameworks
    like TensorFlow and PyTorch facilitates comprehensive workflows from data ingestion
    through analysis, visualization, and publication.
  requires_registration: true
  url: https://cloud.google.com/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:69
    category: B2AI:Application
    name: Cloud-Based Biomedical AI Infrastructure and Model Deployment
    description: Google Cloud Platform is extensively used in AI applications for
      scalable biomedical data processing, large-scale model training, and deployment
      of clinical AI systems with healthcare-compliant infrastructure. AI researchers
      leverage GCP's specialized services including Vertex AI for managed ML pipelines,
      BigQuery for analyzing massive clinical datasets, and Cloud Healthcare API for
      FHIR-based data integration. The platform enables distributed training of deep
      learning models on genomic sequences, medical images, and electronic health
      records at population scale, while providing HIPAA-compliant infrastructure
      for clinical AI deployment. GCP's TPU infrastructure accelerates training of
      large biomedical language models and computer vision systems, and its AutoML
      capabilities democratize AI development for healthcare institutions.
    used_in_bridge2ai: false
    references:
    - https://cloud.google.com/healthcare-api
    - https://cloud.google.com/vertex-ai
- id: B2AI_STANDARD:768
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - graphdataplatform
  - machinelearningframework
  concerns_data_topic:
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Graph Representation leArning, Predictions and Evaluation library
  formal_specification: https://github.com/AnacletoLAB/grape
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:14
  - B2AI_SUBSTRATE:15
  is_open: true
  name: GrAPE
  publication: doi:10.48550/arXiv.2110.06196
  purpose_detail: A fast graph processing and embedding library, designed to scale
    with big graphs and to run on both off-the-shelf laptop and desktop computers
    and High Performance Computing clusters of workstations.
  requires_registration: false
  url: https://github.com/AnacletoLAB/grape
  has_application:
  - id: B2AI_APP:70
    category: B2AI:Application
    name: Graph Embedding and Network Medicine AI
    description: GrAPE (Graph Automatic Programming Environment) is used in biomedical
      AI for creating graph embeddings and training graph neural networks on biological
      networks, enabling applications in drug discovery, protein function prediction,
      and disease gene prioritization. Researchers leverage GrAPE's efficient graph
      processing capabilities to learn low-dimensional representations of large-scale
      biological networks including protein-protein interactions, gene regulatory
      networks, and knowledge graphs. The tool supports AI applications that predict
      novel drug-target interactions, identify disease modules, and perform link prediction
      in biomedical knowledge graphs. GrAPE's optimization for large graphs enables
      training on genome-scale networks and integration of multi-omics data through
      graph-based representations.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1093/gigascience/giab044
- id: B2AI_STANDARD:769
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:32
  - B2AI_TOPIC:16
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Half-Edge Semantic Measures Library
  formal_specification: https://github.com/jjlastra/HESML
  is_open: true
  name: HESML
  publication: doi:10.1186/S12859-021-04539-0
  purpose_detail: HESML is an efficient, scalable and large Java software library
    of ontology-based semantic similarity measures and Information Content (IC) models
    based on WordNet, SNOMED-CT, MeSH or any other OBO-based ontology.
  related_to:
  - B2AI_STANDARD:553
  requires_registration: false
  url: http://hesml.lsi.uned.es/
- id: B2AI_STANDARD:770
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Hangar
  formal_specification: https://github.com/tensorwerk/hangar-py
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:42
  is_open: true
  name: Hangar
  purpose_detail: Hangar is a version control system specifically designed for managing
    tensor data and numerical arrays in machine learning and scientific computing
    workflows, providing Git-like versioning capabilities for datasets that are too
    large or complex for traditional version control systems. Built with Python and
    optimized for performance, Hangar enables data scientists and ML engineers to
    track changes in training datasets, model weights, embeddings, and experimental
    results with full history, branching, merging, and collaboration features. The
    system uses content-addressable storage with automatic data deduplication, storing
    only changed array blocks rather than entire files, making it efficient for large
    multidimensional datasets common in deep learning (image datasets, genomic sequences,
    time series, medical imaging volumes). Hangar provides atomic commits ensuring
    data consistency, supports concurrent read access for distributed training, and
    enables reproducible machine learning by linking dataset versions to specific
    model training runs. Key features include automatic detection of array schema
    changes, efficient storage of sparse arrays, lazy loading for memory-efficient
    data access, and integration with NumPy, PyTorch, and TensorFlow workflows. Applications
    span ML experiment tracking where different dataset versions are tested with model
    architectures, collaborative dataset curation where multiple researchers contribute
    annotations or corrections, model debugging through inspection of training data
    that caused specific failures, and regulatory compliance in medical AI where dataset
    provenance and versioning history must be maintained. Hangar addresses critical
    gaps in ML operations by providing reproducibility (exact dataset version used
    for published results), collaboration (merge dataset contributions from multiple
    sources), and auditability (full history of dataset modifications with timestamps
    and contributors), essential for rigorous scientific research and production ML
    systems.
  requires_registration: false
  url: https://hangar-py.readthedocs.io/en/stable/
- id: B2AI_STANDARD:771
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Health Analytics Data-to-Evidence Suite
  formal_specification: https://github.com/OHDSI/Hades
  has_relevant_organization:
  - B2AI_ORG:76
  is_open: true
  name: HADES
  purpose_detail: HADES (formally known as the OHDSI Methods Library) is a set of
    open source R packages for large scale analytics, including population characterization,
    population-level causal effect estimation, and patient-level prediction.
  requires_registration: false
  url: https://ohdsi.github.io/Hades/
- id: B2AI_STANDARD:772
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - multimodal
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Holistic AI in Medicine framework
  formal_specification: https://github.com/lrsoenksen/HAIM
  is_open: true
  name: HAIM
  publication: doi:10.1038/s41746-022-00689-4
  purpose_detail: A unified Holistic AI in Medicine (HAIM) framework to facilitate
    the generation and testing of AI systems that leverage multimodal inputs.
  requires_registration: false
  url: https://github.com/lrsoenksen/HAIM
  has_application:
  - id: B2AI_APP:71
    category: B2AI:Application
    name: Holistic Multi-Modal Healthcare AI Integration
    description: HAIM (Holistic AI in Medicine) framework is used for developing and
      evaluating multi-modal AI systems that integrate diverse healthcare data types
      including imaging, time-series, tabular clinical data, and text for comprehensive
      patient assessment. The framework enables training of models that leverage complementary
      information across modalities to improve diagnostic accuracy, risk prediction,
      and clinical decision support beyond single-modality approaches. HAIM provides
      standardized methods for multi-modal data fusion, handles missing modalities
      gracefully, and enables systematic evaluation of how different data types contribute
      to model performance. Applications include integrated ICU monitoring systems,
      comprehensive cancer diagnosis combining radiology and pathology, and patient
      deterioration prediction using all available clinical data streams.
    used_in_bridge2ai: false
    references:
    - https://doi.org/10.1038/s41746-022-00689-4
- id: B2AI_STANDARD:773
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HTSeq
  formal_specification: https://github.com/htseq/htseq
  is_open: true
  name: HTSeq
  publication: doi:10.1093/bioinformatics/btu638
  purpose_detail: A Python library to facilitate the rapid development of high throughput
    sequencing data analysis scripts. HTSeq offers parsers for many common data formats
    in HTS projects, as well as classes to represent data, such as genomic coordinates,
    sequences, sequencing reads, alignments, gene model information and variant calls,
    and provides data structures that allow for querying via genomic coordinates.
  requires_registration: false
  url: https://github.com/htseq/htseq
- id: B2AI_STANDARD:774
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ImJoy
  formal_specification: https://github.com/imjoy-team/ImJoy
  is_open: true
  name: ImJoy
  publication: doi:10.1038/s41592-019-0627-0
  purpose_detail: A plugin powered hybrid computing platform for deploying deep learning
    applications such as advanced image analysis tools. ImJoy runs on mobile and desktop
    environment cross different operating systems, plugins can run in the browser,
    localhost, remote and cloud servers.
  requires_registration: false
  url: https://imjoy.io/
- id: B2AI_STANDARD:775
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Informatics for Integrating Biology and the Bedside platform
  formal_specification: https://github.com/i2b2
  has_relevant_organization:
  - B2AI_ORG:42
  is_open: true
  name: i2b2
  publication: doi:10.1093/jamia/ocv188
  purpose_detail: A system for searching and exchanging clinical data.
  requires_registration: false
  url: https://www.i2b2.org/software/index.html
- id: B2AI_STANDARD:776
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Insight Segmentation and Registration Toolkit
  formal_specification: https://github.com/InsightSoftwareConsortium/ITK
  is_open: true
  name: ITK
  purpose_detail: The Insight Toolkit (ITK) is an open-source, cross-platform toolkit
    for N-dimensional scientific image processing, segmentation, and registration.
  requires_registration: false
  url: https://itk.org/
- id: B2AI_STANDARD:777
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:9
  - B2AI_TOPIC:15
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Integrated digital Multimodal PATIENt daTa framework
  formal_specification: https://github.com/lambda-science/IMPatienT
  is_open: true
  name: IMPatienT
  purpose_detail: A free and open-source web application to digitize, process and
    explore multimodal patient data. IMPatienT has a modular architecture, including
    four components to (i) create a standard vocabulary for a domain, (ii) digitize
    and process free-text data by mapping it to a set of standard terms, (iii) annotate
    images and perform image segmentation, and (iv) generate an automatic visualization
    dashboard to provide insight on the data and perform automatic diagnosis suggestions.
  requires_registration: false
  url: https://impatient.lbgi.fr
- id: B2AI_STANDARD:778
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:27
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Integrative Modeling Platform
  formal_specification: https://github.com/salilab/imp
  is_open: true
  name: IMP
  purpose_detail: An open source C++ and Python toolbox for solving complex modeling
    problems, and a number of applications for tackling some common problems in a
    user-friendly way. IMP can also be used from the Chimera molecular modeling system,
    or via one of several web applications.
  requires_registration: false
  url: https://integrativemodeling.org/
- id: B2AI_STANDARD:779
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - notebookplatform
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Jupyter Notebook
  formal_specification: https://github.com/jupyter/notebook
  is_open: true
  name: Jupyter
  purpose_detail: A web-based notebook environment for interactive computing.
  requires_registration: false
  url: https://jupyter.org/
  used_in_bridge2ai: true
- id: B2AI_STANDARD:780
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Kaldi Speech Recognition Toolkit
  formal_specification: https://github.com/kaldi-asr/kaldi
  is_open: true
  name: Kaldi
  purpose_detail: Kaldi is an open-source toolkit for speech recognition research
    and development, written primarily in C++ with scripting interfaces, providing
    comprehensive implementations of state-of-the-art automatic speech recognition
    (ASR) algorithms including deep neural networks (DNNs), hidden Markov models (HMMs),
    Gaussian mixture models (GMMs), and modern end-to-end architectures. Developed
    by researchers at Johns Hopkins University and maintained by a global community,
    Kaldi emphasizes flexibility, efficiency, and reproducibility in speech recognition
    pipelines. The toolkit provides modular components for acoustic feature extraction
    (MFCCs, PLPs, filterbank energies, pitch features), acoustic modeling (DNN-HMM
    hybrid systems, time-delay neural networks TDNNs, chain models with lattice-free
    MMI training), language modeling (n-gram models, recurrent neural network language
    models), and decoding (weighted finite-state transducers WFSTs for efficient search).
    Kaldi supports both speaker-independent and speaker-adaptive recognition, with
    tools for vocal tract length normalization, feature-space maximum likelihood linear
    regression (fMLLR), and i-vector/x-vector speaker embeddings for robust recognition
    across diverse acoustic conditions. The toolkit includes recipes for training
    ASR systems on standard benchmarks (LibriSpeech, WSJ, Switchboard, Fisher, TED-LIUM,
    AMI, CHiME) with documented pipelines enabling researchers to replicate published
    results and adapt models to new datasets and languages. Applications span transcription
    of clinical conversations for medical documentation, analysis of patient-physician
    interactions, voice biomarker extraction for neurological disease monitoring (Parkinson's,
    ALS, dementia), mental health assessment through speech characteristics, and accessibility
    tools for hearing-impaired patients. Kaldi's extensive documentation, active mailing
    list, and well-tested recipes make it accessible for both research and production
    deployment despite its complexity, bridging traditional statistical ASR approaches
    with modern deep learning methods.
  requires_registration: false
  url: https://kaldi-asr.org/
- id: B2AI_STANDARD:781
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Keepsake
  formal_specification: https://github.com/replicate/keepsake
  is_open: true
  name: Keepsake
  purpose_detail: Version control for machine learning. A Python library that uploads
    files and metadata (like hyperparameters) to Amazon S3 or Google Cloud Storage.
  requires_registration: false
  url: https://keepsake.ai/
- id: B2AI_STANDARD:782
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: khmer
  formal_specification: https://github.com/dib-lab/khmer/
  is_open: true
  name: khmer
  publication: doi:10.12688/f1000research.6924.1
  purpose_detail: khmer is a free, open-source software library and suite of command-line
    tools for efficient analysis of high-throughput DNA sequencing data, including
    genomes, transcriptomes, metagenomes, and single-cell datasets. It implements
    advanced algorithms for probabilistic k-mer counting, digital normalization, compressible
    De Bruijn graph representation, and graph partitioning, enabling scalable de novo
    assembly, error correction, and data reduction. khmer is designed for use in UNIX
    environments and is supported by extensive documentation, community protocols,
    and integration with popular bioinformatics workflows. It is widely used for preprocessing
    and quality control in large-scale sequencing projects.
  related_to:
  - B2AI_STANDARD:183
  requires_registration: false
  url: https://khmer.readthedocs.io/
- id: B2AI_STANDARD:783
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Koza data transformation framework
  formal_specification: https://github.com/monarch-initiative/koza
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:6
  has_relevant_organization:
  - B2AI_ORG:58
  is_open: true
  name: Koza
  purpose_detail: Transform csv, json, yaml, jsonl, and xml and converting them to
    a target csv, json, or jsonl format based on your dataclass model. Koza also can
    output data in the KGX format. Write data transforms in semi-declarative Python.
    Configure source files, expected columns/json properties and path filters, field
    filters, and metadata in yaml. Create or import mapping files to be used in ingests
    (eg id mapping, type mappings). Create and use translation tables to map between
    source and target vocabularies.
  related_to:
  - B2AI_STANDARD:346
  requires_registration: false
  url: https://github.com/monarch-initiative/koza
- id: B2AI_STANDARD:784
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:7
  - B2AI_TOPIC:25
  - B2AI_TOPIC:35
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: LIkelihood Ratio Interpretation of Clinical AbnormaLities
  formal_specification: https://github.com/TheJacksonLaboratory/LIRICAL
  has_relevant_organization:
  - B2AI_ORG:58
  is_open: true
  name: LIRICAL
  publication: doi:10.1016/j.ajhg.2020.06.021
  purpose_detail: Performs phenotype-driven prioritization of candidate diseases and
    genes in the setting of genomic diagnostics (exome or genome) in which the phenotypic
    abnormalities are described as Human Phenotype Ontology (HPO) terms.
  related_to:
  - B2AI_STANDARD:468
  requires_registration: false
  url: https://lirical.readthedocs.io/
- id: B2AI_STANDARD:785
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - cloudservice
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Linode
  is_open: false
  name: Linode
  purpose_detail: Linode (now part of Akamai Connected Cloud) is a developer-focused
    cloud computing platform emphasizing simplicity, predictable pricing, and high
    performance for compute, storage, and networking services across a globally distributed
    infrastructure. Core compute offerings include Essential Compute instances (Shared
    CPU, Dedicated CPU, High Memory, and Premium CPU), GPU instances for parallel
    processing workloads including machine learning and scientific computing, and
    Accelerated Compute for specialized workloads. Container orchestration is provided
    through Linode Kubernetes Engine (LKE) for managed Kubernetes clusters, and App
    Platform for rapid cloud-native application deployment with automated infrastructure
    management. Storage solutions encompass Block Storage for scalable persistent
    volumes, Object Storage with S3-compatible API for unstructured data, and automated
    Backups for data protection. Managed Databases support MySQL and PostgreSQL with
    automated maintenance, backups, and high availability configurations. Networking
    features include NodeBalancers for load distribution, Cloud Firewall for security,
    DNS Manager, Private Networking (VLAN), and DDoS protection. Linode differentiates
    itself through transparent flat pricing bundling CPU, transfer, storage, and
    RAM without hidden egress fees or complex tier structures, making it particularly
    attractive for budget-conscious researchers and startups. The platform provides
    comprehensive API access, CLI tools, Terraform provider, and extensive documentation
    for programmatic infrastructure management. Following acquisition by Akamai in
    2022, Linode benefits from integration with Akamai's global CDN network and security
    capabilities while maintaining its developer-friendly approach and competitive
    pricing. Linode's global presence includes data centers across North America,
    Europe, Asia-Pacific, and emerging markets, providing edge computing capabilities
    and reduced latency for distributed applications. The platform offers free DDoS
    protection, 24/7/365 support, and a Cloud Computing Foundations Certification
    program for developer education. While more streamlined than hyperscale competitors,
    Linode provides essential cloud services suitable for web applications, data pipelines,
    containerized workloads, and computational research without vendor lock-in through
    standard open-source technologies.
  requires_registration: true
  url: https://www.linode.com/
- id: B2AI_STANDARD:786
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - scrnaseqanalysis
  concerns_data_topic:
  - B2AI_TOPIC:34
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Markov Affinity-based Graph Imputation of Cells
  formal_specification: https://github.com/KrishnaswamyLab/MAGIC
  is_open: true
  name: MAGIC
  publication: doi:10.1016/j.cell.2018.05.061
  purpose_detail: A method for imputing missing values in scRNA-seq data.
  requires_registration: false
  url: https://www.krishnaswamylab.org/projects/magic
- id: B2AI_STANDARD:787
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Marquez
  formal_specification: https://github.com/MarquezProject/marquez
  is_open: true
  name: Marquez
  purpose_detail: Marquez is an open source metadata service for the collection, aggregation,
    and visualization of a data ecosystem's metadata. It maintains the provenance
    of how datasets are consumed and produced, provides global visibility into job
    runtime and frequency of dataset access, centralization of dataset lifecycle management,
    and much more. Marquez was released and open sourced by WeWork.
  requires_registration: false
  url: https://lfaidata.foundation/projects/marquez/
- id: B2AI_STANDARD:788
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - machinelearningframework
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Medical Open Network for AI
  formal_specification: https://github.com/Project-MONAI/MONAI
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:33
  is_open: true
  name: MONAI
  purpose_detail: MONAI (Medical Open Network for AI) is a PyTorch-based, open-source
    framework specifically designed for deep learning in healthcare imaging, providing
    domain-optimized data loaders, transforms, network architectures, loss functions,
    and training utilities tailored to the unique challenges of medical image analysis
    including 3D volumetric data, high-resolution whole slide images, multi-modal
    imaging, and clinical deployment constraints. Developed through collaboration
    between NVIDIA, King's College London, and a global consortium of academic medical
    centers and industry partners, MONAI addresses the gap between general-purpose
    deep learning frameworks and the specialized requirements of medical imaging AI
    by offering production-ready implementations of state-of-the-art architectures
    (U-Net, SegResNet, DenseNet, EfficientNet, Vision Transformers adapted for medical
    imaging), pretrained model weights from large-scale medical imaging datasets,
    and composable building blocks that accelerate development while maintaining flexibility
    for research innovation. The framework provides comprehensive data preprocessing
    pipelines with medical imaging-specific transforms including intensity normalization
    (z-score, percentile clipping, histogram matching), spatial operations (resampling,
    cropping, padding with orientation-aware handling for DICOM/NIfTI), augmentation
    strategies proven effective for medical data (elastic deformations, MixUp, CutMix,
    RandAugment), and handling of metadata from clinical imaging standards (DICOM
    headers, NIfTI affines). MONAI's data loading infrastructure optimizes memory
    and compute efficiency for large 3D medical volumes through smart caching, deterministic
    transforms for reproducibility, distributed data parallel training, and ThreadDataLoader/ThreadBuffer
    for CPU-bound preprocessing during GPU training. The framework includes task-specific
    modules for organ and lesion segmentation (including cascaded approaches, nnU-Net-style
    architectures), image registration (affine and deformable, learning-based and
    classical), anomaly detection, image synthesis and super-resolution, classification
    and detection, and federated learning for privacy-preserving multi-institutional
    collaboration via NVIDIA FLARE integration. MONAI Core provides the foundational
    library, while the MONAI ecosystem extends to MONAI Label (interactive annotation
    with active learning), MONAI Deploy (containerized deployment for clinical PACS
    integration), MONAI Bundle (shareable pretrained models and workflows with standardized
    configuration), and Auto3DSeg (automated neural architecture search and ensemble
    learning for segmentation). The framework supports integration with clinical workflows
    through DICOM/NIfTI I/O via pydicom and nibabel, deployment tools for FDA/CE
    mark regulatory pathways, model explainability modules (Grad-CAM, occlusion sensitivity),
    and quantitative evaluation metrics specific to medical imaging (Dice score, Hausdorff
    distance, surface distance). MONAI has become the de facto standard framework
    for academic medical imaging AI research with adoption by hundreds of institutions
    worldwide, forms the foundation for international challenges (Medical Segmentation
    Decathlon, BraTS), and powers production clinical AI systems for automated organ
    segmentation, tumor detection, treatment response assessment, and pathology slide
    analysis deployed in hospital PACS environments.
  related_to:
  - B2AI_STANDARD:816
  requires_registration: false
  url: https://monai.io/
  has_application:
  - id: B2AI_APP:72
    category: B2AI:Application
    name: Medical Imaging Deep Learning Framework and Workflows
    description: MONAI (Medical Open Network for AI) is a specialized PyTorch-based
      framework extensively used for developing deep learning models on 3D medical
      imaging data across radiology, pathology, and microscopy. Researchers and clinical
      AI developers leverage MONAI's domain-specific transforms (intensity normalization,
      resampling, augmentation), pretrained models for medical imaging tasks, and
      optimized data loaders for large 3D volumes to accelerate development of segmentation,
      classification, and detection models. The framework provides standardized workflows
      for common medical imaging AI tasks, federated learning capabilities for multi-institutional
      collaboration, and Auto3DSeg for automated model development. MONAI's integration
      with clinical imaging standards and deployment tools makes it a bridge between
      research and clinical implementation.
    used_in_bridge2ai: false
  - id: B2AI_APP:179
    category: B2AI:Application
    name: Generative Medical Image Synthesis with MONAI Generative Models
    description: 'MONAI Generative Models extends the framework with latent diffusion
      and GAN-based architectures to synthesize multi-modality medical images for
      data augmentation, privacy-preserving dataset generation, and methodological
      research. The package provides implementations of "Latent Diffusion Model comprising
      an autoencoder (compression) and a diffusion generative model" with perceptual,
      spectral, and patch-based adversarial losses, plus a "2.5D perceptual-loss strategy
      for 3D" volumes. Concrete applications span chest X-ray synthesis on MIMIC-CXR
      (96,161 images, FID 8.8325, MS-SSIM 0.4276), mammography on CSAW-M (9,523 images),
      retinal OCT (84,483 images), 2D brain MRI slices (UK Biobank, 360,525 slices),
      and 3D T1 brain volumes (UK Biobank, 41,162 volumes, FID 0.0051, MS-SSIM 0.9217).
      MONAI components include pretrained networks (RadImageNet for 2D, MedicalNet
      for 3D) and standardized metrics (MS-SSIM for autoencoder reconstruction, FID
      and MS-SSIM for sample fidelity/diversity, CLIP Score for conditioning). The
      package releases "experiment code publicly available" with pretrained models
      for reproducible benchmarking, enabling researchers to generate synthetic medical
      images that preserve anatomical realism while avoiding patient identifiers,
      supporting data augmentation for rare diseases and methodological studies of
      generative model behavior in medical imaging domains.'
    references:
    - https://doi.org/10.48550/arxiv.2307.15208
    used_in_bridge2ai: false
  - id: B2AI_APP:180
    category: B2AI:Application
    name: Clinical Prostate MRI Segmentation Deployment with MONAI Deploy
    description: 'MONAI Deploy Express enables production clinical deployment of AI
      segmentation and lesion detection pipelines integrated with PACS and biopsy
      planning workflows. A research-based clinical deployment at a major medical
      center used MONAI Deploy to operationalize a prostate mpMRI pipeline running
      on "a dedicated inference server" that generates "binary lesion prediction and
      probability maps" presented in biopsy preparation platforms where clinicians
      approve or reject AI-identified targets. Prospective evaluation across Phase
      1 (58 patients) and Phase 2 (35 patients, 72 radiologist-accepted lesions with
      23 AI+/Rad- lesions) demonstrated that "concordant AI+/Rad+ lesions showed highest
      cancer detection rate (CDR)82.1% for PCa and 42.9% for csPCa" with added value
      from AI-only ROIs showing "47.8% CDR when AI-proposed ROIs were accepted" and
      clinically significant cancer found in 4 of 23 AI+/Rad- lesions. The deployment
      achieved "results similar to original model development" with "excellent concordance
      for PIRADS 5" while maintaining clinical integration through user approval workflows.
      This exemplifies MONAI Deploy''s capability to package trained models into containerized
      applications that integrate with radiology information systems, enable clinician
      oversight of AI predictions, and deliver actionable outputs within clinical
      decision-making timelines for interventional procedures.'
    references:
    - https://doi.org/10.1007/s00261-025-05014-7
    used_in_bridge2ai: false
  - id: B2AI_APP:181
    category: B2AI:Application
    name: Interactive Annotation and Active Learning with MONAI Label
    description: 'MONAI Label provides interactive annotation workflows with active
      learning that accelerate dataset curation and iteratively improve model performance
      through human-in-the-loop refinement. At Karolinska University Hospital within
      the MAIA platform, a CT vertebral metastasis segmentation project began with
      30 labeled subjects and used MONAI Label to enable "radiologists to load new
      batches of unlabeled data, visualize model predictions, and perform corrective
      annotations," scaling to 150 cases through iterative labeling cycles. Model
      performance improved dramatically: "segmentation model sensitivity improved
      from 55% (30 cases) to 82% (150 cases)" through MONAI Label-assisted expansion.
      The workflow was "containerized, enabling on-demand instantiation for prediction
      on new patient data" with end-to-end Kubeflow orchestration managing "DICOM
      loading and NIfTI conversion through preprocessing, segmentation, optional correction,
      and postprocessing, to the final DICOM output." MONAI Label integrates with
      clinical viewers (OHIF, 3D Slicer, XNAT) where radiologists review and correct
      predictions, with "updated annotations returned to the server" for model retraining.
      This active learning approach addresses the annotation bottleneck in medical
      AI by prioritizing uncertain predictions for expert review, progressively refining
      models with minimal labeling effort while maintaining clinical DICOM format
      compatibility throughout the annotation-training-deployment lifecycle.'
    references:
    - https://doi.org/10.1038/s44387-025-00042-6
    - https://openarchive.ki.se/articles/thesis/Design_and_integration_of_AI_solutions_in_oncology_and_healthcare_infrastructures_bridging_the_gap_between_AI_innovation_and_clinical_practice/30466691/1/files/59123939.pdf
    used_in_bridge2ai: false
  - id: B2AI_APP:182
    category: B2AI:Application
    name: Clinical Chest X-ray Device Detection with MONAI Frameworks
    description: 'MONAI-based frameworks operationalize clinical AI pipelines for
      safety-critical applications including detection and identification of MRI-hazardous
      implanted devices on chest radiographs. At Mayo Clinic, a production system
      uses "MONAI-based frameworks to operationalize the pipeline" with "two cascading
      modelsdetection with a Faster R-CNN followed by identification with a multi-class
      CNN" to identify leadless electronic devices (e.g., Bravo esophageal reflux
      pH capsules) that pose MRI safety risks. The system processes "frontal chest
      x-rays at Mayo Clinic" with continuous learning and reports exemplar performance
      such as devices "correctly detected and identified (with 10/10 certainty)."
      Clinical integration includes "integration into a robust viewer with back-end
      data recording and adjudication by radiologists," enabling continuous quality
      monitoring and iterative improvement from real-world deployment feedback. The
      application demonstrates MONAI''s role in building clinical decision support
      systems for patient safety, where automated detection flags potentially hazardous
      conditions for radiologist review before MRI procedures, combining computer
      vision with clinical workflow integration and human oversight to prevent adverse
      events while maintaining audit trails for quality assurance and model refinement.'
    references:
    - https://doi.org/10.2196/55833
    used_in_bridge2ai: false
  - id: B2AI_APP:183
    category: B2AI:Application
    name: MONAI Application Packages for Standards-Compliant Clinical Deployment
    description: 'MONAI Deploy App SDK structures clinical inference as MONAI Application
      Packages (MAPs) that are "DAGs of Operators" providing standardized components
      for DICOM-native medical AI deployment with EMR integration. Operators include
      StudyLoaderOperator, SeriesSelectorOperator, SeriesToVolumeOperator for DICOM
      parsing, preprocessing operators (Gaussian filter, threshold), and model prediction
      operators that "parse DICOM into numpy arrays for inference" with models retrievable
      from "MONAI Model Zoo." Clinical outputs are formatted as "DICOM Segmentation
      or DICOM Structured Reports" that integrate with PACS systems, with deep integration
      via "DICOM Router (Laurel Bridge)" enabling "routing rules to normalize and
      filter studies (e.g., thin vs thick slices)" and "parallel routing to reduce
      latency" for real-time clinical throughput. Advanced integration includes generation
      of "HL7 v2 messages from DICOM SR (via custom C# on the router) for EMR/worklist
      integration," enabling AI predictions to populate electronic health records
      and trigger clinical workflows. This architecture addresses the gap between
      research prototypes and clinical deployment by providing reusable, standards-compliant
      components that handle medical imaging complexities (modality-specific preprocessing,
      series selection, coordinate systems) while maintaining interoperability with
      radiology IT infrastructure through DICOM/HL7 messaging standards and supporting
      regulatory requirements for medical device software through documented, version-controlled
      deployment packages.'
    references:
    - https://doi.org/10.48550/arxiv.2311.10840
    used_in_bridge2ai: false
- id: B2AI_STANDARD:789
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Metacat
  formal_specification: https://github.com/Netflix/metacat
  has_relevant_organization:
  - B2AI_ORG:65
  is_open: true
  name: Metacat
  purpose_detail: Metacat is a unified metadata exploration API service. You can explore
    Hive, RDS, Teradata, Redshift, S3 and Cassandra. Metacat provides you information
    about what data you have, where it resides and how to process it.
  requires_registration: false
  url: https://github.com/Netflix/metacat
- id: B2AI_STANDARD:790
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - cloudservice
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Microsoft Azure
  has_relevant_organization:
  - B2AI_ORG:56
  is_open: false
  name: Azure
  purpose_detail: Microsoft Azure is a comprehensive enterprise-grade cloud computing
    platform offering 200+ services spanning compute, storage, databases, networking,
    AI/machine learning, analytics, IoT, and developer tools with deep integration
    into Microsoft's enterprise ecosystem. Core compute services include Azure Virtual
    Machines for Windows and Linux workloads, Azure App Service for web application
    hosting, Azure Kubernetes Service (AKS) for container orchestration, Azure Container
    Instances for serverless containers, and Azure Functions for event-driven serverless
    computing. For AI and machine learning, Azure provides Azure AI Studio (including
    Microsoft Foundry for AI agent development), Azure Machine Learning for end-to-end
    ML workflows, Azure Cognitive Services for pre-trained AI models (vision, speech,
    language), and Azure OpenAI Service for accessing GPT, Codex, and DALL-E models.
    Data and analytics capabilities include Azure Synapse Analytics for unified data
    warehousing and big data analytics, Azure Databricks for Apache Spark-based analytics,
    Azure Data Lake for scalable data storage, and Microsoft Fabric for unified data
    platform integration. Database services encompass Azure SQL Database for managed
    relational databases, Azure Cosmos DB for globally distributed NoSQL, Azure Database
    for PostgreSQL/MySQL/MariaDB, and Azure DocumentDB for MongoDB-compatible document
    storage. Healthcare and life sciences benefit from Azure Health Data Services
    for FHIR-based health data management, Azure Genomics for scalable genomics pipeline
    execution, Azure Healthcare APIs, and compliance with HIPAA, HITRUST, GxP, and
    FDA 21 CFR Part 11 regulations. Researchers leverage Azure Machine Learning for
    model training with GPU/FPGA acceleration, automated ML (AutoML) capabilities,
    MLOps for model lifecycle management, and integration with popular frameworks
    like TensorFlow, PyTorch, and scikit-learn. Azure's hybrid cloud capabilities
    through Azure Arc enable consistent management across on-premises, multi-cloud,
    and edge environments, crucial for institutions with existing infrastructure investments.
    Enterprise integration with Active Directory, Microsoft 365, Windows Server, SQL
    Server, and Visual Studio provides seamless authentication, collaboration, and
    development workflows. Azure's global infrastructure spans 60+ regions worldwide
    with data residency and compliance options for international regulations. Security
    features include Azure Security Center, Azure Sentinel for SIEM, Azure Active
    Directory for identity management, encryption at rest and in transit, and extensive
    compliance certifications across healthcare, government, and financial sectors.
  requires_registration: true
  url: https://azure.microsoft.com/
- id: B2AI_STANDARD:791
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - cloudservice
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MinIO
  formal_specification: https://github.com/minio/
  is_open: true
  name: MinIO
  purpose_detail: MinIO is a high-performance object storage system that implements
    the Amazon S3 API for cloud-native environments. The system is designed as software-defined
    storage and provides S3-compatible operations with reported latency under 10ms
    and high throughput capabilities. MinIO is designed to run on Kubernetes and can
    be deployed across public cloud platforms, private cloud infrastructure, and edge
    environments. The architecture uses a distributed design that supports scaling
    beyond traditional storage system limits, with deployment capabilities from single
    servers to exabyte-scale installations using a single namespace. The platform
    includes automated data management features such as self-healing, load balancing,
    and data protection with erasure coding. Security features include encryption
    at rest and in transit, identity and access management integration, and policy-based
    access controls. MinIO is used in AI and machine learning workflows, providing
    storage integration with frameworks like PyTorch and data lakehouse technologies
    including Apache Iceberg. The system supports data lakehouse analytics engines
    such as Apache Spark and Trino for structured and unstructured data processing.
    Client libraries are available for multiple programming languages including Go,
    Python, Java, .NET, Rust, and JavaScript. Associated tools include mc (command-line
    client for object operations), DirectPV (a Kubernetes CSI driver), and a Kubernetes
    operator for cluster deployment and management. The source code is released under
    the GNU AGPLv3 license. The project reports over 2 billion downloads and maintains
    an active development community with more than 50,000 stars on GitHub.
  requires_registration: false
  url: https://min.io/
- id: B2AI_STANDARD:792
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ML Metadata
  formal_specification: https://github.com/google/ml-metadata
  has_relevant_organization:
  - B2AI_ORG:37
  is_open: true
  name: MLMD
  purpose_detail: ML Metadata (MLMD) is a library for recording and retrieving metadata
    associated with ML developer and data scientist workflows.
  requires_registration: false
  url: https://www.tensorflow.org/tfx/guide/mlmd
- id: B2AI_STANDARD:793
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MLflow platform
  formal_specification: https://github.com/mlflow/mlflow/
  is_open: true
  name: MLflow
  purpose_detail: MLflow is an open source platform to manage the ML lifecycle, including
    experimentation, reproducibility, deployment, and a central model registry
  requires_registration: false
  url: https://mlflow.org/
- id: B2AI_STANDARD:794
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - machinelearningframework
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MLPro framework
  formal_specification: https://github.com/fhswf/MLPro
  is_open: true
  name: MLPro
  publication: doi:10.1016/j.simpa.2022.100421
  purpose_detail: 'MLPro (Machine Learning Professional) is a middleware framework
    for standardized machine learning development in Python. The framework provides
    an object-oriented architecture that enables the integration and standardization
    of multiple machine learning paradigms through reusable process models and templates.
    MLPro consists of several sub-frameworks organized as modules: MLPro-BF (Basic
    Functions) provides cross-sectional infrastructure including mathematics, data
    management, plotting capabilities, and logging; MLPro-SL supports supervised learning
    workflows; MLPro-RL implements reinforcement learning processes; MLPro-GT covers
    game theory applications; and MLPro-OA focuses on online adaptive machine learning.
    The architecture uses standardized interfaces that allow ML models to be embedded
    into training pipelines and operational workflows in a modular fashion. The framework
    integrates with external machine learning libraries including River (for online
    learning), scikit-learn (for supervised learning), and OpenML (for dataset and
    experiment management), providing wrappers that standardize their usage within
    MLPro processes. Development follows object-oriented design principles with test-driven
    development for quality assurance and continuous integration/deployment (CI/CD)
    practices. The framework includes an extension hub for third-party integrations,
    comprehensive documentation with API references, and an example pool demonstrating
    various use cases. MLPro supports hybrid ML applications that combine multiple
    learning paradigms and real-time adaptive systems. The project is developed and
    maintained by the Group for Automation Technology and Learning Systems at South
    Westphalia University of Applied Sciences in Germany. The source code is released
    under the Apache 2.0 license and is available through PyPI for installation.'
  requires_registration: false
  url: https://mlpro.readthedocs.io/
  has_application:
  - id: B2AI_APP:73
    category: B2AI:Application
    name: Reinforcement Learning Framework for Adaptive Healthcare Systems
    description: MLPro is used in biomedical AI for developing reinforcement learning
      agents that learn optimal treatment strategies, adaptive monitoring protocols,
      and personalized intervention policies through interaction with healthcare environments.
      Researchers leverage MLPro's modular framework to implement and compare different
      RL algorithms for applications like dynamic treatment regimen optimization,
      adaptive clinical trial design, and automated ICU management. The platform supports
      simulation-based learning where agents train on synthetic patients before clinical
      deployment, multi-agent systems for coordinated care, and safe exploration strategies
      essential for healthcare applications. MLPro enables systematic evaluation of
      RL approaches for sequential decision-making problems in medicine where actions
      have long-term consequences.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:795
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - scrnaseqanalysis
  concerns_data_topic:
  - B2AI_TOPIC:23
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MoClust
  is_open: true
  name: MoClust
  publication: doi:10.1093/bioinformatics/btac736
  purpose_detail: A novel joint clustering framework that can be applied to several
    types of single-cell multi-omics data. A selective automatic doublet detection
    module that can identify and filter out doublets is introduced in the pretraining
    stage to improve data quality. Omics-specific autoencoders are introduced to characterize
    the multi-omics data.
  requires_registration: false
  url: https://zenodo.org/record/7306504
- id: B2AI_STANDARD:796
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - modelcards
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Model Card Toolkit
  formal_specification: https://github.com/tensorflow/model-card-toolkit
  has_relevant_organization:
  - B2AI_ORG:37
  is_open: true
  name: MCT
  publication: doi:10.48550/arXiv.1810.03993
  purpose_detail: The Model Card Toolkit (MCT) streamlines and automates generation
    of Model Cards, machine learning documents that provide context and transparency
    into a model's development and performance.
  requires_registration: false
  url: https://github.com/tensorflow/model-card-toolkit
- id: B2AI_STANDARD:797
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MongoDB
  formal_specification: https://github.com/mongodb/mongo
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:13
  - B2AI_SUBSTRATE:22
  - B2AI_SUBSTRATE:9
  is_open: true
  name: MongoDB
  purpose_detail: A non-relational document database that provides support for JSON-like
    storage.
  requires_registration: false
  url: https://www.mongodb.com/
- id: B2AI_STANDARD:798
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Monocle 2
  formal_specification: https://github.com/cole-trapnell-lab/monocle-release
  is_open: true
  name: Monocle2
  publication: doi:10.1038/nmeth.4402
  purpose_detail: An algorithm that uses reversed graph embedding to describe multiple
    fate decisions in a fully unsupervised manner.
  requires_registration: false
  url: https://github.com/cole-trapnell-lab/monocle-release
- id: B2AI_STANDARD:799
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - multimodal
  concerns_data_topic:
  - B2AI_TOPIC:23
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Multi-Omic Integration by Machine Learning
  formal_specification: https://github.com/jessegmeyerlab/MIMaL
  is_open: true
  name: MIMaL
  publication: doi:10.1093/bioinformatics/btac631
  purpose_detail: MIMaL is a new method for integrating multiomic data using SHAP
    model explanations.
  requires_registration: false
  url: https://mimal.app/
- id: B2AI_STANDARD:800
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - multimodal
  concerns_data_topic:
  - B2AI_TOPIC:26
  - B2AI_TOPIC:28
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Multi-Scale Integrated Cell pipeline
  formal_specification: https://github.com/idekerlab/MuSIC
  has_relevant_organization:
  - B2AI_ORG:116
  is_open: true
  name: MuSIC
  publication: doi:10.1038/s41586-021-04115-9
  purpose_detail: MuSIC is a hierarchical map of human cell architecture created from
    integrating immunofluorescence images in the Human Protein Atlas with affinity
    purification experiments from the BioPlex resource. Integration involves configuring
    each approach to produce a general measure of protein distance, then calibrating
    the two measures using machine learning.
  requires_registration: false
  url: https://nrnb.org/music/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:74
    category: B2AI:Application
    name: Multi-Scale Cell Type Annotation and Classification
    description: MuSIC (Multi-Scale Integrated Cell classification) is used in AI
      applications for automated cell type annotation in single-cell RNA-seq data
      by leveraging reference datasets and hierarchical classification strategies.
      The tool employs machine learning algorithms that integrate multiple sources
      of evidence including marker genes, reference atlases, and cross-dataset mapping
      to assign cell type labels with confidence scores. AI systems build upon MuSIC's
      probabilistic framework to develop more sophisticated deep learning models for
      cell state identification, rare cell type discovery, and cross-species cell
      type mapping. The multi-scale approach enables AI models to make predictions
      at varying levels of granularity, from broad cell classes to fine-grained subtypes,
      which is essential for biological interpretation and downstream analysis.
    used_in_bridge2ai: false
  - id: B2AI_APP:184
    category: B2AI:Application
    name: Multimodal Representation Learning and Contrastive Co-Embedding
    description: 'MuSIC applies deep learning-based representation learning to fuse
      immunofluorescence imaging and protein-protein interaction (PPI) network data
      for integrated mapping of subcellular architecture. The pipeline uses "a deep
      convolutional neural network" to embed each protein''s IF images as a 1024-dimension
      feature vector and "node2vec embeds AP-MS interaction neighborhoods as a second
      1024-dimension vector," computing protein-protein similarities as cosine distances
      in these high-dimensional embeddings. The CM4AI extension employs "contrastive
      deep learning to integrate those embeddings into a joint co-embedding per protein"
      and applies "graph representation learning (PPI networks processed with node2vec)"
      plus "image representation learning (IF images processed with a Human Protein
      Atlas deep learning model)" to produce integrated representations. MuSIC calibrates
      and combines the two distance measures "using machine learning, sampling known
      subcellular components of varying physical size to relate embedding distances
      to spatial scale," enabling cross-modal integration where "pairs close by one
      modality are enriched for closeness by the other." Downstream unsupervised ML
      tasks include "community detection and hierarchy creation from all-by-all similarities
      in co-embedding space" to produce hierarchical cell maps, with MuSIC 1.0 resolving
      69 subcellular systems. This demonstrates end-to-end application of deep learning
      for image feature extraction, graph embedding for network topology, metric learning
      for cross-modal calibration, and unsupervised clustering for biological system
      discovery.'
    references:
    - https://doi.org/10.1101/2020.06.21.163709
    - https://doi.org/10.1101/2024.05.21.589311
    used_in_bridge2ai: false
  - id: B2AI_APP:185
    category: B2AI:Application
    name: AI-Ready Dataset Packaging with FAIRSCAPE and RO-Crate
    description: 'CM4AI operationalizes MuSIC as an AI-ready data production pipeline
      that packages outputs with comprehensive provenance and metadata for reproducible
      machine learning benchmarking. MuSIC processes input datasets (Human Protein
      Atlas microscopy, BioPlex AP-MS interactions) "to RO-Crate outputs" with "FAIRSCAPE
      provenance graphs, persistent identifiers, and JSON-Schema characterization"
      that establish "FAIR metadata and persistent identifiers; computing machine-readable
      provenance graphs for inputs, computations, software, and outputs; and characterizing/validating
      datasets with JSON-Schema mini-data-dictionaries." The standardized outputs
      are "shared via NDEx/HiView/Cytoscape for downstream ML use and benchmarking,"
      enabling researchers to access hierarchical cell maps with "communities derived
      from embeddings that integrate protein image and affinity purification data"
      through interactive visualization that supports "zooming into nested communities,
      showing underlying interaction networks and providing lists of proteins per
      community." CM4AI frames AI-readiness around explicit criteria: "FAIR, Provenanced,
      Explainable, Ethical," emphasizing "interoperability, provenance metadata, statistical
      characterization, and ethical documentation." Released data packages include
      "software links, input datasets, dataset schemas, and deep provenance graphs"
      under CC-BY-NC-SA 4.0 licensing with Data Access Committee oversight. This standardization
      infrastructure "targets reproducible training/validation and structured benchmarking
      of multimodal cell-architecture models" and provides production-ready datasets
      for foundation model pretraining, graph neural network development, and multimodal
      integration model evaluation.'
    references:
    - https://doi.org/10.1101/2024.05.21.589311
    used_in_bridge2ai: false
  - id: B2AI_APP:186
    category: B2AI:Application
    name: Model Interpretability via Visible Neural Networks and LLM Annotation
    description: 'MuSIC provides interpretability mechanisms for AI models through
      biological grounding, structural validation, and natural language annotation
      of learned protein assemblies. The pipeline "align[s] cell-map communities to
      GO/Reactome for biological validation" and "use[s] an LLM to name protein assemblies
      with confidence scores, supporting interpretability and human-readable labels
      for ML evaluation," enabling researchers to understand what biological processes
      and structures correspond to learned representations. CM4AI "ranks communities
      by available structural data (PDB, AlphaFoldDB, crosslinking) and performs integrative
      structure modeling, which can provide structural restraints and validation data
      for model training and interpretation." The framework explicitly supports "visible
      machine learning" or "visible neural networks (VNNs)" that "link protein assemblies
      to cell-level phenotype predictions, supporting interpretability" by mapping
      learned neural network components directly to interpretable biological modules
      from MuSIC hierarchies. This approach enables "human-interpretable model inspection
      and hypothesis generation" where each layer or module of a neural architecture
      corresponds to a specific protein complex or cellular system with known functional
      annotations and structural evidence. The combination of ontology alignment,
      structural evidence ranking, and LLM-based natural language naming creates a
      multi-layered interpretability strategy that makes black-box models transparent
      to biologists, facilitates validation against orthogonal data sources (crystallographic
      structures, pathway databases), and supports explainability requirements for
      clinical or diagnostic AI applications.'
    references:
    - https://doi.org/10.1101/2024.05.21.589311
    used_in_bridge2ai: true
  - id: B2AI_APP:187
    category: B2AI:Application
    name: Foundation Models and Graph Neural Network Training on Cell Architecture
      Maps
    description: 'MuSIC outputs serve as training corpora and benchmarks for foundation
      models, graph neural networks, and multimodal generative models that require
      multi-scale, multi-modal, perturbation-aware cell architecture data. While not
      MuSIC-specific, AI guidance highlights that "MuSIC-style multi-modal, multi-scale,
      perturbation-aware datasets enable foundation-model pretraining, graph neural
      networks on protein assemblies, multimodal generative/integration models, and
      causal/perturbation-aware evaluation." MuSIC provides hierarchical protein assembly
      graphs with rich node features (image embeddings, interaction profiles, localization
      patterns) and edge attributes (co-membership scores, physical interactions)
      suitable for graph convolutional networks and attention-based architectures
      that learn from network topology. The availability of "co-measured dynamic/perturbation
      cohorts" from single-cell CRISPR screens integrated with MuSIC maps enables
      "action-response analyses (causal or intervention-aware training and validation)"
      where models can learn how genetic perturbations propagate through cellular
      systems. Foundation models (scGPT, transformers, Stofm) can be pretrained on
      MuSIC-derived representations to learn generalizable features of cellular organization
      transferable across cell types and experimental conditions. Multimodal integration
      models (totalVI, Multivi) benefit from MuSIC''s calibrated fusion of imaging
      and interaction modalities as reference implementations for cross-modal alignment
      strategies. The standardized, FAIR-compliant packaging of MuSIC outputs with
      "platform-aware alignment and leakage-aware partitions" supports "fair train/validation
      splits and generalization testing," enabling reproducible benchmarking of model
      architectures and rigorous evaluation of generalization to held-out cell lines,
      conditions, or perturbations.'
    references:
    - https://doi.org/10.1101/2024.05.21.589311
    - https://doi.org/10.48550/arxiv.2510.12498
    used_in_bridge2ai: false
- id: B2AI_STANDARD:801
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: MySQL
  formal_specification: https://github.com/mysql/mysql-server
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:23
  - B2AI_SUBSTRATE:37
  - B2AI_SUBSTRATE:9
  is_open: true
  name: MySQL
  purpose_detail: MySQL is an open-source relational database management system (RDBMS)
    developed and maintained by Oracle Corporation that implements the SQL (Structured
    Query Language) standard for storing, organizing, and retrieving structured data
    in tables with defined schemas, supporting ACID (Atomicity, Consistency, Isolation,
    Durability) transactions for data integrity and concurrent access control essential
    to enterprise applications and web services. As one of the world's most popular
    databases, MySQL powers millions of websites, applications, and data-driven systems
    ranging from small personal projects to massive-scale platforms including Facebook,
    Twitter, YouTube, Wikipedia, and major financial institutions requiring reliable,
    high-performance data storage. The database engine supports multiple storage engines
    (InnoDB for transactional workloads with foreign key constraints and crash recovery,
    MyISAM for read-heavy scenarios, Memory for temporary in-memory tables) allowing
    administrators to optimize performance characteristics per table based on access
    patterns and consistency requirements. MySQL provides comprehensive indexing strategies
    (B-tree, hash, full-text, spatial indexes for GIS data), query optimization with
    EXPLAIN plan analysis, replication topologies (primary-replica, multi-source,
    group replication for high availability), partitioning for horizontal scaling
    of large tables, and connection pooling for efficient resource utilization under
    concurrent load. The system integrates with virtually all programming languages
    through native drivers and ODBC/JDBC interfaces (Python MySQLdb/PyMySQL, PHP
    mysqli, Java JDBC, Node.js mysql2, R RMySQL), supporting both embedded applications
    and client-server architectures across operating systems (Linux, Windows, macOS,
    BSD). For data science and analytics, MySQL serves as a backend for data warehousing
    solutions, ETL pipelines, reporting dashboards (Tableau, Power BI, Looker), and
    machine learning feature stores where structured transactional or reference data
    must be joined with model training datasets or queried during inference for entity
    lookups and business rule application. MySQL's role in AI/ML workflows includes
    storing labeled training data with annotations and metadata, maintaining experiment
    tracking databases for hyperparameter tuning and model versioning (MLflow, Weights
    & Biases backends), serving feature values in real-time inference pipelines through
    low-latency point queries, and hosting patient registries, clinical trial data,
    EHR-derived tables, and biomedical ontology mappings in translational research
    platforms. The database supports stored procedures and triggers for complex business
    logic, views for data abstraction and security through row-level access control,
    JSON data type for semi-structured data alongside relational schemas, and full-text
    search capabilities for unstructured text fields. MySQL's ecosystem includes management
    tools (MySQL Workbench for visual design and administration, phpMyAdmin for web-based
    management), monitoring solutions (MySQL Enterprise Monitor, Prometheus exporters),
    and cloud-managed services (Amazon RDS for MySQL, Google Cloud SQL, Azure Database
    for MySQL) that provide automated backups, patch management, and scaling without
    infrastructure overhead. The database's mature security model offers authentication
    mechanisms (native passwords, LDAP/Kerberos integration, SSL/TLS encrypted connections),
    privilege management at database/table/column granularity, and audit logging for
    compliance with healthcare (HIPAA) and financial regulations. MySQL continues
    active development with regular releases introducing performance improvements
    (query optimizer enhancements, parallel replication), modern features (CTEs, window
    functions, recursive queries), and compatibility with cloud-native architectures
    (Kubernetes operators, containerized deployments), maintaining relevance as a
    foundational data persistence layer for contemporary data-intensive applications
    requiring structured data storage, referential integrity, and transactional guarantees.
  requires_registration: false
  url: https://www.mysql.com/
- id: B2AI_STANDARD:802
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - graphdataplatform
  concerns_data_topic:
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Neo4j Graph Data Platform
  formal_specification: https://github.com/neo4j/neo4j
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:14
  - B2AI_SUBSTRATE:15
  - B2AI_SUBSTRATE:25
  - B2AI_SUBSTRATE:9
  is_open: true
  name: Neo4j
  purpose_detail: "Neo4j is a native graph database platform implementing property
    graph model with ACID-compliant transactions, designed for storing and querying
    highly connected data through nodes, relationships, and properties, enabling efficient
    traversal of complex relationship patterns at scale. Developed by Neo4j, Inc.
    and available in both open-source Community Edition and commercial Enterprise
    Edition, Neo4j uses the declarative Cypher query language allowing pattern-matching
    queries that naturally express graph relationships (e.g., `MATCH (person:Person)-[:FRIENDS_WITH]->(friend)
    RETURN person, friend`) without complex join operations required in relational
    databases. The architecture employs index-free adjacency where each node directly
    references its adjacent nodes enabling constant-time traversals regardless of
    graph size, native graph storage optimized for relationship-heavy queries, and
    clustered deployment supporting high availability, horizontal scaling, and causal
    consistency across distributed systems. Key features include rich graph algorithms
    library (Graph Data Science) implementing PageRank, community detection, shortest
    path, centrality measures, and similarity algorithms; APOC (Awesome Procedures
    on Cypher) extending functionality with graph refactoring, data integration, and
    advanced algorithms; support for temporal queries and multiple graph projections;
    and integration with analytics tools (Apache Spark, Python data science stack)
    and machine learning frameworks. Biomedical and AI applications span knowledge
    graphs integrating biomedical ontologies, drug databases, genomic data, and literature
    for drug repurposing and target discovery; clinical decision support modeling
    patient-symptom-disease-treatment relationships for diagnosis recommendation;
    biological network analysis representing protein-protein interactions, metabolic
    pathways, gene regulatory networks for systems biology research; healthcare interoperability
    mapping relationships between FHIR resources, HL7 messages, and clinical terminologies;
    and ML feature engineering extracting graph embeddings, network features, and
    relationship patterns as input for predictive models. Neo4j enables real-time
    recommendation systems, fraud detection networks, identity and access management,
    supply chain optimization, and social network analysis, essential for data scientists
    working with connected data, bioinformaticians building biological knowledge bases,
    clinical informaticians developing patient care pathways, and ML engineers requiring
    graph-structured features."
  requires_registration: false
  url: https://neo4j.com/
- id: B2AI_STANDARD:803
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:16
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: NETME
  is_open: true
  name: NETME
  publication: doi:10.1007/S41109-021-00435-X
  purpose_detail: Starting from a set of fulltext obtained from PubMed, through an
    easy-to-use web interface, interactively extracts a group of biological elements
    stored into a selected list of ontological databases and then synthesizes a network
    with inferred relations among such elements.
  requires_registration: false
  url: https://netme.click/#/
- id: B2AI_STANDARD:804
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - scrnaseqanalysis
  concerns_data_topic:
  - B2AI_TOPIC:34
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: NetSeekR network analysis R package
  formal_specification: https://github.com/igbb-popescu-lab/NetSeekR
  is_open: true
  name: NetSeekR
  publication: doi:10.1186/S12859-021-04554-1
  purpose_detail: NetSeekR is an R package that implements a network analysis pipeline
    specifically designed for RNA-Seq time series data, enabling researchers to identify
    dynamic gene regulatory networks and temporal patterns of gene expression changes
    by integrating differential expression analysis, gene co-expression network construction,
    and time-dependent module detection to reveal biological processes activated or
    repressed across experimental time courses. Published in BMC Bioinformatics (2021),
    NetSeekR addresses the challenge that traditional differential expression analysis
    identifies individual genes changing over time but fails to capture coordinated
    regulation among functionally related genes and the temporal dynamics of regulatory
    network rewiring during biological processes such as development, disease progression,
    drug response, or environmental adaptation. The package provides an end-to-end
    workflow that begins with RNA-Seq count matrices and sample metadata, performs
    temporal differential expression analysis using methods like DESeq2 or edgeR to
    identify significantly changing genes, constructs weighted gene co-expression
    networks (WGCN) based on pairwise correlation of expression profiles across time
    points, and applies graph clustering algorithms to identify gene modules representing
    co-regulated functional units. NetSeekR implements time-aware network inference
    that accounts for temporal ordering of samples, allowing detection of early-response
    and late-response gene programs, identification of hub genes (highly connected
    regulators likely controlling module behavior), and tracking of how module membership
    and connectivity evolve across time series stages. The pipeline integrates functional
    enrichment analysis to annotate identified modules with Gene Ontology terms, pathway
    databases (KEGG, Reactome), and disease associations, providing biological context
    for temporal network patterns and enabling hypothesis generation about regulatory
    mechanisms. NetSeekR supports visualization of time series expression profiles,
    network topology with module-level summaries, and temporal trajectories of module
    activity scores, facilitating interpretation of complex multi-gene dynamics through
    dimensionality reduction from thousands of genes to dozens of interpretable modules.
    The package is designed for common RNA-Seq time series experimental designs including
    longitudinal sampling in developmental biology (embryogenesis, organogenesis),
    disease progression studies (cancer evolution, viral infection time courses, neurodegeneration),
    drug treatment response profiling (pharmacodynamics, resistance emergence), and
    environmental perturbation experiments (stress response, circadian rhythms). For
    machine learning and systems biology applications, NetSeekR-derived network modules
    serve as biologically informed feature sets for predictive modeling (using module
    eigengenes as features instead of individual genes), enable transfer learning
    across related time series datasets by mapping conserved modules, and provide
    mechanistic interpretations for AI models trained on temporal omics data by linking
    predictions to specific regulatory programs. The package outputs can be integrated
    with causal inference methods to distinguish correlation from regulatory causation,
    combined with ChIP-seq or ATAC-seq data to validate predicted transcription factor-target
    relationships, and used to prioritize candidate drug targets by identifying hub
    genes in disease-associated temporal modules. NetSeekR is implemented in R with
    dependencies on Bioconductor packages (DESeq2, edgeR, WGCNA), provides example
    datasets and vignettes for reproducible analysis, and outputs network structures
    compatible with Cytoscape and other network visualization tools, supporting integrative
    analysis of temporal transcriptomics in developmental biology, immunology, cancer
    research, and pharmacology where understanding time-dependent gene regulation
    is essential for mechanistic insight and therapeutic intervention.
  requires_registration: false
  url: https://github.com/igbb-popescu-lab/NetSeekR
- id: B2AI_STANDARD:805
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - graphdataplatform
  concerns_data_topic:
  - B2AI_TOPIC:21
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Network Data Exchange
  is_open: true
  name: NDEx
  purpose_detail: The NDEx Project provides an open-source framework where scientists
    and organizations can store, share, manipulate, and publish biological network
    knowledge.
  requires_registration: true
  url: https://www.ndexbio.org/
- id: B2AI_STANDARD:806
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - scrnaseqanalysis
  concerns_data_topic:
  - B2AI_TOPIC:34
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: NeuCA - Neural-network based Cell Annotation tool
  formal_specification: https://github.com/haoharryfeng/NeuCA
  is_open: true
  name: NeuCA
  publication: doi:10.1038/s41598-021-04473-4
  purpose_detail: A R/Bioconductor tool for cell type annotation using single-cell
    RNA-seq data. It is a supervised cell label assignment method that uses existing
    scRNA-seq data with known labels to train a neural network-based classifier, and
    then predict cell labels in single-cell RNA-seq data of interest.
  requires_registration: false
  url: https://github.com/haoharryfeng/NeuCA
  has_application:
  - id: B2AI_APP:75
    category: B2AI:Application
    name: Cell Type Annotation in Single-Cell Genomics
    description: NeuCA (Neural network-based Cell Annotation) is used in AI applications
      for automated cell type identification in single-cell RNA sequencing data, leveraging
      deep learning to achieve accurate, scalable annotation across diverse tissues
      and species. The tool employs neural network architectures optimized for single-cell
      expression profiles to classify cells based on marker gene expression patterns,
      enabling rapid analysis of large-scale atlas projects and clinical samples.
      NeuCA's transfer learning capabilities allow models trained on reference atlases
      to annotate new datasets with limited manual curation, supporting applications
      in cancer cell identification, immune profiling, and developmental biology.
      The method provides confidence scores for cell type assignments and can identify
      novel or transitional cell states.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:807
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:20
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Nextflow
  formal_specification: https://github.com/nextflow-io/nextflow
  has_relevant_organization:
  - B2AI_ORG:90
  is_open: true
  name: Nextflow
  purpose_detail: Enables scalable and reproducible scientific workflows using software
    containers. It allows the adaptation of pipelines written in the most common scripting
    languages.
  requires_registration: false
  url: https://www.nextflow.io/
- id: B2AI_STANDARD:808
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - cloudplatform
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: NHGRI Analysis Visualization and Informatics Lab-space
  formal_specification: https://github.com/anvilproject
  has_relevant_organization:
  - B2AI_ORG:73
  is_open: true
  name: AnVIL
  purpose_detail: AnVIL is NHGRI's Genomic Data Science Analysis, Visualization, and
    Informatics Lab-Space.
  requires_registration: true
  url: https://anvilproject.org/
- id: B2AI_STANDARD:809
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:23
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: OmicsEV
  formal_specification: https://github.com/bzhanglab/OmicsEV
  is_open: true
  name: OmicsEV
  publication: doi:10.1093/bioinformatics/btac698
  purpose_detail: An R package for quality evaluation of omics data tables. For each
    data table, OmicsEV uses a series of methods to evaluate data depth, data normalization,
    batch effect, biological signal, platform reproducibility, and multi-omics concordance,
    producing comprehensive visual and quantitative evaluation results that help assess
    data quality of individual data tables and facilitate the identification of the
    optimal data processing method and parameters for the omics study under investigation.
  requires_registration: false
  url: https://github.com/bzhanglab/OmicsEV
- id: B2AI_STANDARD:810
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology Access Kit
  formal_specification: https://github.com/INCATools/ontology-access-kit
  has_relevant_organization:
  - B2AI_ORG:58
  is_open: true
  name: OAK
  purpose_detail: OAK provides a collection of interfaces for various ontology operations.
  requires_registration: false
  url: https://incatools.github.io/ontology-access-kit/
- id: B2AI_STANDARD:811
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Ontology Development Kit
  formal_specification: https://github.com/INCATools/ontology-development-kit
  has_relevant_organization:
  - B2AI_ORG:58
  is_open: true
  name: ODK
  publication: doi:10.1093/database/baac087
  purpose_detail: A toolkit and workflow system for managing the ontology life-cycle.
  requires_registration: false
  url: https://github.com/INCATools/ontology-development-kit
- id: B2AI_STANDARD:812
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:18
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: OpenHealth
  is_open: true
  name: OpenHealth
  publication: doi:10.1109/MDAT.2019.2906110
  purpose_detail: An open-source platform for wearable health monitoring. It aims
    to design a standard set of hardware/software and wearable devices that can enable
    autonomous collection of clinically relevant data.
  requires_registration: false
  url: https://sites.google.com/view/openhealth-wearable-health/home
- id: B2AI_STANDARD:813
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: pandas
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:29
  - B2AI_SUBSTRATE:8
  is_open: true
  name: pandas
  purpose_detail: An open source data analysis and manipulation tool built on top
    of the Python programming language.
  requires_registration: false
  url: https://pandas.pydata.org/
- id: B2AI_STANDARD:814
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - datavisualization
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Panel
  formal_specification: https://github.com/holoviz/panel
  is_open: true
  name: Panel
  purpose_detail: An open-source Python library that lets you create custom interactive
    web apps and dashboards by connecting user-defined widgets to plots, images, tables,
    or text.
  requires_registration: false
  url: https://panel.holoviz.org/
- id: B2AI_STANDARD:815
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: PostgreSQL
  formal_specification: https://github.com/postgres/postgres
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:31
  - B2AI_SUBSTRATE:37
  - B2AI_SUBSTRATE:9
  is_open: true
  name: PostgreSQL
  purpose_detail: An open source object-relational database system.
  requires_registration: false
  url: https://www.postgresql.org/
- id: B2AI_STANDARD:816
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - machinelearningframework
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: PyTorch
  formal_specification: https://github.com/pytorch/pytorch
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:33
  is_open: true
  name: PyTorch
  purpose_detail: A popular machine learning platform.
  related_to:
  - B2AI_STANDARD:354
  requires_registration: false
  url: https://pytorch.org/
  has_application:
  - id: B2AI_APP:76
    category: B2AI:Application
    name: Deep Learning for Biomedical Research and Clinical AI
    description: PyTorch is the dominant framework for biomedical AI research, used
      extensively for developing deep learning models across medical imaging, genomics,
      drug discovery, and clinical prediction. Researchers leverage PyTorch's dynamic
      computational graphs, extensive ecosystem (torchvision, torchaudio, TorchIO),
      and pretrained models to build custom neural architectures for tasks like cancer
      detection in pathology images, protein structure prediction, medical report
      generation, and patient outcome forecasting. PyTorch's flexibility enables rapid
      prototyping of novel architectures, its strong academic community supports reproducible
      research through shared model implementations, and its production deployment
      tools (TorchServe, TorchScript) facilitate clinical translation of research
      models.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:817
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - notebookplatform
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Quarto publishing system
  formal_specification: https://github.com/quarto-dev/quarto-cli
  is_open: true
  name: Quarto
  purpose_detail: An open-source scientific and technical publishing system built
    on Pandoc.
  requires_registration: false
  url: https://quarto.org/
- id: B2AI_STANDARD:818
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - cloudplatform
  concerns_data_topic:
  - B2AI_TOPIC:4
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Rare Disease Cures Accelerator-Data and Analytics Platform
  has_relevant_organization:
  - B2AI_ORG:31
  is_open: false
  name: RDCA-DAP
  purpose_detail: The Rare Disease Cures Accelerator-Data and Analytics Platform (RDCA-DAP)
    is an FDA-funded initiative that provides a centralized and standardized infrastructure
    to support and accelerate rare disease characterization, with the goal of accelerating
    therapy development across rare diseases.
  requires_registration: true
  url: https://c-path.org/programs/rdca-dap/
- id: B2AI_STANDARD:819
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: refget API
  formal_specification: https://samtools.github.io/hts-specs/refget.html
  has_relevant_organization:
  - B2AI_ORG:34
  is_open: true
  name: refget
  purpose_detail: Enables access to reference genomic sequences without ambiguity
    from different databases and servers using a checksum identifier based on the
    sequence content itself.
  requires_registration: false
  url: https://samtools.github.io/hts-specs/refget.html
- id: B2AI_STANDARD:820
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - machinelearningframework
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Relexi
  formal_specification: https://github.com/flexi-framework/relexi
  is_open: true
  name: Relexi
  publication: doi:10.1016/j.simpa.2022.100422
  purpose_detail: Relexi is an open source reinforcement learning (RL) framework written
    in Python and based on TensorFlow's RL library TF-Agents. Relexi allows to employ
    RL for environments that require computationally intensive simulations like applications
    in computational fluid dynamics. For this, Relexi couples legacy simulation codes
    with the RL library TF-Agents at scale on modern high-performance computing (HPC)
    hardware using the SmartSim library. Relexi thus provides an easy way to explore
    the potential of RL for HPC applications.
  requires_registration: false
  url: https://github.com/flexi-framework/relexi
  has_application:
  - id: B2AI_APP:77
    category: B2AI:Application
    name: Explainable Reinforcement Learning for Clinical Decision Support
    description: Relexi is used in biomedical AI for developing interpretable reinforcement
      learning systems that can explain their decision-making process, crucial for
      clinical applications where treatment recommendations must be understandable
      to physicians. The framework enables training of RL agents for sequential clinical
      decisions (medication dosing, ventilator management, treatment timing) while
      maintaining explainability through attention mechanisms and policy distillation.
      Relexi supports safe exploration in healthcare settings by incorporating domain
      constraints and enabling clinicians to understand why specific actions are recommended.
      Applications include explainable sepsis treatment protocols, interpretable insulin
      dosing algorithms, and transparent clinical trial enrollment strategies where
      understanding the agent's reasoning is essential for clinical trust and regulatory
      approval.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:821
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:31
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Research Electronic Data Capture
  has_relevant_organization:
  - B2AI_ORG:114
  - B2AI_ORG:117
  is_open: true
  name: REDCap
  publication: doi:10.1016/j.jbi.2008.08.010
  purpose_detail: REDCap (Research Electronic Data Capture) is a mature, metadata-driven
    software platform designed for building and managing online databases and surveys
    for clinical and translational research studies, developed at Vanderbilt University
    and now maintained by an international consortium of over 6,000 institutional
    partners across 150 countries. The platform provides a comprehensive web-based
    application with an intuitive interface for rapid development of electronic data
    capture instruments using a data dictionary that defines field types, validation
    rules, branching logic, and calculated fields without requiring programming expertise.
    Core features include secure user authentication with role-based access controls,
    audit trails tracking all data changes with timestamps and user attribution, automated
    data quality checks through validation rules and range constraints, repeating
    instruments and events for longitudinal data collection, survey distribution with
    public and private links, and mobile-responsive data entry forms accessible from
    any device. REDCap supports complex study designs through its project management
    capabilities including multi-site studies, randomization modules, scheduling and
    calendar features, file upload repositories, and automated survey invitations
    with customizable reminder logic. The platform emphasizes data security and regulatory
    compliance with HIPAA-compatible infrastructure, 21 CFR Part 11 validation documentation,
    encrypted data transmission and storage, automatic de-identification tools for
    exporting datasets, and comprehensive logging for audit purposes. REDCap's extensibility
    is enabled through a robust RESTful API supporting programmatic data import/export,
    project metadata retrieval, and integration with external systems, plus the REDCap
    External Modules framework allowing custom functionality development. The Clinical
    Data Interoperability Services (CDIS) extension provides FHIR-based integration
    enabling bidirectional data exchange with electronic health records, supporting
    real-time EHR data extraction and write-back capabilities. Data analysis features
    include built-in descriptive statistics, graphical data visualization, data export
    to multiple formats (CSV, SPSS, SAS, R, Stata, Excel), and the REDCap-ETL tool
    for transforming and loading data into relational databases for advanced analytics.
    The platform has been extensively adopted in AI and machine learning workflows
    as a standardized data capture layer feeding training datasets, with documented
    applications spanning multimodal ML combining clinical tabular data with imaging
    features, federated rare disease registries supporting deep learning on aggregated
    cohorts, in-app risk scoring and triage algorithms implemented via calculated
    fields, reproducible ETL pipelines to SQLite/Python/scikit-learn for nuclear
    medicine ML, FHIR-mediated interoperable AI data pipelines for oncology research,
    and prospective ML model monitoring frameworks with continuous retraining tied
    to REDCap-collected outcomes. REDCap's metadata-driven architecture, standardized
    common data elements support, API-driven ETL capabilities, and integration with
    research informatics ecosystems make it essential infrastructure for rigorous,
    reproducible clinical research and AI-ready biomedical data collection.
  requires_registration: true
  url: https://www.project-redcap.org/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:78
    category: B2AI:Application
    name: Clinical Research Data Capture for ML Model Development
    description: REDCap is widely used in AI applications as a platform for collecting
      high-quality, structured clinical research data that feeds machine learning
      model development and validation studies. AI researchers leverage REDCap's data
      dictionaries, validation rules, and standardized data collection instruments
      to create clean training datasets for predictive models in clinical trials,
      cohort studies, and patient registries. REDCap's API enables automated data
      extraction for ML pipelines, and its audit trails and data quality features
      ensure reproducibility in AI research. The platform is particularly valuable
      for multi-site AI studies where standardized data collection across institutions
      is essential for model generalizability.
    used_in_bridge2ai: false
  - id: B2AI_APP:159
    category: B2AI:Application
    name: Interoperable AI Data Pipelines via REDCap-to-FHIR ETL
    references:
    - https://doi.org/10.1101/2024.03.15.24303032
    description: The EuCanImage consortium uses REDCap as a standardized intermediary
      data store with Python ETL pipelines that map to FHIR endpoints and common
      data models to create AI-ready datasets for oncology research. REDCap Clinical
      Data Interoperability Services (CDIS) enables high-fidelity extraction of treatment
      data from EHRs via FHIR, positioning REDCap within AI data pipelines by unlocking
      structured, trustworthy inputs for downstream modeling. This approach enables
      harmonized multicenter data capture and transformation for AI model development
      and submission to permanent repositories, demonstrating how REDCap supports
      FAIR data principles and interoperability standards essential for collaborative
      AI research.
    used_in_bridge2ai: false
  - id: B2AI_APP:160
    category: B2AI:Application
    name: Multimodal Machine Learning Combining Imaging and Clinical Data
    references:
    - https://doi.org/10.48550/arxiv.2501.11535
    description: REDCap-derived tabular clinical variables are combined with radiomics
      features in multimodal machine learning models for cancer staging and prognosis.
      In hepatocellular carcinoma studies, XGBoost models trained on combined imaging
      and REDCap tabular data achieved superior performance compared to single-modality
      models, with feature importance analyses identifying critical REDCap-captured
      variables such as age and tumor characteristics. This application demonstrates
      REDCap's role in providing curated clinical features that complement imaging
      biomarkers, with preprocessing workflows that filter REDCap columns based on
      data quality thresholds and domain relevance before model training.
    used_in_bridge2ai: false
  - id: B2AI_APP:161
    category: B2AI:Application
    name: Federated Rare Disease Registries Supporting Deep Learning
    references:
    - https://doi.org/10.1093/jamia/ocad132
    description: The National Mesothelioma Virtual Bank uses REDCap with standardized
      Common Data Elements (CDEs) and an API-driven ETL pipeline (CSV export via REDCap
      API, processed through R and JavaScript, served via AWS Lambda) to power cohort
      discovery and deidentified data provisioning for advanced analytics and deep
      learning on rare-disease cohorts. This federated REDCap deployment aggregates
      over 2,000 cases across multiple institutions, enabling scalable multicenter
      datasets with faceted search capabilities and honest-broker data protection
      models. The standardized data capture and automated deidentification workflows
      make these registries valuable resources for training ML models on rare diseases
      where data scarcity is a major challenge.
    used_in_bridge2ai: false
  - id: B2AI_APP:162
    category: B2AI:Application
    name: REDCap-Embedded Risk Scoring and Triage Algorithms
    references:
    - https://doi.org/10.1017/cts.2024.1134
    description: Clinical decision support algorithms are implemented directly within
      REDCap using calculated fields and conditional logic to perform real-time risk
      prediction and automated triage. The Mayo Clinic deployed a Risk Prediction
      and Management (RPM) scoring tool in REDCap that evaluates operational risks
      for incoming clinical studies, automatically scoring approximately 200 projects
      and routing high-risk cases to physician leadership for review. This in-app
      decision support approach leverages REDCap's computational capabilities to operationalize
      algorithmic scoring without requiring external systems, demonstrating how REDCap
      can serve not only as a data capture platform but also as a deployment environment
      for prediction models in operational workflows.
    used_in_bridge2ai: false
  - id: B2AI_APP:163
    category: B2AI:Application
    name: Prospective ML Model Monitoring and Continuous Retraining
    references:
    - https://doi.org/10.1186/s13017-025-00594-7
    description: The MINERVA study protocol uses REDCap as the secure primary data
      repository for training a deep learning relapse-risk model with plans for continuous
      real-time monitoring, automated logging, and iterative retraining tied to REDCap-collected
      outcomes. The framework includes web-based monitoring of model performance metrics
      (sensitivity, specificity, PPV, NPV), automated capture of instances where predictions
      diverge from clinical outcomes, root-cause analysis workflows, and data validation
      checks that feed back into model refinement. This application demonstrates how
      REDCap can support the full ML lifecycle from initial training data collection
      through deployment monitoring and model maintenance, with role-based access
      controls and deidentification protocols ensuring data security throughout the
      process.
    used_in_bridge2ai: false
  - id: B2AI_APP:164
    category: B2AI:Application
    name: Reproducible ML Pipelines for Nuclear Medicine
    references:
    - https://iris.unito.it/bitstream/2318/2067667/2/01_PhD_thesis_Rovera_Guido.pdf
    description: REDCap-centered workflows in nuclear medicine use REDCap for standardized
      clinical data capture with ETL pipelines (REDCap-ETL or custom Python/R scripts)
      that export to SQLite databases for machine learning analyses with scikit-learn.
      These pipelines emphasize anonymization, reproducibility through version-controlled
      scripts, dynamic real-time data retrieval, and automated computation of derived
      variables to support ML workflows. The use of REDCap's repeating instruments
      feature enables capture of relational time-series events, while project XML
      templates support multi-center reproducibility. This approach demonstrates how
      REDCap integrates into end-to-end ML pipelines for medical imaging and clinical
      prediction tasks, with synthetic data generation capabilities for testing model
      scalability.
    used_in_bridge2ai: false
  - id: B2AI_APP:165
    category: B2AI:Application
    name: Clinical Decision Support for Post-Surgical Risk and STI Recommendations
    references:
    - https://doi.org/10.1017/cts.2021.529
    description: REDCap implementations have been documented generating ML-based risk
      predictions for post-surgical outcomes, producing sexually transmitted infection
      (STI) test recommendations and orders, and improving clinical handoffs in surgical
      oncology through decision support workflows. These applications leverage REDCap's
      data capture capabilities combined with statistical and machine learning methods
      to deliver actionable clinical recommendations, though published documentation
      of such novel uses remains limited. The integration patterns demonstrate REDCap's
      potential to bridge research data collection and clinical decision support,
      enabling evidence-based interventions derived from ML models trained on REDCap-captured
      data.
    used_in_bridge2ai: false
  - id: B2AI_APP:166
    category: B2AI:Application
    name: Translational Research Frameworks Integrating Multimodal Data for Predictive
      Modeling
    references:
    - https://iris.unito.it/bitstream/2318/2067667/2/01_PhD_thesis_Rovera_Guido.pdf
    description: Preclinical and translational research frameworks use REDCap instruments
      for single-measure clinical data capture with customized API and ETL pipelines
      to integrate multimodal datasets (clinical, imaging, molecular) for dashboards
      and predictive modeling applications. These frameworks demonstrate REDCap's
      role as a data backbone that feeds analytic workflows, supporting integrated
      datasets that combine REDCap-captured clinical assessments with laboratory results,
      imaging features, and genomic data. The metadata-driven structure of REDCap
      projects facilitates standardized variable definitions across data modalities,
      enabling machine learning models to leverage complementary information sources
      for improved prediction accuracy in translational medicine.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:822
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Sage Synapse
  formal_specification: https://github.com/Sage-Bionetworks/Synapse-Repository-Services
  has_relevant_organization:
  - B2AI_ORG:86
  is_open: true
  name: Synapse
  publication: doi:10.2139/ssrn.3502410
  purpose_detail: Synapse is a collaborative research platform developed by Sage Bionetworks
    that provides web services and tools for aggregating, organizing, analyzing, and
    sharing scientific data, code, and insights across biomedical research communities.
    The platform implements a comprehensive data management infrastructure through
    its Repository Services, offering RESTful JSON APIs for entity management, file
    storage, versioning, and access control. Core functionality includes hierarchical
    project organization with entities such as Projects, Folders, Files, Tables, and
    Views that support structured data organization and metadata annotation. Synapse
    provides advanced data access control through Access Requirements and Access Approvals,
    enabling researchers to implement controlled access policies including ACTAccessRequirement
    for managed access to sensitive data. The platform supports collaborative research
    through Teams, subscription services for change notifications, messaging capabilities,
    and wiki documentation integrated with research artifacts. Data governance features
    include a Qualified Research Program that balances broad researcher access with
    participant protections, research project management for data access requests,
    and submission workflows for access review by the Access and Compliance Team (ACT).
    Technical capabilities include multi-part file uploads, version tracking with
    provenance through Activity records, search indexing using OpenSearch for entity
    discovery, DOI minting for permanent identifiers, and integration with OAuth2
    authentication and OpenID Connect. Synapse implements the GA4GH DRS (Data Repository
    Service) API specification for standardized data access, supports tabular data
    through Table entities with SQL-like querying, and provides form-based data collection
    with review workflows. The platform serves as infrastructure for open science
    initiatives, enabling data sharing while encoding contractual protections for
    research participants, and has been deployed in numerous biomedical studies including
    mHealth research such as the mPower Parkinson disease observational study.
  requires_registration: true
  url: https://www.synapse.org/
  used_in_bridge2ai: true
- id: B2AI_STANDARD:823
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: SemEHR
  formal_specification: https://github.com/CogStack/CogStack-SemEHR
  is_open: true
  name: SemEHR
  publication: doi:10.1093/jamia/ocx160
  purpose_detail: An open source semantic search and analytics tool for EHRs.
  requires_registration: false
  url: https://github.com/CogStack/CogStack-SemEHR
- id: B2AI_STANDARD:824
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - scrnaseqanalysis
  concerns_data_topic:
  - B2AI_TOPIC:34
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Single-cell Analysis Via Expression Recovery
  formal_specification: https://github.com/mohuangx/SAVER
  is_open: true
  name: SAVER
  publication: doi:10.1038/s41592-018-0033-z
  purpose_detail: A regularized regression prediction and empirical Bayes method to
    recover the true gene expression profile in noisy and sparse scRNA-seq data.
  requires_registration: false
  url: https://mohuangx.github.io/SAVER/
- id: B2AI_STANDARD:825
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Snorkel
  formal_specification: https://github.com/snorkel-team/snorkel
  is_open: false
  name: Snorkel
  purpose_detail: Snorkel is a platform for automated data labeling. It has since
    been extended into a full machine learning platform (see https://snorkel.ai/).
  requires_registration: true
  url: https://www.snorkel.org/
  collection:
  - deprecated
- id: B2AI_STANDARD:826
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:25
  - B2AI_TOPIC:35
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: SnpEff
  formal_specification: https://github.com/pcingola/SnpEff
  is_open: true
  name: SnpEff
  publication: doi:10.4161/fly.19695
  purpose_detail: SnpEff is a variant annotation and effect prediction tool. It annotates
    and predicts the effects of genetic variants (such as amino acid changes).
  requires_registration: false
  url: https://pcingola.github.io/SnpEff/se_introduction/
- id: B2AI_STANDARD:827
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - scrnaseqanalysis
  concerns_data_topic:
  - B2AI_TOPIC:34
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Souporcell
  formal_specification: https://github.com/wheaton5/souporcell
  is_open: true
  name: Souporcell
  publication: doi:10.1038/s41592-020-0820-1
  purpose_detail: A method to cluster cells using the genetic variants detected within
    the scRNA-seq reads.
  requires_registration: false
  url: https://github.com/wheaton5/souporcell
- id: B2AI_STANDARD:828
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:33
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Spliced Transcripts Alignment to a Reference
  formal_specification: https://github.com/alexdobin/STAR
  is_open: true
  name: STAR
  publication: doi:10.1093/bioinformatics/bts635
  purpose_detail: Software based on an RNA-seq alignment algorithm that uses sequential
    maximum mappable seed search in uncompressed suffix arrays followed by seed clustering
    and stitching procedure.
  requires_registration: false
  url: https://github.com/alexdobin/STAR
- id: B2AI_STANDARD:829
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Systems Biology Layout and Rendering Service
  formal_specification: https://github.com/iVis-at-Bilkent/syblars
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:36
  is_open: true
  name: SyBLaRS
  publication: doi:10.1371/journal.pcbi.1010635
  purpose_detail: A web service for automatic layout of biological data in various
    standard formats as well as construction of customized images in both raster image
    and scalable vector formats of these maps. Some of the supported standards are
    more generic such as GraphML and JSON, whereas others are specialized to biology
    such as SBGNML (The Systems Biology Graphical Notation Markup Language) and SBML
    (The Systems Biology Markup Language).
  related_to:
  - B2AI_STANDARD:337
  - B2AI_STANDARD:289
  requires_registration: false
  url: http://syblars.cs.bilkent.edu.tr/
- id: B2AI_STANDARD:830
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Task Execution Service
  has_relevant_organization:
  - B2AI_ORG:34
  is_open: true
  name: TES
  purpose_detail: The Task Execution Service (TES) API is a proposed standard for
    describing and executing tasks in a platform-agnostic way. Includes a TES API
    validator and a Task Execution API specification.
  requires_registration: false
  url: https://ga4gh.github.io/task-execution-schemas/docs/
- id: B2AI_STANDARD:831
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - machinelearningframework
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Tensorflow
  formal_specification: https://github.com/tensorflow/tensorflow
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:42
  has_relevant_organization:
  - B2AI_ORG:37
  is_open: true
  name: TF
  purpose_detail: TensorFlow is an end-to-end open source platform for machine learning.
    It has an ecosystem of tools, libraries and community resources that lets researchers
    and developers easily build and deploy ML powered applications.
  related_to:
  - B2AI_STANDARD:354
  requires_registration: false
  url: https://www.tensorflow.org/
  has_application:
  - id: B2AI_APP:79
    category: B2AI:Application
    name: Production-Scale Healthcare AI Systems and Deployment
    description: TensorFlow is widely used for deploying production-grade AI systems
      in healthcare, particularly for applications requiring high-throughput inference,
      mobile deployment, and integration with enterprise IT infrastructure. Healthcare
      organizations leverage TensorFlow's mature ecosystem (TF Serving, TF Lite, TF.js)
      to deploy models for real-time clinical decision support, mobile diagnostic
      apps, and edge computing in medical devices. The framework's strong support
      for model optimization, quantization, and hardware acceleration (TPUs, GPUs)
      enables efficient deployment of complex models like retinal disease screening
      systems, ECG interpretation algorithms, and clinical NLP pipelines. TensorFlow
      Extended (TFX) provides production ML pipelines for managing data validation,
      model training, and continuous monitoring in regulated healthcare environments.
    used_in_bridge2ai: false
    references:
    - https://www.tensorflow.org/tfx
- id: B2AI_STANDARD:832
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - cloudplatform
  concerns_data_topic:
  - B2AI_TOPIC:20
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Terra Community Workbench
  formal_specification: https://github.com/DataBiosphere/terra-ui
  has_relevant_organization:
  - B2AI_ORG:71
  is_open: true
  name: Terra
  purpose_detail: Terra is a cloud-native platform for biomedical researchers to access
    data, run analysis tools, and collaborate.
  requires_registration: true
  url: https://app.terra.bio/
- id: B2AI_STANDARD:833
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: The R Project for Statistical Computing
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:34
  - B2AI_SUBSTRATE:35
  - B2AI_SUBSTRATE:40
  is_open: true
  name: R
  purpose_detail: A free software environment for statistical computing and graphics.
  requires_registration: false
  url: https://www.r-project.org/
  used_in_bridge2ai: true
- id: B2AI_STANDARD:834
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - deprecated
  - machinelearningframework
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Theano
  formal_specification: https://github.com/Theano/Theano
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:1
  is_open: true
  name: Theano
  purpose_detail: A Python library that allows you to define, optimize, and evaluate
    mathematical expressions involving multi-dimensional arrays efficiently. It is
    being continued as aesara.
  related_to:
  - B2AI_STANDARD:354
  requires_registration: false
  url: https://github.com/Theano/Theano
  has_application:
  - id: B2AI_APP:80
    category: B2AI:Application
    name: Legacy Deep Learning Models and Reproducible Research
    description: Theano was historically important in early biomedical deep learning
      research and continues to be relevant for reproducing published models and maintaining
      legacy clinical AI systems. Many influential early papers in medical AI used
      Theano, and researchers still need to run these models for comparison benchmarks,
      reproduce published results, and maintain systems deployed before modern frameworks
      emerged. The library's symbolic computation approach and automatic differentiation
      influenced the design of current frameworks, and understanding Theano remains
      valuable for historical context in AI research. While active development has
      ceased (succeeded by Aesara), Theano-based code remains in production in some
      clinical settings and research archives.
    used_in_bridge2ai: false
    references:
    - https://theano-pymc.readthedocs.io/
- id: B2AI_STANDARD:835
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Tool Registry Service
  has_relevant_organization:
  - B2AI_ORG:34
  is_open: true
  name: TRS
  purpose_detail: A proposed standard for sharing and discovering tools and workflows
    in a platform-agnostic way. Includes a Tool Registry Search validator and a Tool
    Discovery API specification.
  requires_registration: false
  url: https://ga4gh.github.io/tool-registry-service-schemas/
- id: B2AI_STANDARD:836
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:4
  - B2AI_TOPIC:13
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: UAB Biomedical Research Information Technology Enhancement Commons
    Program
  has_relevant_organization:
  - B2AI_ORG:94
  is_open: false
  name: U-BRITE
  purpose_detail: U-BRITE (UAB Biomedical Research Information Technology Enhancement)
    assembles new and existing HIPAA-compliant, high-performance informatics tools
    to provide researchers with a means to better manage and analyze clinical and
    genomic data sets and implements a translational research commons to facilitate
    and enable interdisciplinary team science across geographical locations.
  requires_registration: true
  url: https://ubrite.org/
- id: B2AI_STANDARD:837
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Usagi
  formal_specification: https://github.com/OHDSI/Usagi
  has_relevant_organization:
  - B2AI_ORG:76
  has_training_resource:
  - B2AI_STANDARD:844
  is_open: true
  name: Usagi
  purpose_detail: An application to help create mappings between coding systems and
    the Vocabulary standard concepts.
  requires_registration: false
  url: https://github.com/OHDSI/Usagi
- id: B2AI_STANDARD:838
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:34
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Vireo
  formal_specification: https://github.com/single-cell-genetics/vireo
  is_open: true
  name: Vireo
  publication: doi:10.1186/s13059-019-1865-2
  purpose_detail: A computationally efficient Bayesian model to demultiplex single-cell
    data from pooled experimental designs.
  requires_registration: false
  url: https://github.com/single-cell-genetics/vireo
- id: B2AI_STANDARD:839
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - cloudservice
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Wasabi Cloud Storage
  is_open: false
  name: Wasabi
  purpose_detail: 'Wasabi Cloud Storage is a high-performance, S3-compatible object storage service providing hot cloud storage with predictable pricing, no egress fees, and 80% lower total cost of ownership compared to hyperscaler alternatives (AWS S3, Azure Blob, Google Cloud Storage), designed for data-intensive workloads requiring frequent access and large-scale data archival. Founded in 2017 by Carbonite co-founders, Wasabi operates 16 globally distributed storage regions across North America, Europe, Asia-Pacific, with SOC-2, ISO 27001, and PCI-DSS certified data centers providing enterprise-grade security, immutability features (WORM, Object Lock), and compliance certifications (HIPAA, GDPR, FERPA) suitable for regulated industries. Wasabi''s "always hot" architecture eliminates tiered storage complexity by storing all data on high-performance disk arrays with consistent millisecond latency for reads/writes, avoiding cold storage retrieval delays that plague glacier-tier alternatives while maintaining cost parity with archival storage services. The platform''s S3 API compatibility ensures drop-in replacement for existing AWS workflows, supporting standard S3 operations (PUT, GET, LIST, multipart uploads), bucket policies, IAM-style access controls, and seamless integration with S3-compatible tools (AWS CLI, SDKs, third-party backup software, media asset managers). Wasabi''s zero-fee model for egress, API requests, and reads eliminates the unpredictable costs that typically double hyperscaler storage bills, enabling cost-effective access for AI/ML training pipelines, video surveillance archives, genomic datasets, and medical imaging repositories where frequent data retrieval is essential. The service provides native integrations with major backup platforms (Veeam, Commvault, Rubrik), media workflows (Adobe Premiere, Frame.io), surveillance systems (Milestone, Hanwha), and object storage gateways, supporting hybrid cloud architectures where on-premises applications seamlessly tier to Wasabi for capacity expansion and disaster recovery. For AI/ML workloads, Wasabi''s high-throughput object storage (up to 10 Gbps per connection) supports rapid dataset ingestion for model training, low-latency access to training data during distributed training across GPU clusters, and cost-effective storage for model checkpoints, experiment artifacts, and inference result archives without egress penalties for iterative model development and hyperparameter tuning requiring repeated dataset access.'
  requires_registration: true
  url: https://wasabi.com/
- id: B2AI_STANDARD:840
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Weights and Balances platform
  is_open: true
  name: W&B
  purpose_detail: Platform for tracking, comparing, and visualizing machine learning
    experiments.
  requires_registration: true
  url: https://wandb.ai/
- id: B2AI_STANDARD:841
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Workflow Execution Service
  has_relevant_organization:
  - B2AI_ORG:34
  is_open: true
  name: WES
  purpose_detail: The Workflow Execution Service (WES) is a GA4GH standard API specification
    that provides a platform-agnostic approach for submitting and monitoring workflow
    execution across different computational environments. WES enables researchers
    to run standardized workflows, currently supporting Common Workflow Language (CWL)
    and Workflow Description Language (WDL) formats, on multiple platforms, clouds,
    and execution systems using a consistent interface. The API specification is written
    in OpenAPI and embodies RESTful service principles, using JSON for requests and
    responses with standard HTTP/HTTPS transport. Core functionality includes workflow
    submission with parameter passing, run status monitoring through detailed logs
    capturing stdout and stderr output, task-level execution tracking with timing
    and exit codes, and workflow cancellation capabilities. WES addresses critical
    use cases such as "bring your code to the data" scenarios where researchers submit
    custom analyses to run on externally-owned datasets without data transfer, and
    best-practices pipeline execution where researchers discover workflows from shared
    repositories like Dockstore and execute them over controlled data environments.
    The service implements OAuth2 bearer token authentication and authorization, with
    implementations responsible for verifying user credentials and enforcing submission
    policies. WES provides comprehensive service introspection through its service-info
    endpoint, reporting supported workflow types, versions, filesystem protocols,
    workflow engines, and system state information. The API supports paginated listing
    of workflow runs, detailed run logs with output file locations, and granular task-level
    monitoring. Run states include QUEUED, INITIALIZING, RUNNING, COMPLETE, EXECUTOR_ERROR,
    SYSTEM_ERROR, CANCELED, and PREEMPTED, enabling detailed tracking of workflow
    execution lifecycle. As a GA4GH standard, WES promotes interoperability across
    bioinformatics workflow execution platforms and supports integration with related
    GA4GH standards including the Data Object Service for credential management and
    the Task Execution Service for extended task definitions.
  requires_registration: false
  url: https://ga4gh.github.io/workflow-execution-service-schemas/docs/
- id: B2AI_STANDARD:842
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Xethub
  is_open: true
  name: Xethub
  purpose_detail: Git-based collaboration to large scale repositories of data, code,
    or any combination of files.
  requires_registration: true
  url: https://xethub.com/assets/docs/
- id: B2AI_STANDARD:843
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - machinelearningframework
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: ZenML
  formal_specification: https://github.com/zenml-io/zenml
  is_open: true
  name: ZenML
  purpose_detail: ZenML is an extensible, open-source MLOps framework for creating
    portable, production-ready MLOps pipelines.
  requires_registration: false
  url: https://zenml.io/
  has_application:
  - id: B2AI_APP:81
    category: B2AI:Application
    name: MLOps Orchestration for Biomedical AI Pipelines
    description: ZenML is used in biomedical AI for building reproducible, production-grade
      machine learning pipelines with comprehensive experiment tracking, model versioning,
      and deployment orchestration. Healthcare AI teams leverage ZenML to standardize
      workflows from data ingestion through model deployment, ensuring compliance
      with regulatory requirements for traceability and reproducibility. The platform
      integrates with diverse healthcare data sources, model registries, and deployment
      targets while maintaining complete lineage tracking essential for clinical AI
      validation. ZenML enables teams to implement best practices for ML operations
      including automated testing, continuous training, and model monitoring in healthcare
      settings where reliability and auditability are critical.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:844
  category: B2AI_STANDARD:TrainingProgram
  concerns_data_topic:
  - B2AI_TOPIC:4
  - B2AI_TOPIC:52
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: 2019 OHDSI Tutorials - OMOP Common Data Model and Standardized Vocabularies
  formal_specification: https://github.com/OHDSI/Tutorial-CDM
  has_relevant_organization:
  - B2AI_ORG:76
  is_open: true
  name: OHDSI Tutorials
  purpose_detail: This workshop is for data holders who want to apply OHDSI's data
    standards to their own observational datasets and researchers who want to be aware
    of OHDSI's data standards, so they can leverage data in OMOP CDM format for their
    own research purposes.
  requires_registration: false
  url: https://www.ohdsi.org/2019-tutorials-omop-common-data-model-and-standardized-vocabularies/
- id: B2AI_STANDARD:845
  category: B2AI_STANDARD:TrainingProgram
  concerns_data_topic:
  - B2AI_TOPIC:52
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CDC Introduction to FHIR - Training Recordings
  has_relevant_organization:
  - B2AI_ORG:40
  is_open: true
  name: CDC Introduction to FHIR
  purpose_detail: A series of HL7 FAIR training lecture recordings made available
    through YouTube.
  requires_registration: false
  url: https://www.cdc.gov/nchs/data/nvss/modernization/Introductory-Training-FHIR.pdf
- id: B2AI_STANDARD:846
  category: B2AI_STANDARD:TrainingProgram
  concerns_data_topic:
  - B2AI_TOPIC:52
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FHIR Drills
  has_relevant_organization:
  - B2AI_ORG:40
  is_open: true
  name: FHIR Drills
  purpose_detail: This set of pages contains a series of FHIR tutorials for those
    just beginning to learn the new specification. The tutorials require no prior
    knowledge of FHIR or REST. At present these tutorials are in their beta stage
    of development and we would appreciate any feedback you may have as we plan to
    build upon these in time to create a full set of tutorials from the very basic
    to the more complex.
  requires_registration: false
  url: https://fhir-drills.github.io/
- id: B2AI_STANDARD:847
  category: B2AI_STANDARD:TrainingProgram
  concerns_data_topic:
  - B2AI_TOPIC:52
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: HL7 FHIR Fundamentals Course
  formal_specification: https://courses.hl7fundamentals.org/campus/
  has_relevant_organization:
  - B2AI_ORG:40
  is_open: false
  name: FHIR Fundamentals
  purpose_detail: This is an asynchronous, instructor-led online course that allows
    you to work at your own pace. Learning takes place through discussions with the
    instructor, tutors and peers. Assessments are in the form of weekly assignments,
    quizzes, exams and projects. Plan on spending 5 to 7 hours per week. There are
    no live lectures to attend.
  requires_registration: true
  url: https://www.hl7.org/training/fhir-fundamentals.cfm
- id: B2AI_STANDARD:848
  category: B2AI_STANDARD:TrainingProgram
  concerns_data_topic:
  - B2AI_TOPIC:52
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Learn LOINC
  has_relevant_organization:
  - B2AI_ORG:53
  is_open: true
  name: Learn LOINC
  purpose_detail: Welcome to the LOINC Library. This is our A to Z collection of resources
    that we've collected to help you learn about LOINC and get connected to the community.
  requires_registration: false
  url: https://loinc.org/learn/
- id: B2AI_STANDARD:849
  category: B2AI_STANDARD:TrainingProgram
  concerns_data_topic:
  - B2AI_TOPIC:15
  - B2AI_TOPIC:52
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Microsoft Learn - Work with medical imaging data and DICOM
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:11
  has_relevant_organization:
  - B2AI_ORG:25
  - B2AI_ORG:56
  is_open: true
  name: Microsoft Medical Imaging
  purpose_detail: Learn why DICOM standards are important. Explore the DICOM standards
    and DICOM service. Review the use case for radiology data in cancer treatment
    with examples.
  requires_registration: false
  url: https://learn.microsoft.com/en-us/training/modules/medical-imaging-data/
- id: B2AI_STANDARD:850
  category: B2AI_STANDARD:TrainingProgram
  concerns_data_topic:
  - B2AI_TOPIC:52
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Udemy - Introduction to FHIR
  has_relevant_organization:
  - B2AI_ORG:40
  is_open: true
  name: Udemy FHIR
  purpose_detail: This course will help you understand the basics of FHIR. It is a
    FREE sample of a comprehensive hands-on introductory course (details inside).
    The full course includes direct access to the course creator via a private members-only
    Slack room.
  requires_registration: true
  url: https://www.udemy.com/course/introduction-to-fhir/
- id: B2AI_STANDARD:851
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Fast Healthcare Interoperability Resources - US Core
  has_relevant_organization:
  - B2AI_ORG:103
  - B2AI_ORG:117
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: FHIR US Core
  purpose_detail: This is subset of all FHIR profiles for the US Realm, i.e., those
    supporting the minimum requirements for clinical data exchange in the United States.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  subclass_of:
  - B2AI_STANDARD:109
  url: https://build.fhir.org/ig/HL7/US-Core/index.html
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:82
    category: B2AI:Application
    name: US-Specific Clinical AI and Regulatory Compliance
    description: FHIR US Core Implementation Guide is used in AI applications that
      require compliance with US healthcare regulations and interoperability requirements,
      enabling standardized data extraction from US-based EHR systems for training
      clinical prediction models. AI systems leverage US Core's profiles for patient
      demographics, vital signs, laboratory results, medications, and conditions to
      build models that meet ONC certification requirements and support CMS quality
      measures. The implementation guide ensures AI applications can reliably access
      structured clinical data across diverse US healthcare systems, supporting use
      cases in risk adjustment, quality metric prediction, social determinants of
      health analysis, and value-based care optimization. US Core's mandatory data
      elements provide a consistent feature set for federated learning across US hospitals.
    used_in_bridge2ai: false
    references:
    - https://www.healthit.gov/isa/united-states-core-data-interoperability-uscdi
- id: B2AI_STANDARD:852
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Fast Healthcare Interoperability Resources - US Core Data for Interoperability
  has_relevant_organization:
  - B2AI_ORG:103
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: FHIR USCDI
  purpose_detail: USCDI is the set of basic healthcare data types expected to supported
    by other systems. This is the FHIR US Core profile with all elements required
    by USCDI.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  subclass_of:
  - B2AI_STANDARD:851
  url: https://build.fhir.org/ig/HL7/US-Core/uscdi.html
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:83
    category: B2AI:Application
    name: Standardized Data Element Mapping for Healthcare AI
    description: FHIR USCDI (US Core Data for Interoperability) implementation is
      used in AI applications to ensure consistent access to core clinical data elements
      required by federal regulations, enabling interoperable AI systems across US
      healthcare. Machine learning models leverage USCDI-compliant data elements including
      allergies, procedures, immunizations, lab results, and clinical notes to train
      on standardized features that are guaranteed to be available across certified
      EHR systems. This standardization is critical for AI applications that need
      to be deployed broadly across healthcare organizations, ensuring model portability
      and consistent performance. USCDI compliance enables AI researchers to develop
      models that align with national interoperability goals and can participate in
      health information exchange networks.
    used_in_bridge2ai: false
    references:
    - https://www.healthit.gov/isa/united-states-core-data-interoperability-uscdi
- id: B2AI_STANDARD:853
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Fast Healthcare Interoperability Resources - US Core Data for Interoperability,
    version 1
  has_relevant_organization:
  - B2AI_ORG:103
  - B2AI_ORG:117
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: FHIR USCDI v1
  purpose_detail: USCDI is the set of basic healthcare data types expected to supported
    by other systems. This is the FHIR US Core profile with all elements required
    by USCDI v1.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  subclass_of:
  - B2AI_STANDARD:852
  url: https://build.fhir.org/ig/HL7/US-Core/uscdi.html
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:84
    category: B2AI:Application
    name: Foundation Models on Core Clinical Data Elements
    description: FHIR USCDI v1 provides the baseline data elements for AI applications
      requiring backward compatibility and stable data definitions for longitudinal
      model training. AI systems developed against USCDI v1 can reliably access essential
      clinical information including patient demographics, problems, medications,
      allergies, lab results, vital signs, and procedures across time, enabling training
      of models on historical data and ensuring consistency in production deployments.
      This version stability is crucial for validating AI models in clinical trials,
      meeting regulatory requirements for locked algorithms, and maintaining model
      performance monitoring over multi-year periods. USCDI v1 compliance ensures
      AI applications can function across healthcare systems at different stages of
      EHR modernization.
    used_in_bridge2ai: false
    references:
    - https://www.healthit.gov/isa/united-states-core-data-interoperability-uscdi-v1
- id: B2AI_STANDARD:854
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Fast Healthcare Interoperability Resources - US Core Data for Interoperability,
    version 4
  has_relevant_organization:
  - B2AI_ORG:103
  - B2AI_ORG:117
  has_training_resource:
  - B2AI_STANDARD:845
  - B2AI_STANDARD:846
  - B2AI_STANDARD:847
  - B2AI_STANDARD:850
  is_open: true
  name: FHIR USCDI v4
  purpose_detail: USCDI is the set of basic healthcare data types expected to supported
    by other systems. This is the FHIR US Core profile with all elements required
    by USCDI v4.
  requires_registration: false
  responsible_organization:
  - B2AI_ORG:40
  subclass_of:
  - B2AI_STANDARD:852
  url: https://build.fhir.org/ig/HL7/US-Core/uscdi.html
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:85
    category: B2AI:Application
    name: Advanced Clinical AI with Expanded Data Elements
    description: FHIR USCDI v4 enables next-generation AI applications with access
      to expanded data elements including social determinants of health, mental health
      assessments, substance use information, and care team details. Machine learning
      models leverage these additional data classes to develop more comprehensive
      prediction models that account for social, behavioral, and environmental factors
      affecting health outcomes. AI systems benefit from USCDI v4's enhanced data
      elements for health equity research, population health modeling, and holistic
      patient risk assessment that goes beyond traditional clinical variables. The
      expanded scope supports AI applications in addressing health disparities, improving
      care coordination, and developing interventions that consider the full spectrum
      of factors influencing patient health.
    used_in_bridge2ai: false
    references:
    - https://www.healthit.gov/isa/united-states-core-data-interoperability-uscdi-v4
- id: B2AI_STANDARD:855
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - multimodal
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-03-10'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Dedoose app
  is_open: false
  name: Dedoose
  purpose_detail: 'Dedoose is a cloud-based, cross-platform application specifically
    designed for analyzing qualitative and mixed methods research data, developed
    by social scientists with expertise spanning education, psychology, anthropology,
    and marketing to address the practical needs of researchers across diverse disciplines.
    The platform supports nearly any type of qualitative data including interview
    transcripts, focus group recordings, open-ended survey responses, field notes,
    documents (PDFs, Word files), multimedia content (video, audio, images), social
    media posts, and website content, enabling comprehensive mixed methods analysis
    that integrates both qualitative coding and quantitative metrics within a unified
    workspace. Dedoose provides a hierarchical coding system where researchers create
    code trees with parent and child codes, apply codes to excerpts of text or segments
    of media, attach descriptors and demographic variables to documents and participants,
    and conduct inter-rater reliability testing with pooled Cohen''s kappa and other
    statistics to validate coding consistency across multiple analysts. The platform''s
    real-time collaboration capabilities allow distributed research teams to work
    simultaneously on the same project at no additional cost, with version control,
    audit trails tracking all coding decisions, user permissions managing access levels,
    and synchronized updates ensuring all team members see changes instantly. Dedoose
    generates dozens of interactive visualizations including code frequency charts
    showing prevalence across documents or demographic groups, code co-occurrence
    matrices revealing relationships between themes, word clouds emphasizing prominent
    terms, heat maps displaying patterns across cases, timeline visualizations for
    longitudinal data, and customizable charts that can be filtered dynamically by
    descriptor variables to explore subgroup differences. The platform operates entirely
    through web browsers on Mac, PC, and Linux systems without requiring software
    installation, stores all data on secure servers with enterprise-grade encryption,
    complies with data protection standards, provides automatic backups, and ensures
    researchers maintain full control over project data with export options in multiple
    formats (Excel, CSV, PDF reports, coded excerpts). Dedoose is widely used across
    healthcare research for analyzing patient interviews and clinical qualitative
    data, education research for evaluating teaching methods and student feedback,
    market and user experience research for synthesizing customer insights and usability
    testing results, program evaluation for assessing intervention effectiveness through
    mixed methods frameworks, social science research for thematic analysis of ethnographic
    data, and policy research for analyzing stakeholder perspectives and public comments.
    The platform''s mixed methods integration is particularly valuable for AI and
    machine learning applications where qualitative coding can inform feature engineering,
    coded themes can serve as training labels for natural language processing models,
    researcher annotations can validate AI-generated classifications, descriptive
    statistics from Dedoose can be incorporated into predictive models, and iterative
    human-in-the-loop workflows can combine automated text analysis with expert qualitative
    interpretation to refine algorithms and improve classification accuracy on nuanced
    social science constructs.'
  requires_registration: true
  url: https://www.dedoose.com/
- id: B2AI_STANDARD:856
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-03-13'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FAIR Data Station
  is_open: true
  name: FAIR Data Station
  publication: doi:10.1093/gigascience/giad014
  purpose_detail: A lightweight application written in Java, that aims to support
    researchers in managing research metadata according to the FAIR principles.
  requires_registration: false
  url: https://fairbydesign.nl/
- id: B2AI_STANDARD:857
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-03-14'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Patient-Led Research Scorecards
  is_open: true
  name: Patient-Led Research Scorecards
  purpose_detail: The Council of Medical Specialty Societies (CMSS) and Patient-Led
    Research Collaborative (PLRC) have developed a sustainable collaborative model
    of CER based on information from and the expertise of patient communities, researchers,
    funders, and clinical research organizations. This model takes the form of scorecards
    which serve to evaluate how effective a patient group and research partner collaboration
    will be at conducting truly patient-led research.
  requires_registration: false
  url: https://patientresearchcovid19.com/storage/2023/02/Patient-Led-Research-Scorecards.pdf
- id: B2AI_STANDARD:858
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:32
  contribution_date: '2023-03-16'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Zshot
  formal_specification: https://github.com/IBM/zshot
  has_relevant_organization:
  - B2AI_ORG:104
  is_open: true
  name: Zshot
  purpose_detail: A framework for performing Zero and Few shot named entity recognition.
  requires_registration: false
  url: https://ibm.github.io/zshot/
- id: B2AI_STANDARD:859
  category: B2AI_STANDARD:Registry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-03-21'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: NIH Common Data Elements Repository
  formal_specification: https://cde.nlm.nih.gov/api
  has_relevant_organization:
  - B2AI_ORG:74
  is_open: true
  name: NIH CDE
  purpose_detail: A registry of standardized, precisely defined questions, paired
    with sets of allowable responses, used systematically across different sites,
    studies, or clinical trials to ensure consistent data collection.
  requires_registration: false
  url: https://cde.nlm.nih.gov/
- id: B2AI_STANDARD:860
  category: B2AI_STANDARD:Registry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-03-24'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Basic Register of Thesauri, Ontologies & Classifications
  is_open: true
  name: BARTOC
  purpose_detail: A database of Knowledge Organization Systems and KOS related registries.
  requires_registration: false
  url: https://bartoc.org/
- id: B2AI_STANDARD:861
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-03-24'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: DuckDB
  formal_specification: https://github.com/duckdb/duckdb
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:9
  is_open: true
  name: DuckDB
  purpose_detail: A database platform designed for working with tabular data.
  requires_registration: false
  url: https://duckdb.org/
- id: B2AI_STANDARD:862
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-03-27'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Polars library
  formal_specification: https://github.com/pola-rs/polars/
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:8
  is_open: true
  name: Polars
  purpose_detail: Polars is a high-performance DataFrame library written in Rust with
    bindings for Python, Node.js, and R, designed as a fast alternative to pandas.
    The library features a multi-threaded query engine with lazy evaluation, query
    optimization, and streaming capabilities for processing larger-than-RAM datasets.
    Polars utilizes Apache Arrow columnar format for zero-copy data sharing and SIMD
    vectorization for cache-coherent algorithms. It supports both eager and lazy execution
    modes, with the lazy API enabling automatic query optimization and parallel execution
    across CPU cores. The library handles diverse data formats including CSV, JSON,
    Parquet, Delta Lake, AVRO, Excel, and direct database connections to MySQL, PostgreSQL,
    SQLite, and cloud storage systems. Polars provides an intuitive expression API
    for data manipulation operations while maintaining minimal dependencies and fast
    import times (70ms vs 520ms for pandas), making it suitable for data analysis,
    ETL pipelines, and analytical workloads requiring high performance.
  requires_registration: false
  url: https://www.pola.rs/
- id: B2AI_STANDARD:863
  category: B2AI_STANDARD:Registry
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-03-27'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Linked Open Vocabularies
  formal_specification: https://github.com/pyvandenbussche/lov
  is_open: true
  name: LOV
  purpose_detail: A collection of searchable ontologies and vocabularies, spanning
    multiple fields.
  requires_registration: false
  url: https://lov.linkeddata.es/
- id: B2AI_STANDARD:864
  category: B2AI_STANDARD:Registry
  concerns_data_topic:
  - B2AI_TOPIC:4
  contribution_date: '2023-03-27'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: PhenX Toolkit
  is_open: true
  name: PhenX Toolkit
  purpose_detail: A catalog of recommended measurement protocols for biomedical research.
  requires_registration: false
  url: https://www.phenxtoolkit.org/
- id: B2AI_STANDARD:866
  category: B2AI_STANDARD:OntologyOrVocabulary
  concerns_data_topic:
  - B2AI_TOPIC:23
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Sequence Alignment Ontology
  is_open: true
  name: SALON
  publication: doi:10.1186/s12859-023-05190-7
  purpose_detail: An OWL2 ontology for representing and semantically annotating pairwise
    and multiple sequence alignments.
  requires_registration: false
  url: https://benhid.com/SALON/
- id: B2AI_STANDARD:867
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - diagnosticinstrument
  concerns_data_topic:
  - B2AI_TOPIC:4
  contribution_date: '2023-05-23'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Glasgow Coma Scale
  is_open: true
  name: GCS
  publication: doi:10.1016/s0140-6736(74)91639-0
  purpose_detail: 'A neurological assessment tool used to evaluate level of consciousness
    based on patient responses in three categories: eye-opening, verbal response,
    and motor response, with a higher score indicating a more favorable neurological
    status.'
  requires_registration: false
  url: https://www.glasgowcomascale.org/
- id: B2AI_STANDARD:868
  category: B2AI_STANDARD:DataStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-05-23'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Frictionless data standards
  formal_specification: https://github.com/frictionlessdata/specs
  is_open: true
  name: Frictionless
  purpose_detail: Frictionless data standards provide a comprehensive suite of lightweight,
    extensible specifications for describing datasets, data files, and tabular data
    to enhance FAIR (Findability, Accessibility, Interoperability, Reusability) principles.
    The core specifications include Data Package for dataset-level metadata and resource
    collections, Data Resource for individual file descriptions, and Table Schema
    for tabular data structure definition with field types, constraints, and relationships.
    These specifications combine to create specialized formats like Tabular Data Packages
    that integrate CSV/JSON data with JSON Schema-based metadata descriptors. The
    standards follow a "small pieces, loosely joined" philosophy, enabling individual
    components to be used independently or combined for complex data scenarios. They
    support cross-technology implementation with human-readable JSON metadata that
    facilitates machine processing, data validation, and automated discovery. The
    specifications enable data portability, version control, and collaborative data
    workflows while maintaining compatibility with existing data formats and tools
    in the data science ecosystem.
  requires_registration: false
  url: https://specs.frictionlessdata.io/
- id: B2AI_STANDARD:869
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:29
  contribution_date: '2023-06-20'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Epic Compass Rose module
  has_relevant_organization:
  - B2AI_ORG:105
  - B2AI_ORG:115
  is_open: false
  name: Compass Rose
  purpose_detail: Care coordination module focused on social determinant of health
    factors. Part of the Epic EHR platform.
  requires_registration: true
  url: https://www.epic.com/software/population-health
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:87
    category: B2AI:Application
    name: Population Health Analytics and Risk Stratification
    description: Compass Rose (Epic's population health tool) is used in AI applications
      for large-scale risk stratification, care gap identification, and population-level
      outcome prediction across Epic's extensive user base. Machine learning models
      leverage Compass Rose's aggregated clinical data, standardized quality measures,
      and longitudinal patient tracking to develop predictive algorithms for chronic
      disease management, preventive care optimization, and resource allocation. AI
      systems built on this platform can identify high-risk patient populations, predict
      hospital readmissions, and recommend targeted interventions at scale. The tool's
      integration with Epic's EHR ecosystem enables real-time AI inference during
      clinical encounters and automated outreach programs guided by ML-based risk
      scores.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:870
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-06-20'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Great Expectations platform
  formal_specification: https://github.com/great-expectations/great_expectations
  is_open: true
  name: GX
  purpose_detail: A platform for organizing, testing, and validating data.
  requires_registration: false
  url: https://greatexpectations.io/
- id: B2AI_STANDARD:871
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-06-20'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Pinecone vector database
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:54
  - B2AI_SUBSTRATE:55
  - B2AI_SUBSTRATE:9
  is_open: false
  name: Pinecone
  purpose_detail: A database platform built around creating vector representations
    of data. The basic implementation is a managed, cloud-native product, though there
    is a free tier.
  requires_registration: true
  url: https://www.pinecone.io/
- id: B2AI_STANDARD:872
  category: B2AI_STANDARD:DataStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-06-20'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Command Line Interface Guidelines
  is_open: true
  name: CLIG
  purpose_detail: An open-source guide to help with writing command-line programs,
    based on UNIX principles.
  requires_registration: false
  url: https://clig.dev/
- id: B2AI_STANDARD:873
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:13
  contribution_date: '2023-09-25'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: 0000-0001-5705-7831
  description: Minimum Information about a high-throughput nucleotide SEQuencing Experiment
  formal_specification: https://zenodo.org/record/5706412
  is_open: true
  name: MINSEQE
  purpose_detail: Five elements of experimental description considered essential when
    making sequencing data available.
  requires_registration: false
  url: https://www.fged.org/projects/minseqe/
- id: B2AI_STANDARD:874
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:18
  contribution_date: '2023-09-25'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: 0000-0001-5705-7831
  description: GGIR accelerometry package
  formal_specification: https://github.com/wadpac/GGIR
  is_open: true
  name: GGIR
  purpose_detail: An R package to process multi-day raw accelerometer data for physical
    activity and sleep research.
  requires_registration: false
  url: https://cran.r-project.org/web/packages/GGIR/vignettes/GGIR.html
- id: B2AI_STANDARD:875
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:18
  contribution_date: '2023-09-25'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: 0000-0001-5705-7831
  description: Metadata Concepts for Advancing the Use of Digital Health Technologies
    in Clinical Research
  is_open: true
  name: Badawy et al. 2019
  publication: doi:10.1159/000502951
  purpose_detail: A proposed metadata set for digital health studies.
  requires_registration: false
  url: https://figshare.com/articles/dataset/Supplementary_Material_for_Metadata_Concepts_for_Advancing_the_Use_of_Digital_Health_Technologies_in_Clinical_Research/9944303
- id: B2AI_STANDARD:876
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-09-25'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: 0000-0001-5705-7831
  description: GSCID/BRC Clinical Metadata Standard
  has_relevant_organization:
  - B2AI_ORG:118
  is_open: true
  name: GSCID/BRC CMS v1.5
  purpose_detail: A general standard for clinical metadata.
  requires_registration: false
  url: https://www.niaid.nih.gov/research/clinical-metadata-standard
- id: B2AI_STANDARD:877
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - minimuminformationschema
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-09-25'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: 0000-0001-5705-7831
  description: Minimum information about clinical artificial intelligence modeling
  formal_specification: https://github.com/beaunorgeot/MI-CLAIM
  is_open: true
  name: MI-CLAIM
  publication: doi:10.1038/s41591-020-1041-y
  purpose_detail: MI-CLAIM (Minimum Information about CLinical AI Modeling) is a reporting
    standard and documentation checklist developed to address transparency and reproducibility
    challenges in clinical artificial intelligence research, published in Nature Medicine
    in 2020 by a multidisciplinary team of clinical and data scientists. MI-CLAIM
    serves two primary purposes - enabling direct assessment of clinical impact including
    fairness considerations, and allowing rapid replication of the technical design
    process for clinical AI studies. The standard provides a comprehensive checklist
    in MS Word table format covering essential reporting elements including study
    design and data characteristics (patient demographics, inclusion/exclusion criteria,
    data sources, temporal validation), model development details (feature engineering,
    architecture selection, hyperparameter tuning, training procedures), performance
    evaluation (metrics across demographic subgroups, confidence intervals, comparison
    to clinical standards), clinical implementation considerations (decision thresholds,
    interpretability mechanisms, failure modes), and ethical aspects (bias assessment,
    fairness metrics, regulatory status). The repository encourages community feedback
    through GitHub Issues to continuously improve the standard as the field evolves,
    promoting best practices for transparent, reproducible, and equitable clinical
    AI development and deployment across healthcare applications.
  requires_registration: false
  url: https://github.com/beaunorgeot/MI-CLAIM
  has_application:
  - id: B2AI_APP:88
    category: B2AI:Application
    name: Clinical AI Reporting Standards and Model Documentation
    description: MI-CLAIM (Minimum Information about Clinical Artificial Intelligence
      Modeling) checklist is used to standardize reporting of clinical AI studies,
      ensuring reproducibility, transparency, and appropriate evaluation of machine
      learning models in healthcare. Researchers leverage MI-CLAIM guidelines to document
      essential details about model development, validation approaches, and clinical
      context that enable others to assess reliability and reproduce findings. The
      standard supports automated model card generation, structured documentation
      for regulatory submissions, and systematic reviews of clinical AI literature
      by providing a consistent framework for reporting. MI-CLAIM compliance facilitates
      responsible AI development by ensuring key information about data provenance,
      model limitations, and intended use cases is explicitly documented.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:878
  category: B2AI_STANDARD:DataStandardOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-09-25'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: 0000-0001-5705-7831
  description: CSV on the Web
  formal_specification: https://w3c.github.io/csvw/syntax/
  has_relevant_data_substrate:
  - B2AI_SUBSTRATE:6
  is_open: true
  name: CSVW
  purpose_detail: A standard for describing and clarifying the content of CSV tables.
  requires_registration: false
  url: https://csvw.org/
- id: B2AI_STANDARD:879
  category: B2AI_STANDARD:DataStandardOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2023-09-25'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: 0000-0001-5705-7831
  description: BagIt file packaging format
  formal_specification: https://datatracker.ietf.org/doc/rfc8493/
  is_open: true
  name: BagIt
  publication: doi:10.17487/RFC8493
  purpose_detail: A set of hierarchical file layout conventions for storage and transfer
    of arbitrary digital content.
  requires_registration: false
  url: https://datatracker.ietf.org/doc/rfc8493/
- id: B2AI_STANDARD:880
  category: B2AI_STANDARD:DataStandardOrTool
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2024-11-02'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: 0000-0001-5705-7831
  description: Unity Catalog
  formal_specification: https://github.com/unitycatalog/unitycatalog
  is_open: true
  name: Unity Catalog
  purpose_detail: A universal catalog for data and AI. It includes a core set of APIs
    for tables, unstructured data, and AI assets.
  requires_registration: false
  url: https://www.unitycatalog.io/
  has_application:
  - id: B2AI_APP:89
    category: B2AI:Application
    name: Unified Data and AI Asset Governance
    description: Unity Catalog is used in biomedical AI for centralized governance
      of data assets, ML models, and AI artifacts across multi-cloud and hybrid healthcare
      IT environments. Healthcare organizations leverage Unity Catalog to implement
      fine-grained access controls for sensitive patient data used in model training,
      track lineage from raw clinical data through processed features to trained models,
      and ensure HIPAA compliance across distributed AI development teams. The catalog
      provides a single source of truth for data discovery, enables secure data sharing
      across research and clinical domains, and maintains comprehensive audit logs
      for regulatory compliance. Unity Catalog's integration with major ML platforms
      facilitates governed AI development where data scientists can access approved
      datasets while maintaining security and privacy requirements.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:881
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:36
  contribution_date: '2023-12-06'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Senselab package
  formal_specification: https://github.com/sensein/senselab
  is_open: true
  name: Senselab
  purpose_detail: A Python package for streamlining the processing and analysis of
    behavioral data, such as voice and speech patterns, with robust and reproducible
    methodologies.
  requires_registration: false
  url: https://sensein.group/senselab/
- id: B2AI_STANDARD:882
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - guidelines
  concerns_data_topic:
  - B2AI_TOPIC:15
  contribution_date: '2023-12-06'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Checklist for Artificial Intelligence in Medical Imaging (CLAIM)
  is_open: true
  name: CLAIM
  publication: doi:10.1148/ryai.2020200029
  purpose_detail: CLAIM is modeled after the STARD guideline and has been extended
    to address applications of AI in medical imaging that include classification,
    image reconstruction, text analysis, and workflow optimization. It is intended
    to provide a framework for the development and validation of AI algorithms in
    medical imaging.
  requires_registration: false
  url: https://doi.org/10.1148/ryai.2020200029
- id: B2AI_STANDARD:883
  category: B2AI_STANDARD:SoftwareOrTool
  concerns_data_topic:
  - B2AI_TOPIC:15
  contribution_date: '2023-12-06'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Pydicom software package
  formal_specification: https://github.com/pydicom/pydicom
  is_open: true
  name: pydicom
  purpose_detail: Pydicom is a pure Python package that provides comprehensive tools
    for reading, modifying, and writing DICOM (Digital Imaging and Communications
    in Medicine) files, serving as the de facto standard library for medical imaging
    data manipulation in the Python ecosystem. As a foundational dependency for medical
    imaging AI research and clinical software development, pydicom implements the
    complex DICOM standard (PS3.5-PS3.10) without requiring external C libraries,
    making it portable across platforms and compatible with environments from laptops
    to high-performance computing clusters. The library provides both high-level abstractions
    for common workflows and low-level access to DICOM data elements, enabling researchers
    to extract pixel arrays, access metadata tags, handle diverse transfer syntaxes
    (including compressed formats like JPEG 2000, JPEG-LS, and RLE), and manipulate
    DICOM data structures programmatically. Pydicom's Dataset class represents DICOM
    information objects as Python dictionaries, allowing natural Pythonic access to
    attributes like patient demographics, imaging parameters, and clinical context
    using standard dictionary notation or attribute-style access with keyword lookups.
    The library handles multi-frame images, color spaces, overlay data, and private
    tags, while supporting both implicit and explicit value representations. For AI/ML
    workflows, pydicom seamlessly integrates with NumPy to convert pixel data into
    arrays suitable for deep learning frameworks, automatically applying rescale slopes,
    intercepts, and window/level transformations. It forms the data loading foundation
    for major medical imaging AI libraries including MONAI (Medical Open Network for
    AI), TorchIO, and Cornerstone.js, and is extensively used in research pipelines
    for tasks ranging from tumor segmentation to fracture detection to radiomics feature
    extraction. Pydicom supports batch processing of large DICOM datasets through
    memory-efficient streaming, enables conversion between DICOM and research formats
    like NIfTI, and provides utilities for anonymization and de-identification critical
    for privacy-compliant research. The library maintains compatibility with thousands
    of DICOM implementations from medical device manufacturers, handles institution-specific
    private tags, and supports DICOM-RT (radiotherapy) structures, DICOM-SR (structured
    reports), and DICOM-SEG (segmentation) objects increasingly used in clinical AI
    deployments. With over 1 million monthly downloads and adoption by virtually all
    Python-based medical imaging projects, pydicom has become indispensable infrastructure
    for translating medical images from clinical PACS systems into AI/ML pipelines
    and returning model predictions in formats compatible with clinical workflows
    and regulatory requirements.
  related_to:
  - B2AI_STANDARD:98
  requires_registration: false
  url: https://pydicom.github.io/
  has_application:
  - id: B2AI_APP:90
    category: B2AI:Application
    name: Medical Imaging Data Processing for Deep Learning
    description: pydicom is the essential Python library for AI researchers working
      with medical imaging data, enabling reading, writing, and manipulation of DICOM
      files in machine learning pipelines. Virtually all medical imaging AI research
      using Python leverages pydicom to extract pixel data and metadata from DICOM
      images, convert images to NumPy arrays for neural network input, and create
      DICOM-compliant outputs for clinical integration. The library handles diverse
      DICOM transfer syntaxes and encodings, enables efficient batch processing of
      large imaging datasets, and provides the foundation for medical imaging data
      loaders in PyTorch and TensorFlow. pydicom's ability to preserve clinical metadata
      during AI processing ensures that model outputs maintain appropriate associations
      with patient context and imaging parameters.
    used_in_bridge2ai: false
  - id: B2AI_APP:170
    category: B2AI:Application
    name: DICOM Loading and Model Output Encoding via Highdicom
    description: "Highdicom builds on pydicom to provide standardized encoding of
      image annotations and ML model outputs for pathology and radiology workflows.
      The library uses pydicom to read DICOM files and interpret image metadata and
      pixel data, passing pixel data as NumPy arrays to ML models for inference on
      CT and slide microscopy images. Model outputs (NumPy arrays) are encoded back
      into DICOM Structured Reporting (SR) and Segmentation (SEG) objects constructed
      from pydicom.Dataset instances, enabling storage through DICOMweb and visualization
      in clinical viewers (OHIF, Slim). Highdicom explicitly \"copies this metadata
      directly from the source images provided as evidence to the constructor\" using
      pydicom, ensuring proper propagation of patient, study, and specimen metadata
      to derived objects for dataset curation, model validation, and audit trails.
      The library exposes parsing methods that decode DICOM annotations into NumPy
      label maps for use as training targets, demonstrating bidirectional integration
      of pydicom-based DICOM I/O with PyTorch and TensorFlow ML frameworks."
    references:
    - https://doi.org/10.1007/s10278-022-00683-y
    used_in_bridge2ai: false
  - id: B2AI_APP:171
    category: B2AI:Application
    name: Real-time Radiology ML Pipelines with Niffler Metadata Extraction
    description: 'The Niffler DICOM framework for machine learning against real-time
      radiology images explicitly "uses the Pydicom library to extract metadata and
      process the DICOM images" at institutional scale (715 scanners, up to 350 GB/day).
      Niffler''s Python3 core runs a periodic extract_metadata thread that iterates
      series and extracts metadata (by default one image per series) into a metadata
      store indexed by PatientID/StudyInstanceUID/SeriesInstanceUID, enabling dataset
      curation, real-time feature collection, and operational analytics for downstream
      ML pipelines. The system converts received compressed DICOM to PNG using GDCM
      "for the ML pipelines to consume in a de-identified manner," with ML workloads
      running as Docker containers "on the images and metadata that it stores." Niffler
      demonstrates pydicom''s role in production-scale metadata extraction, format
      conversion, and continuous ingestion workflows supporting real-time ML applications
      such as IVC filter detection and segmentation pipelines.'
    references:
    - https://doi.org/10.1007/s10278-021-00491-w
    used_in_bridge2ai: false
  - id: B2AI_APP:172
    category: B2AI:Application
    name: DICOM Annotation Encoding and Decoding for ML Training
    description: 'Highdicom implements Python classes derived from pydicom.Dataset
      to encode ML model outputs (NumPy arrays) plus metadata into DICOM annotation
      objects and decode DICOM annotations (pydicom objects) into NumPy arrays used
      as training targets. The library provides validated constructors for DICOM Segmentation
      and Structured Reporting that handle "highly nested and interdependent structure"
      of annotation IODs, addressing limitations of pydicom''s low-level API which
      "has no concept of IODs" and requires manual attribute setting. Highdicom uses
      pydicom for coded vocabularies ("the SCT vocabulary built in to pydicom" via
      pydicom.sr.codedict.codes) and file loading (pydicom.dcmread), then constructs
      SR and SEG objects (hd.sr.Comprehensive3DSR.from_dataset, hd.seg.Segmentation.from_dataset)
      that expose methods to collect ROI polygons (get_planar_roi_measurement_groups)
      and produce NumPy label maps (seg_image.get_pixels_by_source_instance). This
      enables standardized integration of DICOM annotations with PyTorch and TensorFlow
      training/inference pipelines while maintaining clinical interoperability.'
    references:
    - https://doi.org/10.1007/s10278-022-00683-y
    used_in_bridge2ai: false
  - id: B2AI_APP:173
    category: B2AI:Application
    name: DICOM De-identification and Anonymization for AI Datasets
    description: Pydicom is used for metadata de-identification in DICOM anonymization
      pipelines preparing imaging datasets for AI research. A peer-reviewed pipeline
      implemented "a custom Python program utilizing the Pydicom library (v2.2.2)"
      to programmatically remove PHI from DICOM attributes while preserving required
      attributes for standard compliance, achieving successful metadata de-identification
      on all images in a dataset sampled from 37,866 prior de-identification requests
      (1,391 exam validation sample). The XNAT/MIDI-B deidentification pipeline explicitly
      names pydicom among Python libraries used for "deidentification of metadata"
      within broader workflows that combine metadata redaction with OCR-based pixel
      text removal for burned-in annotations. A knee MRI preprocessing study for deep
      learning lists "PyDicom and Simple ITK" as Python packages used for DICOM preprocessing
      alongside dataset anonymization steps. These applications demonstrate pydicom's
      role in institutional-scale anonymization workflows that strip/modify DICOM
      metadata attributes to prepare privacy-compliant datasets for ML development,
      often integrated with pixel-processing tools for comprehensive de-identification.
    references:
    - https://doi.org/10.1007/s10278-024-01098-7
    - https://doi.org/10.48550/arxiv.2504.20657
    - https://doi.org/10.26574/maedica.2024.19.3.526
    used_in_bridge2ai: false
- id: B2AI_STANDARD:884
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2025-02-16'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: CSVY format
  formal_specification: https://github.com/leeper/csvy
  is_open: true
  name: CSVY
  purpose_detail: CSVY is a lightweight file format that enhances comma-separated
    values (CSV) files by embedding a YAML metadata header at the beginning of the
    file, enabling self-describing tabular datasets that combine the simplicity and
    ubiquity of CSV with structured metadata that documents data provenance, column
    definitions, licenses, and processing history. The format consists of a YAML
    front matter section delimited by triple-dash lines (---) followed by standard
    CSV data, allowing a single file to carry both human-readable and machine-readable
    documentation alongside the actual data values. The YAML header can specify field
    names, data types, units of measurement, missing value codes, column descriptions,
    dataset-level metadata like title, author, creation date, version, license terms,
    and citations, making CSVY particularly valuable for open science, data repositories,
    and reproducible research where data provenance and documentation are essential.
    Unlike traditional CSV files that rely on external documentation or README files
    to explain column meanings and data collection context, CSVY embeds this information
    directly within the data file itself, reducing the risk of metadata loss when
    files are shared, archived, or transferred between collaborators. The format maintains
    full backward compatibility with CSV parsers when they ignore the YAML header,
    while CSVY-aware tools can extract both the metadata and data, enabling applications
    to automatically validate data types, apply unit conversions, handle missing values
    according to documented conventions, and present contextual information to users.
    CSVY has gained adoption in scientific computing, environmental monitoring, survey
    research, and government open data initiatives where datasets require rich documentation
    but must remain accessible to users with basic spreadsheet tools. Programming
    libraries in R (csvy package), Python (pandas with yaml), and Julia provide native
    CSVY support, allowing researchers to read metadata-annotated tabular data with
    a single function call that populates data frames with appropriate column types
    and preserves documentation in object attributes. The format addresses a common
    pain point in data science workflows where CSV files are exchanged without adequate
    context about variable definitions, measurement protocols, or quality flags, often
    leading to misinterpretation or improper analysis. For machine learning applications,
    CSVY enables self-documenting training datasets where the header can specify feature
    engineering details, class labels, train/test splits, preprocessing steps, and
    benchmark results, facilitating reproducibility and proper citation of datasets
    used in published models. CSVY exemplifies the trend toward literate data formats
    that integrate documentation with data representation, making tabular datasets
    more FAIR (Findable, Accessible, Interoperable, Reusable) compliant while maintaining
    the practical simplicity that has made CSV a universal data exchange format for
    decades.
  requires_registration: false
  url: https://github.com/leeper/csvy
- id: B2AI_STANDARD:885
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:37
  contribution_date: '2025-02-16'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: openSMILE software
  formal_specification: https://github.com/audeering/opensmile
  has_relevant_organization:
  - B2AI_ORG:117
  is_open: true
  name: openSMILE
  purpose_detail: openSMILE (open-source Speech and Music Interpretation by Large-space
    Extraction) is a comprehensive open-source toolkit for audio feature extraction,
    processing, and classification specifically designed for speech and music signal
    analysis, widely applied in affective computing for automatic emotion recognition,
    speaker identification, automatic speech recognition, paralinguistic analysis,
    beat tracking, and audio scene classification. Originally developed in 2008 at
    Technische Universitt Mnchen within the EU-funded SEMAINE project by Dr. Florian
    Eyben, Martin Wllmer, and Prof. Bjrn Schuller as a real-time speech and emotion
    analysis component for a virtual agent with affective skills, the toolkit has
    been owned and maintained since 2013 by audEERING GmbH while remaining freely
    available for academic, research, and educational use, accumulating over 150,000
    downloads and 2,650+ citations in scientific publications. Written purely in C++
    for performance efficiency, openSMILE provides a modular, configurable architecture
    running on desktop (Linux, Windows, macOS), mobile (Android, iOS), and embedded
    platforms (Raspberry Pi), with version 3.0 introducing major improvements including
    a standalone opensmile-python package with easy-to-use Python API, new C API
    with Python/.NET wrappers, modern CMake build system, enhanced iOS support, updated
    Android integration, and FFmpeg audio source component. Core capabilities span
    signal processing (FFT, mel-scale filterbank, voice activity detection, pitch
    tracking, formant estimation), speech-related features (mel-frequency cepstral
    coefficients MFCC for speech recognition, linear predictive coding LPC and perceptual
    linear prediction PLP, fundamental frequency F0 and voicing for prosody, jitter/shimmer
    voice quality measures, harmonics-to-noise ratio, spectral flux/rolloff, formants
    F1-F4 for vowel characterization), music-related features (chroma vectors for
    chord recognition, beat tracking and tempo estimation, onset detection, spectral
    centroid and bandwidth for timbre), extensive statistical functionals over time
    windows (mean, standard deviation, percentiles, regression coefficients capturing
    trends, skewness and kurtosis), and data processing utilities (delta coefficients
    for feature dynamics, normalization including cepstral mean subtraction removing
    channel effects, resampling). Pre-configured feature sets enable rapid prototyping
    where ComParE (Computational Paralinguistics Challenge) extracts 6,373 acoustic
    features widely used as emotion recognition baseline, eGeMAPS (extended Geneva
    Minimalistic Acoustic Parameter Set) provides 88 carefully selected features balancing
    theoretical significance and automatic extractability for affective computing
    and clinical voice analysis, and INTERSPEECH IS09-IS13 feature sets from annual
    challenges capture progressive refinements for paralinguistic tasks. Applications
    position openSMILE as de facto standard feature extractor where emotion recognition
    systems classify discrete emotions (happiness, anger, sadness, fear) or dimensional
    affect (arousal, valence) from speech acoustics using machine learning models
    (SVMs, random forests, neural networks) trained on databases like IEMOCAP, speaker
    recognition extracts voice biometrics for authentication, automatic speech recognition
    pipelines use MFCCs or PLP as front-end acoustic representations, clinical voice
    analysis assesses pathological speech in Parkinson's disease and dysarthria using
    perturbation measures, depression and mental health screening leverages paralinguistic
    cues (reduced pitch variability, slower speech rate), call center analytics performs
    sentiment analysis and customer satisfaction prediction, music information retrieval
    applications including genre classification and mood detection use chroma and
    spectral features, and audio scene classification distinguishes acoustic environments
    supporting context-aware systems. Machine learning integration where openSMILE
    serves as feature extraction front-end for diverse architectures including classical
    ML (SVMs, random forests, gradient boosting XGBoost/LightGBM, Gaussian mixture
    models), deep neural networks (CNNs for spectrogram-like representations, RNNs/LSTMs
    capturing temporal dependencies, attention mechanisms and Transformers, hybrid
    architectures combining openSMILE features with end-to-end learned representations
    from wav2vec 2.0 or HuBERT), transfer learning adapting pre-trained models to
    specific tasks, multi-task learning jointly optimizing emotion/gender/age recognition,
    and ensemble methods combining feature-based and spectrogram-based models. Research
    applications across computational paralinguistics where annual INTERSPEECH ComParE
    challenges (2009-present) use openSMILE as official baseline feature extractor
    for emotion recognition, speaker traits estimation, breathing/heart sounds, COVID-19
    cough detection, AVEC challenges for multimodal emotion recognition combining
    audio with facial expression and physiological signals, cross-corpus emotion recognition
    evaluating generalization across languages and conditions, continuous dimensional
    emotion recognition predicting time-varying arousal/valence trajectories, depression
    severity estimation from clinical interviews, Parkinson's disease detection from
    sustained phonations, and autism spectrum disorder screening from atypical prosody.
    Dual licensing where openSMILE is freely available under audEERING Research License
    for private, research, and educational use enabling widespread academic adoption,
    but commercial products require purchasing commercial license from audEERING GmbH.
    Integration with broader ecosystem where opensmile-python provides Pythonic interface
    with NumPy/Pandas, compatibility with scikit-learn pipelines enables seamless
    ML workflows (feature scaling, cross-validation), audformat/audb libraries facilitate
    dataset management with feature caching, and extensive documentation at audeering.github.io/opensmile/
    with active GitHub community (748 stars, 96 forks) ensures openSMILE remains
    the most widely adopted open-source toolkit for acoustic feature extraction in
    speech and affective computing research, serving as essential infrastructure for
    reproducible computational paralinguistics, supporting Bridge2AI voice biomarker
    projects, and enabling clinical translation of speech-based digital phenotyping
    for neurological, psychiatric, and respiratory conditions.
  requires_registration: false
  url: https://www.audeering.com/research/opensmile/
  used_in_bridge2ai: true
- id: B2AI_STANDARD:886
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:36
  contribution_date: '2025-02-16'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Praat software
  formal_specification: https://github.com/praat/praat
  has_relevant_organization:
  - B2AI_ORG:117
  is_open: true
  name: Praat
  purpose_detail: Praat is open-source software for phonetic analysis and speech
    research, developed by Paul Boersma and David Weenink at the University of Amsterdam's
    Institute of Phonetic Sciences. Since its initial release in 1992, Praat has
    become the standard tool for doing phonetics by computer, enabling researchers
    to analyze, synthesize, manipulate, and annotate speech with high-quality publication-ready
    graphics. The software is cross-platform with binaries available for Windows
    (Intel64/Intel32/ARM64), macOS (universal Intel/Apple Silicon), Linux (Intel64/ARM64/s390x),
    Chromebook, and Raspberry Pi, distributed under GNU General Public License v3
    or later with source code written in C (67.4%) and C++ (29.9%). Praat provides
    comprehensive speech analysis capabilities including spectral analysis with spectrograms
    and cochleagrams (simulating inner ear sound reception), pitch analysis with
    fundamental frequency (F0) extraction, formant analysis identifying vocal tract
    resonances F1-F4, intensity analysis measuring sound pressure levels, and voice
    quality metrics including jitter (frequency perturbation), shimmer (amplitude
    perturbation), and voice breaks for clinical voice assessment. The software enables
    speech synthesis through multiple methods including acoustic synthesis from user-created
    pitch curves and filters, articulatory synthesis from simulated muscle activities
    modeling the vocal tract, and Klatt synthesis. Speech manipulation features allow
    modification of pitch contours, intensity envelopes, and duration patterns in
    existing utterances via PSOLA (pitch-synchronous overlap-add) and filtering techniques.
    Labeling and segmentation tools support annotation using the International Phonetic
    Alphabet (IPA) with multi-tier TextGrid files enabling time-aligned transcription,
    word segmentation, phoneme boundaries, and prosodic marking, handling sound files
    up to 2 gigabytes (~3 hours), with multi-language text-to-speech facilities for
    automatic segmentation. Praat includes learning algorithms for computational
    modeling including feedforward neural networks with backpropagation, discrete
    Optimality Theory constraint ranking, Harmonic Grammar weighted constraints,
    and stochastic grammar learning. Statistical analysis modules support multidimensional
    scaling (MDS) for visualizing similarity structures, principal component analysis
    (PCA) for dimensionality reduction, and discriminant analysis for classification
    tasks common in phonetic research. The programmable scripting language enables
    automation of repetitive analyses, batch processing, custom experiments, and
    extensibility, with scripts using Python-like syntax supporting loops, conditionals,
    arrays, and procedure definitions. Inter-process communication is facilitated
    through the sendpraat utility enabling external programs to send commands to
    running Praat instances. Graphics capabilities produce publication-quality outputs
    exportable as PDF, PNG, or EPS with integrated phonetic symbols (IPA), mathematical
    notation, and customizable fonts for professional presentations and manuscripts.
    Praat supports diverse file format I/O including WAV, AIFF, AIFC, FLAC, MP3
    (reading), and proprietary formats, with machine-independent binary files ensuring
    cross-platform compatibility. The software facilitates listening experiments
    for perceptual phonetics including identification tests (categorical perception)
    and discrimination tests (same-different, AX, ABX paradigms) with stimulus presentation
    and response collection. With 1.8k GitHub stars, 270 forks, 546 releases (latest
    v6.4.47 November 2025), and extensive documentation including integrated tutorials
    and searchable manuals accessible via Help menus, Praat remains the most widely
    adopted tool in phonetic sciences supporting research in speech production, perception,
    prosody, sociophonetics, clinical voice analysis, language documentation, and
    computational phonology.
  related_to:
  - B2AI_STANDARD:887
  requires_registration: false
  url: https://www.fon.hum.uva.nl/praat/
  used_in_bridge2ai: true
- id: B2AI_STANDARD:887
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:36
  contribution_date: '2025-03-05'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Parselmouth software
  formal_specification: https://github.com/YannickJadoul/Parselmouth
  has_relevant_organization:
  - B2AI_ORG:117
  is_open: true
  name: Parselmouth
  purpose_detail: Parselmouth is a Python library for working with Praat software.
    Parselmouth directly accesses Praat's C/C++ code (which means the algorithms and
    their output are exactly the same as in Praat) and provides efficient access to
    the program's data, but also provides an interface that looks no different from
    any other Python library.
  related_to:
  - B2AI_STANDARD:886
  requires_registration: false
  url: https://parselmouth.readthedocs.io/en/stable/
  used_in_bridge2ai: true
- id: B2AI_STANDARD:888
  category: B2AI_STANDARD:DataStandard
  collection:
  - audiovisual
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2025-03-13'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: BigTIFF format
  is_open: true
  name: BigTIFF
  purpose_detail: BigTIFF is an image format. It is a variant of the TIFF format that
    uses 64-bit offsets thereby supporting files up to 18,000 petabytes in size, vastly
    transcending TIFF's normal 4 GB limit. Since the format also supports all of the
    normal features and header tags of TIFF_6 and the extended metadata offered by
    GeoTIFF, it provides good service in the GIS domain, medical imaging, and other
    applications that employ large scanners or cameras.
  related_to:
  - B2AI_STANDARD:383
  requires_registration: false
  url: https://www.loc.gov/preservation/digital/formats/fdd/fdd000328.shtml
- id: B2AI_STANDARD:889
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:37
  contribution_date: '2025-03-13'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: TorchAudio library
  has_relevant_organization:
  - B2AI_ORG:117
  is_open: true
  name: torchaudio
  purpose_detail: TorchAudio is an official PyTorch domain library that provides
    building blocks for audio and speech processing in deep learning research and
    production applications, offering I/O operations, signal transforms, datasets,
    and pretrained models optimized for GPU acceleration and seamless integration
    with PyTorch's tensor operations and neural network modules. As part of the PyTorch
    ecosystem, torchaudio implements essential audio preprocessing primitives including
    waveform loading from diverse formats (WAV, MP3, FLAC, Vorbis, Opus), resampling,
    spectrogram computation (STFT, mel-scale, MFCC), audio augmentation (time/frequency
    masking, pitch shifting, noise injection), and feature extraction methods used
    in speech recognition, speaker identification, audio classification, and music
    information retrieval. The library provides unified access to standard audio datasets
    (LibriSpeech, VCTK, CommonVoice, YesNo) with consistent preprocessing pipelines,
    enabling reproducible benchmarking across research groups. TorchAudio includes
    reference implementations and pretrained weights for state-of-the-art models including
    Wav2Vec2, HuBERT, Conformer, Emformer, and Tacotron2, allowing researchers to
    fine-tune speech foundation models on domain-specific tasks or use them as feature
    extractors for downstream applications. The library's functional API separates
    stateless transforms from stateful modules, enabling flexible composition of audio
    processing pipelines that can be scripted with TorchScript for deployment in production
    environments without Python dependencies. TorchAudio supports both CPU and CUDA
    operations with automatic batching and differentiable transforms, allowing audio
    preprocessing to be included in end-to-end trainable pipelines where gradient
    flow through feature extraction improves model performance for tasks like source
    separation, speech enhancement, and audio generation. The library maintains compatibility
    with torchaudio.pipelines for high-level task-specific interfaces (automatic speech
    recognition, speaker verification, speech synthesis) that bundle models, preprocessing,
    and post-processing into single callable units suitable for inference workflows.
    TorchAudio is extensively used in industry and academia for applications ranging
    from voice assistants and podcast transcription to environmental sound classification,
    music genre recognition, acoustic event detection, and bioacoustic monitoring
    of wildlife vocalizations. For biomedical applications, torchaudio processes respiratory
    sounds (cough, wheeze, lung sounds), cardiac auscultation recordings, speech biomarkers
    for neurological disorders, and voice-based mental health assessment, enabling
    AI-driven diagnostics in telehealth and remote patient monitoring scenarios. The
    library's active development by Meta AI Research and the PyTorch Audio SIG ensures
    regular updates with new models, optimized kernels (Sox, FFmpeg, Kaldi-compatible
    implementations), and compatibility with the latest PyTorch features including
    torch.compile for accelerated inference and distributed training primitives for
    scaling audio model training across multiple GPUs and nodes.
  related_to:
  - B2AI_STANDARD:816
  requires_registration: false
  url: https://pytorch.org/audio/stable/index.html
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:91
    category: B2AI:Application
    name: Biomedical Audio Analysis and Speech-Based Diagnostics
    description: torchaudio is used in AI applications for processing and analyzing
      biomedical audio signals including speech patterns for neurological assessment,
      respiratory sounds for pulmonary diagnosis, and cardiac auscultation for automated
      heart disease detection. Deep learning models built with torchaudio process
      audio biomarkers such as cough sounds for COVID-19 screening, voice characteristics
      for Parkinson's disease detection, and lung sounds for pneumonia classification.
      The library's preprocessing capabilities, pretrained models, and integration
      with PyTorch enable researchers to develop audio-based AI diagnostics that can
      be deployed on mobile devices for remote patient monitoring, telehealth applications,
      and resource-limited settings where traditional diagnostic equipment is unavailable.
    used_in_bridge2ai: false
  - id: B2AI_APP:174
    category: B2AI:Application
    name: Self-Supervised Learning Pipelines for Speech Foundation Models
    description: 'TorchAudio provides pretrained self-supervised learning (SSL) pipelines
      and fine-tuning recipes for speech foundation models including Wav2Vec 2.0,
      HuBERT, XLS-R, and WavLM, enabling researchers to leverage large-scale pretrained
      encoders for downstream tasks. The library supplies "models and pre-trained
      pipelines for Wav2Vec 2.0, HuBERT, XLS-R, and WavLM" where "each pre-trained
      pipeline relies on the weights... and thus produces identical outputs" to upstream
      implementations, ensuring reproducibility when fine-tuning on domain-specific
      data. TorchAudio includes "end-to-end training recipes that allow for pre-training
      and fine-tuning HuBERT models from scratch" on LibriSpeech and custom datasets.
      A downstream spoken language understanding study explicitly "used the pretrained
      ASR models provided by TorchAudio," including WAV2VEC2_ASR_BASE_960H and HUBERT_ASR_LARGE
      checkpoints, to build joint-CTC SLU systems for intent classification and dialogue
      act recognition. The pretrained SSL encoders are designed for CTC fine-tuning
      and serve as feature extractors for tasks beyond ASR, demonstrating TorchAudio''s
      role as the primary distribution mechanism for PyTorch-compatible speech foundation
      models in research and production.'
    references:
    - https://doi.org/10.48550/arxiv.2310.17864
    - https://doi.org/10.1109/icassp49357.2023.10096795
    used_in_bridge2ai: false
  - id: B2AI_APP:175
    category: B2AI:Application
    name: ASR Model Training, Streaming Inference, and CTC Decoding
    description: 'TorchAudio implements complete automatic speech recognition training
      and inference pipelines including Conformer transducer, RNN-Transducer, and
      streaming Emformer architectures with production-ready decoders and forced alignment.
      The library provides "end-to-end training recipes" for Conformer and RNN-T,
      and introduces a "streaming-capable transformer-based acoustic model Emformer"
      that enables "real-time inference on CPU" for audio-visual ASR. For decoding,
      TorchAudio supplies a high-performance CTC beam search decoder supporting "both
      lexicon and lexicon-free decoding" with KenLM language model integration and
      custom neural network support, plus the "only publicly available CUDA-compatible
      CTC decoder" for GPU-accelerated inference. The library includes the "first
      publicly available GPU-based solution for computing forced alignments" via a
      "CTC-based forced alignment implementation" that supports both CPU and CUDA
      execution. StreamReader/StreamWriter APIs enable "chunked streaming of audio/video
      tensors from files and remote sources," supporting online ASR pipelines entirely
      in PyTorch with examples demonstrating how to "stream audio chunk by chunk from
      a remote video file." These components operationalize full ASR workflows from
      data loading through model training, streaming inference, beam search decoding,
      and timestamp alignment for subtitling and transcription applications.'
    references:
    - https://doi.org/10.48550/arxiv.2310.17864
    used_in_bridge2ai: false
  - id: B2AI_APP:176
    category: B2AI:Application
    name: Audio Preprocessing, Feature Extraction, and Data Augmentation
    description: 'TorchAudio''s functional, transform, and sox_effects modules provide
      comprehensive preprocessing and augmentation capabilities for ML training pipelines,
      with GPU-accelerated and TorchScript-compatible implementations suitable for
      production deployment. The library implements core preprocessing operations including
      "Spectrogram/InverseSpectrogram, mel-frequency cepstrum coefficients (MFCC),
      spectral centroid, phase vocoder and Griffin-Lim, resampling, and other filters"
      as torch.nn.Module subclasses that integrate seamlessly with PyTorch model pipelines
      via torch.nn.Sequential. The sox_effects module exposes "58 different sound
      effects, such as resampling, pitch shift" for data augmentation, with all transforms
      "fully torchscriptable" to enable compilation for inference optimization. TorchAudio
      emphasizes "GPU compute capability, automatic differentiation, and production
      readiness," allowing audio preprocessing to be included in end-to-end trainable
      pipelines where gradient flow through feature extraction can improve model performance.
      The library serves as "the foundational audio layer for downstream PyTorch toolkits
      (ESPnet, SpeechBrain, NeMo, fairseq)" by providing standardized, well-tested
      building blocks that avoid reimplementation of basic operations across projects.
      Transforms support runtime and inference acceleration through JIT compilation
      and "support [for] TorchScript and PyTorch-native quantization and leverage
      PyTorch 2.0''s Accelerated Transformers... to speed up training and inference,"
      with benchmarking on AWS p4d A100 hardware demonstrating performance parity
      or improvements over reference implementations.'
    references:
    - https://doi.org/10.48550/arxiv.2310.17864
    - https://doi.org/10.1109/icassp43922.2022.9747236
    used_in_bridge2ai: false
  - id: B2AI_APP:177
    category: B2AI:Application
    name: Multichannel Speech Enhancement and MVDR Beamforming
    description: 'TorchAudio 2.1 introduces mask-based MVDR (Minimum Variance Distortionless
      Response) beamforming and multichannel speech enhancement components that integrate
      with STFT/ISTFT processing and can be trained end-to-end with ASR models. The
      library implements a "mask-based MVDR beamforming model" where "the MVDR module
      is applied to the masks and multi-channel spectrum to produce the beamforming
      weights" and the "enhanced waveform is derived via inverse STFT," with training
      using "Ci-SDR as the loss function." The enhancement pipeline is designed for
      joint optimization with downstream tasks, as demonstrated by experiments "jointly
      leveraging TorchAudio''s mask-based MVDR beamforming model, Conformer transducer
      model, and TorchAudio-Squim" to perform "multi-channel speech enhancement, ASR,
      and speech quality assessment all within TorchAudio." The MVDR module supports
      integration with both Conformer transducer and Wav2Vec-2.0-based ASR models,
      enabling researchers to build end-to-end pipelines from multichannel microphone
      array input through beamforming, enhancement, recognition, and quality assessment.
      Experiments validate "the efficacy of TorchAudio''s MVDR module" for improving
      ASR performance in noisy and reverberant environments, with all components implemented
      as differentiable PyTorch modules that support GPU acceleration and gradient-based
      optimization of enhancement parameters jointly with acoustic model weights.'
    references:
    - https://doi.org/10.48550/arxiv.2310.17864
    used_in_bridge2ai: false
  - id: B2AI_APP:178
    category: B2AI:Application
    name: Reference-Less Speech Quality Assessment and Evaluation
    description: TorchAudio-Squim provides reference-less speech quality assessment
      tools that estimate perceptual and objective metrics including STOI (Short-Time
      Objective Intelligibility), PESQ (Perceptual Evaluation of Speech Quality),
      Si-SDR (Scale-Invariant Signal-to-Distortion Ratio), and MOS (Mean Opinion Score)
      without requiring clean reference signals, enabling integrated evaluation and
      training-time feedback within speech processing pipelines. The tool allows researchers
      to "estimate STOI, PESQ, and Si-SDR" during model training and inference, supporting
      quality-aware optimization and automated assessment of enhancement, separation,
      and synthesis outputs. TorchAudio-Squim is designed for integration with other
      library components, as demonstrated in experiments combining MVDR beamforming,
      Conformer/Wav2Vec-2.0 ASR, and "TorchAudio-Squim for quality assessment," providing
      a unified framework for training and evaluating multichannel enhancement pipelines
      with perceptual quality feedback. The reference-less approach is particularly
      valuable for real-world applications where clean reference signals are unavailable,
      such as live enhancement of telephony, conferencing, or broadcast audio, and
      for self-supervised or unsupervised training regimes where quality metrics guide
      model optimization without paired clean/noisy training data. By providing standardized,
      GPU-accelerated quality estimators as PyTorch modules, TorchAudio-Squim enables
      researchers to incorporate perceptual loss functions and quality monitoring
      directly into training loops, supporting development of enhancement and generation
      models optimized for human-perceived speech quality.
    references:
    - https://doi.org/10.48550/arxiv.2310.17864
    used_in_bridge2ai: false
- id: B2AI_STANDARD:890
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2025-03-13'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: RadElement Common Data Elements (CDEs) for Radiology
  is_open: true
  name: RadElement
  purpose_detail: RadElement is an open-source initiative that defines and curates
    standardized Common Data Elements (CDEs) for radiology research, providing machine-readable
    structured reporting templates that enable consistent capture of imaging findings,
    measurements, and clinical assessments across institutions, PACS vendors, and
    research consortia. Developed by the Radiological Society of North America (RSNA)
    with collaborative input from specialty societies, RadElement addresses the fragmentation
    of radiology reporting by creating consensus-driven data element sets organized
    into modular templates for specific clinical scenarios (lung nodule characterization,
    liver lesion reporting, prostate MRI, breast density assessment, stroke imaging
    protocols) that specify precise terminology, permissible values, measurement methods,
    and reporting conventions. Each CDE includes metadata describing the element's
    definition, units of measurement, data type, value sets drawn from controlled
    vocabularies (RadLex, SNOMED CT, LOINC), cardinality constraints, and provenance
    documentation linking to source guidelines from ACR, RSNA, and disease-specific
    professional societies. RadElement templates are expressed in JSON Schema format
    and distributed through the RadElement.org web platform, which provides search,
    browsing, and API access to the CDE library, enabling software developers to integrate
    standardized reporting into radiology information systems, structured reporting
    tools, and clinical decision support applications. By standardizing the representation
    of imaging observations, RadElement facilitates secondary use of radiology data
    for clinical research, quality improvement, and AI model development, as structured
    reports can be automatically parsed and aggregated across large patient cohorts
    without manual chart review or natural language processing of free-text reports.
    For AI/ML applications, RadElement provides ground-truth annotations and training
    labels derived from structured clinical reports, enables automated quality control
    of annotated imaging datasets, and supports development of AI-generated structured
    reports that populate CDE fields with model predictions for radiologist review.
    The initiative has released CDE sets for high-impact imaging applications including
    lung cancer screening (Lung-RADS), liver tumor response assessment (LI-RADS),
    prostate cancer staging (PI-RADS), traumatic brain injury, COVID-19 pneumonia,
    and pediatric appendicitis, with ongoing expansion driven by community contributions
    and feedback from pilot implementations. RadElement supports FAIR data principles
    by making imaging research data more Findable (standardized metadata schema),
    Accessible (open web platform and APIs), Interoperable (controlled vocabularies
    and FHIR mappings), and Reusable (versioned templates with clear licensing), addressing
    a critical need for semantic interoperability in multi-site imaging studies and
    federated learning scenarios where consistent data representation is essential
    for combining datasets. The platform tracks CDE usage statistics, version history,
    and validator tools that check report conformance to template specifications,
    promoting reproducibility and data quality in radiology research while reducing
    the burden on investigators to develop ad hoc data dictionaries for each new study.
  requires_registration: false
  url: https://www.radelement.org/
- id: B2AI_STANDARD:891
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2025-03-13'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: FAIR Genomes Semantic Model
  formal_specification: https://github.com/fairgenomes/fairgenomes-semantic-model/blob/main/fair-genomes.yml
  is_open: true
  name: FAIRGenomes
  purpose_detail: FAIR Genomes is a semantic metadata model that provides a comprehensive,
    extensible framework for describing human genomic and phenotypic datasets in accordance
    with FAIR principles (Findable, Accessible, Interoperable, Reusable), addressing
    the critical challenge of standardizing metadata across diverse genomics studies,
    biobanks, and clinical genomics programs to enable data discovery, integration,
    and responsible sharing. The model defines a structured vocabulary organized into
    modular components covering study metadata (project descriptions, consent codes,
    data access policies), subject/patient attributes (demographics, ancestry, clinical
    diagnoses using ontology terms from HPO, MONDO, ORDO), sample characteristics
    (tissue types, preservation methods, collection protocols referencing UBERON and
    OBI), sequencing information (platforms, library preparation, coverage metrics,
    file formats), and variant/molecular data annotations (variant calling pipelines,
    reference genomes, quality metrics). FAIR Genomes is expressed as a machine-readable
    YAML schema (fair-genomes.yml) that specifies element definitions, data types,
    cardinality constraints, recommended ontologies and controlled vocabularies for
    each field, and mappings to existing standards including GA4GH Phenopackets, European
    Genome-phenome Archive (EGA) metadata schema, and Dublin Core terms. The model
    emphasizes use of established biomedical ontologies (Human Phenotype Ontology
    for phenotypes, NCIT for cancer types, LOINC for lab measurements, SNOMED CT
    for clinical concepts) to ensure semantic interoperability and enable automated
    reasoning, cross-dataset queries, and ontology-based data harmonization essential
    for meta-analyses and federated research networks. FAIR Genomes supports capture
    of consent and data use restrictions through structured encoding of informed consent
    types, data use ontology (DUO) terms, and return-of-results policies, facilitating
    compliant data sharing and automated assessment of whether a dataset can be used
    for a proposed research purpose. The schema is designed for extensibility through
    defined extension mechanisms that allow research communities to add domain-specific
    modules (rare disease registries, cancer genomics, population cohorts) while maintaining
    core compatibility, and it provides versioning and provenance tracking to document
    schema evolution over time. Implementation tooling includes Python and R libraries
    for validating metadata against the FAIR Genomes schema, converting between FAIR
    Genomes and other metadata formats (VCF headers, PED files, Phenopackets JSON),
    and generating metadata submission templates for data producers. For clinical
    genomics and precision medicine applications, FAIR Genomes enables integration
    of research genomic datasets with electronic health records, supports clinical
    variant interpretation workflows that require patient phenotype and family history
    context, and facilitates development of machine learning models that leverage
    rich phenotypic annotations for variant pathogenicity prediction and genomic risk
    scoring. The model has been adopted by European genomic data infrastructures including
    the European Rare Disease Research Infrastructure (RD-Connect), Solve-RD consortium,
    and national genome initiatives implementing federated analysis frameworks where
    standardized metadata is essential for distributed query execution without moving
    sensitive patient data across institutional boundaries.
  requires_registration: false
  url: https://github.com/fairgenomes/fairgenomes-semantic-model
- id: B2AI_STANDARD:893
  category: B2AI_STANDARD:BiomedicalStandard
  concerns_data_topic:
  - B2AI_TOPIC:15
  contribution_date: '2025-03-13'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Recommended Metadata for Biological Images (REMBI)
  is_open: true
  name: REMBI
  publication: doi:10.1038/s41592-021-01166-8
  purpose_detail: REMBI (Recommended Metadata for Biological Images) is a community-developed
    specification that defines minimum metadata standards for biological imaging experiments
    across light microscopy, electron microscopy, and other imaging modalities, addressing
    the critical need for reproducible, interpretable, and reusable image data in
    biomedical research by establishing consensus guidelines for documenting experimental
    context, sample preparation, imaging acquisition parameters, and image processing
    workflows. Published in Nature Methods (2021) through a collaborative effort involving
    microscopy core facilities, imaging scientists, data managers, and standards organizations,
    REMBI provides a hierarchical framework organized into four tiers of metadata
    granularityfrom essential contextual information required for all imaging datasets
    to detailed technical parameters needed for advanced image analysis and computational
    modeling. The specification covers study-level metadata (biological question,
    experimental design, organism/cell line identifiers linked to ontologies), specimen
    preparation details (fixation methods, labeling strategies, mounting media), imaging
    acquisition parameters specific to each modality (objective magnification, numerical
    aperture, excitation/emission wavelengths, laser power, detector settings, pixel
    size, bit depth, acquisition speed), and image processing provenance (software
    versions, algorithms applied, parameter settings, visualization adjustments like
    brightness/contrast). REMBI emphasizes use of controlled vocabularies and ontologies
    including the Biological Imaging Methods Ontology (FBbi), Cell Ontology (CL),
    Uberon anatomy ontology, and Chemical Entities of Biological Interest (ChEBI)
    for fluorophores and dyes, ensuring semantic consistency and enabling automated
    metadata validation and cross-dataset queries. The guidelines are intentionally
    flexible to accommodate the diversity of biological imagingfrom high-content
    screening and live-cell time-lapse to super-resolution and correlative light-electron
    microscopywhile maintaining core requirements that ensure datasets can be properly
    cited, understood without direct contact with original investigators, and computationally
    processed using documented protocols. REMBI is designed for integration with emerging
    imaging data repositories (Image Data Resource, BioImage Archive, Cell Image
    Library), OMERO image management systems, and Electronic Lab Notebook platforms,
    with implementation efforts producing metadata capture templates, validation tools,
    and export utilities that embed REMBI-compliant metadata into OME-TIFF and OME-ZARR
    file formats. For AI/ML applications in biological imaging, REMBI metadata enables
    proper documentation of training datasets, supports domain adaptation by revealing
    systematic differences in imaging protocols across labs, facilitates development
    of imaging biomarkers by linking acquisition parameters to biological outcomes,
    and ensures that deep learning models trained on specific microscopy modalities
    can be appropriately applied to new datasets or flagged when imaging conditions
    fall outside training distribution. The standard has been adopted by major microscopy
    facilities as a framework for user training and data management policies, is referenced
    in data management plans for imaging-intensive grants, and serves as a foundation
    for specialized extensions addressing modality-specific requirements in cryo-electron
    microscopy, spatial transcriptomics imaging, and high-throughput phenotypic screening.
  requires_registration: false
  url: https://docs.google.com/spreadsheets/d/1Ck1NeLp-ZN4eMGdNYo2nV6KLEdSfN6oQBKnnWU6Npeo/edit?gid=1023506919#gid=1023506919
- id: B2AI_STANDARD:894
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - dataregistry
  concerns_data_topic:
  - B2AI_TOPIC:5
  - B2AI_TOPIC:23
  description: PRoteomics IDEntifications Database
  has_relevant_organization:
  - B2AI_ORG:116
  is_open: true
  name: PRIDE
  publication: doi:10.1093/nar/gkae1011
  purpose_detail: The PRIDE PRoteomics IDEntifications (PRIDE) Archive database is
    a centralized, standards compliant, public data repository for mass spectrometry
    proteomics data, including protein and peptide identifications and the corresponding
    expression values, post-translational modifications and supporting mass spectra
    evidence (both as raw data and peak list files). PRIDE is a core member in the
    ProteomeXchange (PX) consortium, which provides a standardised way for submitting
    mass spectrometry based proteomics data to public-domain repositories. Datasets
    are submitted to ProteomeXchange via PRIDE and are handled by expert bio-curators.
    All PRIDE public datasets can also be searched in ProteomeCentral, the portal
    for all ProteomeXchange datasets.
  requires_registration: false
  url: https://www.ebi.ac.uk/pride/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:92
    category: B2AI:Application
    name: Proteomics Data Mining and Peptide Identification
    description: PRIDE (PRoteomics IDEntifications) database is used in AI applications
      for training deep learning models on mass spectrometry proteomics data, enabling
      improved peptide identification, protein quantification, and post-translational
      modification prediction. Machine learning systems leverage PRIDE's extensive
      repository of annotated spectra to develop neural networks for de novo peptide
      sequencing, spectral quality assessment, and cross-linking analysis. AI models
      trained on PRIDE data improve upon traditional database search algorithms by
      learning complex patterns in fragmentation spectra, enabling identification
      of novel peptides, non-canonical modifications, and proteoforms. The database's
      standardized mzML and mzIdentML formats facilitate reproducible AI research
      in clinical proteomics, biomarker discovery, and precision medicine applications.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:895
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  - implementation_maturity_production
  - standards_process_maturity_final
  concerns_data_topic:
  - B2AI_TOPIC:5
  description: American Standard Code for Information Interchange ASCII File Format
    Guidelines for Earth Science Data
  formal_specification: https://www.earthdata.nasa.gov/s3fs-public/imported/ESDS-RFC-027v1.1.pdf
  has_relevant_organization:
  - B2AI_ORG:114
  is_open: true
  name: ASCII File Format Guidelines for Earth Science Data
  purpose_detail: The ASCII File Format Guidelines for Earth Science Data is a NASA
    Earth Science Data and Information System (ESDIS) standard (ESDS-RFC-027v1.1)
    that establishes conventions for structuring plain-text ASCII files to store,
    distribute, and archive Earth science measurements, model outputs, and derived
    products in a human-readable, platform-independent format that remains accessible
    across decades of data preservation while supporting automated ingestion into
    scientific analysis workflows. Developed to address the proliferation of inconsistent
    ASCII file formats across NASA missions and data centers, the guidelines specify
    best practices for file organization including mandatory header sections containing
    metadata (data provider, creation date, geographic coverage, temporal range, variable
    definitions, units of measurement, missing value codes, quality flags), delimited
    data records (comma-separated, tab-separated, or fixed-width columns), and documentation
    of coordinate systems and datum references essential for geospatial data interpretation.
    The standard emphasizes self-describing files where all information necessary
    to interpret the data is embedded within the file itself, reducing dependency
    on external documentation that can be lost or separated from data files during
    long-term archival or inter-institutional data sharing. ASCII format conventions
    include character encoding specifications (7-bit ASCII for maximum compatibility),
    line terminator guidelines (handling platform differences between Unix, Windows,
    Mac), numeric precision recommendations, date/time representation standards (ISO
    8601 preferred), and strategies for representing hierarchical or multidimensional
    data within the inherent flatness of text files. The guidelines address practical
    considerations for Earth science data including representation of gridded datasets
    (latitude/longitude arrays, satellite swath data, model output grids), time series
    from ground stations and buoys, trajectory data from aircraft and autonomous vehicles,
    and derived products like statistical summaries and aggregated climatologies.
    While acknowledging that binary formats (NetCDF, HDF5, GeoTIFF) are more efficient
    for large-scale scientific datasets, the ASCII guidelines serve critical use cases
    including rapid prototyping and exploratory analysis, data exchange with legacy
    systems and non-specialist users, archival copies for human readability and format
    migration resilience, and compliance with data publication requirements from journals
    and funding agencies mandating human-readable supplementary data files. For interdisciplinary
    research connecting Earth science with biomedical and public health applications,
    ASCII representations of environmental exposure variables (air quality indices,
    temperature extremes, precipitation patterns, vegetation indices, land use changes)
    enable integration with epidemiological datasets often stored in simple text or
    CSV formats, supporting studies of climate-health relationships, environmental
    justice assessments, and One Health research connecting human, animal, and ecosystem
    health. The standard has influenced broader scientific data practices including
    the design of CSV dialect specifications, contributed to development of metadata
    standards for observational data, and serves as a reference for NASA Earth science
    data producers preparing data for distribution through the Earthdata system and
    long-term preservation in NASA's Earth Observing System Data and Information System
    (EOSDIS) archive.
  requires_registration: false
  url: https://www.earthdata.nasa.gov/about/esdis/esco/standards-practices/ascii-file-format-guidelines-earth-science-data
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:93
    category: B2AI:Application
    name: Environmental and Climate Health Data Integration
    description: ASCII File Format Guidelines for Earth Science Data are used in AI
      applications that integrate environmental and climate data with biomedical datasets
      to model health impacts of environmental change, predict disease patterns related
      to climate factors, and develop early warning systems for climate-sensitive
      health outcomes. Machine learning models leverage standardized ASCII representations
      of temperature, precipitation, air quality, and other environmental variables
      to train on relationships between environmental exposures and health outcomes
      such as vector-borne disease transmission, heat-related illness, respiratory
      disease exacerbation, and mental health impacts. The standardized format enables
      AI systems to integrate NASA Earth observations with clinical and epidemiological
      data for One Health applications bridging human, animal, and environmental health.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:896
  category: B2AI_STANDARD:SoftwareOrTool
  collection:
  - datamodel
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:14
  description: Geographic Informations Systems Toolchain
  has_relevant_organization:
  - B2AI_ORG:76
  - B2AI_ORG:115
  is_open: true
  name: GIS Toolchain
  purpose_detail: The GIS toolchain consists of extensions to the OMOP schema, extensions
    to the OMOP Vocabulary, and GIS-specific software for acquiring and working with
    geospatial data. Together, these enable researchers to use health-related attributes
    of the regions where patients live in OHDSI study cohort definitions. For example,
    you can use the GIS toolchain to define cohorts that include regional data on
    exposure to toxicants or social deprivation along with EHR data on relevant health
    outcomes. The toolchain also includes informatics resources that support integration
    with the main OHDSI tool stack (HADES) and integration with externally supported
    solutions for geocoding and for finding and deriving relevant data sources from
    catalogs of available data sources. Importantly, the toolchain allows integrated
    analysis of geospatial and EHR data without sharing any sensitive patient location
    data.
  requires_registration: false
  url: https://ohdsi.github.io/GIS/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:94
    category: B2AI:Application
    name: Geospatial Health Analytics and Environmental Exposure Modeling
    description: GIS Toolchain (from OHDSI) is used in AI applications for integrating
      geographic information with clinical data to train models that account for environmental
      exposures, social determinants of health, and spatial disease patterns. Machine
      learning systems leverage geospatial features derived from this toolchain to
      develop prediction models for infectious disease spread, environmental health
      impacts, cancer cluster detection, and health equity analysis. AI applications
      combine geocoded patient addresses with environmental data layers (air quality,
      greenspace, food access) to create location-aware risk models that inform population
      health interventions and resource allocation. The toolchain's integration with
      OMOP CDM enables spatiotemporal analysis at scale across observational health
      databases.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:897
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  - standards_process_maturity_development
  - implementation_maturity_pilot
  concerns_data_topic:
  - B2AI_TOPIC:4
  description: Medical Imaging Common Data Model
  has_relevant_organization:
  - B2AI_ORG:76
  - B2AI_ORG:115
  is_open: true
  name: MI-CDM
  publication: doi:10.1007/s10278-024-00982-6
  purpose_detail: The rapid growth of artificial intelligence (AI) and deep learning
    techniques require access to large inter-institutional cohorts of data to enable
    the development of robust models, e.g., targeting the identification of disease
    biomarkers and quantifying disease progression and treatment efficacy. The Observational
    Medical Outcomes Partnership Common Data Model (OMOP CDM) has been designed to
    accommodate a harmonized representation of observational healthcare data. This
    study proposes the Medical Imaging CDM (MI-CDM) extension, adding two new tables
    and two vocabularies to the OMOP CDM to address the structural and semantic requirements
    to support imaging research. The tables provide the capabilities of linking DICOM
    data sources as well as tracking the provenance of imaging features derived from
    those images. The implementation of the extension enables phenotype definitions
    using imaging features and expanding standardized computable imaging biomarkers.
    This proposal offers a comprehensive and unified approach for conducting imaging
    research and outcome studies utilizing imaging features.
  related_to:
  - B2AI_STANDARD:98
  - B2AI_STANDARD:243
  requires_registration: false
  used_in_bridge2ai: true
  url: https://doi.org/10.1007/s10278-024-00982-6
  has_application:
  - id: B2AI_APP:95
    category: B2AI:Application
    name: Medical Imaging AI Data Standardization and Multi-Site Studies
    description: MI-CDM (Medical Imaging Common Data Model) is used in AI applications
      to standardize heterogeneous imaging metadata across institutions, enabling
      training of robust deep learning models on diverse multi-site imaging datasets.
      The common data model facilitates AI development by providing consistent representation
      of imaging protocols, scanner parameters, patient demographics, and clinical
      annotations across different PACS systems and imaging centers. Machine learning
      systems leverage MI-CDM to perform federated learning on distributed imaging
      data without raw image transfer, enable systematic bias detection across sites,
      and develop models that generalize across different scanner manufacturers and
      acquisition protocols. The standardization is critical for regulatory-grade
      AI where model validation requires diverse, well-characterized datasets.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:898
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:21
  description: Cytoscape Exchange
  has_relevant_organization:
  - B2AI_ORG:116
  is_open: true
  name: CX
  purpose_detail: Cytoscape Exchange (CX) format is a network exchange format, designed
    as a flexible structure for transmission of networks. It is designed for flexibility,
    modularity, and extensibility, and as a message payload in common REST protocols.
    It is not intended as an in-memory data model for use in applications.
  requires_registration: false
  url: https://cytoscape.org/cx/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:96
    category: B2AI:Application
    name: Network Biology and Graph Neural Networks
    description: CX (Cytoscape Exchange) format is used in AI applications for representing
      biological networks that serve as input to graph neural networks for tasks such
      as protein function prediction, drug-target interaction prediction, and disease
      gene prioritization. Machine learning models leverage CX's standardized representation
      of nodes, edges, and network attributes to train graph-based deep learning architectures
      that capture complex biological relationships. AI systems use CX-encoded networks
      for multi-omics data integration, pathway analysis, and systems biology modeling
      where network topology and node features inform predictions. The format enables
      sharing of network models across platforms and reproducible AI research in network
      medicine, synthetic biology, and computational drug discovery.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:899
  category: B2AI_STANDARD:DataStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:5
  description: Anndata h5ad format
  formal_specification: https://github.com/scverse/anndata/blob/main/src/anndata/_io/h5ad.py
  has_relevant_organization:
  - B2AI_ORG:116
  is_open: true
  name: h5ad
  purpose_detail: h5ad is an HDF5-based file format specifically designed for storing
    annotated data matrices produced by the AnnData Python package, serving as the
    standard data structure for single-cell genomics analysis and computational biology
    workflows in the scverse ecosystem. The format efficiently stores high-dimensional
    biological datasets by representing data as an AnnData object containing a primary
    data matrix (typically cell-by-gene expression counts stored as sparse or dense
    arrays), cell-level annotations (obs), gene-level annotations (var), unstructured
    annotations (uns), pairwise cell relationships (obsp), pairwise gene relationships
    (varp), and multi-dimensional arrays indexed by observations or variables (obsm,
    varm) such as dimensionality reduction embeddings (PCA, UMAP, t-SNE) or batch-effect
    correction results. Built on the HDF5 hierarchical data format, h5ad enables partial
    I/O operations allowing selective reading and writing of specific dataset components
    without loading entire files into memory, critical for datasets with millions
    of cells that exceed typical RAM capacity. The format supports both sparse matrix
    representations (CSR, CSC formats from scipy.sparse) for efficient storage of
    scRNA-seq data where most gene expression values are zero, and dense arrays for
    continuous measurements or embeddings. h5ad files preserve complete analytical
    provenance by storing preprocessing parameters, quality control metrics, normalization
    methods, clustering results, differential expression statistics, and trajectory
    inference outcomes alongside raw and processed data in a single self-contained
    file. Integration with the broader scverse ecosystem (Scanpy for analysis, scVI
    for deep generative modeling, CellRank for trajectory inference, Squidpy for spatial
    transcriptomics) ensures interoperability across tools, while compatibility with
    R through the anndata R package and Seurat conversion utilities enables cross-platform
    workflows. The format is extensively used in machine learning applications for
    cell type annotation, batch correction with deep learning models (scVI, scANVI),
    gene regulatory network inference, spatial transcriptomics analysis, and multi-modal
    data integration combining scRNA-seq with scATAC-seq, CITE-seq antibody counts,
    or spatial imaging data. h5ad has become the de facto standard for single-cell
    data sharing in repositories like the Human Cell Atlas, CZI CELLxGENE, and Single
    Cell Portal, enabling reproducible research and collaborative data analysis across
    the single-cell genomics community.
  requires_registration: false
  url: https://anndata.readthedocs.io/en/stable/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:97
    category: B2AI:Application
    name: Single-Cell Genomics and Deep Learning Integration
    description: h5ad format (HDF5-based AnnData) is the standard file format for
      AI applications in single-cell genomics, enabling efficient storage and access
      of large-scale single-cell RNA-seq datasets for training deep learning models.
      AI systems leverage h5ad's structured representation of cell-by-gene expression
      matrices, cell metadata, and dimensionality reduction embeddings to perform
      cell type classification, trajectory inference, gene regulatory network inference,
      and batch effect correction. Deep learning frameworks integrate seamlessly with
      h5ad through the AnnData API, supporting applications in cancer cell identification,
      developmental biology, immune profiling, and drug response prediction at single-cell
      resolution. The format's efficient sparse matrix storage enables training on
      datasets with millions of cells.
    used_in_bridge2ai: false
  - id: B2AI_APP:168
    category: B2AI:Application
    name: scvi-tools Probabilistic Modeling and Integration with AnnData
    references:
    - https://doi.org/10.1038/s41587-021-01206-w
    description: The scvi-tools Python library uses h5ad/AnnData as the primary data
      interface for probabilistic deep learning models in single-cell genomics, with
      models consuming AnnData objects directly for training and inference. The framework
      provides setup_anndata to register tensor locations from AnnData objects, enabling
      models like scVI, scANVI, and totalVI to operate on h5ad files via code such
      as "SCVI.setup_anndata(adata, batch_key='donor')" followed by "model = SCVI(adata)".
      The AnnDataLoader component "reads data from the AnnData object and automatically
      structures it for training or for downstream analysis with the trained model,"
      supporting joint multimodal processing of transcriptomics and proteomics data.
      Batch integration is enabled through batch_key parameters, and model-specific
      requirements (UMI counts, labels_key) are handled through the AnnData interface.
      The reimplementation of methods like Stereoscope and CellAssign within scvi-tools
      allows these models to "be used from Jupyter notebooks and with AnnData objects,"
      with visualization via Scanpy taking less than 20 lines of code. This demonstrates
      h5ad as the standard format for training variational autoencoders, performing
      batch correction, cell type annotation, and multimodal integration in the single-cell
      ecosystem.
    used_in_bridge2ai: false
  - id: B2AI_APP:169
    category: B2AI:Application
    name: PyTorch Integration for ML Training on Single-Cell Data
    references:
    - https://doi.org/10.21105/joss.04371
    - https://doi.org/10.1101/2021.12.16.473007
    description: AnnData provides native PyTorch integration for machine learning
      workflows through the AnnLoader DataLoader interface and AnnCollection for lazy
      concatenation of larger-than-memory datasets, enabling seamless training of
      deep learning models on h5ad files. The format uses "language-independent, hierarchical
      on-disk formats HDF5 and zarr (commonly .h5ad)" with a one-to-one mapping between
      in-memory and on-disk structures, allowing compressed sparse matrices (CSR,
      CSC) to be stored as data, indices, and indptr arrays while tabular data is
      stored columnar. This design enables partial I/O operations where models can
      read specific data components without loading entire files into memory, critical
      for training on million-cell datasets. The PyTorch integration via AnnLoader
      and AnnCollection allows h5ad-backed objects to serve directly as model input,
      supporting training workflows that combine numpy-centered tools (scikit-learn,
      UMAP) with PyTorch-based deep learning. Major consortia including the Human
      Cell Atlas and HuBMAP distribute single-cell datasets through h5ad format, and
      the ecosystem explicitly includes scvi-tools for deep generative modeling, demonstrating
      h5ad as the interchange format connecting data repositories, preprocessing workflows,
      and ML model training in single-cell genomics.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:900
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:9
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Clinical Data Interchange Standards Consortium (CDISC) Dataset-JavaScript
    Object Notation
  formal_specification: https://www.cdisc.org/standards/data-exchange/dataset-json/dataset-json-v1-1
  is_open: true
  name: Dataset-JSON
  purpose_detail: CDISC Dataset-JSON is a JSON-based schema specifically designed
    for exchanging tabular datasets in clinical studies. It is based on CDISC Dataset-JSON
    version 1.0 with enhancements, including smaller file sizes, additional metadata,
    and simpler processing. The format supports file and Application Programming Interface
    based data exchange, is widely supported across technologies, and can link to
    Define-XML for additional metadata. Dataset-JSON has the potential to replace
    Statistical Analysis System (SAS) version 5 XPORT Transport Format (XPT) for submission
    of electronic study data to regulatory agencies.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:15
  url: https://www.cdisc.org/standards/data-exchange/dataset-json
- id: B2AI_STANDARD:901
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:37
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: European Data Format Plus
  has_application:
  - id: B2AI_APP:262
    category: B2AI:Application
    name: I-CARE EDF+ coma EEG ML prognostication resource
    description: Large-scale harmonized clinical continuous EEG resource for machine learning-based prognostication in comatose post-cardiac arrest patients, with original EEG explicitly converted to de-identified EDF+ files supporting annotations, events, and interrupted recordings. The I-CARE (International Cardiac Arrest Research) consortium dataset comprises 56,676 hours of continuous EEG from 1,020 patients across multiple centers, standardized to EDF+ format to enable interoperable ML analytics pipelines. EDF+ capabilities for storing annotations alongside signal data facilitate labeling of seizures, burst suppression, periodic discharges, and outcome markers; interrupted recording support accommodates clinical gaps in monitoring without file fragmentation; and event annotations document stimuli, medication administration, and clinical interventions. This EDF+-based resource enables training and validation of deep learning models (CNNs on raw EEG spectrograms, LSTMs on time-series features, transformers on multi-channel sequences) for automated outcome prediction, seizure detection, and quantitative EEG biomarker extraction in critically ill patients, supporting development of clinical decision support systems for neurological prognostication after cardiac arrest where timely, accurate predictions can guide withdrawal of life-sustaining therapy decisions.
    references:
    - https://doi.org/10.1038/s41597-023-02810-8
  has_relevant_organization:
  - B2AI_ORG:115
  is_open: true
  name: EDF+
  publication: doi:10.1016/S1388-2457(03)00123-8
  purpose_detail: An extension of the European Data Format (EDF) that maintains EDF
    compatibility while adding capability to store annotations, events, and stimuli
    alongside the signal data. It also allows for storing interrupted (non-contiguous)
    recordings in a single file. EDF+ can save most EEG, PSG, ECG, EMG, and Evoked
    Potential data that cannot be saved into common hospital information systems.
    The Persyst universal EEG reader supports this format.
  related_to:
  - B2AI_STANDARD:105
  requires_registration: false
  url: https://www.edfplus.info/specs/edfplus.html
  used_in_bridge2ai: true
- id: B2AI_STANDARD:902
  category: B2AI_STANDARD:DataStandard
  collection:
  - datamodel
  - standards_process_maturity_draft
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:5
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Archival Resource Key Persistent Identifiers
  formal_specification: https://datatracker.ietf.org/doc/draft-kunze-ark/
  has_relevant_organization:
  - B2AI_ORG:116
  is_open: true
  name: ARK PIDs
  purpose_detail: Archival Resource Key (ARK) identifiers are persistent URLs designed
    to support long-term access to information objects. ARKs can identify digital
    objects (documents, databases, images, software), physical objects (books, artifacts),
    living beings, and intangible objects (concepts, services). ARKs are characterized
    by their internal "ark:" label, their NAAN (Name Assigning Authority Number) identifying
    the naming organization, and features like metadata access through inflections
    (adding ? to the URL). They are free to create and use, with no fees to assign
    or use ARKs, and can be hosted on any web server or through the global N2T.net
    resolver.
  requires_registration: false
  url: https://arks.org/about/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:98
    category: B2AI:Application
    name: Persistent Dataset Identification for ML Reproducibility
    description: ARK (Archival Resource Key) persistent identifiers are used in AI
      applications to create stable, long-term references to training datasets, model
      artifacts, and research outputs, ensuring reproducibility and provenance tracking
      in machine learning research. AI systems leverage ARK identifiers to maintain
      citations to specific versions of datasets used in model training, enabling
      verification of results and compliance with data governance policies. The identifier
      scheme's flexibility supports both fine-grained object identification (individual
      data files) and collections (entire datasets), which is essential for documenting
      complex ML pipelines. ARK PIDs facilitate data sharing in federated AI research
      while maintaining clear attribution and enabling auditable access logs for sensitive
      biomedical data.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:903
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - datamodel
  concerns_data_topic:
  - B2AI_TOPIC:7
  - B2AI_TOPIC:4
  contribution_date: '2025-05-29'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Clinical Dataset Structure
  formal_specification: https://github.com/AI-READI/cds-specification
  has_relevant_organization:
  - B2AI_ORG:114
  is_open: true
  name: CDS
  publication: doi:10.5281/zenodo.10867040
  purpose_detail: The Clinical Dataset Structure (CDS) is a standardized way to organize
    and describe clinical research datasets to make them readily interoperable and
    easily reusable by humans and machines. It addresses the challenge of integrating
    multiple data modalities from clinical studies by providing a simple, intuitive
    file and directory structure. CDS organizes data by datatype at the root level,
    with each datatype directory structured according to applicable standards or a
    recommended hierarchy of modality/device/participant directories. The standard
    includes specifications for metadata files that document the dataset content,
    structure, and participant information, optimizing datasets for AI-readiness and
    secondary analysis.
  requires_registration: false
  url: https://cds-specification.readthedocs.io/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:99
    category: B2AI:Application
    name: Clinical Decision Support Rule Integration and ML Hybridization
    description: CDS (Clinical Decision Support) specification is used in AI applications
      to integrate machine learning models with traditional rule-based clinical decision
      support systems, enabling hybrid AI approaches that combine interpretable rules
      with data-driven predictions. AI systems leverage CDS Hooks and standardized
      interfaces to deploy ML models as FHIR-based services that provide real-time
      recommendations during clinical workflows, such as drug-drug interaction checking
      augmented with personalized risk predictions, or sepsis alerts that combine
      guideline-based criteria with neural network early warning scores. The specification
      enables AI applications to surface predictions within EHR user interfaces at
      appropriate decision points, while maintaining auditability through standardized
      request/response formats and provenance metadata.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:904
  category: B2AI_STANDARD:DataStandard
  collection:
  - datamodel
  - standards_process_maturity_final
  - implementation_maturity_production
  concerns_data_topic:
  - B2AI_TOPIC:5
  contribution_date: '2025-05-29'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: A collaborative, community-developed schema for structured data on
    the Internet
  formal_specification: https://github.com/schemaorg/schemaorg
  has_relevant_organization:
  - B2AI_ORG:116
  is_open: true
  name: Schema.org
  purpose_detail: Schema.org is a collaborative, community activity founded by Google,
    Microsoft, Yahoo, and Yandex with a mission to create, maintain, and promote schemas
    for structured data on the Internet. It provides a common vocabulary that webmasters
    can use to mark up their pages in ways that can be understood by major search
    engines and other applications. The Schema.org vocabulary consists of a set of
    types (e.g., Person, Event, Organization), properties (e.g., name, location, startDate),
    and relationships that can be used with many different encoding formats including
    RDFa, Microdata, and JSON-LD. As of 2024, over 45 million web domains use Schema.org
    markup with over 450 billion Schema.org objects. The vocabulary is continuously
    evolving through an open community process managed by the W3C Schema.org Community
    Group.
  requires_registration: false
  url: https://schema.org/
  used_in_bridge2ai: true
  has_application:
  - id: B2AI_APP:100
    category: B2AI:Application
    name: Biomedical Knowledge Extraction and Semantic Search
    description: Schema.org vocabularies, particularly the biomedical extensions (BioSchemas),
      are used in AI applications for automated knowledge extraction from web resources,
      semantic annotation of datasets, and training large language models on structured
      biomedical information. AI systems leverage Schema.org markup to extract structured
      data about proteins, genes, diseases, clinical trials, and medical conditions
      from web pages, enabling automated knowledge base construction and question-answering
      systems. The vocabulary supports AI-driven dataset discovery, metadata standardization
      for machine learning pipelines, and training of biomedical language models that
      understand relationships between biological entities. Schema.org's widespread
      adoption makes it valuable for web-scale biomedical data mining and federated
      search applications.
    used_in_bridge2ai: false
- id: B2AI_STANDARD:905
  category: B2AI_STANDARD:BiomedicalStandard
  collection:
  - fileformat
  concerns_data_topic:
  - B2AI_TOPIC:28
  contribution_date: '2025-05-29'
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: Thermo Fisher RAW mass spectrometry data format
  has_relevant_organization:
  - B2AI_ORG:116
  is_open: false
  name: Thermo Fisher RAW
  purpose_detail: The Thermo Fisher RAW format is a proprietary file format designed
    for storing mass spectrometry data generated by Thermo Fisher Scientific instruments.
    It contains detailed information about mass spectra, chromatograms, instrument
    parameters, and metadata from experimental runs. The format supports multiple
    mass spectrometry techniques and is accessed through Thermo's software tools like
    MSFileReader or more recent RawFileReader. While it's proprietary, various conversion
    tools allow transformation to open formats like mzML or mzXML for broader compatibility
    with third-party analysis software. The format is widely used in proteomics, metabolomics,
    and other mass spectrometry-based research applications where preserving the complete
    experimental context is essential for data interpretation and analysis.
  requires_registration: true
  responsible_organization:
  - B2AI_ORG:123
  url: https://www.thermofisher.com/us/en/home/industrial/mass-spectrometry.html
  used_in_bridge2ai: true
- id: B2AI_STANDARD:906
  category: B2AI_STANDARD:TrainingProgram
  concerns_data_topic:
  - B2AI_TOPIC:4
  - B2AI_TOPIC:52
  contributor_github_name: caufieldjh
  contributor_name: Harry Caufield
  contributor_orcid: ORCID:0000-0001-5705-7831
  description: 'Tutorial: Developing and Evaluating Your Extract, Transform, Load
    (ETL) Process to the OMOP CDM'
  has_relevant_organization:
  - B2AI_ORG:76
  is_open: true
  name: OHDSI 2024 Global Symposium ETL Tutorial
  purpose_detail: In this video tutorial, students will learn about the tools and
    practices developed by the OHDSI community to support the journey to establish
    and maintain an ETL to standardize your data to OMOP CDM and enable standardized
    evidence generation across a data network.
  requires_registration: false
  url: https://www.youtube.com/watch?v=H69dC7f-edQ
